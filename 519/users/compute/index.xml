<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>EGI Documentation â€“ Compute Services</title><link>/users/compute/</link><description>Recent content in Compute Services on EGI Documentation</description><generator>Hugo -- gohugo.io</generator><atom:link href="/users/compute/index.xml" rel="self" type="application/rss+xml"/><item><title>Users: Federated Cloud Compute</title><link>/users/compute/cloud-compute/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/cloud-compute/</guid><description>
&lt;p>The &lt;a href="https://www.egi.eu/services/cloud-compute/">EGI Federated Cloud Compute&lt;/a> (FedCloud)
service offers a multi-cloud IaaS federation that brings together
research clouds as a scalable computing platform for data and/or compute driven
applications and services for research and science.&lt;/p>
&lt;p>This documentation focuses on using the service. Those resource providers
willing to integrate into the service, please see the
&lt;a href="../../../providers/cloud-compute">EGI Federated Cloud Integration documentation&lt;/a>.&lt;/p>
&lt;p>Cloud Compute gives you the ability to deploy and scale virtual machines
on-demand. It offers computational resources in a secure and isolated
environment controlled via APIs without the overhead of managing physical
servers.&lt;/p>
&lt;p>Cloud Compute service is provided through a federation of IaaS cloud sites that
offer:&lt;/p>
&lt;ul>
&lt;li>Single Sign-On via &lt;a href="https://www.egi.eu/services/check-in/">EGI Check-in&lt;/a>,
users can login into every provider with their institutional credentials and
use modern industry standards like
&lt;a href="https://openid.net/connect/">OpenID Connect&lt;/a>.&lt;/li>
&lt;li>Global VM image catalogue at &lt;a href="https://appdb.egi.eu">AppDB&lt;/a> with pre-configured
Virtual Machine images that are automatically replicated to every provider
based on your community needs.&lt;/li>
&lt;li>Resource discovery features to easily understand which providers are
supporting your community and what are their capabilities.&lt;/li>
&lt;li>&lt;a href="https://accounting.egi.eu/cloud/">Global accounting&lt;/a> that aggregates and
allows visualisation of usage information across the whole federation.&lt;/li>
&lt;li>&lt;a href="https://argo.egi.eu/egi/report-status/Critical/SITES?filter=FedCloud">Monitoring of Availability and Reliability of the providers&lt;/a>
to ensure SLAs are met.&lt;/li>
&lt;/ul>
&lt;p>The flexibility of the Infrastructure as a Service can benefit various use cases
and usage models. Besides serving compute/data intensive analysis workflows, Web
services and interactive applications can be also integrated with and hosted on
this infrastructure. Contextualisation and other deployment features can help
application operators fine tune services in the cloud, meeting software (OS and
software packages), hardware (number of cores, amount of RAM, etc.) and other
types of needs (e.g. orchestration, scalability).&lt;/p>
&lt;p>Since the opening of the EGI Federated Cloud, the following usage models have
emerged:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Service hosting&lt;/strong>: the EGI Federated Cloud can be used to host any IT
service as web servers, databases, etc. Cloud features, as elasticity, can
help users to provide better performance and reliable services.
&lt;ul>
&lt;li>Example:
&lt;a href="https://www.egi.eu/use-cases/scientific-applications-tools/nbis-toolkit/">NBIS Web Services&lt;/a>,
&lt;a href="https://www.egi.eu/news/peachnote-in-unison-with-egi/">Peachnote analysis platform&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Compute and data intensive applications&lt;/strong>: for those applications needing
considerable amount of resources in terms of computation and/or memory and/or
intensive I/O. Ad-hoc computing environments can be created in the EGI cloud
providers to satisfy extremly intensive HW resource requirements.
&lt;ul>
&lt;li>Example:
&lt;a href="https://www.egi.eu/news/new-egi-use-case-a-close-look-at-the-amatrice-earthquake/">VERCE platform&lt;/a>,
&lt;a href="https://www.egi.eu/use-cases/research-stories/the-genetics-of-salmonella-infections/">The Genetics of Salmonella Infections&lt;/a>,
&lt;a href="https://www.egi.eu/use-cases/research-stories/new-viruses-implicated-in-fatal-snake-disease/">The Chipster Platform&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Datasets repository&lt;/strong>: the EGI Cloud can be used to store and manage large
datasets exploiting the large amount of disk storage available in the
Federation.&lt;/li>
&lt;li>&lt;strong>Disposable and testing environments&lt;/strong>: environments for training or testing
new developments.
&lt;ul>
&lt;li>Example:
&lt;a href="https://www.egi.eu/services/training-infrastructure/">Training infrastructure&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Eager to test this service? Have a look at
&lt;a href="../../tutorials/create-your-first-virtual-machine">how to create your first Virtual Machine in EGI&lt;/a>.&lt;/p></description></item><item><title>Users: Cloud Container Compute</title><link>/users/compute/cloud-container-compute/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/cloud-container-compute/</guid><description>
&lt;p>The
&lt;a href="https://www.egi.eu/services/cloud-container/">EGI Cloud Container Compute service&lt;/a>
allows you to run container-based applications on the providers of the EGI
Federated Cloud. There are two main ways of executing containers:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Using docker (or a similar container runtime) on a VM, so you can just
interact directly with the container runtime to run your applications. This
fits simpler applications that can easily fit on one node and are composed by
a small number of containers.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using a container orchestration platform, e.g.
&lt;a href="https://kubernetes.io">kubernetes&lt;/a> on a set of VMs to manage the
applications in an automated way for you. This is usually suited for more
complex applications that spawn several nodes and are composed of several
containers that need to cooperate to deliver the expected functionality.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Follow the guides below to learn more about them.&lt;/p>
&lt;p>The EGI Cloud Container Compute service was presented in one of the
&lt;a href="https://www.egi.eu/webinars/">EGI Webinars&lt;/a>. See more details on the
&lt;a href="https://indico.egi.eu/event/5492/">indico page&lt;/a> and a video recording on
&lt;a href="https://youtu.be/cZ3M47ON0pg">YouTube&lt;/a>.&lt;/p></description></item><item><title>Users: High Throughput Compute</title><link>/users/compute/high-throughput-compute/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/high-throughput-compute/</guid><description>
&lt;h2 id="what-is-it">What is it?&lt;/h2>
&lt;p>High Throughput Compute (HTC) is a computing paradigm that focuses on the
&lt;strong>efficient execution of a large number of loosely-coupled tasks&lt;/strong> (e.g. data
analysis jobs). HTC systems execute independent tasks that can be individually
scheduled on many different computing resources, across multiple administrative
boundaries. Users submit these tasks to the infrastructure as jobs. After a job
have been scheduled and executed, the output can be collected from the
service(s) that executed the job.&lt;/p>
&lt;h2 id="target-users">Target users&lt;/h2>
&lt;p>The target customers for EGI High Throughput Compute are research communities
that need to share, store, process, and produce large sets of data. Typically,
their research collaborations involve organizations across Europe and the
World.Some may already have local resources (e.g. universities, research
institutions) that can only be accessed by local users in accordance to the
respective organisation&amp;rsquo;s access policies.&lt;/p>
&lt;p>In case of local compute resources researchers can request access to the local
compute cluster from their IT department. However, when researchers join
collaborations that need to share their research activities, data collections,
and repositories, they need a homogenous and coordinated operation of the
compute resources, which are not uniformly accessible. In addition, nowadays
many research collaborations generate large amounts of data, and managing such
data volumes is time consuming and error-prone.&lt;/p>
&lt;p>The EGI High Throughput Compute service provides access to compute resources,
and offers a set of high-level tools that allow managing large amounts of data
in a collaborative way (e.g authorization and access control tools can be
regulated by the research collaboration in a central manner, data can be
uniformly distributed in the EGI Cloud, etc.).&lt;/p>
&lt;h2 id="features">Features&lt;/h2>
&lt;p>EGI High Throughput Compute provides easy, uniform access to shared computating
and data services of EGI service providers. Most software deployed in the
distributed resource centers is based on
&lt;a href="https://en.wikipedia.org/wiki/Open_standard">open standards&lt;/a> and
&lt;a href="https://en.wikipedia.org/wiki/Open_source">open source&lt;/a> middleware services.&lt;/p>
&lt;p>The main features of the EGI High Throughput Compute are:&lt;/p>
&lt;ul>
&lt;li>Access to high-quality computing resources&lt;/li>
&lt;li>Integrated monitoring and accounting tools provide information about the
availability and resource consumption&lt;/li>
&lt;li>Workload and data management tools to manage all computational tasks&lt;/li>
&lt;li>Large amounts of processing capacity over long periods of time&lt;/li>
&lt;li>Faster results for your research&lt;/li>
&lt;li>Shared resources enable collaborative research&lt;/li>
&lt;/ul>
&lt;h2 id="the-egi-htc-infrastructure">The EGI HTC infrastructure&lt;/h2>
&lt;p>The EGI High Throughput Compute infrastructure is the federation of GRID
resources provided by EGI providers. Its aim is to share in a secure way the
distributed IT resources that are part of the EGI Cloud. It comprises of:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Compute Resources&lt;/strong> &amp;ndash; execution environment for computing tasks, organized
into clusters distributed across multiple resource centers in Europe and the
World.&lt;/li>
&lt;li>&lt;strong>Data Infrastructure&lt;/strong> &amp;ndash; storage servers from different resource centers
where users can store their data/files in a distributed manner.&lt;/li>
&lt;li>&lt;strong>Federated Operations&lt;/strong> &amp;ndash; global operational tasks (e.g., AAI, accounting,
helpdesk) needed to federate the heterogeneous resources of resource centers
and their operational activities.&lt;/li>
&lt;li>&lt;strong>User Support&lt;/strong> &amp;ndash; EGI provides the central user support and coordinates
support activities of EGI providers, who offer user support for the
services/resources they contribute to the EGI ecosystem.&lt;/li>
&lt;/ul>
&lt;h3 id="architecture-and-service-components">Architecture and service components&lt;/h3>
&lt;p>&lt;img src="htc_archtecture.png" alt="EGI High Throughput Compute architecture">&lt;/p>
&lt;p>The key components of the EGI High Throughput Compute architecture are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../../data/management/data-transfer">Data Transfer service&lt;/a> (FTS)&lt;/li>
&lt;li>&lt;a href="../../data/storage">Online Storage services&lt;/a>&lt;/li>
&lt;li>&lt;strong>Computing Elements&lt;/strong> (CEs) are compute resources made available through GRID
interfaces. The most common implementations of CEs in the EGI infrastructure
are &lt;a href="https://htcondor.com/htcondor-ce/">HTCondor-CE&lt;/a> and
&lt;a href="http://www.nordugrid.org/arc/ce/">ARC-CE&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="access-model">Access model&lt;/h3>
&lt;p>Access to HTC resources in the EGI infrastructure is based on X.509 certificates
and &lt;a href="../../aai/check-in/vos">Virtual Organisations&lt;/a> (VOs).&lt;/p>
&lt;p>VOs are fully managed by research communities, allowing communitites to manage
their users and grant access to their services and resources. This means
communities can either own their resources and use EGI services to share
(federate) them, or can use the resources available in the EGI infrastructure
for their scientific needs.&lt;/p>
&lt;p>Before users can access EGI HTC services, they have to:&lt;/p>
&lt;ol>
&lt;li>Obtain an X.509 certficate. The certificates are issued by Certification
Authorities (CAs) part of the
&lt;a href="https://www.eugridpma.org">European Policy Management Authority for Grid Authentication&lt;/a>
(EUGridPMA), which is also part of the
&lt;a href="https://www.igtf.net">International Global Trust Federation&lt;/a> (IGTF).&lt;/li>
&lt;li>Enroll into a VO before they can use the services, as users are not
individually granted access to resources.&lt;/li>
&lt;li>Add the certificate to their internet browser of choice, or import it into
the appropriate certificate store of their local machine (on Windows).&lt;/li>
&lt;li>Proceed to &lt;a href="../../compute/orchestration/workload-manager">Workload Manager&lt;/a> to submit HTC jobs or
retrieve job results, login using &lt;a href="../../aai/check-in">EGI Check-in&lt;/a> when prompted.&lt;/li>
&lt;/ol></description></item><item><title>Users: Compute Orchestration</title><link>/users/compute/orchestration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/orchestration/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>One of the challenges for researchers in recent years is to manage an ever
increasing amount of compute and storage services, which then form ever more
complex end user applications or platforms.&lt;/p>
&lt;p>To address this need, EGI provides several orchestrators that facilitate the
manangement of your workload using resources on the federation. These tools have
different levels of abstractions and features. See below to understand which to
choose (or gets used automatically) in specific scenarios:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Service Name&lt;/th>
&lt;th>Workload Type&lt;/th>
&lt;th>Use-Case&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;a href="im">Infrastructure Manager&lt;/a>&lt;/td>
&lt;td>VMs, containers, storage&lt;/td>
&lt;td>Used to run workloads on a single IaaS Cloud provider.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;a href="ec3">Elastic Cloud Compute Cluster&lt;/a>&lt;/td>
&lt;td>VMs, containers, storage&lt;/td>
&lt;td>Used when you need to run workloads on clusters that can be elastically scaled and potentially span more than one IaaS Cloud provider.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;a href="workload-manager">Workload Manager&lt;/a>&lt;/td>
&lt;td>Jobs&lt;/td>
&lt;td>Used to efficiently distribute, manage, and monitor computing workloads.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;a href="dodas">Dynamic On-Demand Analysis Service&lt;/a>&lt;/td>
&lt;td>Containers, storage (caches)&lt;/td>
&lt;td>Used when you need to process your data either interactively or via a batch system.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
| [PaaS Orchestrator](indigo-paas) | VMs, containers, HTC jobs | Used when you have both IaaS Cloud and HTC workloads. The DEEP Platform uses it for ML/DL workloads. |
-->
&lt;!-- markdownlint-enable line-length -->
&lt;p>The following sections offer more details about each of these orchestrators.&lt;/p></description></item><item><title>Users: Content Distribution</title><link>/users/compute/content-distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/content-distribution/</guid><description>
&lt;p>This page is about the CVMFS service operated for EGI by RAL.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The CernVM-File System (CVMFS) provides a scalable, reliable and low-maintenance
software distribution service. It was developed to assist High Energy Physics
collaborations to deploy software on the worldwide distributed computing
infrastructure used to run data processing applications. CVMFS is implemented as
a POSIX read-only file system in user space. Files and directories are hosted on
standard web servers and mounted in the universal namespace /cvmfs. CernVM-FS
uses outgoing HTTP connections only, thereby it avoids most of the firewall
issues of other network file systems. It transfers data and metadata on demand
and verifies data integrity by cryptographic hashes. CVMFS is actively used by
small and large collaborations. In many cases, it replaces package managers and
shared software areas on cluster file systems as means to distribute the
software used to process experiment data.&lt;/p>
&lt;p>This documentation is for the VO content managers.&lt;/p>
&lt;h2 id="official-cvmfs-pages">Official CVMFS pages&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://cvmfs.readthedocs.io/en/latest/">CVMFS Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cernvm-forum.cern.ch/">Q&amp;amp;As and Discussion Forum&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="requesting-the-creation-of-a-new-repository">Requesting the creation of a new repository&lt;/h2>
&lt;p>In the case of a new repository for EGI, steps are described
&lt;a href="https://ims.egi.eu/display/EGIPP/PROC22+Support+for+CVMFS+replication+across+the+EGI+Infrastructure">in PROC22&lt;/a>.&lt;/p>
&lt;h2 id="onboarding-new-content-managers">Onboarding new Content Managers&lt;/h2>
&lt;p>Steps for a new VO Content Manager to be granted access to the Stratum-0.&lt;/p>
&lt;h3 id="requesting-access">Requesting access&lt;/h3>
&lt;p>Request access to the service sending an email to &lt;a href="mailto:cvmfs-support@gridpp.rl.ac.uk">cvmfs-support@gridpp.rl.ac.uk&lt;/a>
In the email, include the following information:&lt;/p>
&lt;ul>
&lt;li>Name of the VO or CVMFS repository.&lt;/li>
&lt;li>Distinguish Name (DN) from your X509 grid certificate.&lt;/li>
&lt;/ul>
&lt;h3 id="mailing-list">Mailing list&lt;/h3>
&lt;p>All VO content managers should join the CVMFS-UPLOADER-USERS mailing list in
&lt;a href="https://www.jiscmail.ac.uk/cgi-bin/webadmin?A0=cvmfs-uploader-users">JISCMAIL&lt;/a>.&lt;/p>
&lt;h2 id="distributing-new-content">Distributing new content&lt;/h2>
&lt;p>To login to the service, make sure you have a valid X509 proxy (with the same DN
provided &lt;a href="#requesting-access">in this step&lt;/a>), and execute the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ gsissh -p &lt;span style="color:#0000cf;font-weight:bold">1975&lt;/span> cvmfs-upload01.gridpp.rl.ac.uk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are the Content Manager for more than one repository, you would need to
specify explicit which account you want to login to:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ gsissh -p &lt;span style="color:#0000cf;font-weight:bold">1975&lt;/span> &amp;lt;myreposgm&amp;gt;@cvmfs-upload01.gridpp.rl.ac.uk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To copy data:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ gsiscp -P &lt;span style="color:#0000cf;font-weight:bold">1975&lt;/span> &amp;lt;source&amp;gt; cvmfs-upload01.gridpp.rl.ac.uk:&amp;lt;destination&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After login, you will find a single directory in the home directory:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>myreposgm@cvmfs-uploader02 ~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>$ ls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cvmfs_repo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Add to that directory the new content you want to distribute.&lt;/p>
&lt;p>Files and directories cannot be distributed with CVMFS if they are not
world-wide readable. You may want to ensure they have the right permissions with
the following commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ find . -type d -exec chmod go+rx &lt;span style="color:#ce5c00;font-weight:bold">{}&lt;/span> &lt;span style="color:#4e9a06">\;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ find . -type f -exec chmod go+r &lt;span style="color:#ce5c00;font-weight:bold">{}&lt;/span> &lt;span style="color:#4e9a06">\;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="building-your-software">Building your software&lt;/h3>
&lt;p>CVMFS is an infrastructure to distribute software world-wide. However, the
uploader host should not be used for the purposes of building and compiling it
prior to distribution.&lt;/p>
&lt;p>The right approach is for you to have your own local building environment, and
use the uploader host only to upload the new content for distribution.&lt;/p>
&lt;p>If you have non-relocatable software, then you will need a &lt;code>/cvmfs/&amp;lt;myrepo&amp;gt;/&lt;/code>
directory on your building host. One option is to use an actual CVMFS client, so
you have ready all the existing content being already distributed by CVMFS. By
default, the &lt;code>/cvmfs/&lt;/code> directory on a CVMFS client host is read-only, but that
can be solved using an
&lt;a href="https://cvmfs.readthedocs.io/en/latest/cpt-enter.html">ephemeral writable container&lt;/a>.&lt;/p></description></item></channel></rss>