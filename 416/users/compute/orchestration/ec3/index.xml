<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>EGI Documentation – Elastic Cloud Compute Clusters</title><link>/users/compute/orchestration/ec3/</link><description>Recent content in Elastic Cloud Compute Clusters on EGI Documentation</description><generator>Hugo -- gohugo.io</generator><atom:link href="/users/compute/orchestration/ec3/index.xml" rel="self" type="application/rss+xml"/><item><title>Users: EC3 portal</title><link>/users/compute/orchestration/ec3/portal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/orchestration/ec3/portal/</guid><description>
&lt;h2 id="deploy-cluster">Deploy cluster&lt;/h2>
&lt;p>Through a &amp;ldquo;job wizard&amp;rdquo; interface the user can login to the
&lt;a href="https://servproject.i3m.upv.es/ec3-ltos/index.php">Elastic Cloud Compute Cluster (EC3) portal&lt;/a>
and configure the virtual cluster with the related tools and applications to be
deployed in the EGI Cloud. Click on the “Deploy your cluster” button to create a
cluster in the EGI Cloud.&lt;/p>
&lt;p>&lt;img src="ec3-portal.png" alt="EC3 Portal.">&lt;/p>
&lt;p>The cluster is composed of a front node, where a batch job scheduler is running,
and a number of compute nodes. These compute nodes will be dynamically deployed
and provisioned to fit increasing load, and un-deployed when they are in idle
status. The installation and configuration of the cluster is performed by means
of the execution of Ansible receipts.&lt;/p>
&lt;p>A wizard will guide the user during the configuration process of the cluster,
allowing to configure details like the operating system, the characteristics of
the nodes, the maximum number of nodes of the cluster or the pre-installed
software packages. Specifically, the general wizard steps include:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>LRMS&lt;/strong> selection: choose &lt;strong>Torque&lt;/strong> from the list of LRMSs (Local Resource
Management System) that can be automatically installed and configured by EC3.&lt;/p>
&lt;p>&lt;img src="ec3-lrms.png" alt="Configure the LRMS of the EC3 cluster">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Endpoint&lt;/strong>: the endpoints of the providers where to deploy the elastic
cluster. The endpoints serving the
&lt;a href="https://operations-portal.egi.eu/vo/view/voname/vo.access.egi.eu">vo.access.egi.eu VO&lt;/a>
are dynamically retrieved from the
&lt;a href="https://appdb.egi.eu/">EGI Application DataBase&lt;/a> using REST APIs.&lt;/p>
&lt;p>&lt;img src="ec3-endpoint.png" alt="Select the provider’s endpoint where deploy the cluster">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Operating System&lt;/strong>: choose one of the available EGI base OS images
available to create the cluster (e.g. CentOS7, or EGI Ubuntu 18.04 LTS).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Instance details&lt;/strong>: in terms of CPU and RAM to allocate for the front-end
and the working nodes.&lt;/p>
&lt;p>&lt;img src="ec3-nodes.png" alt="Configure of the cluster nodes">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cluster’s size and name&lt;/strong>: the name of the cluster and the maximum number
of nodes of the cluster, without including the frontend. This value
indicates the maximum number of working nodes that the cluster can scale.
Initially, the cluster is created with the frontend and only one working
node: the other working nodes are powered on on-demand.&lt;/p>
&lt;p>&lt;img src="ec3-size.png" alt="Select the maximum number of nodes">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Resume and Launch&lt;/strong>: a summary of the chosen cluster configuration. To
start the deployment process, click the Submit button.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
The configuration of the cluster may
take some time. Please wait for its completion before starting to use the
cluster!
&lt;/div>
&lt;p>When the frontend node of the cluster has been successfully deployed, the user
will be notified with the credentials to access via SSH.&lt;/p>
&lt;p>The cluster details are available by clicking on the &lt;strong>&amp;ldquo;Manage your deployed
clusters&amp;rdquo;&lt;/strong> link on the front page.&lt;/p>
&lt;h2 id="accessing-the-ec3-cluster">Accessing the EC3 cluster&lt;/h2>
&lt;p>To access the frontend of the elastic cluster:&lt;/p>
&lt;ol>
&lt;li>Download the SSH private key provided by the EC3 portal;&lt;/li>
&lt;li>Change its permissions to 600;&lt;/li>
&lt;li>Access via SSH providing the key as identity file for public key
authentication.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>$ ssh -i key.pem cloudadm@&amp;lt;CLUSTER_PUBLIC_IP&amp;gt;
Last login: Mon Nov &lt;span style="color:#0000cf;font-weight:bold">18&lt;/span> 11:37:29 &lt;span style="color:#0000cf;font-weight:bold">2019&lt;/span> from torito.i3m.upv.es
&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>cloudadm@server ~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>$
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Both the frontend and the working node are configured by Ansible. This process
usually takes some time. User can monitor the status of the cluster
configuration using the &lt;code>is_cluster_ready&lt;/code> command-line tool:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># is_cluster_ready&lt;/span>
Cluster is still configuring.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The cluster is successfully configured when the command returns the following
message:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># is_cluster_ready&lt;/span>
Cluster configured!
node state enabled &lt;span style="color:#204a87">time&lt;/span> stable &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> used &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> total
&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>$ clues status
node state enabled &lt;span style="color:#204a87">time&lt;/span> stable &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> used &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> total
-----------------------------------------------------------------------
wn1 off enabled 00h04&lt;span style="color:#4e9a06">&amp;#39;20&amp;#34; 0,0 1,-1
&lt;/span>&lt;span style="color:#4e9a06">wn2 off enabled 00h04&amp;#39;&lt;/span>20&lt;span style="color:#4e9a06">&amp;#34; 0,0 1,-1
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="operate-the-ec3-cluster">Operate the EC3 cluster&lt;/h2>
&lt;p>To force CLUES to spawn a new node of the cluster, please use the command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>$ clues poweron wn1
node wn1 powered on
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The configuration triggers the execution of several ansible processes to
configure the node and may take some time. To monitor the configuration of the
node, you can use the &lt;code>is_cluster_ready&lt;/code> command.&lt;/p>
&lt;p>To avoid that clues powers off the node in case of inactivities, once the node
is configured, you can disable the node as it follows:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>$ clues disable wn1
node wn1 successfully disabled
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="log-files">Log files&lt;/h2>
&lt;ul>
&lt;li>Cluster logs files are available in &lt;code>/var/tmp/.im/&amp;lt;cluster_id&amp;gt;&lt;/code>&lt;/li>
&lt;li>IM log files are in &lt;code>/var/log/im/im.log&lt;/code>&lt;/li>
&lt;li>CLUES log files are in &lt;code>/var/log/clues2/clues2.log&lt;/code>&lt;/li>
&lt;/ul></description></item><item><title>Users: EC3 CLI</title><link>/users/compute/orchestration/ec3/cli/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/orchestration/ec3/cli/</guid><description>
&lt;p>You can find here documentation on how to deploy a sample SLURM cluster, which
you can then adapt to create other kind of clusters easily.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>We will use docker for running EC3, direct installation is also possible and
described at
&lt;a href="https://ec3.readthedocs.io/en/devel/intro.html#installation">EC3 documentation&lt;/a>.
First get the docker image:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">docker pull grycap/ec3
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And check that you can run a simple command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ docker run grycap/ec3 list
name state IP nodes
------------------------
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For convenience we will create a directory to keep the deployment configuration
and status together.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">mkdir ec3-test
&lt;span style="color:#204a87">cd&lt;/span> ec3-test
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can list the available templates for clusters with the &lt;code>templates&lt;/code> command:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ docker run grycap/ec3 templates
name kind summary
----------------------------------------------------------------------------------------------------------------------
blcr component Tool &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> checkpointing applications.
&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>...&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
sge main Install and configure a cluster SGE from distribution repositories.
slurm main Install and configure a cluster using the grycap.slurm ansible role.
slurm-repo main Install and configure a cluster SLURM from distribution repositories.
&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>...&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;p>We will use the &lt;code>slurm&lt;/code> template for configuring our cluster.&lt;/p>
&lt;h2 id="site-details">Site details&lt;/h2>
&lt;p>EC3 needs some information on the site that you are planning to use to deploy
your cluster:&lt;/p>
&lt;ol>
&lt;li>authentication information&lt;/li>
&lt;li>network identifiers&lt;/li>
&lt;li>VM image identifiers&lt;/li>
&lt;/ol>
&lt;p>We will use the &lt;a href="../../../../getting-started/cli">FedCloud client&lt;/a> to discover
the required information. Set your credentials as shown in
&lt;a href="../../../../aai/auth/#check-in-and-access-tokens">the authentication guide&lt;/a>
and create the autorisation files needed for ec3 (in this case for CESGA with VO
vo.access.egi.eu):&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">fedcloud ec3 init --site CESGA --vo vo.access.egi.eu
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will generate an &lt;code>auth.dat&lt;/code> file with your credentials to access the site
and a &lt;code>templates/refresh.radl&lt;/code> with a refresh token to allow long running
clusters to be managed on the infrastructure.&lt;/p>
&lt;p>Let&amp;rsquo;s get also some needed site information. Start getting the available
networks, we will need both a public and private network:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ fedcloud openstack --site CESGA --vo vo.access.egi.eu network list
+--------------------------------------+----------------------+--------------------------------------+
&lt;span style="color:#000;font-weight:bold">|&lt;/span> ID &lt;span style="color:#000;font-weight:bold">|&lt;/span> Name &lt;span style="color:#000;font-weight:bold">|&lt;/span> Subnets &lt;span style="color:#000;font-weight:bold">|&lt;/span>
+--------------------------------------+----------------------+--------------------------------------+
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 12ffb5f7-3e54-433f-86d0-8ffa43b52025 &lt;span style="color:#000;font-weight:bold">|&lt;/span> net-vo.access.egi.eu &lt;span style="color:#000;font-weight:bold">|&lt;/span> 754342b1-92df-4fc8-9499-2ee8b668141f &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 &lt;span style="color:#000;font-weight:bold">|&lt;/span> public00 &lt;span style="color:#000;font-weight:bold">|&lt;/span> 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 &lt;span style="color:#000;font-weight:bold">|&lt;/span>
+--------------------------------------+----------------------+--------------------------------------+
&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;p>Then, get the list of images available:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ fedcloud openstack --site CESGA --vo vo.access.egi.eu image list
+--------------------------------------+----------------------------------------------------------+--------+
&lt;span style="color:#000;font-weight:bold">|&lt;/span> ID &lt;span style="color:#000;font-weight:bold">|&lt;/span> Name &lt;span style="color:#000;font-weight:bold">|&lt;/span> Status &lt;span style="color:#000;font-weight:bold">|&lt;/span>
+--------------------------------------+----------------------------------------------------------+--------+
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 9d22cb3b-e6a3-4467-801a-a68214338b22 &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> CernVM3 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>CentOS/6/QEMU-KVM&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> b03e8720-d88a-4939-b93d-23289b8eed6c &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> CernVM4 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>CentOS/7/QEMU-KVM&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 06cd7256-de22-4e9d-a1cf-997b5c44d938 &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> Chipster &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>Ubuntu/16.04/KVM&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 8c4e2568-67a2-441a-b696-ac1b7c60de9c &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> EGI CentOS &lt;span style="color:#0000cf;font-weight:bold">7&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>CentOS/7/VirtualBox&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> abc5ebd8-f65c-4af9-8e54-a89e3b5587a3 &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> EGI Docker &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>Ubuntu/18.04/VirtualBox&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 22064e93-6af9-430b-94a1-e96473c5a72b &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> EGI Ubuntu 16.04 LTS &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>Ubuntu/16.04/VirtualBox&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> d5040b3e-ef33-4959-bb88-5505e229f579 &lt;span style="color:#000;font-weight:bold">|&lt;/span> Image &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> EGI Ubuntu 18.04 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>Ubuntu/18.04/VirtualBox&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
&lt;span style="color:#000;font-weight:bold">|&lt;/span> 79fadf3f-6092-4bb7-ab78-9a322f0aad33 &lt;span style="color:#000;font-weight:bold">|&lt;/span> cirros &lt;span style="color:#000;font-weight:bold">|&lt;/span> active &lt;span style="color:#000;font-weight:bold">|&lt;/span>
+--------------------------------------+----------------------------------------------------------+--------+
&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;p>For our example we will use the EGI CentOS 7 with id
&lt;code>8c4e2568-67a2-441a-b696-ac1b7c60de9c&lt;/code>.&lt;/p>
&lt;p>Finally, with all this information we can create the &lt;code>images&lt;/code> template for EC3
that specifies the site configuration for our deployment. Save this file as
&lt;code>templates/centos.radl&lt;/code>:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-plaintext" data-lang="plaintext">description centos-cesga (
kind = &amp;#39;images&amp;#39; and
short = &amp;#39;centos7-cesga&amp;#39; and
content = &amp;#39;CentOS7 image at CESGA&amp;#39;
)
network public (
provider_id = &amp;#39;public00&amp;#39; and
outports contains &amp;#39;22/tcp&amp;#39;
)
network private (provider_id = &amp;#39;net-vo.access.egi.eu&amp;#39;)
system front (
cpu.arch = &amp;#39;x86_64&amp;#39; and
cpu.count &amp;gt;= 2 and
memory.size &amp;gt;= 2048 and
disk.0.os.name = &amp;#39;linux&amp;#39; and
disk.0.image.url = &amp;#39;ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c&amp;#39; and
disk.0.os.credentials.username = &amp;#39;centos&amp;#39;
)
system wn (
cpu.arch = &amp;#39;x86_64&amp;#39; and
cpu.count &amp;gt;= 2 and
memory.size &amp;gt;= 2048 and
ec3_max_instances = 5 and # maximum number of worker nodes in the cluster
disk.0.os.name = &amp;#39;linux&amp;#39; and
disk.0.image.url = &amp;#39;ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c&amp;#39; and
disk.0.os.credentials.username = &amp;#39;centos&amp;#39;
)
&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;p>Note we have used &lt;code>public00&lt;/code> as public network and opened port &lt;code>22&lt;/code> to allow ssh
access. The private network uses &lt;code>net-vo.access.egi.eu&lt;/code>. We have two kind of VMs
in almost every deployment: the &lt;code>front&lt;/code>, that runs the batch system, and the
&lt;code>wn&lt;/code>, that will execute the jobs. In our example, both will use the same CentOS
image, which is specified with the
&lt;code>disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c'&lt;/code>
line: &lt;code>ost&lt;/code> refers to OpenStack, &lt;code>fedcloud-osservices.egi.cesga.es&lt;/code> is the
hostname of the URL obtained above with &lt;code>fedcloud endpoint list&lt;/code> and
&lt;code>8c4e2568-67a2-441a-b696-ac1b7c60de9c&lt;/code> is the ID of the image in OpenStack. The
size of the VM is also specified.&lt;/p>
&lt;h2 id="launch-cluster">Launch cluster&lt;/h2>
&lt;p>We are ready now to deploy the cluster with ec3 (this can take several minutes):&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ docker run -it -v &lt;span style="color:#000">$PWD&lt;/span>:/root/ -w /root grycap/ec3 launch mycluster slurm ubuntu refresh -a auth.dat
Creating infrastructure
Infrastructure successfully created with ID: 74fde7be-edee-11ea-a6e9-da8b0bbd7c73
Front-end configured with IP 193.144.46.234
Transferring infrastructure
Front-end ready!
&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;p>We can check the status of the deployment:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ docker run -it -v &lt;span style="color:#000">$PWD&lt;/span>:/root/ -w /root grycap/ec3 list
name state IP nodes
----------------------------------------------
mycluster configured 193.144.46.234 &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And once configured, ssh to the front node. The &lt;code>is_cluster_ready&lt;/code> command will
report whether the cluster is fully configured or not:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ docker run -it -v &lt;span style="color:#000">$PWD&lt;/span>:/root/ -w /root grycap/ec3 ssh mycluster
Warning: Permanently added &lt;span style="color:#4e9a06">&amp;#39;193.144.46.234&amp;#39;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>ECDSA&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> to the list of known hosts.
Last login: Thu Sep &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span> 14:07:46 &lt;span style="color:#0000cf;font-weight:bold">2020&lt;/span> from torito.i3m.upv.es
$ bash
cloudadm@slurmserver:~$ is_cluster_ready
Cluster configured!
cloudadm@slurmserver:~$
&lt;/code>&lt;/pre>&lt;/div>&lt;p>EC3 will deploy &lt;a href="https://www.grycap.upv.es/clues/eng/index.php">CLUES&lt;/a>, a
cluster management system that will power on/off nodes as needed depending on
the load. Initially all the nodes will be off:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">node state enabled &lt;span style="color:#204a87">time&lt;/span> stable &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> used &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> total
-----------------------------------------------------------------------------------------------
wn1 off enabled 00h03&lt;span style="color:#4e9a06">&amp;#39;55&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">wn2 off enabled 00h03&amp;#39;&lt;/span>55&lt;span style="color:#4e9a06">&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">wn3 off enabled 00h03&amp;#39;55&amp;#34;&lt;/span> 0,0.0 1,1073741824.0
wn4 off enabled 00h03&lt;span style="color:#4e9a06">&amp;#39;55&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">wn5 off enabled 00h03&amp;#39;&lt;/span>55&lt;span style="color:#4e9a06">&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;p>SLURM will also report nodes as down:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">PARTITION AVAIL TIMELIMIT NODES STATE NODELIST
debug* up infinite &lt;span style="color:#0000cf;font-weight:bold">5&lt;/span> down* wn&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>1-5&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>As we submit a first job, some nodes will be powered on to meet the request. You
can also start them manually with &lt;code>clues poweron&lt;/code>.&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cloudadm@slurmserver:~$ srun hostname
srun: Required node not available &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>down, drained or reserved&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
srun: job &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> queued and waiting &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> resources
srun: job &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> has been allocated resources
wn1.localdomain
cloudadm@slurmserver:~$ clues status
node state enabled &lt;span style="color:#204a87">time&lt;/span> stable &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> used &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>cpu,mem&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> total
-----------------------------------------------------------------------------------------------
wn1 idle enabled 00h07&lt;span style="color:#4e9a06">&amp;#39;45&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">wn2 off enabled 00h52&amp;#39;&lt;/span>25&lt;span style="color:#4e9a06">&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">wn3 off enabled 00h52&amp;#39;25&amp;#34;&lt;/span> 0,0.0 1,1073741824.0
wn4 off enabled 00h52&lt;span style="color:#4e9a06">&amp;#39;25&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">wn5 off enabled 00h52&amp;#39;&lt;/span>25&lt;span style="color:#4e9a06">&amp;#34; 0,0.0 1,1073741824.0
&lt;/span>&lt;span style="color:#4e9a06">cloudadm@slurmserver:~&lt;/span>$&lt;span style="color:#4e9a06"> sinfo
&lt;/span>&lt;span style="color:#4e9a06">PARTITION AVAIL TIMELIMIT NODES STATE NODELIST
&lt;/span>&lt;span style="color:#4e9a06">debug* up infinite 4 down* wn[2-5]
&lt;/span>&lt;span style="color:#4e9a06">debug* up infinite 1 idle wn1
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length -->
&lt;h2 id="destroying-the-cluster">Destroying the cluster&lt;/h2>
&lt;p>Once you are done with the cluster and want to destroy it, you can use the
&lt;code>destroy&lt;/code> command. If your cluster was created more than one hour ago, your
credentials to access the site will be expired and need to refreshed first with
&lt;code>fedcloud ec3 refresh&lt;/code>:&lt;/p>
&lt;!-- markdownlint-disable line-length -->
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ fedcloud ec3 refresh &lt;span style="color:#8f5902;font-style:italic"># refresh your auth.dat&lt;/span>
$ docker run -it -v &lt;span style="color:#000">$PWD&lt;/span>:/root/ -w /root grycap/ec3 list &lt;span style="color:#8f5902;font-style:italic"># list your clusters&lt;/span>
name state IP nodes
----------------------------------------------
mycluster configured 193.144.46.234 &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
$ docker run -it -v &lt;span style="color:#000">$PWD&lt;/span>:/root/ -w /root grycap/ec3 destroy mycluster -a auth.dat -y
WARNING: you are going to delete the infrastructure &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>including frontend and nodes&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>.
Success deleting the cluster!
&lt;/code>&lt;/pre>&lt;/div>&lt;!-- markdownlint-enable line-length --></description></item><item><title>Users: EC3 applications and tools</title><link>/users/compute/orchestration/ec3/apps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/orchestration/ec3/apps/</guid><description>
&lt;p>This section documents how to run some applications and use the existing tools
with EC3.&lt;/p>
&lt;h2 id="how-to-run-scientific-applications-in-ec3">How to run scientific applications in EC3&lt;/h2>
&lt;h3 id="namd-cluster">NAMD cluster&lt;/h3>
&lt;p>To deploy &lt;a href="https://www.ks.uiuc.edu/Research/namd/">NAMD&lt;/a> clusters, please select
one of the available LRMS (Local Resource Management System) and choose NAMD
from the list of applications.&lt;/p>
&lt;h2 id="how-to-use-generic-toolspractices-in-ec3">How to use generic tools/practices in EC3&lt;/h2>
&lt;h3 id="ecas-cluster">ECAS cluster&lt;/h3>
&lt;p>Check the dedicated &lt;a href="./ecas/">ECAS documentation&lt;/a>.&lt;/p>
&lt;h3 id="kubernetes">Kubernetes&lt;/h3>
&lt;p>Check the
&lt;a href="../../../cloud-container-compute">Cloud Container Compute documentation&lt;/a>.&lt;/p>
&lt;h3 id="mesos--marathon--chronos">Mesos + Marathon + Chronos&lt;/h3>
&lt;p>To deploy a virtual cluster with
&lt;a href="https://mesosphere.github.io/marathon/">Marathon&lt;/a>,
&lt;a href="http://mesos.apache.org/">Mesos&lt;/a>, and
&lt;a href="https://mesos.github.io/chronos/">Chronos&lt;/a> as an orchestration, please select
Mesos + Marathon + Chronos from the list of available LRMS.&lt;/p>
&lt;h3 id="oscar-cluster">OSCAR cluster&lt;/h3>
&lt;p>To deploy
&lt;a href="https://www.egi.eu/about/newsletters/serverless-computing-for-data-processing-applications-in-egi/">Serverless computing for data-processing applications&lt;/a>
in EGI, please select OSCAR from the list of LRMS (Local Resource Management
System). OSCAR supports data-driven serverless computing for file-processing
applications. A file upload, to the object storage backend &lt;a href="http://minio.io">MinIO&lt;/a>,
will trigger the execution of a chosen shell script running inside a user-defined
container. These will be orchestrated as Kubernetes batch jobs. The output data
will be uploaded to any object storage backends support. Synchronous invocations
are also available.&lt;/p>
&lt;p>As external object storage providers, the following services can be used:&lt;/p>
&lt;ul>
&lt;li>External &lt;a href="https://min.io">MinIO&lt;/a> servers, which may be in clusters other than the
platform.&lt;/li>
&lt;li>&lt;a href="https://aws.amazon.com/s3/">Amazon S3&lt;/a>, the Amazon&amp;rsquo;s object storage service that
offers industry-leading scalability, data availability, security, and performance
in the public Cloud.&lt;/li>
&lt;li>&lt;a href="https://onedata.org/">Onedata&lt;/a>, the global data access solution for science used
in the &lt;a href="https://datahub.egi.eu/">EGI DataHub&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>See the documentation to deploy an elastic Kubernetes cluster with the OSCAR
platform with EC3:
&lt;a href="https://docs.oscar.grycap.net/deploy-ec3/">Deploy OSCAR with EC3&lt;/a>&lt;/p>
&lt;p>See some use cases of applications that use the OSCAR framework for event-driven
high-throughput processing of files (you can found it in the GitHub repository
&lt;a href="https://github.com/grycap/oscar/tree/master/examples">examples folder&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>Inference of a machine learning model: See full description at
&lt;a href="https://oscar.grycap.net/blog/post-oscar-faas-scalable-ml-inference/">OSCAR Blog entry&lt;/a>.&lt;/li>
&lt;li>Mask detection: See full description at
&lt;a href="https://oscar.grycap.net/blog/post-oscar-serverless-ai-models/">OSCAR Blog entry&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/indigo-dc/plant-classification-theano">Plants Classification&lt;/a>,
an application that performs plant classification using Lasagne/Theano.&lt;/li>
&lt;li>&lt;a href="https://www.imagemagick.org/">ImageMagick&lt;/a>, a tool to manipulate images.&lt;/li>
&lt;li>&lt;a href="https://github.com/eubr-atmosphere/radiomics">Radiomics&lt;/a>, a use case about
the handling of Rheumatic Heart Disease (RHD) through image computing and
Artificial Intelligence (AI).&lt;/li>
&lt;/ul>
&lt;p>More information in the &lt;a href="https://oscar.grycap.net/">OSCAR web page&lt;/a>&lt;/p>
&lt;h3 id="slurm-cluster">SLURM cluster&lt;/h3>
&lt;p>To deploy &lt;a href="https://slurm.schedmd.com/documentation.html">SLURM&lt;/a> clusters, please
select SLURM from the list of available LRMS. See also the dedicated guide on
&lt;a href="./htc/">HTC clusters&lt;/a>&lt;/p></description></item><item><title>Users: Instructions for developers</title><link>/users/compute/orchestration/ec3/developers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/users/compute/orchestration/ec3/developers/</guid><description>
&lt;p>This section documents how to integrate a new application in EC3.&lt;/p>
&lt;h2 id="about">About&lt;/h2>
&lt;p>The process to integrate a new application in EC3 is described by the
following process:&lt;/p>
&lt;ul>
&lt;li>Describe the application to be integrated with Ansible, the open-source
automation engine that automates software provisioning, configuration
management, and application deployment.&lt;/li>
&lt;li>Use the Ansible receipt to create a new RADL template&lt;/li>
&lt;/ul>
&lt;h3 id="documentations">Documentations&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://docs.ansible.com/">Ansible documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://ec3.readthedocs.io/en/devel/templates.html">EC3 documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/grycap/ec3/tree/master/templates">RADL&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="contacts">Contacts&lt;/h3>
&lt;ul>
&lt;li>Miguel Caballer at: micafer1 &lt;code>&amp;lt;at&amp;gt;&lt;/code> upv &lt;code>&amp;lt;dot&amp;gt;&lt;/code> es&lt;/li>
&lt;li>Amanda Calatrava at: amcaar &lt;code>&amp;lt;at&amp;gt;&lt;/code> i3m &lt;code>&amp;lt;dot&amp;gt;&lt;/code> upv &lt;code>&amp;lt;dot&amp;gt;&lt;/code> es&lt;/li>
&lt;/ul></description></item></channel></rss>