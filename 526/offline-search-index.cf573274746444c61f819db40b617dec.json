










































































































































































































[{"body":"Introduction The Operations Start Guide will help you start with EGI Operations duties. It covers the responsibilities of the various parties involved in the running of the EGI infrastructure and guide how to join operations. As a newcomer, you need to understand the structure of the infrastructure and roles of operators at different levels. Reading the whole document will give you a complete overall picture of daily operations within EGI.\nResource Centres and Resource Infrastructures Resources are geographically distributed and are contributed by Resource Centres. A Resource Centre is the smallest resource administration domain within EGI. It can be either localized or geographically distributed. A Resource Centre is also known as a site. It provides a minimum set of local or remote IT Services, such as High Throughput Compute (HTC), Cloud Compute, Storage and Data Transfer, compliant to well-defined IT Capabilities necessary to make resources accessible to authorised users. Users can access the services using common interfaces.\nThe EGI Federation is a Resource Infrastructure federating Resource Centres to constitute a homogeneous operational domain, and the Resource Infrastructure Provider is the legal organisation that is responsible of establishing, managing and of operating directly or indirectly the operational services to an agreed level of quality needed by the Resource Centres and the user community. It holds the responsibility of integrating them in EGI to enable uniform resource access and sharing for the benefit of their consuming end users. Examples of a Resource infrastructure Provider are the European Intergovernmental Research Organisations (EIRO) and the NGIs.\nIn Europe, Resource Centres are required to be affiliated to the respective NGIs, which (a) have a mandate to represent their national users community in all matters falling within the scope of the EGI Infrastructure, and (b) are the only organization having the mandate described in (a) for its country and thus provide a single contact point at the national level.\nOperations Centres A Resource Infrastructure Provider is responsible for delivering, to different groups of customers, a number of services, either technical or humans : services supporting the activities of the given Resource Infrastructure Provider and facilitating and making secure the access to the provided resources and services.\nIn order to contribute resources to EGI, a Resource Infrastructure Provider must be associated to an Operations Centre which will deliver the services on behalf of them.\nA single Operations Centre can be federated and operate services for multiple Resource Infrastructure Providers. Federation of operations can be a cost-effective approach for Resource Infrastructure Providers wishing to share their effort and services, or in order to operate the infrastructures in an early stage of maturity.\nThe Operations Centre provides services in collaboration with the respective Resource Centres via the Resource Centre Operations Manager, and globally with EGI Foundation and other Operations Centres.\nLocally, Operations Centres are responsible for supporting the Resource Centres, for monitoring their Quality of Service (QoS), for collecting requirements and representing them in the EGI operations boards. Globally they are in charge of contributing to the evolution of the EGI Infrastructure. Roles The following sections covers the roles that are commonly involved in the operations of the EGI Infrastructure. The correspondent roles defined in Configuration Database (GOCDB) give specific rights in the Configuration Database itself and in other EGI services. There are roles whose scope is limited to the operation of a Resource Centre. A number of other roles act on a higher level, involving the operations activities related either to the NGI (Regional level) or to the EGI infrastructure as a whole (global level). Other terms and definitions can be found in the EGI Glossary.\nResource Centre level Resource Centre Administrator An individual responsible for installing, operating, maintaining and supporting one or more Resources or IT Services in a Resource Centre. In the scope of Operations, Resource Centres (RC) administrators primarily receive and react on incidents at their Resource Centre and on service requests notified through tickets created on the EGI Helpdesk service. They should respond to the tickets in a suitable time frame as defined in the Resource Centre Operational Level Agreement (OLA) and be aware of the alarms at their site, eg. through the Operations Dashboard. Sites MUST only operate supported middleware versions. This implies upgrading it regularly. Emergency releases are treated in a special way. Resource Centre Administrators MUST react to security issues that are at a global level, but affect their site. See SEC03 EGI-CSIRT Critical Vulnerability Handling.\nResource Centre Operations Manager An individual who leads the Resource Centre operations, who is the official technical contact person in the connected organisation, and who is locally supported by a team of Resource Centre Administrators. The Resource Centre Operations Manager is responsible for the site at the political and legal levels and for signing the Operational Level Agreement) between the Resource Centre and its hosting NGI. The Resource Centre Operations Manager is also responsible for enforcing the EGI policies and procedures by the Resource Centre and for assigning and approving the other site roles in the Configuration Database. Further, they should ensure that administrators are subscribed to the relevant mailing lists.\nResource Centre Security Officer The person responsible for keeping the site compliant with the EGI security policies. They are the primary contact for the NGI Security officer and EGI Computer Security Incident Response Team (CSIRT). The Site Security Officer deals with security incidents and shall respond to enquiries in a timely fashion as defined in the collection of security procedures and policies.\nRegional level Regional Operator on Duty (ROD) A team responsible for solving problems and incidents in the infrastructure according to agreed procedures. ROD teams monitor the Resource Centres in their region, react to incidents identified by the monitoring tools, and oversee incidents and related problems through to their resolution. They ensure that incidents are properly recorded and that the solutions progress according to specified time lines. They also provide support to Resource Centres and Virtual Organisations (VOs) and provide support to oversight bodies in cases of unresponsive Resource Centres. They ensure that all the necessary information is available to all parties. The team is provided by each NGI and requires procedural knowledge on the process (rather than technical skills) for their work. New ROD team members are required to read the ROD Welcome page and be familiar with ROD wiki page.\nNGI Security officer The NGI Security Officer is member of the EGI-CSIRT.\nThe NGI Security Officer coordinates the security activities within its NGI and serves as the primary contact for all security related requests, in particular from EGI-CSIRT’s IRTF concerning issues with the sites within the NGI.\nFurther, the NGI Security Officer is responsible for overseeing the security related aspects of the operations of the NGI and coordinates the security activities within its NGI.\nNGIs and Sites MUST respond in a timely manner to the security requests and alerts coming from the EGI-CSIRT’s IRTF.\nThe NGI Security Officer name and contact address needs to be registered in the Configuration Database (GOCDB) and the information maintained by the NGI.\nNGI Operations manager The NGI Operations manager is the contact point for all operational matters and represents the NGI within the Operations Management Board (OMB).\nThey are mainly responsible for:\nkeeping up to date the NGI entry in the Configuration Database and managing the status of all sites under their NGI, and ensuring their information is also kept current addressing problems with Site availability or reliability. The reports are issued on a monthly basis and the NGI operations managers have 10 days to respond to identified problems attending regular Operations Management Board meetings All NGI operations management responsibilities are listed in the Resource Infrastructure Provider OLA document.\nGlobal level Chief Operations Officer The Chief Operations Officer leads EGI Operations, and is responsible for coordinating the operations of the infrastructure across the project.\nEGI-CSIRT EGI-CSIRT is the official security coordination team and contact point at project level.\nEGI-CSIRT site EGI-CSIRT profile according to RFC-2350 EGI-CSIRT Terms of References EGI-CSIRT’s IRTF The Incident Response Team for the Federation (IRTF) is a subgroup of EGI-CSIRT which acts as the primary contact for all security related requests concerning the Federation and the projects the EGI Foundation is involved in.\nEGI-CSIRTs IRTF provides the Security Officer on Duty role on a weekly rota basis. Details are in the EGI-CSIRT Terms of References.\nEGI Security Officer The EGI Security Officer, having the EGI CSIRT Officer role in the Configuration Database, is leading and coordinating EGI-CSIRT.\nThe role of the EGI Security Officer is provided by a member of EGI-CSIRT’s IRTF on a weekly rota basis.\nEGI Foundation SDIS team The EGI Foundation Service Delivery and Information Security (SDIS) team, formerly known as the EGI Operations team, is responsible of coordinating and supporting the operational activities of all the EGI Infrastructure.\nVO A Virtual Organisation (VO) is a group of users and, optionally, resources, often not bound to a single institution or national borders, who, by reason of their common membership and in sharing a common goal, are given authority to use a set of resources. Each VO member signs the VO Acceptable Usage Policy (AUP) (during registration) which is the policy describing the goals of the VO thereby defining the expected and acceptable use of the resources by the users of the VO. User documentation can be found in the users section.\nVO manager An individual responsible for the management of the membership registry of the VO ensuring its accuracy and integrity.\nJoining operations In order to join any of the organisational groups in your NGI, you will need to go through the following steps in order:\nAuthentication In general the authentication in EGI infrastructure works either with X509 personal certificates or through federated identities using Check-in. For some services (HTC and Storage) the access is granted only by using a X509 personal certificate due to legacy reasons: the process of moving the authentication and authorisation mechanism to federated identities has started but it will takes time before having everything compliant with federated identities.\nObtaining a X509 personal certificate If you do not already have a X509 personal certificate the EUGridPMA worlmap provides a map of all certification authorities according to the country (or NGI). Select your country on the map to find out who is your local CA. Follow the procedure of your local CA to request a certificate. When you have received your certificate, install it into your web browser.\nIf case of setting up a new Resource Centre please request Host certificates.\nEUGridPMA provides a web page allowing to test your certificate. Please use this resource and contact your CA if your certificate does not work.\nCreate a federated identity: registration in EGI Check-in As soon as you try to access an EGI service with your federated identity, you will be requested to register an account in EGI Check-in if not existing yet. The Check-in sign up guide explains how to sign up for an EGI account. If you already own an account, you will be simply asked to login through EGI Check-in.\nJoining dteam VO It is recommended to join the dteam VO at the dteam Registration page. You should request group membership for /dteam and /dteam/YOUR_NGI. The dteam group manager will then be notified and review your application. The membership to dteam VO is possible only by using a X09 personal certificate and it is useful to test the RCs and to debug related issues.\nRequesting GOCDB access Read Input System User Documentation first. Go to the Configuration Database and follow the instruction. All members should notify their NGI operations manager about their role requests, to be sure they are considered on time.\nRegistering into GGUS To register into GGUS please follow the Central GGUS registration link. GGUS can be accessed either with your X509 personal certificate or with your federated identity. Do not forget to apply for the support role as well. (The GGUS support staff will approve you quickly as they get the notification automatically). To get the supporter role with your federated identity, please enroll to the GGUS Supporters group in Check-in.\nIf your NGI also have a local helpdesk interfaced with GGUS, please ensure that you are properly registered also there: your NGI managers will take care of that.\nSubscribing to mailing lists NGIs and Sites have local mailing lists for ROD team members and Site Administrators respectively. Please ensure that you subscribe to them. Depending on your role ask your NGI operations manager or Site operations manager to have you included on the necessary mailing lists if there is no automatic subscription process.\nNGI operations manager should contact operations@egi.eu and state that wish to be subscribed to noc-managers mailing list noc-managers@mailman.egi.eu.\nDocumentation Procedures and policies are accessible on the EGI Policies and Procedures space. Additional documentation relevant to EGI operations is available at EGI Documentation.\nTools A list of services relevant to EGI operations can be found at the section about internal services.\n","categories":"","description":"Starting with EGI Operations duties","excerpt":"Starting with EGI Operations duties","ref":"/providers/operations-manuals/operations-start-guide/","tags":"","title":"Operations Start Guide"},{"body":"Jupyter is an extensible environment that supports different programming languages. For the EGI service we have enabled two different environments:\nthe default environment including commonly used languages like Python, R, and Julia. the MATLAB environment, for running MATLAB. Additionally, you can bring your own environment via Binder. This will allow you to build reproducible and shareable environments for your notebooks.\nAfter logging into the service, you will be shown a form for selecting the environment, pick the desired one and click start\n","categories":"","description":"Environments available in EGI Notebooks\n","excerpt":"Environments available in EGI Notebooks\n","ref":"/users/dev-env/notebooks/kernels/","tags":"","title":"Notebooks Environments"},{"body":"This page provides an overview of what are the required steps to get a new Resource Centre integrated into the EGI Federation.\nBe aware of the security policies.\nAcceptance of the RC OLA (agreed with the NGI the RC belongs to).\nRegistration on the Configuration Database:\nan entry for the resource centre was created Required mailing lists are registered people operating the services are registered service endpoints are registered associated to the proper service types (e.g. for cloud providers), and the flags monitored and production are set to yes Support through the EGI Helpdesk service:\nThe RC name is listed in the GGUS fields “Affected site” and “Notify Site” The site administrators can modify and reply to the tickets assigned to their RC The Supporter role can be requested either directly on GGUS by using the own X509 personal certificate or by enrolling to the ggus-supporters group in Check-in. Security:\nHTC: pakiti is installed and the outcome of the EGI CSIRT assessment is positive; Cloud: the EGI security Survey was sent to the EGI CSIRT and the outcome was positive. AAI:\nHTC/Storage Compliant with X509 certificates and VOMS attributes OpenStack clouds integration with Check-in General integration with Check-in for SPs Monitoring: the registered endpoints are automaticall detected by ARGO and monitored according to their type registered in the Configuration Database.\nAccounting: The accounting records are properly sent to the accounting repository and displayed by the Accounting Portal.\nAccounting probes and APEL SSM are properlly installed and configured: HTC: APEL client Cloud: cASO. Information discovery:\nHTC/Storage: the information about compute and storage endpoints are published by the site-bdii into the Top-BDIIs. Cloud: site is added to the fedcloud-catchall-operations repository Middleware: latest version of technology products are installed using the UMD and CMD releases.\nSoftware distribution:\nHTC (Optional): CVMFS Cloud: VM image synchronisation is configured with a HEPIX VM image list compliant software (e.g. cloudkeeper) ","categories":"","description":"Steps required to integrate a Resource Centre","excerpt":"Steps required to integrate a Resource Centre","ref":"/providers/operations-manuals/integration-checklist/","tags":"","title":"Resource Centre Integration Check List"},{"body":"The EGI Notebooks service relies on the following technologies to provide its functionality:\nJupyterHub with custom EGI Check-in oauthentication configured to spawn pods on Kubernetes. Kubernetes as container orchestration platform running on top of EGI Cloud resources. Within the service it is in charge of managing the allocated resources and providing the right abstraction to deploy the containers that build the service. Resources are provided by EGI Federated Cloud providers, including persistent storage for users notebooks. CA authority to allocate recognised certificates for the HTTPS server Prometheus for monitoring resource consumption. Specific EGI hooks for monitoring, accounting and backup. VO-Specific storage/Big data facilities or any pluggable tools into the notebooks environment can be added to community specific instances. Kubernetes A Kubernetes (k8s) cluster deployed into a resource provider is in charge of managing the containers that will provide the service. On this cluster there are:\n1 master node that manages the whole cluster Support for load balancer or alternatively 1 or more edge nodes with a public IP and corresponding public DNS name (e.g. notebooks.egi.eu) where a k8s ingress HTTP reverse proxy redirects requests from user to other components of the service. The HTTP server has a valid certificate from one CA recognised at most browsers (e.g. Let's Encrypt). 1 or more nodes that host the JupyterHub server, the notebooks servers where the users will run their notebooks. Hub is deployed using the JupyterHub helm charts. These nodes should have enough capacity to run as many concurrent user notebooks as needed. Main constraint is usually memory. Support for Kubernetes PersistentVolumeClaims for storing the persistent folders. Default EGI-Notebooks installation uses NFS, but any other volume type with ReadWriteOnce capabilities can be used. Prometheus installation to monitor the usage of resources so accounting records are generated. All communication with the user goes via HTTPS and the service only needs a publicly accessible entry point (public IP with resolvable name)\nMonitoring and accounting are provided by hooking into the respective monitoring and accounting EGI services.\nThere are no specific hardware requirements and the whole environment can run on commodity virtual machines.\nEGI Customisations EGI Notebooks is deployed as a set of customisations of the JupyterHub helm charts.\nAuthentication EGI Check-in can be easily configured as a OAuth2.0 provider for JupyterHub's oauthenticator. See below a sample configuration for the helm chart using Check-in production environment:\nhub: extraEnv: OAUTH2_AUTHORIZE_URL: https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/auth OAUTH2_TOKEN_URL: https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token OAUTH_CALLBACK_URL: https://\u003cyour host\u003e/hub/oauth_callback auth: type: custom custom: className: oauthenticator.generic.GenericOAuthenticator config: login_service: \"EGI Check-in\" client_id: \"\u003cyour client id\u003e\" client_secret: \"\u003cyour client secret\u003e\" oauth_callback_url: \"https://\u003cyour host\u003e/hub/oauth_callback\" username_key: \"sub\" token_url: \"https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token\" userdata_url: \"https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo\" scope: [\"openid\", \"profile\", \"email\", \"eduperson_scoped_affiliation\", \"eduperson_entitlement\"] To simplify the configuration and to add refresh capabilities to the credentials, we have created a new EGI Check-in authenticator that can be configued as follows:\nauth: state: enabled: true cryptoKey: \u003csome unique crypto key\u003e type: custom custom: className: oauthenticator.egicheckin.EGICheckinAuthenticator config: client_id: \"\u003cyour client id\u003e\" client_secret: \"\u003cyour client secret\u003e\" oauth_callback_url: \"https://\u003cyour host\u003e/hub/oauth_callback\" scope: - openid - profile - email - offline_access - eduperson_scoped_affiliation - eduperson_entitlement The auth.state configuration allows to store refresh tokens for the users that will allow to get up-to-date valid credentials as needed.\nAccounting Warning This is Work in progress, expect changes! Accounting module generates VM-like accounting records for each of the notebooks started at the service. It's available as a helm chart that can be deployed in the same namespace as the JupyterHub chart. The only needed configuration for the chart is an IGTF-recognised certificate for the host registered in GOCDB as accounting.\nssm: hostcert: |- \u003chostcert\u003e hostkey: |- \u003chostkey\u003e Monitoring Monitoring is performed by trying to execute a user notebook every hour. This is accomplished by registering a new service in the hub that has admin permissions. Monitoring is then deployed as a helm chart that must be deployed in the same namespace as the JupyterHub chart. Configuration of JupyterHub must include this section:\nhub: services: status: url: \"http://status-web/\" admin: true apiToken: \"\u003ca unique API token\u003e\" Likewise the monitoring chart is configured as follows:\nservice: api_token: \"\u003csame API token as above\u003e\" Docker images Our service relies on custom images for the hub and the single-user notebooks. Dockerfiles are available at EGI Notebooks images git repository and automatically build for every commit pushed to the repository to eginotebooks @ dockerhub.\nHub image Builds from the JupyterHub k8s-hub image and adds:\nEGI and D4Science authenticators EGISpawner EGI look and feel for the login page Single-user image Builds from Jupyter datasicence-notebook and adds a wide range of libraries as requested by users of the services. We are currently looking into alternatives for better managing this image with CVMFS as a possible solution.\nSample helm configuration If you want to build your own EGI Notebooks instance, you can start from the following sample configuration and adapt to your needs by setting:\nsecret tokens (for proxy.secretToken, hub.services.status.api_token, auth.state.cryptoKey). They can be generated with openssl rand -hex 32. A valid hostname (\u003cyour notebooks host\u003e below) that resolves to your Kubernetes Ingress Valid EGI Check-in client credentials, these can be obtained by creating a new Service for the demo instance of Check-in through the EGI Federation Registry. When moving to EGI Check-in production environment, make sure to remove the hub.extraEnv.EGICHECKIN_HOST variable. --- proxy: secretToken: \"\u003csome secret\u003e\" service: type: NodePort ingress: enabled: true annotations: kubernetes.io/tls-acme: \"true\" hosts: [\u003cyour notebooks host\u003e] tls: - hosts: - \u003cyour notebooks host\u003e secretName: acme-tls-notebooks enabled: true hosts: [\u003cyour notebooks host\u003e] singleuser: storage: capacity: 1Gi dynamic: pvcNameTemplate: claim-{userid}{servername} volumeNameTemplate: vol-{userid}{servername} storageAccessModes: [\"ReadWriteMany\"] memory: limit: 1G guarantee: 512M cpu: limit: 2 guarantee: .02 defaultUrl: \"/lab\" image: name: eginotebooks/single-user tag: c1b2a2a hub: image: name: eginotebooks/hub tag: c1b2a2a extraConfig: enable-lab: |- c.KubeSpawner.cmd = ['jupyter-labhub'] volume-handling: |- from egispawner.spawner import EGISpawner c.JupyterHub.spawner_class = EGISpawner extraEnv: JUPYTER_ENABLE_LAB: 1 EGICHECKIN_HOST: aai-demo.egi.eu services: status: url: \"http://status-web/\" admin: true api_token: \"\u003cmonitor token\u003e\" auth: type: custom state: enabled: true cryptoKey: \"\u003ca unique crypto key\u003e\" admin: access: true users: [\u003clist of EGI Check-in users with admin powers\u003e] custom: className: oauthenticator.egicheckin.EGICheckinAuthenticator config: client_id: \"\u003cyour egi checkin_client_id\u003e\" client_secret: \"\u003cyour egi checkin_client_secret\u003e\" oauth_callback_url: \"https://\u003cyour notebooks host\u003e/hub/oauth_callback\" enable_auth_state: true scope: - openid - profile - email - offline_access - eduperson_scoped_affiliation - eduperson_entitlement ","categories":"","description":"Internal Service Architecture","excerpt":"Internal Service Architecture","ref":"/providers/notebooks/architecture/","tags":"","title":"Architecture"},{"body":"The EGI Federated Cloud (FedCloud) is a multi-national cloud system that integrates community, private and/or public clouds into a scalable computing platform for research. The Federation pools resources from a heterogeneous set of cloud providers using a single authentication and authorisation framework that allows the portability of workloads across multiple providers, and enables bringing computing to data. The current implementation is focused on Infrastructure-as-a-Service (IaaS) services, but can be easily applied to Platform-as-a-Service (PaaS) and Software-as-a-Servcice (SaaS) layers.\nEach resource centre of the federated infrastructure operates a Cloud Management Framework (CMF) according to its own preferences and constraints and joins the federation by integrating this CMF with components of the EGI service portfolio. CMFs must at least be integrated with EGI Authentication and Authorization Infrastructure (AAI) so users can access services with a single identity, integration with other components and APIs to be provided are agreed by the community the resource centre provides services to.\nEGI follows a Service Integration and Management (SIAM) approach to manage the federation with processes that cover the different aspects of the IT Service Management. Providers in the federation keep complete control of their services and resources. EGI creates Virtual Organizations (VOs) for each research community, and EGI VO Operation Level Agreements (OLAs) establish a reliable, trust-based communication channel between the community and the providers, by agreeing on the services, their levels and the types of support.\nNote EGI VO OLAs are not legal contracts but, as agreements, they outline the clear intentions to collaborate and support research. Federated IaaS The EGI FedCloud IaaS resource centres deploy a Cloud Management Framework (CMF) that provide users with an API-based service for management of Virtual Machines and associated Block Storage to enable persistence and Networks to enable connectivity of the Virtual Machines (VMs) among themselves and third party resources.\nThe IaaS federation is a thin layer that brings the providers together with:\nFederated authentication Resource discovery Central VM image catalogue Usage accounting Monitoring The IaaS capabilities (VM, block storage, network management, etc.) must be provided via community agreed APIs (OpenStack is supported at the moment) that allow integration with EGI Check-in for authentication and authorisation of users.\nNote Those providers that limit the interaction to web dashboards and do not expose APIs to direct consumption for users cannot be considered part of the EGI IaaS Cloud services. Users and Community platforms built on top of the EGI IaaS can interact with the cloud providers at three different layers:\nDirectly using the IaaS APIs or CLIs to manage individual resources. This option is recommended for pre-existing use cases with requirements on specific APIs. Using federated access tools that allow managing the complexity of dealing with different providers in a uniform way. These tools include: Provisioning systems allow users to define infrastructure as code, then manage and combine resources from different providers, thus enabling the portability of application deployments between them (e.g. Infrastructure Manager or Terraform), and Cloud brokers provide matchmaking for workloads to available providers (e.g. the INDIGO-DataCloud Orchestrator). Using the VMOps dashboard. EGI provides ready-to-use software components to enable the federation for OpenStack. These components rely on public APIs of the IaaS system and use Check-in accounts for authenticating into the provider.\nImplementation Authentication and authorization Federated identity ensures that users of the federation can use a single account for accessing the resources.\nOpenID Connect Providers of the EGI Cloud support authentication with OAuth2 tokens provided by Check-in OpenID Connect Identity provider. Support builds on the AAI guide for SPs with detailed configuration provided at the EGI IaaS Service providers documentation.\nThe integration relies on the OpenStack Keystone OS-FEDERATION API.\nInformation discovery The Configuration Database contains the list of resource centres and their endpoints, while the AppDB Information System collects this information in a central service for discovery, providing a real-time view of the actual capabilities of federation participants (can be used by both human users and machine services).\nConfiguration Database The EGI Configuration Database is used to catalogue the static information of the production infrastructure topology (e.g. the list of resource centres and their endpoints).\nTo allow resource providers to expose IaaS federation endpoints, the following service types are avialable:\norg.openstack.horizon org.openstack.nova org.openstack.swift eu.egi.cloud.accounting eu.egi.cloud.vm-metadata.marketplace All providers must enter cloud service endpoints into the Configuration Database to enable integration with EGI.\nThe Cloud Info Provider extracts information from the resource centres using their native APIs and formats it following Glue, an OGC recommended standard. This information is pushed to the Argo Messaging System and consumed by AppDB to provide a central information discovery service that aggregates several other sources of information about the infrastructure.\nVirtual Machine Image management In a distributed, federated IaaS service, users need solutions for efficiently managing and distributing their VM images across multiple resource providers. EGI provides a catalogue of VM images (VMIs) that allows any user to share their VMI, and communities to select those VMIs relevant for distribution across providers. These images are automatically replicated at the providers supporting the community and converted as needed to ensure the correct instantiation when used.\nAppDB includes a Virtual Appliance Marketplace supporting Virtual Appliances (VAs), which are clean and lean virtual machine images designed to run on a virtualisation platform, that provide a software solution out-of-the-box, ready to be used with minimal or no set-up.\nAppDB allows representatives of research communities (VOs) to generate a VM image list that resource centres subscribe to. The subscription enables the periodic download, conversion and storage of those images to the image repository of the indicated resource centres, using HEPiX image list format. cloudkeeper provides this automated synchronisation between AppDB and the cloud provider.\nAccounting Federated Accounting provides an integrated view about resource/service usage: it pulls together usage information from the federated sites and services, integrates the data and presents them in such a way that both individual users as well as whole communities can monitor their own resource/service usage across the whole federation.\nUsage of resources is gathered centrally using EGI Accounting repository and available for visualisation at EGI Accounting portal.\nCloud Usage Record The federated cloud task force has agreed on a Cloud Usage Record, which inherits from the OGF Usage Record. This record defines the data that resource providers must send to EGI’s central Accounting repository.\nVersion 0.4 of the Cloud Accounting Usage Record was agreed at the FedCloud Face to Face in Amsterdam in January 2015. A summary table of the format is shown below:\nCloud Usage Record Property Type Null Definition VMUUID varchar(255) No Virtual Machine's Universally Unique Identifier concatenation of CurrentTime, SiteName and MachineName SiteName varchar(255) No GOCDB SiteName - GOCDB now has cloud service types and a cloud-only site is allowed. CloudComputeService (NEW) varchar(255) Name identifying cloud resource within the site. Allows multiple cloud resources within a sitei.e. a level of granularity. MachineName varchar(255) No VM ID - the site name for the VM LocalUserId varchar(255) Local username LocalGroupId varchar(255) Local group name GlobalUserName varchar(255) Global identity of user (certificate DN) FQAN varchar(255) Use if VOs part of authorization mechanism Status varchar(255) Completion status - completed, started or suspended StartTime datetime Must be set when Status = started EndTime datetime Set to NULL until Status = completed SuspendDuration datetime Set when Status = suspended (Timestamp) WallDuration int WallClock time - actual time used CpuDuration int CPU time consumed (Duration) CpuCount int Number of CPUs allocated NetworkType varchar(255) Needs clarifying NetworkInbound int GB received NetworkOutbound int GB sent PublicIPCount (NEW) int Number of public IP addresses assigned to VM Not used. Memory int Memory allocated to the VM Disk int Size in GB allocated to the VM BenchmarkType (NEW) varchar(255) Name of benchmark used for normalization of times (eg HEPSPEC06) Benchmark (NEW) Decimal Value of benchmark of VM using ServiceLevelType benchmark’ StorageRecordId varchar(255) Link to other associated storage record Need to check feasibility ImageId varchar(255) Every image has a unique ID associated with it. For images from the EGI FedCloud AppDB this should be VMCATCHER_EVENT_AD_MPURI; for images from other repositories it should be a vmcatcher equivalent; for local images - local identifier of the image. CloudType varchar(255) Type of cloud infrastructure: OpenNebula; OpenStack; Synnefo; etc. Public IP Usage Record The fedcloud task force has agreed on an IP Usage Record. The format uses many of the same fields as the Cloud Usage Record. The Usage Record should be a \"snapshot\" of the number of IPs currently assigned to a user. A table defining v0.2 of the format is shown below:\nCloud Usage Record Property Type Null Definition Notes MeasurementTime datetime No The time the usage was recorded. In the message format, must be a UNIX timestamp, i.e. the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970) SiteName varchar(255) No The GOCDB site assigning the IP CloudComputeService varchar(255) Yes See Cloud Usage Record CloudType varchar(255) No See Cloud Usage Record LocalUser varchar(255) No See Cloud Usage Record LocalGroup varchar(255) No See Cloud Usage Record GlobalUserName varchar(255) No See Cloud Usage Record FQAN varchar(255) No See Cloud Usage Record IPVersion byte No 4 or 6 IPCount int(11) No The number of IP addresses of IPVersion this user currently assigned to them A JSON schema defining a valid Public IP Usage message can be found at: https://github.com/apel/apel/blob/9476bd86424f6162c3b87b6daf6b4270ceb8fea6/apel/db/__init__.py\nGPU Usage Record The fedcloud task force has agreed on an GPU Usage Record. The format uses many of the same fields as the Cloud Usage Record. A table defining Draft 4 – 24/02/2021 is shown below:\nGPU Usage Record Property Type Null Definition MeasurementMonth int No The month/year the reported usage should be assigned to. If the month/year is the current month/year, the usage should be up to the point of reporting. MeasurementYear int No AssociatedRecordType varchar(255) No The context in which the reported usage was used. I.e. “cloud” for an accelerator attached to a VM. AssociatedRecord varchar(255) No VMUUID if AssociatedRecordType is “cloud” GlobalUserName varchar(255) Yes See the definition of your AssociatedRecordType FQAN varchar(255) No See the definition of your AssociatedRecordType SiteName varchar(255) No See the definition of your AssociatedRecordType Count decimal No A count of the Accelerators attached to the VM. At the moment Accelerators are not shared among VMs but it will change when Accelerator virtualization is applied, so we should have the field at decimal type instead of integer (e.g. Count = 0.5 when it is shared between two VMs). Cores int(11) Yes Total number of cores. i.e. So if an Accelerator has 64 cores and a VM has 2 like that attached then we would report: Count=2 and Processors=128 ActiveDuration int(11) Yes Actual usage duration of the Accelerator in seconds for the given month/year (in case some systems could report actual usage). At the moment, ActiveDuration will be the same as the AvailableDuration due to the limitation of currently used technologies (impossible to get ACCELERATOR utilization from outside of the VM, no ACCELERATOR hot-plug into running VM) but it may change in near future so it is good to have the fields separately. Set to AvailableDuration if AcitveDuration is omitted from the record AvailableDuration int(11) No Time accelerator was available in seconds for the given month/year (Wall)Time that a GPU was attached to a VM. BenchmarkType varchar(255) Yes Name of benchmark used for normalization of times Benchmark decimal Yes Value of benchmark of Accelerator Type varchar(255) No High level description of accelerator, i.e. GPU, FPGA, Other Model varchar(255) Yes model number, spec, some other concept that 2 ACCELERATORs with the same number of cores might be different etc APEL and accounting portal Once generated, records are delivered to the central accounting repository using APEL SSM (Secure STOMP Messenger). SSM client packages can be obtained at https://apel.github.io. A Cloud Accounting Summary Usage Record has also been defined and summaries created on a daily basis from all the accounting records received from the Resource Providers are sent to the EGI Accounting Portal. The Accounting portal also runs SSM to receive these summaries and provides a web view of the accounting data received from the Resource Providers.\ncASO delivers an implmentation of the extrator probes for OpenStack.\nMonitoring The endpoints published in the Configuration Database are monitored via ARGO. Specific probes to check functionality and availability of services must be provided by service developers.\nThe current set of probes used for monitoring IaaS resources consists of:\nAccounting probe (eu.egi.cloud.APEL-Pub): Checks if the cloud resource is publishing data to the Accounting repository TCP checks (org.nagios.Broker-TCP, org.nagios.CDMI-TCP, and org.nagios.CloudBDII-Check): Basic TCP checks for services. VM Marketplace probe (eu.egi.cloud.AppDB-Update): gets a predetermined image list from AppDB and checks its update interval. Perun probe (eu.egi.cloud.Perun-Check): connects to the server and checks the status by using internal Perun interface Roadmap The TCB-Cloud board defines the roadmap for the technical evolution of the EGI Cloud. All the components are continuously maintained to:\nImprove their programmability, providing complete APIs specification in adequate format for facilitating the generation clients (e.g. following the OpenAPI initiative and Swagger). Lower the barriers to integrate and operate resource centres in the federation by a) minimizing the number of components used; b) contributing code to upstream distributions; and c) use only public APIs of the Cloud Management Frameworks. Currently the EGI FedCloud TaskForce is focused on moving to a central operations model, where providers only need to integrate their system with EGI Check-in but do not need to deploy and configure the different tools (accounting, discovery, VMI management, etc.) locally but delegate this to a central EGI team.\n","categories":"","description":"The architecture of the EGI Federation\n","excerpt":"The architecture of the EGI Federation\n","ref":"/users/getting-started/architecture/","tags":"","title":"EGI Architecture"},{"body":" What is it? Block storage provides block-level storage volumes for use within virtual machines (VMs). Block storage volumes are raw, unformatted block devices, which can be mounted as devices in VMs.\nBlock storage volumes that are attached to a VM are exposed as storage volumes that persist independently from the life of the VM, and need to be explicitly destroyed when data is not needed anymore. Users can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive).\nThe content of block storage volumes can be accessed only from within the VM they are mounted to, and they can be mounted to a single VM at any given time.\nThe main features of block storage:\nBlock storage is recommended for data that must be quickly accessible and requires long-term persistence. Block storage volumes are well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes. Users can create point-in-time snapshots of block storage volumes, which protect data for long-term durability, and they can be used as the starting point for new block storage volumes. Note Block storage volumes can can only be mounted to VMs running at the same provider where the block storage is located. Note Block storage usage is accounted for the entire block storage device, regardless how much of it is actually used. Important There is a limit on the number of block storage devices you can attach on a VM and there is a limit to the maximum size of such virtual disks. These values will depend on the particular provider and your SLA. Manage volumes The block storage in the EGI Cloud is offered via OpenStack deployments that implement the Cinder service.\nUsers can manage block storage using the OpenStack Horizon dashboard of a provider, from a command-line interface (CLI), or via the OpenStack Block Storage API.\nManage from the command-line Multiple command-line interfaces (CLIs) are available to manage block storage:\nThe OpenStack CLI The FedCloud Client is a high-level CLI for interaction with the EGI Federated Cloud (recommended) The Cinder CLI has some advanced features and administrative commands that are not available through the OpenStack CLI The main FedCloud commands for managing volumes are detailed below.\nNote For more information see the documentation about volume management. List volumes For example, to list the volumes in the site IN2P3-IRES via the Pilot VO (vo.access.egi.eu), use the following FedCloud command:\nLinux / Mac Windows PowerShell To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n$ export EGI_SITE=IN2P3-IRES $ export EGI_VO=vo.access.egi.eu $ fedcloud openstack volume list Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e set EGI_SITE=IN2P3-IRES \u003e set EGI_VO=vo.access.egi.eu \u003e fedcloud openstack volume list Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e $Env:EGI_SITE=\"IN2P3-IRES\" \u003e $Env:EGI_VO=\"vo.access.egi.eu\" \u003e fedcloud openstack volume list Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+ Create volume To create a new volume use the FedCloud command below:\n$ fedcloud openstack volume create --size 10 my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume create --size 10 my-volume +---------------------+------------------------------------------------------------------+ | Field | Value | +---------------------+------------------------------------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-08-05T13:10:42.000000 | | description | None | | encrypted | False | | id | a711296-5cff-46ac-bbe3-58e00712ee3e | | multiattach | False | | name | my-volume | | properties | | | replication_status | None | | size | 10 | | snapshot_id | None | | source_volid | None | | status | creating | | type | ceph | | updated_at | None | | user_id | 1a3ef4b64714f86ac71f1c9512345678c157a94ae1b37f167b6a663baa3b915b | +---------------------+------------------------------------------------------------------+ The status of the new volume will probably be returned as creating. To check if the volume finished creating, look at the details of the volume, or list only the newly created volume (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 10 | | +--------------------------------------+-----------+-----------+------+-------------+ When the status of the volume is available the volume is ready to be attached to a VM.\nSee volume details To view details of a volume use the FedCloud command below:\nTip The volume can be specified either by its ID or by its name (if it has one). $ fedcloud openstack volume show Matlab Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume show Matlab +------------------------------+------------------------------------------------------------+ | Field | Value | +------------------------------+------------------------------------------------------------+ | attachments | [{'server_id': 'a4ab00a1-d458-41a9-a091-ee707bc9357e', | | | 'attachment_id': '291111d3-f43c-494c-b64c-5c0abcda3d37', | | | 'attached_at': '2021-08-05T08:50:19.000000', | | | 'host_name': 'sbgcsrv17.in2p3.fr', | | | 'volume_id': '9a4000fb-0bcc-47e8-96fb-85a222295402', | | | 'device': '/dev/vdb', | | | 'id': '9a4000fb-0bcc-47e8-96fb-85a222295402'}] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-08-05T08:48:41.000000 | | description | | | encrypted | False | | id | 9a4000fb-0bcc-47e8-96fb-85a222295402 | | multiattach | False | | name | Matlab | | os-vol-tenant-attr:tenant_id | 7a910xxxxae74ed9yyyy7497zzzz9499 | | properties | | | replication_status | None | | size | 50 | | snapshot_id | None | | source_volid | None | | status | in-use | | type | ceph | | updated_at | 2021-08-05T08:50:20.000000 | | user_id | babzz8c3b4cxxxx6286dacyyyy01e5a3 | +------------------------------+------------------------------------------------------------+ Attach volume to VM Mapping block devices to VMs is described in detail in the OpenStack documentation.\nTo attach a volume to a VM use the FedCloud command below:\nNote To be able to attach a volume to a VM, the volume must not be attached to any VM (volume status must be available). Caution The optional --device argument to specify the device name in the VM should not be used. It does not work properly, and will be removed in the near future. $ fedcloud openstack server add volume my-server my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: server add volume my-server my-volume You can check that the volume got attached to the VM (and with what device name) by either looking at the details of the volume, the details of the VM, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-----------------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-----------------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | in-use | 10 | my-server on /dev/vdc | +--------------------------------------+-----------+-----------+------+-----------------------+ When the volume status is in-use the volume is attached to a VM, and it can be used from the VM.\nDetach volume from VM To detach a volume from a VM use the FedCloud command below:\n$ fedcloud openstack server remove volume my-server my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: server remove volume my-server my-volume You can check that the volume got detached by either looking at the details of the volume, the details of the VM, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 10 | | +--------------------------------------+-----------+-----------+------+-------------+ When the volume status is available the volume is not attached to any VM.\nResize volume To resize a volume set its size property to the desired size. For example, if there is a volume named my-volume with a size of 10GB, it can be resized using the FedCloud command below:\nTip Other volume properties can be altered in the same way. Note To be able to resize a volume, the volume must not be attached to any VM (volume status must be available), unless the volume driver supports in-use extend. $ fedcloud openstack volume set --size 20 my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume set --size 20 my-volume You can check that the volume got resized by either looking at the details of the volume, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 20 | | +--------------------------------------+-----------+-----------+------+-------------+ Snapshot a volume Users can create snapshots of a volume that can later be used to create other volumes or to rollback to a precedent point in time. Volume snapshots are pointers in the read-write history of a volume.\nTo take a snapshot of a volume, use the FedCloud command below:\nTip See also the documentation about the other snapshot management commands. Note To be able to create a snapshot of a volume, the volume must not be attached to any VM (volume status must be available). To create a snapshot while the volume is attached to a VM, use the --force command flag, but be aware that there may be inconsistencies if the VM’s OS is not aware of the snapshot being taken. $ fedcloud openstack volume snapshot create --volume my-volume my-snapshot --force Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume snapshot create --volume my-volume my-snapshot +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2021-08-06T16:08:37.631226 | | description | None | | id | 15149b42-032f-4cec-b6f3-41aa5c958081 | | name | my-snapshot | | properties | | | size | 10 | | status | creating | | updated_at | None | | volume_id | aa711296-5cff-46ac-bbe3-58e0712ee3e9 | +-------------+--------------------------------------+ The status of the snapshot will probably be returned as creating. To check if the snapshot is ready, look at the details of the snapshot, or list only the newly created snapshot (filter by snapshot name or ID):\n$ fedcloud openstack volume snapshot list --name my-snapshot Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume snapshot list --name my-snapshot +--------------------------------------+-------------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+-------------+-------------+-----------+------+ | 15149b42-032f-4cec-b6f3-41aa5c958081 | my-snapshot | None | available | 10 | +--------------------------------------+-------------+-------------+-----------+------+ When the status of the snapshot is available the snapshot is ready, and can be used to create new volumes or to restore the source volume to the point-in-time when the snapshot was taken.\nBackup a volume Users can also create backups of a volume, but backups can only be used later to create or replace other volumes. A volume backup is a copy of a volume saved to cold storage, which is cheaper than the performant storage used for volumes and snapshots.\nTo make a backup of a volume, use the FedCloud command below:\nTip See also the documentation of the other backup management commands. Note Not all OpenStack deployments support volume backups. Note Backups use as much storage as the source volume, albeit in a cheaper storage layer. This means backups take a long time to create (~4h for 500GB), and the space is accounted for the same way as for regular volumes (the entire size of the backed up volume, regardless of how much of the volume space is actually used). Thus backups should be deleted when no longer needed. Note To be able to make a backup of a volume, the volume must not be attached to any VM (volume status must be available). To make a backup while the volume is attached to a VM, use the --force command flag, but be aware that there may be inconsistencies if the VM’s OS is not aware of the backup being taken. $ fedcloud openstack volume backup create --name my-backup my-volume --force Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume backup create --name my-backup my-volume +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2021-08-06T16:08:37.631226 | | description | None | | id | 158bcb42-032f-4cec-b6f3-890a5c958081 | | name | my-backup | | properties | | | size | 10 | | status | creating | | updated_at | None | | volume_id | aa711296-5cff-46ac-bbe3-58e0712ee3e9 | +-------------+--------------------------------------+ The status of the backup will probably be returned as creating. To check if the backup has finished, look at the details of the backup, or list only the newly created backup (filter by backup name or ID):\n$ fedcloud openstack volume backup list --name my-backup Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume backup list --name my-backup +--------------------------------------+-----------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+-----------+-------------+-----------+------+ | 158bcb42-032f-4cec-b6f3-890a5c958081 | my-backup | None | available | 10 | +--------------------------------------+-----------+-------------+-----------+------+ When the status of the backup is available the backup operation is complete.\nDelete volume To delete a volume use the following FedCloud command:\nNote To be able to delete a volume, the volume must not be attached to any VM (volume status must be available). $ fedcloud openstack volume delete my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume delete my-volume This starts deletion of the specified volume (the volume status changes to deleting):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | deleting | 10 | | +--------------------------------------+-----------+----------+------+-------------+ Once deletion of the specified volume is complete, it will no longer show up in the list of volumes.\nAccess from your VMs Block storage volumes attached to a VM will appear as a block device in the VM.\nTo find out the device name that got assigned to a volume when it was attached to a VM, look at the details of the volume, or list only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+--------+------+-----------------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+--------+------+-----------------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | in-use | 10 | my-server on /dev/vdb | +--------------------------------------+-----------+--------+------+-----------------------+ In this example the device name is /dev/vdb. To validate this, run the following command in the VM:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 20G 0 disk └─vda1 252:1 0 20G 0 part / vdb 252:16 0 10G 0 disk Usually these devices are empty upon creation. The first time you attach them to a VM, you will need to partition the device and create filesystem(s) on it, by running the following command in the VM:\nTip Using a file system volume label is useful to avoid the need to find the out the device name, especially when multiple block storage volumes are attached to a VM. It is recommended to use a file system volume label in the VM that is the same as the name of the block storage volume. Tip The filesystem type can be one supported by the Linux distribution in the VM, but xfs and ext4 are the most widely used. Caution Only run this command the first time you use the device, as it deletes all data! Make sure you use the correct device name, otherwise you will destroy data on other devices! $ sudo mkfs.ext4 -L my-volume /dev/vdb Once you created a filesystem on the device, you can mount it at any desired path by running the following command in the VM:\n$ sudo mount /dev/vdb1 /\u003cpath\u003e Continuing with the example above, if we check again the block devices by running the following command in the VM:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 20G 0 disk └─vda1 252:1 0 20G 0 part / vdb 252:16 0 10G 0 disk └─vdb1 252:1 0 10G 0 part /\u003cpath\u003e If non-root users should be able to access the mounted volume similar to the way e.g. /tmp is accessible, set the sticky bit on the mount point, with this command in the VM:\n$ sudo chmod +t /\u003cpath\u003e If the desired behaviour is to mount the file system automatically on VM restart, add it to /etc/fstab. Using the LABEL parameter will ensure the correct volume is chosen if multiple volumes are attached:\nLABEL=my-volume /\u003cpath\u003e ext4 noatime,nodiratime,user_xattr,nofail 0 0\nNote The use of option nofail is recommended in order to skip (and not block on) mounting the file system volume if it is unavailable, e.g. in case of network issues. Remove this option from the fstab line if you want the VM to block the boot process if the volume is unavailable. Note The use of option nobarrier is not recommended as volumes are accessed via a cache, and ignoring the correct ordering of journal commits may result in a corrupted file system in case of a hardware problem. With that you can access /\u003cpath\u003e inside the VM, where all data stored on the volume will be available. Applications will not see any difference between a block storage device and a regular disk, thus no major changes should be required in the application logic.\nStorage Encryption This section describes the usage of the tool cryptsetup to enable the permanent encryption of the data stored in the disk. The tool is available in standard linux distributions, and for this guide we assume the installation in an Ubuntu distribution.\n$ sudo su - $ apt -y install cryptsetup To encrypt the disk, it must be first initialized correctly. In the example below, the disk named /dev/vdb is first filled with random data and then initialized using the cryptsetup luksFormat command below. This first step can be quite long.\n$ dd if=/dev/urandom of=/dev/vdb bs=4k $ cryptsetup -v --cipher aes-xts-plain64 --key-size 512 --hash sha512 \\ --iter-time 5000 --use-random luksFormat /dev/vdb If this last command slows down or even blocks with the following message:\nSystem is out of entropy while generating volume key. Please move mouse or type some text in another window to gather some random events. Generating key (0% done). you can make the cryptsetup luksFormat command running faster by first installing the haveged program in your virtual machine.\nThe following command verifies that the disk is now of type LUKS:\n$ cryptsetup luksDump /dev/vdb LUKS header information for /dev/vdb Version: 1 Cipher name: aes Cipher mode: xts-plain64 Hash spec: sha512 Payload offset: 4096 MK bits: 512 MK digest: c4 f7 4b 02 2a 3f 12 c1 2c ba e5 c9 d2 45 9a cd 89 20 6c 73 MK salt: 98 58 3e f3 f6 88 99 ea 2a f3 cf 71 a0 0d e5 8b d5 76 64 cb d2 5c 9b d1 8a d3 1d 18 0e 04 7a eb MK iterations: 81250 UUID: c216d954-199e-4eab-a167-a3587bd41cb3 Key Slot 0: ENABLED Iterations: 323227 Salt: a0 45 3e 98 fa cf 60 74 c6 09 3d 54 97 89 be 65 5b 96 7c 1c 39 26 47 b4 8b 0e c1 3a c9 94 83 c2 Key material offset: 8 AF stripes: 4000 Key Slot 1: DISABLED Key Slot 2: DISABLED Key Slot 3: DISABLED Key Slot 4: DISABLED Key Slot 5: DISABLED Key Slot 6: DISABLED Key Slot 7: DISABLED The disk is now ready for use. The first time you use it, you must perform the following steps:\nStep 1: Open the encrypted disk with the cryptsetup luksOpen command. The name storage1 is only indicative, you can choose what you want:\n$ cryptsetup luksOpen /dev/vdb storage1 Step 2: Create a filesystem on disk:\n$ mkfs.ext4 /dev/mapper/storage1 Step 3: Create the disk mount point:\n$ mkdir /storage1 Step 4: Mount the disk:\n$ mount -t ext4 /dev/mapper/storage1 /storage1 Step 5: Check available space (this may be slightly different from what was entered during the openstack volume create command):\n$ df -h /storage1 Filesystem Size Used Avail Use% Mounted on /dev/mapper/storage1 2.0G 6.0M 1.9G 1% /storage1 Once the disk is operational, steps 2 and 3 are no longer necessary.\nYou can now send files (for example DATA.dat) from your personal computer to your virtual machine in a secure way, for example with scp:\n$ scp -i ${HOME}/.ssh/cloudkey DATA.dat ubuntu@134.158.151.224:/storage1 DATA.dat 100% 82 0.1KB/s 00:00 When you are done with your work on the disk, you can remove it cleanly with the following commands:\n$ umount /storage1 $ cryptsetup close storage1 For the following uses of the persistent disk, there will be no need to perform all these operations, only the following are necessary:\n$ cryptsetup luksOpen /dev/vdb storage1 $ mkdir /storage1 $ mount -t ext4 /dev/mapper/storage1 /storage1 Access via EGI Data Transfer EGI Data Transfer allows you to move any type of data files asynchronously from one storage to another. If you want to copy data from/to one VM running on the EGI cloud, you will need to run a compatible server (Webdav/HTTPS, GridFTP, xrootd, SRM, S3, GCloud) that can interact with the FTS3 software.\nAn easy way to provide a GridFTP server on your VM is to use the gridftp-le ready2go docker stack for deploying a GridFTP Docker Container with certificates from Let’s Encrypt. Take into account:\nSecurity groups for the VM must allow ports 80, 2811 and the 50000-50200 range.\nThe VM must have a valid DNS entry (you can use Dynamic DNS in FedCloud to get one)\nThe default setup uses /srv as path to expose and maps users to the nobody user. Make sure that nobody is able to read (and write if needed) on that location or set the mapping to the appropriate users.\nThe Let’s Encrypt certificates may not be accepted by some of the EGI infrastructure endpoints, you may want to consider using IGTF certificates instead. Check your CA for instructions on how to get those.\nYou can add direct mappings for specific DNs by adding a /etc/localgridmap.conf file in your running container. See the example below to map the /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Enol Fernandez del Castillo DN to nobody. You can add as many lines as needed:\n\"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Enol Fernandez del Castillo\" nobody You need to specify the environment variables in the docker-compose.yml file for obtaining the Let’s Encrypt certificate. An extra variable GLOBUS_HOSTNAME must be also set:\nenvironment: - TESTCERT - EMAIL=youremail@domain.com - DOMAIN=mygridftp.example.com - GLOBUS_HOSTNAME=mygridftp.example.com If you are running on a site with MTU smaller than 1500 (e.g. CESNET-MCC), make sure that you set the MTU to a value smaller than the interface MTU(you can check this with ip addr). In your docker-compose.yml, add:\nnetworks: default: driver: bridge driver_opts: com.docker.network.driver.mtu: 1434 ","categories":"","description":"Block Storage offered by EGI Cloud providers\n","excerpt":"Block Storage offered by EGI Cloud providers\n","ref":"/users/data/storage/block-storage/","tags":"","title":"Block Storage"},{"body":"Moving the Site BDII to another machine Ensure the new Site BDII is working fine and publishing all the necessary site information\nSee MAN01 for general information about how to configure a Site BDII. In particular, remember that the Site BDII configuration must include the BDII node itself. Put the old Site BDII in scheduled downtime in the Configuration Database for a couple of hours\nRegister the new service in the Configuration Database by adding a new Service Endpoint:\nselect Site BDII in Service Type field fill in at least the hostname select Y for production and monitoring, N for beta service field Properly modify the GIIS URL field with the new SITE-BDII ldap URL\nNote that the site name in the GIIS URL is case-sensitive Mark the old Site BDII as not production and turn off its monitoring\nTop BDIIs updates their sites list every hour so the new Site BDII should be published in less than an hour; instead Nagios updates the monitored hosts every 3 hours\nWhen the new Site BDII is appeared on Nagios and the old one is disappeared, turn off the old Site BDII and remove it from the Configuration Database\nTurning off a Site BDII co-hosted with other services Stop and remove the service\nservice bdii stop yum remove glite-BDII glite-yaim-bdii Delete some info-providers and ldif file ( /opt/glite/etc/gip/ldif/stub-site.ldif, /opt/glite/etc/gip/site-urls.conf, /opt/glite/etc/gip/provider/glite-info-provider-site, /opt/glite/etc/gip/provider/glite-info-provider-service-bdii-site-wrapper, /opt/glite/etc/gip/ldif/glite-info-site.ldif)\nReconfigure with yaim without specifying the BDII_site profile\n","categories":"","description":"Changing the Site BDII","excerpt":"Changing the Site BDII","ref":"/providers/high-throughput-compute/changing_site_bdii/","tags":"","title":"Changing the Site BDII"},{"body":"The integration of OpenStack service providers into the EGI Check-in is a two-step process:\nTest integration with the demo instance of EGI Check-in. This will allow you to check the complete functionality of the system without affecting the production Check-in service. Once the integration is working correctly, register your provider with the production instance of EGI Check-in to allow members of the EGI User Community to access your service. Registration into Check-in demo instance Before your service can use the EGI Check-in OIDC Provider for user login, you need to register a client through the EGI Federation Registry in order to obtain OAuth2.0 credentials and register one or more redirect URIs.\nMake sure that you fill in the following options:\nGeneral tab:\nSet Integration Environment to Demo and fill the form with the information about your Service. Protocol Specific tab:\nSet Select Protocol to OIDC Service Set redirect URL to https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect. Recent versions of OpenStack may deploy Keystone at /identity/, be sure to include that in the \u003cyour keystone endpoint\u003e part of the URL if needed. Enable openid, profile, email, eduperson_entitlement in the Scope field Enable authorization code in the Grant Types field Set Proof Key for Code Exchange (PKCE) Code Challenge Method to SHA-256 hash algorithm (recommended) Make sure Allow calls to the Introspection Endpoint? is enabled in Introspection field Submit the request for review by the Check-in operations team. Once the request has been approved, you will get a client ID and client secret. Save them for the following steps\nKeystone setup Pre-requisites Keystone must run as a WSGI application behind an HTTP server (Apache is used in this documentation, but any server should be possible if it has OpenID connect/OAuth2.0 support). Keystone project has deprecated eventlet, so you should be already running Keystone in such way. Keystone must be run with SSL You need to install mod_auth_openidc for adding support for OpenID Connect to Apache. IGTF CAs EGI monitoring checks that your Keystone accepts clients with certificates from the IGTF CAs. Please ensure that your server is configured with the correct Certificate and Revocation path:\nFor Apache HTTPd HTTPd is able to use CAs and CRLs contained in a directory:\nSSLCACertificatePath /etc/grid-security/certificates SSLCARevocationPath /etc/grid-security/certificates For haproxy CA and CRLS have to be bundled into one file.\nClient verification should be set as optional otherwise accepted CAs won't be presented to the EGI monitoring:\n# crt: concatenated cert, key and CA # ca-file: all IGTF CAs, concatenated as one file # crl-file: all IGTF CRLs, concatenated as one file # verify: enable optional X.509 client authentication bind XXX.XXX.XXX.XXX:443 ssl crt /etc/haproxy/certs/host-cert-with-key-and-ca.pem ca-file /etc/haproxy/certs/igtf-cas-bundle.pem crl-file /etc/haproxy/certs/igtf-crls-bundle.pem verify optional For nginx CA and CRLS have to be bundled into one file.\nClient verification should be set as optional otherwise accepted CAs won't be presented to the EGI monitoring:\nssl_client_certificate /etc/ssl/certs/igtf-cas-bundle.pem; ssl_crl /etc/ssl/certs/igtf-crls-bundle.pem; ssl_verify_client optional; Managing IGTF CAs and CRLs IGTF CAs can be obtained from UMD, you can find repository files for your distribution at EGI CA repository\nIGTF CAs and CRLs can be bundled using the examples command hereafter.\nPlease update CAs bundle after IGTF updates, and CRLs bundle after each CRLs update made by fetch-crl:\ncat /etc/grid-security/certificates/*.pem \u003e /etc/haproxy/certs/igtf-cas-bundle.pem cat /etc/grid-security/certificates/*.r0 \u003e /etc/haproxy/certs/igtf-crls-bundle.pem # Some CRLs files are not ending with a new line # Ensuring that CRLs markers are separated by a line feed perl -pe 's/----------/-----\\n-----/' -i /etc/haproxy/certs/igtf-crls-bundle.pem Apache Configuration Include this configuration on the Apache config for the virtual host of your Keystone service, using the client ID and secret obtained above:\nOIDCResponseType \"code\" OIDCClaimPrefix \"OIDC-\" OIDCClaimDelimiter ; OIDCScope \"openid profile email eduperson_entitlement\" OIDCProviderMetadataURL https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration OIDCClientID \u003cclient id\u003e OIDCClientSecret \u003cclient secret\u003e OIDCCryptoPassphrase \u003csome crypto pass phrase\u003e OIDCRedirectURI https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect # OAuth for CLI access OIDCOAuthIntrospectionEndpoint https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect OIDCOAuthClientID \u003cclient id\u003e OIDCOAuthClientSecret \u003cclient secret\u003e # Increase Shm cache size for supporting long entitlements OIDCCacheShmEntrySizeMax 65536 \u003cLocation ~ \"/v3/auth/OS-FEDERATION/websso/openid\"\u003e AuthType openid-connect Require valid-user \u003c/Location\u003e \u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e If you have multiple keystone hosts, configure an alternative caching mechanism as per https://github.com/zmartzone/mod_auth_openidc/wiki/Caching\nFor example, using memcache\nOIDCCacheType memcache OIDCMemCacheServers \"memcache1 memcache2 memcache3\" Be sure to enable the mod_auth_oidc module in Apache, in Ubuntu:\nsudo a2enmod auth_openidc Note If running Keystone behind a proxy, make sure to correctly set the X-Forwarded-Proto and X-Forwarded-Port request headers, e.g. for haproxy:\nhttp-request set-header X-Forwarded-Proto https if { ssl_fc } http-request set-header X-Forwarded-Proto http if !{ ssl_fc } http-request set-header X-Forwarded-Port %[dst_port] Keystone Configuration Configure your keystone.conf to include in the [auth] section openid in the list of authentication methods:\n[auth] # This may change in your installation # add openid to the list of the methods you support methods = password, token, openid Add a [openid] section as follows:\n[openid] # this is the attribute in the Keystone environment that will define the # identity provider remote_id_attribute = HTTP_OIDC_ISS Add your horizon host as trusted dashboard to the [federation] section:\n[federation] trusted_dashboard = https://\u003cyour horizon\u003e/dashboard/auth/websso/ Finally copy the default template for managing the tokens in horizon to /etc/keystone/sso_callback_template.html. This template can be found in keystone git repository at https://github.com/openstack/keystone/blob/master/etc/sso_callback_template.html\ncurl -L https://raw.githubusercontent.com/openstack/keystone/master/etc/sso_callback_template.html \\ \u003e /etc/keystone/sso_callback_template.html Now restart your Apache (and Keystone if running in uwsgi) so you can configure the Keystone Federation support.\nKeystone Federation Support First, create a new egi.eu identity provider with remote id https://aai-demo.egi.eu/auth/realms/egi:\n$ openstack identity provider create --remote-id https://aai-demo.egi.eu/auth/realms/egi egi.eu +-------------+-----------------------------------------+ | Field | Value | +-------------+-----------------------------------------+ | description | None | | domain_id | 1cac7817dafb4740a249cc9ca6b14ea5 | | enabled | True | | id | egi.eu | | remote_ids | https://aai-demo.egi.eu/auth/realms/egi | +-------------+-----------------------------------------+ Create a group for users coming from EGI Check-in, usual configuration is to have one group per VO you want to support.\n$ openstack group create ops +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | | | domain_id | default | | id | 89cf5b6708354094942d9d16f0f29f8f | | name | ops | +-------------+----------------------------------+ Add that group to the desired local project:\nopenstack role add member --group ops --project ops Define a mapping of users from EGI Check-in to the group just created and restrict with the OIDC-eduperson_entitlement the VOs you want to support for that group. Substitute the group ID and the allowed entitlements for the adequate values for your deployment:\n$ cat mapping.egi.json [ { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"89cf5b6708354094942d9d16f0f29f8f\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai-demo.egi.eu/auth/realms/egi\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:ops:role=vm_operator#aai.egi.eu$\" ] } ] } ] More recent versions of Keystone allow for more elaborated mapping, but this configuration should work for Mitaka and onwards\nCreate the mapping in Keystone:\n$ openstack mapping create --rules mapping.egi.json egi-mapping +-------+--------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-------+--------------------------------------------------------------------------------------------------------------------------------------+ | id | egi-mapping | | rules | [{u'remote': [{u'type': u'HTTP_OIDC_SUB'}, {u'type': u'HTTP_OIDC_ISS', u'any_one_of': [u'https://aai-demo.egi.eu/auth/realms/egi']}, | | | {u'regex': True, u'type': u'OIDC-eduperson_entitlement', u'any_one_of': [u'^urn:mace:egi.eu:.*:ops:vm_operator@egi.eu$']}], | | | u'local': [{u'group': {u'id': u'89cf5b6708354094942d9d16f0f29f8f'}, u'user': {u'name': u'{0}'}}]}] | +-------+--------------------------------------------------------------------------------------------------------------------------------------+ Finally, create the federated protocol with the identity provider and mapping created before:\n$ openstack federation protocol create \\ --identity-provider egi.eu \\ --mapping egi-mapping openid +-------------------+-------------+ | Field | Value | +-------------------+-------------+ | id | openid | | identity_provider | egi.eu | | mapping | egi-mapping | +-------------------+-------------+ Keystone is now ready to accept EGI Check-in credentials.\nHorizon Configuration Edit your local_settings.py to include the following values:\n# Enables keystone web single-sign-on if set to True. WEBSSO_ENABLED = True # Allow users to choose between local Keystone credentials or login # with EGI Check-in WEBSSO_CHOICES = ( (\"credentials\", _(\"Keystone Credentials\")), (\"openid\", _(\"EGI Check-in\")), ) Once horizon is restarted you will be able to choose \"EGI Check-in\" for login.\nCLI Access The OpenStack Client has built-in support for using OpenID Connect Access Tokens to authenticate. You first need to get a valid Access Token from EGI Check-in (e.g. from https://aai-demo.egi.eu/token/) and then use it in a command like:\n$ openstack --os-auth-url https://\u003cyour keystone endpoint\u003e/v3 \\ --os-auth-type v3oidcaccesstoken --os-protocol openid \\ --os-identity-provider egi.eu \\ --os-access-token \u003cyour access token\u003e \\ token issue +---------+---------------------------------------------------------------------------------------+ | Field | Value | +---------+---------------------------------------------------------------------------------------+ | expires | 2017-05-23T11:24:31+0000 | | id | gAAAAABZJA3fbKX....nEMAPi-IsFOCkU9QWGTISYElzYJsI3z0SJGs7QsTJv4aJQq0JDJUBz6uE85SqXDj3 | | user_id | 020864ea9415413f9d706f6b473dbeba | +---------+---------------------------------------------------------------------------------------+ Additional VOs Configuration can include as many mappings as needed in the json file. Users will be members of all the groups matching the remote part of the mapping. For example this file has 2 mappings, one for members of ops and another for members of fedcloud.egi.eu:\n[ { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"66df3a7a0c6248cba8b729de7b042639\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [\"https://aai-demo.egi.eu/auth/realms/egi\"] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:ops:role=vm_operator#aai.egi.eu$\" ] } ] }, { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"e1c04284718f4e19bb0516e5534a24e8\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [\"https://aai-demo.egi.eu/auth/realms/egi\"] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:fedcloud.egi.eu:vm_operator:role=member#aai.egi.eu$\" ] } ] } ] Multiple OIDC providers If your OpenStack deployment needs to support multiple identity providers (besides EGI Check-in) you will need to configure mod_auth_openidc to support multiple providers and use an OAuth2.0 token introspection proxy like ESACO.\nmod_auth_openidc configuration First, create a directory to host each of the providers configuration, in our case we will use /var/lib/apache2/oidc/metadata, but adapt this to your specific needs. Ensure this directory is writable by the user running apache:\nmkdir -p /var/lib/apache2/oidc/metadata chown -R www-data:www-data /var/lib/apache2/oidc/metadata Set in your Apache configuration the OIDCMetadataDir pointing to that directory\nOIDCMetadataDir /var/lib/apache2/oidc/metadata You may remove the OIDCProviderMetadataURL, OIDCClientID and OIDCClientSecret options from the Apache configuration as these will be now set in new files created in the metadata directory. For every provider you will support, you need to create 3 files:\n\u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.provider with the OpenID Connect Discovery OP JSON metadata. The easiest way to create this file is getting its content from the OIDC server itself. For EGI Check-in:\ncurl https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration \u003e \\ /var/lib/apache2/oidc/metadata/aai-demo.egi.eu%2Foidc.provider \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.client with the client credentials. For EGI Check-in (aai-demo.egi.eu%2Foidc.client):\n{ \"client_id\": \"\u003cyour client id\u003e\", \"client_secret\": \"\u003cyour secret id\u003e\" } \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.conf with any extra configuration for the provider. This may not be needed if all your providers are similar. For example to specify the scopes to use for Check-in, use a aai-demo.egi.eu%2Foidc.conf as follows:\n{ \"scope\": \"openid email profile eduperson_entitlement\" } Now add for the providers you support new configuration in Apache to facilitate the use of the dashboard. This is for a configuration of an egi.eu identity provider with openid as protocol:\n\u003cLocation ~ \"/v3/auth/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/websso\"\u003e AuthType openid-connect # This is your Redirect URI with a new iss=\u003cyour idp iss\u003e option added OIDCDiscoverURL https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect?iss=https%3A%2F%2Faai-demo.egi.eu%2Foidc%2F # Ensure that the user is authenticated with the expected iss Require claim iss:https://aai-demo.egi.eu/auth/realms/egi Require valid-user \u003c/Location\u003e In your Horizon configuration, set the list of providers and their mappings:\n# this is the list that will show up in the dropdown menu WEBSSO_CHOICES = ( (\"credentials\", _(\"Keystone Credentials\")), (\"egi.eu\", _(\"EGI Check-in\")), (\"other-idp\", _(\"Other IdP\")), ) # this maps the options above to keystone's idps and protocols WEBSSO_IDP_MAPPING = { \"egi.eu\": (\"egi.eu\", \"openid\"), \"other-idp\": (\"other-idp.com\", \"openid\") } ESACO configuration ESACO will handle OAuth tokens when users hit your Keystone from API/CLI. It needs to run as a daemon that listens (by default) on port 8156. We will use docker for facilitating the deployment:\nCreate a yaml file with the configuration of the different providers (application.yaml):\noidc: clients: - issuer-url: https://aai-demo.egi.eu/auth/realms/egi client-id: \"\u003cyour check-in client id\u003e\" client-secret: \"\u003cyour check-in client secret\u003e\" - issuer-url: \u003canother idp\u003e client-id: \"\u003cyour client id for second idp\u003e\" client-secret: \"\u003cyour client secret for second idp\u003e\" Create a environment file with the ESACO credentials you want to use (esaco.env):\n# User name credential requested from clients introspecting tokens ESACO_USER_NAME=\u003cesaco user name\u003e # Password credential requested from clients introspecting tokens ESACO_USER_PASSWORD=\u003cesaco password\u003e Run the ESACO server (adapt this as it better fits to run on your servers and make it run permanently):\ndocker run -p 8156:8156 -d -env-file=esaco.env \\ -v application.yml:/esaco/config/application.yml:ro \\ indigoiam/esaco:latest Configure Keystone’s Apache to use ESACO as OAuth introspection endpoint:\n# point this to the host where ESACO is running OIDCOAuthIntrospectionEndpoint http://localhost:8156/introspect OIDCOAuthClientID \u003cesaco user name\u003e OIDCOAuthClientSecret \u003cesaco password\u003e OIDCIDTokenIatSlack 3600 Configure also the locations in Apache that should use OAuth:\n\u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e \u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/other_idp/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e Moving to EGI Check-in production instance Once tests in the development instance of Check-in are successful, you can move to the production instance. Go to EGI Federation Registry and submit a Service Request for the production instance of EGI Check-in. After the approval of the request, you will need to update your configuration as follows:\nUpdate the remote-id of the identity provider:\nopenstack identity provider set --remote-id https://aai.egi.eu/auth/realms/egi egi.eu Update the HTTP_OIDC_ISS filter in your mappings, e.g.:\nsed -i 's/aai-demo.egi.eu/aai.egi.eu/' mapping.egi.json openstack mapping set --rules mapping.egi.json egi-mapping Update Apache configuration to use aai.egi.eu instead of aai-demo.egi.eu:\nOIDCProviderMetadataURL https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration OIDCOAuthIntrospectionEndpoint https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Changes in the client settings If you want to make any changes to the client configuration, you need to submit a reconfiguration request through the Federation Registry. Client Migration to Keycloak Check-in is migrating its internal implementation to Keycloak. The Development and Demo environments already using Keycloak since June 24th 2022 and Production environment expected to migrate during July 2022.\nA general guide on migration is available on Check-in documentation, in this section we provide specific information for OpenStack providers.\nChanges in Apache configuration The new Keycloak endpoint has a different URL that needs to be updated in your apache config. You will also need to add PKCE configuration. These are the configuration parameters and their new values:\n# update Metadata URL OIDCProviderMetadataURL https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration # Add PKCE method OIDCPKCEMethod S256 # update introspection endpoint OIDCOAuthIntrospectionEndpoint https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Require claim If you use the Require claim iss:\u003cissuer\u003e in your Apache configuration to restrict the issuer, please allow both the Keycloak and MitreID issuers during the transition period, e.g.:\n\u003cRequireAny\u003e Require claim iss:https://aai.egi.eu/auth/realms/egi Require claim iss:https://aai.egi.eu/oidc/ \u003c/RequireAny\u003e Multiple OIDC providers Note Configuration in this section is only needed if you are using mod_auth_oidc with multiple providers If you are using multiple OpenID Connect providers, you will need to add a new configuration to your metadata directory:\n$ curl https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration \u003e \\ /var/lib/apache2/oidc/metadata/aai.egi.eu%2Fauth%2Frealms%2Fegi.conf # copy credentials from existing client $ cp /var/lib/apache2/oidc/metadata/aai.egi.eu%2Foidc.client \\ /var/lib/apache2/oidc/metadata/aai.egi.eu%2Fauth%2Frealms%2Fegi.client # create configuration for the client $ cat \u003e /var/lib/apache2/oidc/metadata/aai.egi.eu%2Fauth%2Frealms%2Fegi.conf \u003c\u003c EOF { \"scope\": \"openid email profile eduperson_entitlement\", \"pkce_method\": \"S256\" } EOF And update your Apache configuration for the authentication with Horizon:\n\u003cLocation ~ \"/v3/auth/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/websso\"\u003e AuthType openid-connect # This is your Redirect URI with a new iss=\u003cyour idp iss\u003e option added OIDCDiscoverURL https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect?iss=https%3A%2F%2Faai.egi.eu%2Fauth%2Frealms%2Fegi # Ensure that the user is authenticated with the expected iss Require claim iss:https://aai.egi.eu/auth/realms/egi Require valid-user \u003c/Location\u003e Also update ESACO to include the configuration of the new endpoint URL:\noidc: clients: - issuer-url: https://aai.egi.eu/auth/realms/egi client-id: \"\u003cyour check-in client id\u003e\" client-secret: \"\u003cyour check-in client secret\u003e\" # other clients Keystone configuration Similarly to the change from Demo to Production described above, you will need to update the URLs on your configuration:\nUpdate the remote-id of the identity provider to also include the new issuer:\n$ openstack identity provider set --remote-id https://aai.egi.eu/auth/realms/egi --remote-id https://aai.egi.eu/oidc/ egi.eu Add the new issuer to the HTTP_OIDC_ISS filter in your mappings, keep both the https://aai.egi.eu/oidc/ and https://aai.egi.eu/auth/realms/egi so users can still use their existing tokens. Check-in will handle the validation of both kind of tokens automatically. The HTTP_OIDC_ISS section should look as follows:\n{ \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai.egi.eu/auth/realms/egi\", \"https://aai.egi.eu/oidc/\" ] } ```shell $ openstack mapping set --rules mapping.egi.json egi-mapping As the new issuer is included in the remote-id configuration of the Keystone identity provider, there should not be any changes in your users, they will be still be able to manage owned resources after the change.\n","categories":"","description":"Authentication and Authorization integration\n","excerpt":"Authentication and Authorization integration\n","ref":"/providers/cloud-compute/openstack/aai/","tags":"","title":"Check-in"},{"body":"The EGI Federated Cloud Compute (FedCloud) service offers a multi-cloud IaaS federation that brings together research clouds as a scalable computing platform for data and/or compute driven applications and services for research and science.\nThis documentation focuses on using the service. Those resource providers willing to integrate into the service, please see the EGI Federated Cloud Integration documentation.\nCloud Compute gives you the ability to deploy and scale virtual machines on-demand. It offers computational resources in a secure and isolated environment controlled via APIs without the overhead of managing physical servers.\nCloud Compute service is provided through a federation of IaaS cloud sites that offer:\nSingle Sign-On via EGI Check-in, users can login into every provider with their institutional credentials and use modern industry standards like OpenID Connect. Global VM image catalogue at AppDB with pre-configured Virtual Machine images that are automatically replicated to every provider based on your community needs. Resource discovery features to easily understand which providers are supporting your community and what are their capabilities. Global accounting that aggregates and allows visualisation of usage information across the whole federation. Monitoring of Availability and Reliability of the providers to ensure SLAs are met. The flexibility of the Infrastructure as a Service can benefit various use cases and usage models. Besides serving compute/data intensive analysis workflows, Web services and interactive applications can be also integrated with and hosted on this infrastructure. Contextualisation and other deployment features can help application operators fine tune services in the cloud, meeting software (OS and software packages), hardware (number of cores, amount of RAM, etc.) and other types of needs (e.g. orchestration, scalability).\nSince the opening of the EGI Federated Cloud, the following usage models have emerged:\nService hosting: the EGI Federated Cloud can be used to host any IT service as web servers, databases, etc. Cloud features, as elasticity, can help users to provide better performance and reliable services. Example: NBIS Web Services, Peachnote analysis platform. Compute and data intensive applications: for those applications needing considerable amount of resources in terms of computation and/or memory and/or intensive I/O. Ad-hoc computing environments can be created in the EGI cloud providers to satisfy extremly intensive HW resource requirements. Example: VERCE platform, The Genetics of Salmonella Infections, The Chipster Platform. Datasets repository: the EGI Cloud can be used to store and manage large datasets exploiting the large amount of disk storage available in the Federation. Disposable and testing environments: environments for training or testing new developments. Example: Training infrastructure Eager to test this service? Have a look at how to create your first Virtual Machine in EGI.\n","categories":"","description":"Run virtual machines in the EGI Cloud\n","excerpt":"Run virtual machines in the EGI Cloud\n","ref":"/users/compute/cloud-compute/","tags":"","title":"Federated Cloud Compute"},{"body":"Overview This tutorial describes how to create your first Virtual Machine in the EGI Federation.\nStep 1: Signing up Create an EGI account with Check-in.\nStep 2: Enrolling to a Virtual Organisation Once your EGI account is ready you need to join a Virtual Organisation (VO). Here are the steps to join a VO. Explore the list of available VOs in the Operations Portal. We have a dedicated VO called vo.access.egi.eu for piloting purposes. If you are not sure about which VO to enrol to, please request access to the vo.access.egi.eu VO with your EGI account by visiting the enrollment URL. Check AppDB to see the list of Virtual Appliances and Resource Providers participating in the vo.access.egi.eu VO. AppDB is one of the service in the EGI Architecture.\nStep 3: Creating a VM Once your membership to a VO has been approved you are ready to create your first Virtual Machine. There are several ways to achieve this. The simplest way is to use a web dashboard like VMOps Dashboard or Infrastructure Manager Dashboard. On the other hand, advanced users may prefer to use the command-line interface.\nTo know more about the Cloud Compute Service in EGI please visit its dedicated section.\nAsking for help If you find issues please do not hesitate to contact us.\n","categories":"","description":"Step by step guide to get your first Virtual Machine up and running\n","excerpt":"Step by step guide to get your first Virtual Machine up and running\n","ref":"/users/tutorials/create-your-first-virtual-machine/","tags":"","title":"Create your first Virtual Machine"},{"body":"Every user of the EGI Notebooks catch-all instance has a 20GB persistent home to store any notebooks and associated data. The content of this home directory will be kept even if your notebook server is stopped (which can happen if there is no activity for more than 1 hour). Modifications to the notebooks environment outside the home directory are not kept (e.g. installation of libraries). If you need those changes to persist, let us know via a GGUS ticket to the Notebooks Support Unit. You can also ask for increasing the 20GB home via ticket.\nImport notebooks into your workspace The Notebooks service default environment includes nbgitpuller, an extension to sync a git repository one-way to a local path. You can generate a shareable link by filling in the nbgitpuller link generator with your git repository.\nAlternatively, you can also use Binder for providing a link to notebooks and their computing environment.\nGetting data in/out Your notebooks have outgoing internet connectivity so you can connect to any external service to bring data in for analysis. As with input data, you can connect to any external service to deposit the notebooks output.\nThis is convenient for smaller datasets but not practical for larger ones, for those cases we can offer integration with several data services. These are not enabled in the catch-all instance but can be made available on demand.\nEGI DataHub EGI DataHub provides a scalable distributed data infrastructure. It offers a tight integration with Jupyter and notebooks with specific drivers that make the DataHub Spaces accessible from any notebook.\nWhenever you log into the service, supported DataHub spaces will be available under the datahub folder. If you need support for any additional space, please open a ticket in GGUS to add it.\nAlternatively, you can also use the fs-onedatafs library from your code. For convenience, the ONEPROVIDER_HOST environment variable will point to the default oneprovider for the Notebooks and the ONECLIENT_ACCESS_TOKEN variable will contain a valid access token for the service.\nfrom fs.onedatafs import OnedataFS # create the OnedataFS driver using defaults from env odfs = OnedataFS(os.environ['ONEPROVIDER_HOST'], os.environ['ONECLIENT_ACCESS_TOKEN'], force_direct_io=True) # use it to open a file f = odfs.open(\"\u003cdatahub file path\u003e\") The ONEPROVIDER_HOST and ONECLIENT_ACCESS_TOKEN variables are obtained as part of the login process and made available in the notebooks environment automatically. You can also specify a different oneprovider host if needed.\nEUDAT B2DROP EUDAT B2DROP is a low-barrier, user-friendly and trustworthy storage environment which allows users to synchronise their active data across different desktops and to easily share this data with peers. EUDAT offers a free public instance of B2DROP for any researcher with a 20 GB quota.\nThe data on B2DROP can be synchronised with EGI Notebooks so you can share content between the two services. This offers an easy-to-use storage and compute platform for the long-tail of science.\nHere is how you can get them synchronised. First, make sure you have access to B2DROP. Then, configure app username and app password on B2DROP’s security settings. Now, back to EGI Notebooks, click on the B2DROP connection drop-down menu when you start your session:\nEnter the app username and app password created previously, with the option to save them for future logins:\nYou will see a b2drop folder in the list of folders (left panel) of the EGI Notebooks that is synchronised with the content on B2DROP:\nD4Science Workspace D4Science VREs provide a shared workspace via a dedicated API. EGI Notebooks embedded in D4Science VREs will automatically show the user’s workspace at the workspace directory. You can browse and use as any regular file.\nShared folders The Notebooks service can enable shared folders for users, either in read-only or read-write mode. These are specially meant for community instances for easing the sharing of data between all the users of the service. In the catch-all instance the datasets directory serves as an example of such feature.\nOther services We are open for integration with other services for facilitating the access to input and output data. Please contact support _at_ egi.eu with your request so we can investigate the best way to support your needs.\n","categories":"","description":"How to access and manage data in EGI Notebooks\n","excerpt":"How to access and manage data in EGI Notebooks\n","ref":"/users/dev-env/notebooks/data/","tags":"","title":"Data Management in Notebooks"},{"body":"The default environment includes a set of kernels that are automatically built from the EGI-Federation/egi-notebooks-images GitHub repository. These are the ones available:\nPython: Default Python 3 kernel, it includes commonly used data analysis and machine learning libraries. Created from the jupyter/scipy-notebook stack.\nDIRAC / Python 2: A python 2 kernel that includes a DIRAC installation for interacting with EGI Workload Manager.\nJulia: The Julia programming language with the libraries described in jupyter/datascience-notebook.\nR: The R programming language with several packages from the R ecosystem as provided by jupyter/r-notebook and some extra libraries.\nOctave: The Octave programming language installed on its own conda environment (named octave).\nIf you want to add a new kernel, just let us know and we will discuss the best way to support your request.\n","categories":"","description":"The default environment in EGI Notebooks\n","excerpt":"The default environment in EGI Notebooks\n","ref":"/users/dev-env/notebooks/kernels/default/","tags":"","title":"Default Environment"},{"body":"This is the documentation to support the Disaster Mitigation and Agriculture community.\nResearchers How to get access Getting access to the EGI infrastructure consists of the following steps:\nSign-up for an EGI Check-In account. Request to join the vo.environmental.egi.eu Virtual Organisation (VO) by visiting the enrollment URL with your EGI Check-In account. The subscription requires approval from the VO Manager. Access the cloud-based resources at CESNET-MCC: Horizon OpenStack dashboard Project name: vo.environmental.egi.eu Project ID: 29e6fbf618984a0c98ffcdf0222ad815 Network ID: group-project-network Security Group: ingress traffic to port 22 (SSH) is enabled How to bring your scientific application to the EGI infrastructure The official solution to distribute software in the EGI Infrastructure is to use CVMFS, the Software Distribution Service developed to assist High Energy Physics collaborations at CERN. For more details, please refer to the Contend Distribution documentation.\nThe following alternative solutions for sharing scientific software in the EGI Infrastructure are also available:\nUse the EGI Applications Database (AppDB): either packaging your application in a custom Virtual Machine or uploading a Virtual Appliance. Use Docker containers via the EGI Cloud Compute Service. To get started, have a look at the tutorial to create your first Virtual Machine in the EGI Infrastructure.\nHow to bring your data to the EGI infrastructure For managing data, different Data Management services are available in the EGI Infrastructure:\nEGI DataHub EGI Data Transfer EGI Data Orchestrator OpenRDM For more information, please see the Data Management section and the tutorials. The following tutorials may be relevant:\nCreate a Virtual Machine with Jupyter and DataHub Access DataHub from a Virtual Machine Service Providers How to contribute cloud resources The steps that an OpenStack cloud provider needs to follow to add resources to the Disaster Mitigation and Agriculture community Virtual Organisation (i.e. vo.environmental.egi.eu) are available in the VO Configuration guide for providers.\n","categories":"","description":"How to to use the EGI infrastructure for Disaster Mitigation and Agriculture community\n","excerpt":"How to to use the EGI infrastructure for Disaster Mitigation and …","ref":"/users/communities/dmcc/","tags":"","title":"Disaster Mitigation and Agriculture community"},{"body":"The following guide is intended for researchers who want to use ECAS, a complete environment enabling data analysis experiments, in the EGI cloud.\nECAS (ENES Climate Analytics Service) is part of the EOSC-hub service catalog and aims to:\nprovide server-based computation, avoid data transfer, and improve reusability of data and workflows. It relies on Ophidia, a data analytics framework for eScience, which provides declarative, server-side, and parallel data analysis, jointly with an internal storage model able to efficiently deal with multidimensional data and a hierarchical data organization to manage large data volumes (“datacubes”), and on JupyterHub, to give users access to ready-to-use computational environments and resources.\nThanks to the Elastic Cloud Compute Cluster (EC3) platform, operated by the Polytechnic University of Valencia (UPV), researchers will be able to rely on the EGI Cloud Compute service to scale up to larger simulations without being worried about the complexity of the underlying infrastructure.\nThis guide will show how to:\ndeploy an ECAS elastic cluster of VMs in order to automatically install and configure the whole ECAS environment services, i.e. JupyterHub, PyOphidia, several Python libraries such as numpy, matplotlib and Basemap; perform data intensive analysis using the Ophidia HPDA framework; access the ECAS JupyterHub interface to create and share documents containing live code, equations, visualizations and explanatory text. Deploy an ECAS cluster with EC3 In the latest release of the EC3 platform a new Ansible receipt is available for researchers interested to deploy ECAS cluster on the EGI Infrastuctrure. The next sections provide details on how to configure and deploy an ECAS cluster on EGI resources.\nConfigure and deploy the cluster To configure and deploy a Virtual Elastic Cluster using EC3, access the EC3 platform front page and click on the \"Deploy your cluster\" link as shown in the figure below:\nA wizard will guide you through the cluster configuration process. Specifically, the general wizard steps include:\nLRMS selection: choose ECAS from the list of LRMSs (Local Resource Management System) that can be automatically installed and configured by EC3. Endpoint: the endpoints of the providers where to deploy the ECAS elastic cluster. The endpoints serving the vo.access.egi.eu VO are dynamically retrieved from the EGI Application DataBase using REST APIs. Operating System: choose EGI CentOS7 as cluster OS. Instance details, in terms of CPU and RAM to allocate for the front-end and the working nodes. Cluster’s size and name: the name of the cluster and the maximum number of nodes of the cluster, without including the frontend. This value indicates the maximum number of working nodes that the cluster can scale to. Initially, the cluster is created with the frontend and only one working node: the other working nodes will be powered on on-demand. Resume and Launch: a summary of the chosen cluster configuration. To start the deployment process, click the Submit button. When the frontend node of the cluster has been successfully deployed, you will be notified with the credentials to access via SSH.\nThe cluster details are available by clicking on the \"Manage your deployed clusters\" link on the front page:\nNote The configuration of the cluster may take some time. Please wait for its completion before starting to use the cluster. Accessing the cluster To access the frontend of the cluster:\ndownload the SSH private key provided by the EC3 portal; change its permissions to 600; access via SSH providing the key as identity file for public key authentication. [user@localhost EC3]$ ssh -i key.pem cloudadm@\u003cYOUR_CLUSTER_IP\u003e Last login: Mon Nov 18 11:37:29 2019 from torito.i3m.upv.es [cloudadm@oph-server ~]$ sudo su - [root@oph-server ~]# Both the frontend and the working nodes are configured by Ansible. This process usually takes some time. You can monitor the status of the cluster configuration using the is_cluster_ready command-line tool:\n[root@oph-server ~]# is_cluster_ready Cluster is still configuring. The cluster is successfully configured when the command returns the following message:\n[root@oph-server ~]# is_cluster_ready Cluster configured! As SLURM is used as workload manager, it is possible to check the status of the working nodes by using the sinfo command, which provides information about Slurm nodes and partitions.\n[root@oph-server ~]# sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 1 down* oph-io2 debug* up infinite 1 idle oph-io1 Accessing the scientific eco-system ECAS provides two different ways to get access to its scientific eco-system: Ophidia client (oph_term) and JupyterHub.\nPerform some basic operations with Ophidia Run the Ophidia terminal as ophuser user.\nThe default parameters are already defined as environmental variables inside the .bashrc file:\nexport OPH_SERVER_HOST=\"127.0.0.1\" export OPH_SERVER_PORT=\"11732\" export OPH_PASSWD=\"abcd\" export OPH_USER=\"oph-test\" Create an empty container and a new datacube with random data and dimensions.\nNow, you can submit your first operation of data transformation: let’s reduce the whole datacube in a single value for grid point using the average along the time:\nLet’s have a look at the environment by listing the datacubes and containers in the session:\nBy default, the Ophidia terminal will use the last output datacube PID. So, you can use the oph_explorecube operator to visualize the first 100 values.\nFor further details about the Ophidia operators, please refer to the official documentation.\nAccessing the Jupyter interface To access the Jupyter interface, open the browser at https://\u003cYOUR_CLUSTER_IP\u003e:443/jupyter and log in to the system using the username and password specified in the jupyterhub_config.pyp configuration file (see the c.Authenticator.whitelist and c.DummyAuthenticator.password lines) located under the /root folder.\nFrom JupyterHub in ECAS you can do several things such as:\ncreate and run a Jupyter Notebook exploiting PyOphidia and other Python libraries for data manipulation, analysis and visualization (e.g. NumPy, matplotlib, Cartopy); browse the directories, download and update files in the home folder; execute operators and workflows directly from the Ophidia Terminal; access to a read-only data repository hosted in a Onedata space and perform any analysis on this shared data. The ECAS space shared in the ECAS environment through the Onedata services is available at the onedata/ecas_provider/ECAS_space folder located under the /data directory.\nTo get started with the ECAS environment capabilities, open the ECAS_Basics.ipynb notebook available under the notebooks/ folder in the home directory.\nAccessing the Grafana UI This section will show how to monitor the ECAS environment and the resource usage and get aggregated information over time.\nTo access the Grafana monitoring interface, open the browser at https://\u003cYOUR_CLUSTER_IP\u003e:3000 and log in to the system using the admin username and the password specified in the .grafana_pwd file located under the /root folder.\nThe Grafana-based monitoring system provides two dashboards in order to monitor the ECAS cluster both at system and application level.\nThe infrastructure dashboard provides information about the percentage of CPU, RAM, SWAP and disk used on each Node.js (the frontend and the working nodes). frontend node working node The application dashboard shows information about which operator/workflow is being executed and its current execution status and provides aggregated information over time (e.g. number of total, completed and failed workflows/tasks, hourly weighted average of running cores). Destroy the cluster To destroy the running cluster use the delete action from the cluster management page.\nReferences ECASLab CMCC ECASLab DKRZ Ophidia GitHub: ECAS-Lab GitHub: ansible role Ophidia cluster EC3 GitHub EC3 ","categories":"","description":"Using Elastic Cloud Computing Cluster (EC3) platform to create an ECAS environment.\n","excerpt":"Using Elastic Cloud Computing Cluster (EC3) platform to create an ECAS …","ref":"/users/compute/orchestration/ec3/apps/ecas/","tags":"","title":"ECAS"},{"body":"This section offers an introduction to the EGI services, together with tutorials about how to set up, use, and combine these services.\nNote See the Service Providers section for details on how to integrate providers into the EGI Federation. Request for information You can ask for more information about the public EGI services on our site.\n","categories":"","description":"Documentation for public EGI services","excerpt":"Documentation for public EGI services","ref":"/users/","tags":"","title":"User Guides"},{"body":"Use this section to get started quickly with internal EGI services:\nThe complete list of internal EGI services supporting the coordination of the EGI Federation offers insight into how EGI is able to offer advanced public cloud services The Configuration Database records the topology of the sites in the EGI federation Service Monitoring tracks and controls the performance of the services Accounting tracks service and resource usage, providing insights and reports on consumption The Helpdesk lets users and providers report incidents and bugs, or request changes ","categories":"","description":"Introduction to internal EGI services","excerpt":"Introduction to internal EGI services","ref":"/internal/getting-started/","tags":"","title":"Getting Started"},{"body":"Rucio terms Rucio Storage Element (RSE) is another name for an endpoint, or storage solution. Rules are an instruction to Rucio to do a certain thing. This can be to ensure file x has at least 1 copy at storagesite1, or ensure file y is on tape, or even on tape at more than one location, or even file z has 2 copies at any site within a selection of sites. How you set up the RSE and the attributes you give them allows for many different strategies to transfer and ordanise data. Once a rule is created, Rucio will get to work to ensure that the rule is satisfied at all times. File is single file within Rucio. Dataset is a collection of files, which may be a collection or related results, or data. Container is a collection of Datasets which may build a larger subset of a whole experiment. Scope is a collection in which files, datasets, and containers are placed. Users will have their own scope, often user.username. But also experiments, sub-experiments, or however you wish to orgaise the data can also have scopes. Accounts can be given access to scopes by VO admins. Data Identifier (DID) uiniquely identifies data in Rucio. It is made up from the scope and the filename, seperated by a colon (e.g. experiment1:file1). Getting started as a new user Account creation To get set up with a Rucio account please create a ticket on GGUS. Please fill in the form with a subject, description, ticket catagory - service request, priority - less urgent, and under routing information please select Assign to support unit - Rucio). Within the ticket description please include:\nDesired Username (usually initials and surname e.g. John Doe would have jdoe) Your email Name of the experiment / VO you are part of The subject of your eScience certificate If you want password access we can organise a video call to explain or take sensitive information if you prefer\nIn Terms of testing you can join the test VO (dteam) to try Rucio as a service and its capabilities.\nPlease note that we are working on allowing Rucio accounts to be created and accessed with IAM services, and EGI Check-in, but currently only support x509 and password access.\nOnce our team has this information we will create you a Rucio account. Docker container setup You will then need to install a containerised client on your computer.\nInstall Docker to run the container https://www.docker.com/get-started (for windows users I would recommend using WSL2) Follow the docker instructions to ensure it is running correctly. Using openSSL you will need to split your grid certificate bundle into the certificate and key: $ openssl pkcs12 -in \u003c*.pfx\u003e -out /sensible/path/usercert.pem -clcerts -nokeys $ openssl pkcs12 -in \u003c*.pfx\u003e -out /sensible/path/userkey.pem -nocerts -nodes Run the Docker container using the following command:\nWhen running the block of code below please replaces all items within \u003c\u003e with the relevent information. This uses a Rucio container that was setup for the EGI communities.\n$ run \\ -e RUCIO_CFG_RUCIO_HOST=https://rucio-server.gridpp.rl.ac.uk:443 \\ -e RUCIO_CFG_AUTH_HOST=https://rucio-server.gridpp.rl.ac.uk:443 \\ -e RUCIO_CFG_AUTH_TYPE=x509_proxy \\ -e RUCIO_CFG_CLIENT_VO=\u003c3 CHAR VO NAME LOWERCASE\u003e \\ -e RUCIO_CFG_CLIENT_CERT=/opt/rucio/etc/usercert.pem \\ -e RUCIO_CFG_CLIENT_KEY=/opt/rucio/etc/userkey.pem \\ -e RUCIO_CFG_ACCOUNT=\u003cRucio Username\u003e \\ -e RUCIO_CFG_CA_CERT=/opt/rucio/etc/web/ca-first.pem \\ -v \u003cPATH/TO/e-Science CA 2B\u003e:/opt/rucio/etc/web/ca-first.pem \\ -v \u003cPATH/TO/YOUR/USERCERT\u003e:/opt/rucio/etc/usercert \\ -v \u003cPATH/TO/YOUR/USERKEY\u003e:/opt/rucio/etc/userkey \\ --name=rucio-client \\ -it \\ -d egifedcloud/rucioclient:1.23.17 This block of code may look large but it is configuring Rucio to connect to the Multi-VO Rucio at RAL, your account and VO details, where you are loading them into the container, and mounting the authentication details into the container.\nThe UK eScience CA 2B can be obtained here. The 3 character VO name will be provided to you when you sign up for a Rucio account.\nRun the following commands inside the docker container to finalise set up: $ cp /opt/rucio/etc/usercert /opt/rucio/etc/usercert.pem $ cp /opt/rucio/etc/userkey /opt/rucio/etc/userkey.pem $ chmod 600 /opt/rucio/etc/usercert.pem $ chmod 400 /opt/rucio/etc/userkey.pem Rucio configuration setup You need to edit the /opt/rucio/etc/rucio.cfg file, this then needs to be lightly edited to add your account name. This will then be loaded into the Rucio client.\n[common] logdir = /var/log/rucio multi_vo = True loglevel = INFO [client] rucio_host = https://rucio-server.gridpp.rl.ac.uk:443 auth_host = https://rucio-server.gridpp.rl.ac.uk:443 vo = \u003c3 character VO name\u003e account = \u003cyour_account\u003e ca_cert = /opt/rucio/etc/web/ca-first.pem auth_type = x509_proxy client_cert = /opt/rucio/etc/usercert.pem client_key = /opt/rucio/etc/userkey.pem client_x509_proxy = /tmp/x509up_u1000 request_retries = 5 You should now have a fully set up Containerised Client for your Rucio Account and VO which you can start in docker and use whenever you need it.\nIf not please contact Rucio support Getting started as a new VO To get set up with a new VO on Multi-VO Rucio account please create a ticket on ggus. Please fill in the form with a subject, description, ticket catagory - service request, priortiy - less urgent, and under routing information please select ‘assign to support unit’ - Rucio).\nWe will set up a meeting to discuss Rucio, your needs, sites, and current set up to ensure that Rucio can work for you, and will track progress with the ticket.\n","categories":"","description":"How to get started with Rucio","excerpt":"How to get started with Rucio","ref":"/users/data/management/rucio/getting-started/","tags":"","title":"Getting Started with Rucio"},{"body":"Overview EGI is a federation of compute and storage resource providers united by a mission to support research and innovation.\nThe resources in the EGI infrastructure are offered by service providers that either run their own data centers or rely on community, private and/or public cloud services. These service providers offer:\nSingle Sign-On via EGI Check-in allows users to login with their institutional (community) credentials Global image catalogue at AppDB with pre-configured virtual machine images Resource discovery features to easily understand which providers are supporting your community, and what are their capabilities Global accounting that aggregates and allows visualisation of usage information Monitoring of availability and reliability to ensure SLAs are met The EGI infrastructure supports a multitude of science and research communities, each with their own virtualised resources built around open standards. The development of these communities is driven by by their own scientific requirements.\nTip See also an overview of the EGI FedCloud architecture. Accessing resources Access to resources (services) in the EGI infrastructure is based on OpenID Connect (OIDC), which replaces the legacy authentication and authorization based on X.509 certificates.\nNote Some services still rely on X.509 certificates, e.g. High Throughput Compute. EGI uses Virtual Organisations (VOs) to control access to resources. VOs are fully managed by research communities, allowing communitites to manage their users and grant access to their services and resources. This means communities can either own their resources and use EGI services to share (federate) them, or can use the resources available in the EGI infrastructure for their scientific needs.\nBefore users can access an EGI service, they have to:\nObtain a supported ID, by signing up with either EGI Check-in directly, or with one of the community identity providers from the EGI infrastructure. Enroll into one VO. Users need to be part of a VO before using EGI services. Explore the list of available VOs in the Operations Portal. Authenticate to EGI Check-in to obtain an OAuth2 access token (and optionally a refresh token). Manage or use the service by leveraging the access token, either implicitly (web interfaces and dashboards usually hide this from users) or explicitly (e.g. when using command-line tools). Note See the EGI Check-in documentation for a detailed description of the Authentication and Authorization Infrastructure (AAI) of the EGI Federation, and to gain a better understanding of the concepts that act as building blocks for the AAI implementation. Requesting resources Depending on the access conditions, a service (or an instance of the service) may be open for any user, or it may require requesting access (ordering).\nEGI services use the following types of access conditions:\nWide access - Users can freely access the service. Login may be required but it is possible with various institutional accounts (through EduGAIN), or with social accounts (e.g. Google). For example you can create a test Virtual Machine or launch a Jupyter Notebook. Policy based - Users are granted access based on specific policies defined by the service providers. Access needs to be requested, and will be checked for such services. Example: Compute resources and tools allocated to researchers in medical imaging (Biomed VO). Pay-for-use - Services are provided for a fee. Example: FitSM Training The EGI user community support team handles access requests (orders) for the Policy based and Pay-for-use access modes. They will respond to the request within maximum 5 work days. We normally contact you to have a short teleconference meeting to better understand your requirements, and to be able to identify resources and services that best match your needs. The meeting typically covers two topics:\nWhat is the background of your request? Scientific domain, partner countries, user bases, pay-for-use or not, etc. What are the technical details of your use case? How many CPU cores, how much RAM per CPU, which software services, and for how long do you need them, etc. Contact us if you want to discuss further.\nCapacity allocation When EGI is able to support a request for resources, it can do so in two ways:\nWe grant you access to an existing service, for example to compute resource pools (Virtual Organisations) that already exist in EGI for specific scientific disciplines or for researchers in specific regions. You can browse these in the EGI Operations Portal. If there is a suitable VO, we help you join it and use its services. We create a new VO for your community when none of the existing resource pools are suitable for your use case. The procedure is as follows: We will contact our provider and negotiate resources for you If there are providers willing to support you, we will sign a Service Level Agreement (SLA) with you A new VO will be created for your community Pilot your application EGI offers a playground allocation for users to get access to the services and understand how to port applications and develop new data analytics tools that can be turn into online services that can be accessed by scientist worldwide.\nRequirements and user registration Access requires acceptance of Acceptable Use Policy (AUP) and Conditions of the 'EGI Applications on Demand Service'.\nAcknowledgment Users of the service are asked to provide appropriate acknowledgement of the use in scientific publications. The following acknowledgement text can be used for this purpose (you should adapt to match the exact providers in your case):\nThis work used advanced computing resources from the 100%IT, CESGA, CLOUDIFIN, CYFRONET-CLOUD, GSI-LCG2, IFCA-LCG2, IN2P3-IRES, INFN-CATANIA-STACK, INFN-PADOVA-STACK, SCAI, TR-FC1-ULAKBIM, UA-BITP and UNIV-LILLE resource centres of the EGI federation. The services are co-funded by the EGI-ACE project (grant number 101017567).\nWhen requesting access users are guided through a registration process. Members of the EGI support team will perform a lightweight vetting process to validate the users’ requests before granting the access to the resources.\nGet access to pilot allocation Create an EGI Check-In account. Enroll the vo.access.egi.eu Virtual Organisation by following the enrollment URL. Make sure you use your EGI Check-In account for the enrollment. The grant to run applications is initially valid for 6 months and can be extended/renewed upon request. These resources are delivered through the vo.access.egi.eu VO.\nYou can manage those resources via command-line or any of the dashboards of the EGI Cloud: the VMOps dashboard and the IM dashboard.\nYou can also easily access scientific applications, EC3 has a list of applications that you can easily start from the EC3 portal.\nUnused resources Users of the EGI services may gain opportunistic usage to unused resources. These are resources that are not dedicated to the user’s organization, but are accessible when the research center(s) have some spare resources. This enables the most efficient use of resources.\nNote Users should not rely on (unused) resources not dedicated to their organisation, as access can be revoked without warning, and data may be lost if not properly backed up. ","categories":"","description":"Introduction to EGI services\n","excerpt":"Introduction to EGI services\n","ref":"/users/getting-started/","tags":"","title":"Getting Started"},{"body":"For collaboration purposes, it is best if you create a GitHub account and fork the repository to your own account. Once you do this, you will be able to push your changes to your GitHub repository for others to see and use, and you will be able to create pull requests (PRs) in the official EGI documentation repository based on branches in your fork.\nIf you are new to git and GitHub you are advised to start by the two following articles providing simple tutorials:\nStep by step guide to git Creating pull request with GitHub GitHub official documentation is available at docs.github.com.\nTip The first-contributions is a repository allowing anyone to freely learn and test creating a real Pull Request to an existing GitHub repository. Additional documentation about the main steps for working with GitHub is also available in this section.\nThe GitHub contribution flow In order to be able to send code update to the repository you need to:\nfork the repository to your GitHub account clone the repository on your local computer create a feature branch where you will commit your changes push the feature branch to the repository fork in your GitHub account open a Pull Request against the upstream repository In this process three git repositories are used:\nThe upstream repository: EGI-Federation/documentation Your fork, also named origin: \u003cyour_username\u003e/documentation A local clone of your fork, containing references to your fork, its origin and to the upstream repository Add an SSH key to your GitHub account The most convenient way to authenticate with GitHub is to use SSH keys over the SSH protocol.\nYou can add an SSH public key to your GitHub account in the Settings on GitHub, at https://github.com/settings/keys.\nRefer to Connecting to GitHub with SSH for an extensive documentation on using SSH keys with GitHub.\nIt’s worth to mention that your ssh public keys can easily be retrieved using a URL like https://github.com/\u003cyour_username\u003e.keys.\nIn order to manage repositories over ssh, you will will have to clone them via SSH, not HTTPS.\nIf you already have a local clone of a repository created via HTTPS, you can switch it to SSH by following Switching remote URLs from HTTPS to SSH.\nStarting with the GitHub CLI The GitHub command-line interface greatly helps with working with GitHub repositories from a terminal.\nIt can be installed using the packages available on their homepage. There is also a manual.\nOnce installed you will have to start by setting up authentication.\n# Authenticate with GitHub, favor SSH protocol $ gh auth login $ gh config set git_protocol ssh Working with repositories The easiest way is to do it via the GitHub CLI that will also clone it locally. But it can also be done via the web interface, using the fork button and then cloning it locally manually.\nFork and clone This command will fork the repository to your GitHub account and clone a local copy for you to work with.\n$ gh repo fork EGI-Federation/documentation Clone existing fork If you want to clone an existing fork you should use:\n$ gh repo clone \u003cyour_username\u003e/documentation Validate the local clone If your local clone of you fork is correctly setup you should see references to the origin and upstream repositories.\n$ git remote -v origin git@github.com:\u003cyour_username\u003e/documentation (fetch) origin git@github.com:\u003cyour_username\u003e/documentation (push) upstream git@github.com:EGI-Federation/documentation.git (fetch) upstream git@github.com:EGI-Federation/documentation.git (push) Run the site locally The documentation site is built from the source files using Hugo. The repository README can be used as a reference for building instructions.\nRequirements Hugo Node.js and other docsy theme dependencies: postcss-cli autoprofixer Installing dependencies To install npm+Node.js please check the official instructions.\nEverything has been tested with Node.js 12.\nThe dependencies of the docsy theme can be installed as follows:\n# From the root of the repository clone $ npm ci Hugo can be installed following the official documentation.\nHugo (extended) releases can be downloaded at the Hugo releases page.\nBuilding the site To build and run the site, from the repository root:\n$ git submodule update --init --recursive --depth 1 $ hugo --minify Testing the site locally To launch the site locally, from the repository root:\n$ hugo serve -D The site is available locally at: http://localhost:1313/.\nBranches and commits You should submit your patch as a git branch ideally named with a meaningful name related to the changes you want to propose. This is called a feature branch (sometimes also named topic branch). You will commit your modifications to this feature branch and submit a Pull Request (PR) based on the differences between the upstream main branch and your feature branch.\nCreate a feature branch Try to avoid committing changes to the main branch of your clone to simplify management, creating a dedicated feature branch helps a lot. Try to pick a meaningful name for the branch (my_nice_update in the example).\n# This should be done from the up-to-date main branch # Read furthermore to see documentation on updating a local clone $ git checkout -b my_nice_update Write changes The documentation being made of plain text files you are free to use whatever text editor or Integrated Development Environment (IDE) suits you, from neovim to Visual Studio Code.\nSome environments may provide you plugins helping with syntax or offering a preview, they are worth checking.\nBe sure to commit files with having been formated using Prettier as documented in our style guide.\nCommit changes It is the best practice to have your commit message have a summary line that includes the issue number, followed by an empty line and then a brief description of the commit. This also helps other contributors understand the purpose of changes to the code.\n#3 - platform_family and style * use platform_family for platform checking * update notifies syntax to \"resource_type[resource_name]\" instead of resources() lookup * GH-692 - delete config files dropped off by packages in conf.d * dropped debian 4 support because all other platforms have the same values, and it is older than \"old stable\" debian release # Select the modified files to be committed $ git add files1 path2/ # Commit the changes $ git commit -m \u003ccommit_message\u003e Push feature branch to the fork in preparation of a PR From inside a feature branch you can push it to your remote fork.\n# Ask git to keep trace of the link between local and remote branches $ git push --set-upstream Once done, the output will show a URL that you can click to generate a Pull Request (PR). Accessing GitHub upstream of forked repositories may also propose you to submit a PR.\nIf needed GitHub CLI can also be used to prepare the PR:\n$ gh pr create \u003cyour_username\u003e:\u003cfeature_branch\u003e --web Previewing a pull request If a repository maintainer adds the label safe for preview to a pull request it will be possible to preview it using a pull request-specific URL: https://docs.egi.eu/documentation/[PR_NUMBER]\nThe preview can be used as an alternative to testing a pull request locally, and the preview can easily be shared with other contributors.\nOnly collaborators having write permission to the repository are able to mark a pull request as safe for review.\nThis should be carefully considered, especially for external and first time contributors.\nUpdate local feature branch with changes made on the PR Once you PR have been opened it will be reviewed, and reviewers can propose and commit changes to your PR. If you need to make further changes be sure to update the local clone with the remote changes.\n# Retrieve changes made on your PR in the upstream repository $ git pull Then you can commit new changes and push them to your remote fork.\nUpdate repository clone with the upstream changes # If you are still in a branch created for a previous PR, move to main $ git checkout main # Get the latest data from the upstream repository $ git fetch upstream # Update your local copy with this data $ git rebase upstream/main main # Update your remote GitHub fork with those changes $ git push Update local feature branch with changes made on the main branch In case the main branch evolved since the feature branch was created, it may be required to merge the new changes in the feature branch.\nIt can easily be done via the PR page on the GitHub web interface, but it can also be done in your repository clone using git rebase.\n# Retrieve changes made in the upstream repository $ git fetch upstream # Check out the feature branch $ git checkout feature_branch # Apply the new changes on main to your feature branch $ git rebase upstream/main In case some files have been changed on both sides you will will have to merge the conflicts manually.\nClone PR to edit/test/review locally It’s possible to clone a Pull Request to a local branch to test it locally. It’s done using the PR number.\n# List available PR and their identifiers. $ gh pr list # Clone specific PR, updating sudmodules $ gh pr checkout XX --recurse-submodules Once done it’s possible to build and run the site locally:\n# From the root of the repository clone # Here on MacOS X, adapt depending on your platform $ hugo serve -D The documentation will then be accessible on http://localhost:1313.\nPeople having write access to the repository hosting the branch related to the PR (ie. usually the PR author) will be able to add and edit files.\n# From the local clone of the repository $ gh pr checkout XXX --recurse-submodules $ vim yyy.zz $ git add yyy.zz $ git commit yyy.zz -m \u003ccommit_message\u003e $ git push Update a local clone of a PR # It will ask you to merge changes $ git pull Then you can refer to the README.md to see how to test it locally.\nIn case the PR got commits that were forced pushed you may have troubles, in that case it may be easier to delete the local branch and do another checkout of the PR.\nClean a local clone of a PR In case you have troubles updating the local clone, as it can happens if changes were forced pushed to it, it maybe easier to delete the local copy of the PR and recreate it.\n# Switch to main branch $ git checkout main # Check local branches $ git branch -vv # Delete a specific branch $ git branch -d \u003cbranch_name\u003e # If you need to force the deletion use -D $ git branch -D \u003cbranch_name\u003e Using stashes Sometimes we realise just before committing a change that we are not in the correct branch (ie. that we forgot to create a dedicated feature branch), when this happens git stash can be helpful.\n# Saving a change $ git stash save \u003coptional message\u003e # Creating the forgotten branch $ git checkout -b \u003cmy_feature_branch\u003e # Reviewing the saved changes, use TAB completion $ git stash show \u003cTAB\u003e # Applying the saved changes, use TAB completion $ git stash pop \u003cTAB\u003e # Review the changes to be committed $ git diff If you already committed your change(s) you may have to look at git reset.\n# Viewing the diff of the two last commits $ git log -n 2 -p # Reverting the last change, keeping the change in the local directory $ git reset HEAD^ ","categories":"","description":"First steps with Git and GitHub","excerpt":"First steps with Git and GitHub","ref":"/about/contributing/git/","tags":"","title":"Git and GitHub"},{"body":"This page contains information about integrating your identity provider (IdP) with Check-in in order to allow users in your community to access EGI tools and services.\nOrganisations who want to register their IdP in Check-in needs to fill this form in case the IdP is not publishing REFEDS R\u0026S and Sirtfi compliance in eduGAIN. A PDF scan of a printed and signed copy should be sent to operations_at_egi.eu\nIdentity Provider integration workflow To integrate your Identity Provider with the EGI Check-in service, you need to submit a GGUS ticket indicating your request. The responsible support unit is AAI Support. The integration follows a two-step process:\nRegister your Identity Provider and test integration with the development instance of EGI Check-in. The development instance allows for testing authentication and authorisation to EGI services and resources without affecting the production environment of EGI. Note that the development instance is not connected to the production service and no information is shared between the two systems. Register your Identity Provider with the production instance of EGI Check-in to allow members of your Community to access production EGI services and resources protected by Check-in. This requires that your Identity Provider meets all the policy requirements and that integration has been thoroughly tested during Step 1. The most important URLs for each environment are listed in the table below but more information can be found in the protocol-specific sections that follow.\nProduction Demo Development Protocol Production environment SAML https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso OpenID Connect See client registration Protocol Demo environment SAML https://aai-demo.egi.eu/proxy/module.php/saml/sp/metadata.php/sso OpenID Connect See client registration Protocol Development environment SAML https://aai-dev.egi.eu/proxy/module.php/saml/sp/metadata.php/sso OpenID Connect See client registration General requirements for integrating identity providers An institution or a community may connect their IdP with Check-in to allow their users to access EGI services, or any other services that have enabled Check-in as an authentication provider. This section presents the general requirements for integrating an IdP with EGI Check-in, while protocol-specific instructions are provided in the sections that follow.\nAttribute release requirements As a bare minimum, the IdP of a user’s Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community. The unique identifier must be accompanied with a minimum set of attributes which the Check-in Service Provider Proxy will attempt to retrieve from the user’s IdP. If this is not possible, the missing user attributes will be acquired and verified through the user registration process with the EGI Account Registry. The following table describes the data requested from the user’s Home Organisation, which are communicated to the Check-in SP as either SAML attributes or OIDC claims, depending on the protocol supported by the authenticating IdP.\nDescription Notes At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier Preferred name for display purposes For example to be used in a greeting or a descriptive listing First name Surname Email address Affiliation within Home Organisation or Community To be released only if relevant for accessing EGI services Note that the above set of requested attributes, particularly the identifier, name, email and affiliation information, complies with the REFEDS R\u0026S attribute bundle.\nInformation about group membership and role information released by your IdP should follow the URN scheme below (see also AARC-G002):\n\u003cNAMESPACE\u003e:group:\u003cGROUP\u003e[:\u003cSUBGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e where:\n\u003cNAMESPACE\u003e is in the form of urn:\u003cNID\u003e:\u003cDELEGATED-NAMESPACE\u003e[:\u003cSUBNAMESPACE\u003e*], where \u003cNID\u003e is the namespace identifier associated with a URN namespace registered with IANA, as per RFC8141, ensuring global uniqueness. Implementers can and should use one of the existing registered URN namespaces, such as urn:geant and urn:mace; \u003cDELEGATED-NAMESPACE\u003e is a URN sub-namespace delegated from one of the IANA registered NIDs to an organisation representing the e-infrastructure, research infrastructure or research collaboration.\n\u003cGROUP\u003e is the name of a VO, research collaboration or a top level arbitrary group. \u003cGROUP\u003e names are unique within the urn:mace:egi.eu:group namespace; zero or more \u003cSUBGROUP\u003e components represent the hierarchy of subgroups in the \u003cGROUP\u003e; specifying sub-groups is optional the optional \u003cROLE\u003e component is scoped to the rightmost (sub)group; if no group information is specified, the role applies to the VO \u003cGROUP-AUTHORITY\u003e is a non-empty string that indicates the authoritative source for the entitlement value. For example, it can be the FQDN of the group management system that is responsible for the identified group membership information Example entitlement values expressing VO/group membership and role information:\nurn:geant:dariah.eu:group:egi-interop:role=member#aaiproxy.de.dariah.eu urn:geant:dariah.eu:group:egi-interop:role=vm_operator#aaiproxy.de.dariah.eu Operational and security requirements The IdP needs to comply with additional requirements to achieve a higher level of assurance and allow its users to gain access to a wider set of EGI services. A first group of additional requirements are defined by the Sirtfi framework v1.0. Adherence to these requirements can be asserted either by publishing Sirtfi compliance in the eduGAIN metadata or by declaring it in this form. These requirements are in the areas of operational security, incident response, traceability and IdPs and users responsibility.\nBranding requirements Check-in provides a central Discovery Service (or “Where Are You From” - WAYF) page where users in your Home Organisation or Community will be automatically redirected when necessary to select to authenticate at your IdP. You can provide us with a logo of your Organisation or Community (in high-res PNG or preferably in svg format) to include a dedicated login button that will allow users to easily identify your IdP.\nSAML Identity Provider To allow users in your community to sign into federated EGI applications, you need to connect to the EGI AAI SP Proxy as a SAML Identity Provider (IdP). Users of the application will be redirected to the central Discovery Service page of the EGI AAI Proxy where they will able to select to authenticate at your IdP. Once the user is authenticated, the EGI AAI Proxy will return a SAML assertion to the application containing the information returned by your IdP about the authenticated user.\nMetadata registration SAML authentication relies on the use of metadata. Both parties (you as an IdP and the EGI AAI SP) need to exchange metadata in order to know and trust each other. The metadata include information such as the location of the service endpoints that need to be invoked, as well as the certificates that will be used to sign SAML messages. The format of the exchanged metadata should be based on the XML-based SAML 2.0 specification. Usually, you will not need to manually create such an XML document, as this is automatically generated by all major SAML 2.0 IdP software solutions (e.g., Shibboleth, SimpleSAMLphp). It is important that you serve your metadata over HTTPS using a browser-friendly SSL certificate, i.e. issued by a trusted certificate authority.\nTo exchange metadata, please send an email including the following information:\nentityID Metadata URL Depending on the software you are using, the authoritative XML metadata URL for your IdP might be in the following form:\nhttps://your.idp.example.eu/idp/shibboleth (Shibboleth) https://your.idp.example.eu/simplesaml/module.php/saml2/idp/metadata.php (SimpleSAMLphp) Note that if your IdP is part of a federation, then it would be preferred to send us the URL to a signed federation metadata aggregate. We can then cherry pick the appropriate entityID from that.\nYou can get the metadata of the EGI Check-in SP Proxy on a dedicated URL that depends on the integration environment being used:\nProduction Demo Development Production environment https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso Demo environment https://aai-demo.egi.eu/proxy/module.php/saml/sp/metadata.php/sso Development environment https://aai-dev.egi.eu/proxy/module.php/saml/sp/metadata.php/sso For the production environment, it is recommended that you get the metadata for the EGI Check-in SP (entityID: https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso) from a signed eduGAIN metadata aggregate. For example, the following aggregates are provided by GRNET:\nGRNET federation's metadata eduGAIN SP metadata Attribute release The SAML based Identity Provider of your Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community, along with a set of additional information as described in the following table (see also general attribute release requirements):\nDescription SAML attribute At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier SubjectID (public) or eduPersonUniqueIdeduPersonPrincipalNameSubjectID (pairwise) or eduPersonTargetedID or SAML persistent identifier Preferred name for display purposes displayName First name givenName Surname sn Email address mail Affiliation within Home Organisation or Community eduPersonScopedAffiliation Group(s)/role(s) within Home Organisation or Community eduPersonEntitlement OpenID Connect Identity Provider Users in your community can sign into federated EGI applications through the Check-in service using your OpenID Connect or OAuth 2.0 based Identity Provider.\nClient registration To enable your OIDC Identity Provider for user login, Check-in needs to be registered as a client in order to obtain OAuth 2.0 credentials, such as a client ID and client secret, and to register one or more redirect URIs. Once Check-in is registered as a client, your users will be redirected to the central Discovery Service page of Check-in when logging into EGI federated applications, where they will able to select to authenticate at your IdP. Once the user is authenticated, Check-in will be responsible for communicating the information returned by your IdP about the authenticated user to the connected application. Depending on the protocol, this information will be expressed through a SAML assertion, a set of OIDC claims or a (proxy) X.509 certificate.\nProvider configuration Check-in needs to obtain your OpenID Provider's configuration information, including the location of the Authorisation, Token and UserInfo endpoints. Your OpenID Provider is expected to make a JSON document available at the path formed by concatenating the string /.well-known/openid-configuration to the Issuer, following the OpenID Connect Discovery 1.0 specification.\nAttribute release The OpenID Connect or OAuth 2.0 based Identity Provider of your Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community, along with a set of additional information as described in the following table (see also general attribute release requirements):\nDescription OIDC claim At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier sub (public)N/Asub (pairwise) Preferred name for display purposes name First name given_name Surname family_name Email address email Affiliation within Home Organisation or Community eduperson_scoped_affiliation Group(s)/role(s) within Home Organisation or Community eduPerson_entitlement ","categories":"","description":"Check-in guide for Identity Providers","excerpt":"Check-in guide for Identity Providers","ref":"/providers/check-in/idp/","tags":"","title":"Identity Providers"},{"body":"What is it? Infrastructure Manager (IM) is a tool that orchestrates the deployment of custom virtual infrastructures on multiple backends. It streamlines the access and usability of Infrastructure-as-a-Service (IaaS) clouds by automating the configuration, deployment, software installation and update, and monitoring of virtual infrastructures.\nIM is integrated with the EGI Check-in Service and supports a wide variety of backends, either federated (such as EGI Cloud Compute), public (such as Amazon Web Services, Google Cloud or Microsoft Azure) or on-premises (such as OpenStack), thus making user applications cloud agnostic. IM features a web-based GUI, an XML-RPC API, a REST API and a command-line interface (CLI). It supports OASIS TOSCA Simple Profile in YAML.\nTip An easy way to deploy your first VM in the EGI Federation is from the Infrastructure Manager dashboard. A tutorial and demo videos are also available. Note For detailed information about Infrastructure Manager please see its documentation. It was also presented in one of the EGI Webinars, more details are available on the indico page and the video recording is available on YouTube. The following sections document how to use IM from its GUI or CLI.\n","categories":"","description":"Automating deployment of virtual infrastructures on EGI Cloud\n","excerpt":"Automating deployment of virtual infrastructures on EGI Cloud\n","ref":"/users/compute/orchestration/im/","tags":"","title":"Infrastructure Manager"},{"body":"Introduction A Resource Centre (RC) is the smallest resource administration domain in the EGI Federation. It can be either localised or geographically distributed and provides a minimum set of local or remote IT Services compliant with well-defined IT Capabilities (HTC, Cloud, Storage, etc.) necessary to make resources accessible to Users. EGI is a Resource Infrastructure federating RCs to constitute a homogeneous operational domain.\nRegistration and certification Note Related procedure: PROC09 Resource Centre Registration and Certification In order to join the EGI Infrastructure, a RC needs to present the request to the Research Infrastructure Provider (RP) existing in its country. A RP is a legal organisation, part of the EGI Resource Infrastructure, responsible for managing and operating a number of operational services at national level, supporting EGI RCs and user communities. Please have a look at the Operations Start Guide to get familiar with the terms mentioned above, and to have a complete picture of the several actors participating in our landscape.\nThe RP operators are going to guide and support the RC during the registration and certification procedures.\nFirstly, the RC will be asked to read, understand, and accept:\nthe RCs Operational Level Agreement (OLA), an agreement made between the RC and its RP that defines the minimum set of operational services and the respective quality parameters that a Resource Centre is required to provide in EGI; the Security Policies defined in EGI to guarantee that all the security aspects with the service delivery are fulfilled and enforced. Secondly, the RC should be registered in the EGI Configuration Database: the provided information, from the generic contacts and roles of people to the service endpoints details, is needed to trigger the daily operations of other services and activities provided by the EGI Infrastructure such as the Monitoring of the resources, the Accounting, the Support, and the Security activities.\nOnce the entry in the Configuration Database is complete, the RP changes the RC status from “Candidate” to “Uncertified”, and the certification procedure can start: it comprises a series of technical controls to verify that the provided services work according to the expectations defined in the RC OLA. Any identified issue is notified by the RP operators to the RC and investigated until its solution.\nWhen all the certification controls are successfully passed, the RC status is changed to “Certified” meaning that the RC is included in the EGI production infrastructure and its resources can be consumed by the users of the infrastructure.\nResource Centres responsibilities Incidents and service requests Providing support is a fundamental part of the daily activity of a provider participating in a large research infrastructure such as EGI. The support is not only for the users accessing the resources but also for those who are involved in the management and oversight of the infrastructure.\nAs defined in the RC OLA, the RC will handle incidents and service requests registered as tickets in the EGI Helpdesk service, with the expectation to acknowledge and process any notified issue, within the agreed response time associated with the priority of the ticket.\nThe response time is defined by the Quality of Support levels, and for the RCs the level will be Medium, meaning that there will 4 priorities for the incidents (requiring for example up to 5 working days for the “less urgent” tickets and up to 1 working day for the “top priority” ones), while any service request will be processed as “less urgent” ticket.\nSecurity topics The security posture of the infrastructure is framed by the set of policies constituting the Security Policies. Those policies cover different complementary activities including the operation of services, the processing of personal data and the management of security incidents and vulnerabilities.\nDealing with security incidents The Security Incident Response Policy aims at coordinating the incident response across the infrastructure, ensuring that that incidents are promptly reported, and that all incidents are investigated as fully as possible.\nSecurity incidents are to be treated as serious matters and their investigation must be resourced appropriately.\nResources Centres must report suspected security incidents to their RP Security Officer and to EGI Computer Security Incident Response Team (CSIRT) within 4 hours of discovery. This initial step will start the coordination of the incident response as documented in the procedure SEC01 EGI CSIRT Security Incident Handling Procedure.\nThis procedure has been implemented according to the Security Incident Response Policy, to minimise the impact of security incidents affecting the Resource Centres part of the infrastructure It covers guidance on how the incident response should be coordinated, describing the responsibilities of the various parties, and encourages post-mortem analysis and promotes cooperation between Resource Centres. Handling of vulnerabilities The handling of vulnerabilities is a very formal process involving many different entities.\nAnyone can report a software vulnerability via a form or by email contacting the Software Vulnerability Group (SVG).\nThe report will trigger an assessment, following the SEC02 Software Vulnerability Issue Handling procedure, by the Software Vulnerability Group, of the risk level associated with this vulnerability in the context of the activities of the EGI Infrastructure.\nOnce a vulnerability has been identified as presenting a risk to the infrastructure, it will be decided if an advisory should be prepared and circulated to the security contacts of the sites. After an agreed period of time, and depending on their confidentiality, advisories are made public.\nVulnerabilities identified as critical are handled according to the procedure SEC03 EGI-CSIRT Critical Vulnerability Handling. When applicable, this usually involves developing a custom security monitoring probe created to identify on High Throughput Compute RCs if their resources are vulnerable to the vulnerability. The status is closely monitored by the security team and accessible to the affected RCs.\nUsing this information correlated with the one from Pakiti, the patch management service collecting information about the patches deployed at the various High Throughput Compute RCs, the Incident Response Task Force (IRTF) on duty Security Officer will open tickets against the impacted sites according to the WI07 Security Vulnerability Handling procedure.\nThe EGI Service Delivery and Information Security (SDIS) team of the EGI Foundation, formerly known as the EGI Operations team, will follow up with the resource provider to work on resolving the ticket. The first duty of the resource provider is to acknowledge the vulnerability and then work on a prompt resolution as suggested in the ticket. In case a satisfactory resolution is not reached in due time, or a sign of active progress on addressing the vulnerability is not visible, the specific Resource Centre may be suspended.\nServing user communities Once part of the production infrastructure, the RC is ready to deliver its resources to any of the users’ communities consuming the infrastructure.\nThe RC can continue serving local user communities, and at the same time deliver capacity for international user communities that approach EGI and therefore reach the federated RCs.\nInternational communities reach EGI through the following channels:\nThe EGI site where they can request access to services. The EGI-ACE Call for Use Cases. The Marketplace of the European Open Science Cloud. Service and Operation Level Agreements (SLAs, OLAs) The User Community Support team of the EGI Foundation receives these requests and negotiates the details of access, with the involvement of relevant and ‘fit-for-purpose’ RCs. This Service Level Management (SLM) process intervenes as a matchmaker between service expectations and needs of the user communities, acting as ‘customers’, and the capabilities of the RCs. Customers are enabled access to the resources in the form of Virtual Organisations (VOs).\nIn order to select providers for provisioning services to a given customer, technical requirements are collected from the customer then transferred to relevant providers. The Expression of Interests for support (EoIs) are collected from the interested providers during the negotiation phase, resulting in the best match with customer’s requirements and expectations (both technical and financial). Several aspects are considered during the negotiation phase, including the geographical location of the customer, national roadmap and priority of the providers, and costs of the service provisioning in case of a pay-for-use model.\nThe result of the negotiation is a ‘Service Level Agreement’ (SLA), and several ‘Operation Level Agreements’ (OLA), one with each contributing provider. (See SLAs-OLAs examples.)\nSLAs and OLAs are typically signed for at least 1 year, and are automatically renewed, as long as the provider(s) or the customer do not express a decision to terminate the Agreement at least a month before the expiration date.\nPerformance reports: enforcing OLAs As defined in the RCs OLA, the performance of the delivered services should meet the Service Level Targets: the monthly performance of the RCs is monitored, and when the targets are not achieved for three consecutive months, the affected RCs are notified through a ticket about the OLA violation and requested to provide within 10 working days an explanation for the low performance and a plan for improvement.\nThe RCs not providing a satisfactory explanation or not replying at all are eligible for suspension.\nIn order to re-join the EGI Infrastructure, any suspended RC should undergo a new certification procedure. The “Suspended” status cannot last for more than 4 months, after which a RCs is either in production again or definitely closed.\nBesides the Targets defined in the RCs OLA, which are enforced to guarantee the permanence of the RC in the infrastructure, the targets promised to the users in the VO SLAs should also be met on a monthly basis: when a violation occurs, the RC is requested to provide a justification and a plan for improving the quality of the provided services.\nIf repeated violations occur, the SLA can be renegotiated with the customer, either by changing the Service Level Targets, or by choosing a different RCs as a provider.\n","categories":"","description":"Guidelines for Resource Centres to join the EGI Infrastructure","excerpt":"Guidelines for Resource Centres to join the EGI Infrastructure","ref":"/providers/joining/federated-resource-centre/","tags":"","title":"Joining as a Federated Resource Centre"},{"body":"In these guidelines, we describe the set of actions a service provider should follow to join the EGI Infrastructure, and to ensure the high-quality delivery of service according to the EGI policies.\nProviders have different options to become members of the EGI Federation, and deliver services for advanced computing within the EGI Infrastructure. The options are documented below, linking to detailed instructions:\nJoining as a federated resource centre, becoming a provider (that we call Resource Centre) delivering one of the following services: Cloud Compute, HTC Compute, Cloud Container Compute, or Online Storage. More than 200 Resource Centres have been integrated in this way. Joining as a software provider, also called Technology Provider, which is providing middleware deployed on the federated resource centres. Joining as a new provider for an existing service in the Compute and Data Federation or the Platform services, such as DataHub, Notebooks, Training Infrastructure, and Workload Manager Joining as Core/Central service provider for services supporting all the other services of the EGI Infrastructure. Contributing a new service, enabling advanced computing in research and education, and not yet present in the EGI service portfolio. ","categories":"","description":"Guidelines for service providers to join the EGI Infrastructure","excerpt":"Guidelines for service providers to join the EGI Infrastructure","ref":"/providers/joining/","tags":"","title":"Joining EGI as a provider"},{"body":"Document control Property Value Title How to publish Site Information Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Publishing site information in the Information Discovery System Owner SDIS team EGI profile for the use of the GLUE 2.0 Information Schema specifies how the GLUE 2.0 information schema should be used in EGI. It gives detailed guidance on what should be published, how the information should be interpreted, what kinds of uses are likely, and how the information may be validated to ensure accuracy.\nConfiguring a site BDII The site BDII needs to be configured to read from every node in the site which publishes information (meaning that it runs a so-called resource BDII). In YAIM this is defined with the BDII_REGIONS variable, which contains a list of node names which in turn refer to variables called BDII_\u003cNODE\u003e_URL which specify the LDAP URL of each resource BDII.\nSome services may have DNS aliases for multiple hosts, but the BDII_REGIONS must contain the real hostnames for each underlying node - the information in the resource BDII is different for each node, so reading it via an alias would produce inconsistent results. However, it will usually be desirable for the published endpoint URLs to contain the alias rather than the real hostname; that can often be defined with a YAIM variable for the service. For the site BDII itself this variable is SITE_BDII_HOST. (If multiple site or top BDIIs are configured identically their content will also be identical, so reading via an alias does not produce any inconsistencies.)\nMost services now publish themselves, so sites should check that all relevant services are included. In particular, VOMS servers have only published themselves comparatively recently so may be missing from the configuration. If the glite-CLUSTER node type is used this must also be included. Publication has been enabled for Argus in EMI 2, so this may also need to be added. Common services which do not currently publish are APEL and Squid. See the table below for more detailed information.\nIt is important to realise that the site BDII itself has a resource BDII, and this must be explicitly included in the configuration, e.g. with something like\nBDII_REGIONS=\"CE SE BDII\" (...) BDII_BDII_URL=\"ldap://$SITE_BDII_HOST:2170/mds-vo-name=resource,o=grid\" In the past it was common for the site BDII to be colocated with the CE so it did not need to be listed explicitly, but if installed on a dedicated node (which is now the recommended deployment) it must be included.\nTo check that all expected services are published the following command can be used:\n$ ldapsearch -x -h $SITE_BDII_HOST -p 2170 -b mds-vo-name=$SITE_NAME,o=grid \\ objectclass=GlueService \\ | perl -p00e 's/\\r?\\n //g' | grep Endpoint: (replacing SITE_BDII_HOST; and SITE_NAME with the values for your site), which should list all the service URLs.\nIn addition, most services should now be published in GLUE 2 format. There is no explicit configuration needed for GLUE 2, but one thing to be aware of is that the site name (and the other parts like o=grid) in the GIIS URL field in the GOCDB must have the correct case as GLUE 2 is case-sensitive.\nTo verify the GLUE 2 publication use the command:\n$ ldapsearch -x -h $SITE_BDII_HOST -p 2170 -b GLUE2DomainID=$SITE_NAME,o=glue \\ objectclass=GLUE2Endpoint \\ | perl -p00e 's/\\r?\\n //g' | grep URL: Some services, notably storage elements, may be missing or incomplete in GLUE 2 if they are older than the EMI 2 release. The following table shows the publishing status for gLite and WLCG node types (ARC and Unicore have a different structure).\nNode type GLUE 1 GLUE 2 Notes LCG-CE Yes No Obsolete CREAM Yes Yes Full publication only in EMI 2 CLUSTER Yes Yes Full publication only in EMI 2 WMS Yes Yes LB Yes Yes DPM Yes EMI 2 dCache Yes EMI 2 StoRM Yes EMI 2 LFC Yes EMI 2 FTS Yes EMI 2 Channels not yet published in GLUE 2 Hydra EMI 2 EMI 2 Not yet released in EMI 2 AMGA Yes EMI 2 VOMS Yes Yes MyProxy Yes Yes Argus No EMI 2 Internal service, publication for deployment monitoring Site BDII Yes Yes Top BDII Yes Yes R-GMA Yes No Obsolete VOBOX Yes Yes Apel No No Internal service, publishing not yet requested Squid No No Configuration exists but not enabled Nagios Yes Yes Service-related documentation Federated Cloud BDII configuration For information about configuration of a Federated Cloud BDII, please look at the EGI Information System.\nGlueSite Object These are the existing well established attributes in the GlueSite object. All of these MUST remain.\nAttribute Example Schema Notes GlueSiteName RAL-LCG2 Free text, no whitespace Same as GOCDB name if in GOCDB, your choice. GlueSiteUniqueID RAL-LCG2 Identical to your !GlueSiteName Same as GlueSiteName GlueSiteWeb https://cern.ch/it Free Text Valid URL about the site. GlueSiteLatitude 52.42 NN.NN Site Latitute. GlueSiteLongitude 16.91 NN.NN Longitude of Site. GlueSiteDescription Rutherford Lab Free Text A long name for the site. GlueSiteLocation Dublin, Ireland Town, City, Country An decreasing resolution ending with Country, agree a country name within a country. i.e UK != United Kingdom. Scotland and the Balkans should write a dynamic provider. !GlueSiteUserSupportContact mailto:helpdesk@example.com Valid URL URL for getting support. A ticket system if available. !GlueSiteSysAdminContact xmpp://admins@jabber.org Valid URL How to contact the admins. !GlueSiteSecurityContact mailto:security@example.com Valid URL How to contact for security related matters. The GlueSite object in the 1.3 Glue Schema contains an attribute GlueSiteOtherInfo. To quote.\nThe attribute is to be used to publish data that does not fit any other attribute of the site entity. A name=value pair or an XML structure are example[s] of usage.\nAll this extra configuration will be with in the static information for the glue site within the Grid Information Provider system.\nGuidelines for GlueSite Object A format for publishing useful information about sites within the !GlueSiteOtherInfo is needed, as shown in the following table.\nKey Example Type Notes GRID EGI [#validgrid List of valid grid names] Multiple ones can be defined. WLCG_TIER 1 Tier level of site in WLCG context. Either 0, 1 , 2 , 3 , 4 WLCG_PARENT UK-T1-RAL Name of the higher (administrative) tier site in WLCG The WLCG_NAME of the site at a higher tier with WLCG WLCG_NAME IT-ATLAS-federation [#lcgnames Valid WLCG Names] An official WLCG name. WLCG_NAMEICON https://example.com/tier2.png Valid URL URL to WLCGNAME icon, ideally 80x80 pixels. EGEE_ROC Russia Valid federated Operations Centre name Only applicable if your site is still part of a federated Operations Centre (“ROC” according to the old EGEE terminology). Name MUST match the Operations Centre name declared in GOCDB. Note. If the site is now part of a NGI, then EGI_NGI MUST be used (see below). EGI_NGI NGI_CZ Valid NGI Must agree with the GOC DB EGEE_SERVICE prod prod, pps or cert Which EGEE grid your site is part of, multiple attributes is okay. Obsolete in EGI. OLDNAME Bristol text If your !GlueSiteName changes at some point please record your old name here. ICON https://example.com/icon.png Valid URL Icon Image for your site, ideally 80x80 pixels BLOG https://scotgrid.blogspot.com/feeds/posts/default Valid RSS or Atom Feed Your site blog if you have one CONFIG yaim yaim, puppet, quattor, … The configuration tool(s) used at the site Note. Keywords starting with one of the grid names are to some extent reserved for that grid.\nExample GlueSiteName: RAL-LCG2 GlueSiteOtherInfo: BLOG=https://example.com/blog/feed GlueSiteOtherInfo: EGI_NGI=NGI_UK GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: GRID=GRIDPP GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: ICON=https://example.com/images/tierOneSmall.png GlueSiteOtherInfo: WLCG_PARENT=CERN-PROD GlueSiteOtherInfo: WLCG_TIER=1 Distributed Tier1s and Tier2s Within an WLCG context for instance there are instances of distributed Tier2s and Tier1s. If separate component sites want to exist as a single WLCG tier then they might contain common values for their WLCGNAME.\nGlueSiteName: CSCS-LCG2 GlueSiteOtherInfo: CONFIG=yaim GlueSiteOtherInfo: EGI_NGI=NGI_CH GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: WLCG_NAME=CH-CHIPP-CSCS GlueSiteOtherInfo: WLCG_PARENT=FZK-LCG2 GlueSiteOtherInfo: WLCG_TIER=2 Note that WLCG_PARENT is an accounting unit defined in the MOU document, as shown in WLCG CRIC.\nEstablished Grid Name Short Name Long Name URL EGI European Grid Initiative https://www.egi.eu EELA Europe and Latin America https://www.eu-eela.eu/ WLCG World LHC Computing Grid https://cern.ch/lcg GRIDPP UK Particle Physics Grid https://www.gridpp.ac.uk UKNGS National UK Grid Service https://www.ngs.ac.uk OSG Open Science Grid (US) https://www.opensciencegrid.org/ NDGF Nordic DataGrid Facility https://www.ndgf.org/ LondonGrid London Grid https://www.gridpp.ac.uk/tier2/london/ NORTHGRID Northern (UK) Grid https://www.gridpp.ac.uk/northgrid/ SCOTGRID Scottish Grid https://www.scotgrid.ac.uk/ SOUTHGRID Southern (UK) Grid https://www.gridpp.ac.uk/southgrid/ Academic Grid Malaysia Malaysian Grid UPM Campus Grid Universiti Putra Malaysia https://www.upm.edu.my/ AEGIS Academic and Educational Grid Initiative of Serbia https://www.aegis.rs/ BIGGRID Dutch e-science Grid https://www.biggrid.nl/ Consorzio Cometa Consorzio Multi-Ente per la promozione e l’adozione di Tecnologie di calcolo Avanzato (Italy) https://www.consorzio-cometa.it/en D-Grid German Grid https://www.d-grid-gmbh.de/index.php?id=1\u0026amp;L=1 EUMED EU/Mediterranean Grid https://www.eumedgrid.eu/ GILDA Grid INFN Laboratory for Dissemination Activities (Italy) https://gilda.ct.infn.it/ GISELA Grid Initiative for e-Science virtual communities in Europe and Latin America https://www.gisela-grid.eu/ GRISU Griglia del Sud (Southern Italy Grid) https://www.grisu-org.it/ NEUGRID Neuroscience Grid https://neugrid4you.eu/background RDIG Russian Data Intensive Grid https://grid-eng.jinr.ru/?page_id=43 SEE-GRID South Eastern European GRid-enabled eInfrastructure Development https://www.see-grid.org/ Important: The EGEE Grid name was decomissioned on [[Agenda-14-02-2011|14-02-2011]]. All sites need to replace this grid name with EGI.\nBeing part of a grid is just a reference that your site is in some way associated with a particular Resource Infrastructure Provider either technically or as part of a collaboration. The list of Grids can be extended. Please contact operations@egi.eu to request changes.\nValid WLCG Names The WLCG names are the site names that appear within the LCG MOU concerning commitments to LHC computing.\nWLCG Name Current GlueSiteName CA-TRIUMF TRIUMF-LCG2 CERN CERN-PROD DE-KIT FZK-LCG2 ES-PIC pic FR-CCIN2P3 IN2P3-CC IT-INFN-CNAF INFN-T1 NDGF NGDF-T1 NL-T1 SARA-MATRIX TW-ASGC Taiwan-LCG2 UK-T1-RAL RAL-LCG2 US-FNAL-CMS USCMS-FNAL-W1 US-T1-BNL BNL-LCG2 For the tier two names please consult WLCG CRIC. The column marked Accounting Name are the WLCG Names which in the case of Tier2s are the GOCDB names. Use your site GOCDB name as your WLCG_NAME.\nAlso some tier2s live under more than 1 tier1 perhaps for different for different VOs. If your tier2 has more that one WLCG_PARENT then just add two distinct records to show this. Also some tier2s do not have a WLCGNAME at all.\nGlueSiteUniqueId: EENet GlueSiteName: EENet GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: EGI_NGI=NGI_NL GlueSiteOtherInfo: WLCG_TIER=2 GlueSiteOtherInfo: WLCG_PARENT=UK-T1-RAL GlueSiteOtherInfo: WLCG_PARENT=NL-T1 Valid EGI NGI Names The valid names are those published on GOCDB.\nYAIM Instructions YAIM will have to be updated for those sites using yaim. This will be done and submitted to sites in the normal way.\nYAIM Variable and Value Resulting Glue Attribute and Value SITE_NAME=RAL_LCG2 GlueSiteName: RAL-LCG2 SITE_DESC=“Rutherford Lab” GlueSiteDescription: Rutherford Lab SITE_EMAIL= steve@example.com GlueSiteSysAdminContact: mailto:steve@example.com SITE_SUPPORT_EMAIL= steve@example.com GlueSiteUserSupportContact: mailto:steve@example.com SITE_SECURITY_EMAIL= steve@example.com GlueSiteSecurityContact: mailto:steve@example.com SITE_LOC=“Soho, London, United Kingdom” GlueSiteLocation: Soho, London, United Kingdom SITE_LONG=52.45 GlueSiteLongitude: 52.45 SITE_LAT=-12.34 GlueSiteLatitude: -12.34 SITE_WEB=“https://example.com/\" GlueSiteWeb: https://example.com/ SITE_OTHER_GRID=“EGI|WLCG” GlueSiteOtherInfo: GRID=EGI\nGlueSiteOtherInfo: GRID=WLCG SITE_OTHER_EGEE_ROC=“UK/I” GlueSiteOtherInfo: EGEE_ROC=UK/I SITE_OTHER_EGI_NGI=“NGI_CZ” GlueSiteOtherInfo: EGI_NGI=NGI_CZ SITE_OTHER_EGEE_SERVICE=“prod” GlueSiteOtherInfo: EGEE_SERVICE=prod SITE_OTHER_WLCG_TIER=2 GlueSiteOtherInfo: WLCG_TIER=2 SITE_OTHER*=\"|” GlueSiteOtherInfo: KEY=GlueSiteOtherInfo: KEY= If multiple values for GlueSiteOtherInfo are needed, then just delimit your values with a |. The character | must be avoided in values.\nCheck your own GlueSite Object The information published can be checked through an ldap search:\n$ ldapsearch -x -H ldap://$SITE_BDII_HOST:2170 \\ -b 'Mds-Vo-Name=$SITE_NAME,o=Grid' \\ '(ObjectClass=GlueSite)' In addition, VAPOR is a tool which provides a GUI for different views of published information, including a LDAP view.\nSite information in GLUE 2 The GLUE 2 equivalent of the GlueSite object is the GLUE2AdminDomain. The same information should be present although in a slightly different format, and there are separate GLUE2Contact and GLUE2Location objects.\n","categories":"","description":"How to publish site information","excerpt":"How to publish site information","ref":"/providers/operations-manuals/man01_how_to_publish_site_information/","tags":"","title":"MAN01 How to publish site information"},{"body":"The more you go in data analysis, the more you understand that the most suitable tool for coding and visualizing is not pure code, or some integrated development environment (IDE), nor data manipulation diagrams (such as workflows or flowcharts). From some point on you just need a mix of all these – that is what notebook platforms are, Jupyter being the most popular of them.\nWhat is it? EGI Notebooks is a service-like environment based on the Jupyter technology, offering a browser-based tool for interactive data analysis.\nThe Notebooks environment provides users with notebooks where they can combine text, mathematics, computations and rich media output. EGI Notebooks is a multi-user service that can scale on demand, being powered by the compute services of EGI.\nEGI Notebooks provides the well-known Jupyter interface for notebooks, with the following added features:\nIntegration with EGI Check-in allows you to login with any EduGAIN or social accounts (e.g. Google, Facebook) Persistent storage associated with each user is available in the notebooks environment Customisable with new notebook environments, expose any existing notebook to your users Can easily use EGI compute and storage services from your notebooks, as your notebooks run on EGI infrastructure Service Modes We offer different service modes depending on your needs\nNotebooks for researchers Individual users can use the centrally operated service from EGI. Users can log in, write and play and re-play notebooks by:\ncreating an EGI account Enrolling to the one of the supported VOs such as vo.notebooks.egi.eu VO accessing https://notebooks.egi.eu/ This instance has limits on the amount of resources available for each user (2 CPU core, 4 GB RAM and 20 GB of storage). It will also kill inactive sessions after 1 hour.\nThe central instance supports the following VOs:\nvo.notebooks.egi.eu, enroll here vo.access.egi.eu auger biomed vo.reliance-project.eu eiscat.se Notebooks for communities User communities can have their customised EGI Notebooks service instance. EGI offers consultancy, support, and can operate the setup as well. A community specific setup allows the community to use the community's own Virtual Organisation (i.e. federated compute and storage sites) for Jupyter, add custom libraries into Jupyter (e.g. discipline-specific analysis libraries) or have fine grained control on who can access the instance (based on the information available to the EGI Check-in AAI service).\nEGI currently operates community instances for:\nD4Science. These instances are accessed through specific Gateways: SoBigData, Blue-Cloud, D4Science Services and EOSC-Pillar. Check with D4Science support for more information. ","categories":"","description":"Interactive data analysis with EGI Notebooks","excerpt":"Interactive data analysis with EGI Notebooks","ref":"/users/dev-env/notebooks/","tags":"","title":"Notebooks"},{"body":"Overview Online Storage includes services that allow users to store, share and access data using the EGI infrastructure. Different categories of storage are available, depending on how data is stored, the technology used to access and consume data, and the foreseen usage.\nThree major service offerings are available:\nBlock Storage is block-level storage that can be attached to virtual machines (VMs) as volumes, a simple solution for durable data that does not need to be shared beside a single VM. Grid Storage is file storage for High Throughput Compute (HTC) and/or High Performance Compute (HPC) scenarios. Object Storage is persistent, hierarchical blob storage for cloud native applications, archiving, or when data is shared between different VMs or multiple steps of processing workflows. Comparison of storage types The differences between Block, Grid, and Object Storage are summarized below:\nType Sharing Accounting Usage Block From within VMs, only at the same site the VM is located For the entire block POSIX access, use as local disk Grid From any device connected to the internet For the data stored Grid protocols and HTTP/WebDAV Object From any device connected to the internet For the data stored HTTP requests to REST API The following sections offer a more detailed description of each storage service.\n","categories":"","description":"Data storage services in the EGI infrastructure\n","excerpt":"Data storage services in the EGI infrastructure\n","ref":"/users/data/storage/","tags":"","title":"Online Storage"},{"body":"Introduction This page was created to help you, as a ROD, start with ROD duties.\nThe section How to become a ROD member describes steps which needs to be taken before starting working as a ROD. The section ROD duties documents all the tasks that make up the ROD duties. Section Important to read introduces all documents which concern this activity and which are supposed to be read at the beginning. Section Operational Tools describes all tools used by ROD teams. Finally, Contact section is going to inform you how to contact others.\nHow to become a ROD member There are few actions which needs to be taken before you start your work:\nGet a valid grid certificate delivered by Certificate Authorities (CA) - this step is important because most of the tools used during the shift require certificate. Find EUGRIDPMA members. Register to Dteam VO. Dteam membership will give you possibility to test sites and debug problems. Register into GGUS tool as support staff. GGUS is a ticketing system which is used for operational purpose within EGI. With support staff role you will be able to reply on and update recorded tickets. Register in Configuration Database, a central database which contains all the information about EGI Infrastructure (sites and people). To be ROD members you have to be registered in this database. It will allows you to perform step 5. Request the Regional Staff role in the Configuration Database. Thanks to this role you will be recognized automatically in operations tools as ROD member. It gives you a several privileges in the database as well as in other tools. Contact your NGI manager - you need to contact your NGI manager to be approved as Regional Staff and to be added to ROD mailing list in your NGI (this mailing list is a contact point to the whole ROD team within the NGI). Get familiar with the ROD documentation, a single place where you will find all information relevant to your work as a ROD. To see how to perform all those actions please watch video How to become a ROD member (7 steps which should be done to become a ROD member also).\nROD duties The Regional Operations team is responsible for detecting problems, coordinating the diagnosis, and monitoring the problems through to a resolution. It monitors sites in their region, and react to problems identified by the monitors, either directly or indirectly, provide support to sites as needed, add to the knowledge base, and provide informational flow to oversight bodies in cases of non-reactive or non-responsive sites. ROD is a team responsible for solving problems on the infrastructure according to agreed procedures. They ensure that problems are properly recorded and progress according to specified time lines. They ensure that necessary information is available to all parties. The team is provided by each Operation Centre and requires procedural knowledge on the process (rather than technical skills) for their work.\nAll duties listed are mandatory for ROD team:\nHandling incidents. The main responsibility of ROD is to deal with incidents at sites in the region. This includes making sure that the tickets are opened and handled properly. The procedure for handling tickets is described in EGI Infrastructure Oversight escalation procedure Propagate actions from EGI Operations down to sites. ROD is responsible for ensuring that decisions taken on the EGI Operations level are propagated to sites. Putting a site in downtime or suspend for urgent matters. In general, ROD can place a site in downtime (in the Configuration Database) if it is either requested by the site, or ROD sees an urgent need to put the site into downtime. ROD may also suspend a site, under exceptional circumstances, without going through all the steps of the escalation procedure. For example, if a security hazard occurs, ROD must suspend a site on the spot in the case of such an emergency. It is important to know that EGI Operations can also suspend a site in the case of an emergency e.g. security incidents or lack of response. Notify EGI Operations about core or urgent matters. ROD should create Helpdesk tickets to EGI Operations in the case of core or urgent matters. Important to read Before you start your duties you should get familiar with following documents:\nEGI Infrastructure Oversight escalation procedure. This document defines escalation procedure for operational problems. It describes steps and timelines which ROD team should follow. Dashboard How-Tos and Training Guides. A collection of How-Tos and guides for EGI Operations. It includes a Dashboard How-To, Training Guides which can be used as a presentation for training staff and quick sheets. ROD FAQ. Frequently Asked Questions related to ROD work It is also important to watch video tutorials prepared for ROD teams. They will walk you through several topics which are important for your work.\nOperational Tools ROD uses several operational tools to perform theirs duties (Operations tools video):\nOperations Portal. Dashboard tool on the Operations Portal is a main tool which is used by ROD teams. All actions concerning incidents (alarms and tickets) should be performed using this tool. Service Monitoring (ARGO) is the official EGI monitoring system based on Nagios. It checks the availability of the services and creates alarms visible on the Operations Portal dashboard when a failure occurs. Helpdesk is the EGI central helpdesk system designed for reporting and tracking problems. Configuration Database is a central database which contains all static information about the infrastructure (sites and people). Contact Each ROD teams is supposed to provide own mailing list as a contact point to the team. The list of people responsible for ROD in a given NGI and contact points can be found in the EGI Configuration Database.\nAll ROD mailing list are subscribed to “all-central-operator-on-duty AT mailman.egi.eu” mailing list so to contact other ROD teams you can use this list.\nTo contact EGI Operations team you can:\nsend an Helpdesk ticket and assign it to EGI Operation support unit send an email to “operations AT egi.eu” You are welcome to send us questions in case of any doubts concerning ROD duties.\n","categories":"","description":"Overview of the information and guidelines for ROD","excerpt":"Overview of the information and guidelines for ROD","ref":"/providers/rod/overview/","tags":"","title":"Overview"},{"body":"Deploy cluster Through a “job wizard” interface the user can login to the Elastic Cloud Compute Cluster (EC3) portal and configure the virtual cluster with the related tools and applications to be deployed in the EGI Cloud. Click on the “Deploy your cluster” button to create a cluster in the EGI Cloud.\nThe cluster is composed of a front node, where a batch job scheduler is running, and a number of compute nodes. These compute nodes will be dynamically deployed and provisioned to fit increasing load, and un-deployed when they are in idle status. The installation and configuration of the cluster is performed by means of the execution of Ansible receipts.\nA wizard will guide the user during the configuration process of the cluster, allowing to configure details like the operating system, the characteristics of the nodes, the maximum number of nodes of the cluster or the pre-installed software packages. Specifically, the general wizard steps include:\nLRMS selection: choose Torque from the list of LRMSs (Local Resource Management System) that can be automatically installed and configured by EC3.\nEndpoint: the endpoints of the providers where to deploy the elastic cluster. The endpoints serving the vo.access.egi.eu VO are dynamically retrieved from the EGI Application DataBase using REST APIs.\nOperating System: choose one of the available EGI base OS images available to create the cluster (e.g. CentOS7, or EGI Ubuntu 18.04 LTS).\nInstance details: in terms of CPU and RAM to allocate for the front-end and the working nodes.\nCluster’s size and name: the name of the cluster and the maximum number of nodes of the cluster, without including the frontend. This value indicates the maximum number of working nodes that the cluster can scale. Initially, the cluster is created with the frontend and only one working node: the other working nodes are powered on on-demand.\nResume and Launch: a summary of the chosen cluster configuration. To start the deployment process, click the Submit button.\nNote The configuration of the cluster may take some time. Please wait for its completion before starting to use the cluster! When the frontend node of the cluster has been successfully deployed, the user will be notified with the credentials to access via SSH.\nThe cluster details are available by clicking on the “Manage your deployed clusters” link on the front page.\nAccessing the EC3 cluster To access the frontend of the elastic cluster:\nDownload the SSH private key provided by the EC3 portal; Change its permissions to 600; Access via SSH providing the key as identity file for public key authentication. ]$ ssh -i key.pem cloudadm@\u003cCLUSTER_PUBLIC_IP\u003e Last login: Mon Nov 18 11:37:29 2019 from torito.i3m.upv.es [cloudadm@server ~]$ Both the frontend and the working node are configured by Ansible. This process usually takes some time. User can monitor the status of the cluster configuration using the is_cluster_ready command-line tool:\n]# is_cluster_ready Cluster is still configuring. The cluster is successfully configured when the command returns the following message:\n]# is_cluster_ready Cluster configured! node state enabled time stable (cpu,mem) used (cpu,mem) total ]$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------- wn1 off enabled 00h04'20\" 0,0 1,-1 wn2 off enabled 00h04'20\" 0,0 1,-1 Operate the EC3 cluster To force CLUES to spawn a new node of the cluster, please use the command:\n]$ clues poweron wn1 node wn1 powered on The configuration triggers the execution of several ansible processes to configure the node and may take some time. To monitor the configuration of the node, you can use the is_cluster_ready command.\nTo avoid that clues powers off the node in case of inactivities, once the node is configured, you can disable the node as it follows:\n]$ clues disable wn1 node wn1 successfully disabled Log files Cluster logs files are available in /var/tmp/.im/\u003ccluster_id\u003e IM log files are in /var/log/im/im.log CLUES log files are in /var/log/clues2/clues2.log ","categories":"","description":"Deploy a virtual cluster with EC3 using the portal.\n","excerpt":"Deploy a virtual cluster with EC3 using the portal.\n","ref":"/users/compute/orchestration/ec3/portal/","tags":"","title":"EC3 Portal"},{"body":"IaaS providers are very welcome to join the EGI Federated Cloud as a Resource Centres (RC) and joining the Federated Cloud Task Force to contribute to the design, creation and implementation of the federation.\nResource Centers are free to use any Cloud Management Framework (OpenStack, etc...) as long as they are able to integrate with the EGI Federation components as described in the Federated Cloud Architecture. At the moment this compliance is guaranteed for OpenStack.\nThe general minimal requirements are:\nHardware requirements greatly depend on your cloud infrastructure, EGI components in general do lightweigth operations by interacting with your services APIs. cloudkeeper requires enough disk space to download and convert images before uploading into your local catalogue. The number and size of images which will be downloaded depends on the communities you plan to support. For the piloting VO fedcloud.egi.eu, 100GB of disk should be enough. Servers need to authenticate each other in the EGI Federated Cloud context using X.509 certificates. So a Resource Centre should be able to obtain server certificates for some services. User and research communities are called Virtual Organisations (VO). Resource Centres are expected to join: ops and dteam VOs, used for operational purposes as per RC OLA a community-VO that supports EGI users (e.g. vo.access.egi.eu for piloting) EGI provides packages for the following operating systems (others may work but we are not providing packages): CentOS 7 (and in general RHEL-compatible) Ubuntu 16.04 (and in general Debian-based) ","categories":"","description":"Requirements for integration","excerpt":"Requirements for integration","ref":"/providers/cloud-compute/requirements/","tags":"","title":"Requirements"},{"body":"Identity card Property Value Name Collaboration Tools Description Supporting Collaboration across the EGI Federation URL N/A Support Email it-support@egi.eu Helpdesk Support Unit EGI Services and Service Components I__ Collaboration Tools Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=983 Supplier EGI Foundation and CESNET Roadmap N/A Release notes N/A Source code N/A Issue tracker for developers N/A License Every component is having its own licence Privacy Policy Parent policy, components usually have their own policy ","categories":"","description":"Technical details of the Collaboration Tools","excerpt":"Technical details of the Collaboration Tools","ref":"/internal/collaboration-tools/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Configuration Database Description Central registry of the infrastructure topology URL https://goc.egi.eu Support Email gocdb-admin \u003cat\u003e mailman.egi.eu Helpdesk Support Unit EGI Services and Service Components I__ Configuration Database (GOCDB) Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=335 Supplier UKRI Roadmap N/A Release notes Release notes Source code https://github.com/GOCDB Issue tracker for developers https://github.com/GOCDB/gocdb/issues License Apache 2 Privacy Policy Privacy policy ","categories":"","description":"Technical details of the Configuration Database","excerpt":"Technical details of the Configuration Database","ref":"/internal/configuration-database/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Helpdesk Description Central helpdesk providing a single interface to EGI support URL https://helpdesk.egi.eu Support Email support at ggus.eu Helpdesk Support Unit EGI Services and Service Components I__ Helpdesk (GGUS) Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=247 Supplier KIT Roadmap N/A Release notes https://ggus.eu/index.php?mode=release_notes Source code Not available Issue tracker for developers N/A License BMC Software Inc. Privacy Policy https://ggus.eu/?mode=privacy ","categories":"","description":"Technical details of EGI Helpdesk","excerpt":"Technical details of EGI Helpdesk","ref":"/internal/helpdesk/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Messaging service Description Facilitate messages exchange between EGI services URL N/A Support Email argo-ggus-support at grnet.gr Helpdesk Support Unit EGI Services and Service Components I__ Messaging Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=429 Supplier GRNET, SRCE Roadmap Release notes https://github.com/ARGOeu/argo-messaging/releases Source code https://github.com/ARGOeu/argo-messaging Issue tracker for developers https://github.com/ARGOeu/argo-messaging/issues License Apache 2.0 Privacy Policy ","categories":"","description":"Technical details of Messaging Service","excerpt":"Technical details of Messaging Service","ref":"/internal/messaging/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name ARGO Monitoring Service Description Service Monitoring for Availability and Reliability URL https://argo.egi.eu/ Support Email argo-ggus-support at grnet.gr Helpdesk Support Unit EGI Services and Service Components I__ Monitoring (ARGO) Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=641 Supplier CNRS, GRNET, SRCE Roadmap Release notes Source code https://github.com/ARGOeu Issue tracker for developers https://github.com/ARGOeu License Apache 2.0 Privacy Policy https://argo.egi.eu/egi/Critical/policies ","categories":"","description":"Technical details of EGI Service Monitoring","excerpt":"Technical details of EGI Service Monitoring","ref":"/internal/monitoring/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Operation Portal Description Provision of VO management functions and other capabilities supporting the daily operations of EGI URL https://operations-portal.egi.eu/ Support Email cic-information at in2p3.fr Helpdesk Support Unit EGI Services and Service Components I__ Operations Portal Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=568 Supplier CNRS Roadmap Release notes Release notes Source code https://gitlab.in2p3.fr/opsportal/sf3 Issue tracker for developers https://gitlab.in2p3.fr/opsportal/sf3/-/issues License Apache 2 Privacy Policy https://operations-portal.egi.eu/home/a/aup ","categories":"","description":"Technical details of the Operation Portal","excerpt":"Technical details of the Operation Portal","ref":"/internal/operations-portal/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Security coordination Description Enhance local security for a safer global infrastructure URL https://csirt.egi.eu Support Email abuse at egi.eu Helpdesk Support Unit EGI Services and Service Components I__ Security Coordination I__ Security Monitoring Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=968 https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=1127 Supplier UKRI, FOM-Nikhef, CERN, CESNET, GRNET, IJS Roadmap N/A Release notes N/A Source code N/A Issue tracker for developers N/A License N/A Privacy Policy N/A ","categories":"","description":"Technical details of EGI Security Coordination","excerpt":"Technical details of EGI Security Coordination","ref":"/internal/security-coordination/service-information/","tags":"","title":"Service information"},{"body":"Sign up You need to sign up for an account for accessing the EGI services. As part of this process you will be assigned a personal unique EGI ID which will be then used across all EGI tools and services. Follow the instructions below to get started:\nGo to https://aai.egi.eu/signup. This will show you the identity provider discovery page: browse through the list of Identity Providers to find your Home Organisation, or, alternatively, type the name of your Home Organisation in the search box. Note that the names are localised based on the selected language.\nEnter your login credentials to authenticate yourself with your Home Organisation\nAfter successful authentication, you may be prompted by your Home Organisation to consent to the release of personal information to the EGI AAI Service Provider Proxy.\nAfter successful authentication, you will be redirected to the EGI account registration form. On the introductory page, click Begin to start the registration process.\nEGI requires some basic information from you, depending on the attributes released by your Identity Provider, you may need to provide the values of the missing attributes.\nOn the registration form, click Review Terms and Conditions (Acceptable Use Policy and Conditions of Use - EGI AUP)\nIf you agree to the Terms of Use, select the I Agree option.\nImportant You will not be able to agree to the terms until you review them. Finally, click Submit to submit your request.\nImportant You will not be able to submit your request until you agree to the terms. After submitting your request, Check-in will send you an email with a verification link. After you click that link, you’ll be taken to the request confirmation page.\nImportant If you do not find the email in your Inbox, please check your Spam or Junk folder for an email from “EGI AAI Notifications”. If you do find the email in these folders, mark the email as “safe” or “not spam” to ensure that you receive any future notifications about your EGI ID. After reviewing your request, click Confirm and re-authenticate yourself using the Identity Provider you selected before.\nIn the case of the Sign Up registration, you need to wait for an EGI User Sponsor to approve your request to join the EGI User Community. Upon approval, EGI AAI will send you a notification email.\nNote: After your registration has been completed, you can manage your profile through the EGI Account Registry portal.\nViewing user profile information The profile includes all the information related to the user. This information can be categorised as follows:\nBasic profile Includes the basic information about your profile:\nName Identifiers Email addresses VO/Group membership and roles Includes information about the Virtual Organisations (VOs) and groups the user is member of and the roles assigned to the user within those VOs. Check the guide about VOs for more details.\nLinked identities Information about linked identities to your account. Check the guide for linking accounts for more information.\nNext steps Once your Check-in account is ready you can check how to link it with different identities or how to join an existing Virtual Organisation (VO).\n","categories":"","description":"Register an account with Check-in to get access to EGI services\n","excerpt":"Register an account with Check-in to get access to EGI services\n","ref":"/users/aai/check-in/signup/","tags":"","title":"Sign up for an EGI Account"},{"body":"An overview of the use cases and possible deployment scenarios of the EGI DataHub.\nTransparent data access Clients use one ore more providers to access data Data can be accessed over multiple protocols Federation of service providers Heterogeneous backend storage Common interfaces (Web, REST, POSIX, CDMI) Common AAI with Check-in Discovery of Datasets in the EGI DataHub Smart caching Site A hosts data and computing resources Site B hosts only data Site X uses data from A and B without pre-staging Pre-staging can also be done using APIs Data is accessed locally “à la” POSIX with FUSE Publication of datasets PID minting Publishing, discovery and access to datasets Integrating DataHub and EGI Notebooks ","categories":"","description":"Use-cases for EGI DataHub\n","excerpt":"Use-cases for EGI DataHub\n","ref":"/users/data/management/datahub/usecases/","tags":"","title":"DataHub Use-Cases"},{"body":"OpenStack providers of the EGI Cloud Compute service offer native OpenStack features via native APIs integrated with EGI Check-in accounts.\nThe extensive OpenStack user documentation includes details on every OpenStack project most providers offer access to:\nKeystone, for identity Nova, for VM management Glance, for VM image management Cinder, for block storage Swift, for object storage Neutron, for network management Horizon, as a web dashboard The Horizon Web-dashboard of the OpenStack providers can be accessed using your EGI Check-in credentials directly. See the Getting Started guide for more information. The rest of this guide will focus on CLI/API access.\nInstallation The OpenStack client is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure.\nLinux / Mac Windows Installation of the OpenStack client can be done using:\npip install openstackclient As there are non-pure Python packages needed for installation, the Microsoft C++ Build Tools is a prerequisite, please make sure it’s installed with the following options selected:\nC++ CMake tools for Windows C++ ATL for latest v142 build tools (x86 \u0026 x64) Testing tools core features - Build Tools Windows 10 SDK (\u003clatest\u003e) In case you prefer to use non-Microsoft alternatives for building non-pure packages, please see here.\nInstallation of the OpenStack client can be done using:\npip install openstackclient Add IGTF CA to python's CA store:\ncat /etc/grid-security/certificates/*.pem \u003e\u003e $(python -m requests.certs) Authentication Check the documentation at the authentication and authorisation section on how to get the right credentials for accessing the providers.\nOpenStack token for other clients Most OpenStack clients allow authentication with tokens, so you can easily use them with EGI Cloud providers just doing a first step for obtaining the token. With the OpenStack client you can use the following command to set the OS_TOKEN variable with the needed token:\n$ OS_TOKEN=$(openstack --os-auth-type v3oidcaccesstoken \\ --os-protocol openid --os-identity-provider egi.eu \\ --os-auth-url \u003ckeystone url\u003e \\ --os-access-token \u003cyour access token\u003e \\ --os-project-id \u003cyour project id\u003e token issue -c id -f value) You can easily obtain an OpenStack token with the FedCloud client:\nfedcloud openstack --site \u003cNAME_OF_THE_SITE\u003e --vo \u003cNAME_OF_VO\u003e token issue -c id -f value Useful commands with OpenStack CLI Usage of the OpenStack client is described in detail here.\nPlease refer to the nova documentation for a complete guide on the VM management features of OpenStack. We list in the sections below some useful commands for the EGI Cloud.\nRegistering an existing ssh key It’s possible to register an ssh key that can later be used as the default ssh key for the default user of the VM (via the --key-name argument to openstack server create):\nopenstack keypair create --public-key ~/.ssh/id_rsa.pub mykey Creating a VM openstack flavor list FLAVOR=\u003cFLAVOR_NAME\u003e openstack image list IMAGE_ID=\u003cIMAGE_ID\u003e openstack network list # Pick FedCloud network NETWORK_ID=\u003cNETOWRK_ID\u003e openstack security group list openstack server create --flavor $FLAVOR --image $IMAGE_ID \\ --nic net-id=$NETWORK_ID --security-group default \\ --key-name mykey oneprovider # Creating a floating IP openstack floating ip create \u003cNETOWRK_NAME\u003e # Assigning floating IP to server openstack server add floating ip \u003cSERVER_ID\u003e \u003cIP\u003e # Removing floating IP from server openstack server show \u003cSERVER_ID\u003e # Deleting server openstack server remove floating ip \u003cSERVER_ID\u003e \u003cIP\u003e openstack server delete \u003cSERVER_ID\u003e # Deleting floating IP openstack floating ip delete \u003cIP\u003e OpenStack: launch an instance on the provider network OpenStack: Managing IP addresses Using cloud-init openstack server create --flavor m3.medium \\ --image d0a89aa8-9644-408d-a023-4dcc1148ca01 \\ --user-data userdata.txt --key-name My_Key server01.example.com OpenStack: providing user data (cloud-init) cloudinit documentation Shell script data as user data #!/bin/sh adduser --disabled-password --gecos \"\" clouduser cloud-config data as user data #cloud-config hostname: mynode fqdn: mynode.example.com manage_etc_hosts: true Official cloud-config examples Cloud-init example Creating a snapshot image from running VM You can create a new image from a snapshot of an existing VM that will allow you to easily recover a previous version of your VM or to use it as a template to clone a given VM.\nopenstack server image create \u003cyour VM\u003e --name \u003cname of the snapshot\u003e Once the snapshot is ready openstack image show \u003cname of the snapshot\u003e will give your the details you can use it as any other image at the provider:\nopenstack server create --flavor \u003cflavor\u003e \\ --image \u003cname of the snapshot\u003e \\ \u003cname of the new VM\u003e You can override files in the snapshot if needed, e.g. changing the SSH keys:\nopenstack server create --flavor \u003cflavor\u003e \\ --image \u003cname of the snapshot\u003e \\ --file /home/ubuntu/.ssh/authorized_keys=my_new_keys \\ \u003cname of the new VM\u003e Terraform Terraform supports EGI Cloud OpenStack providers by using valid access tokens for Keystone. For using this, just configure your provider as usual in Terraform, but do not include user/password information. Instead, use the FedCloud client client to configure environment variables as follows:\n# export OS_AUTH_URL and OS_PROJECT_ID with $ eval \"$(fedcloud site show-project-id --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e)\" # now get a valid token $ export OS_TOKEN=$(fedcloud openstack --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e \\ token issue -c id -f value) Here is a sample main.tf configuration file for Terraform:\nterraform { required_providers { openstack = { source = \"terraform-provider-openstack/openstack\" } } } # Create a server resource \"openstack_compute_instance_v2\" \"vm\" { name = \"testvm\" image_id = \"...\" flavor_id = \"...\" security_groups = [\"default\"] } Initialize Terraform with:\n$ terraform init Now check the deployment plan:\n$ terraform plan If you are happy with the plan, perform the deployment with:\n$ terraform apply For more information on how to use Terraform with OpenStack please check the OpenStack provider documentation.\nlibcloud Apache libcloud supports OpenStack and EGI authentication mechanisms by setting the ex_force_auth_version to 3.x_oidc_access_token or 2.0_voms respectively. Check the libcloud docs on connecting to OpenStack for details. See below two code samples for using them\nOpenID Connect import requests from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver refresh_data = { 'client_id': '\u003cyour client_id\u003e', 'client_secret': '\u003cyour client_secret\u003e', 'grant_type': 'refresh_token', 'refresh_token': '\u003cyour refresh_token\u003e', 'scope': 'openid email profile', } r = requests.post(\"https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token\", auth=(client_id, client_secret), data=refresh_data) access_token = r.json()['access_token'] OpenStack = get_driver(Provider.OPENSTACK) # first parameter is the identity provider: \"egi.eu\" # Second parameter is the access_token # The protocol 'openid' is specified in ex_tenant_name # and tenant/project cannot be selected :( driver = OpenStack('egi.eu', access_token, ex_tenant_name='openid', ex_force_auth_url='https://keystone_url:5000', ex_force_auth_version='3.x_oidc_access_token') VOMS from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver OpenStack = get_driver(Provider.OPENSTACK) # assume your proxy is available at /tmp/x509up_u1000 # you can obtain a proxy with the voms-proxy-init command # no need for username driver = OpenStack(None, '/tmp/x509up_u1000', ex_tenant_name='EGI_FCTF', ex_force_auth_url='https://sbgcloud.in2p3.fr:5000', ex_force_auth_version='2.0_voms') ","categories":"","description":"How to interact with the OpenStack providers APIs in the EGI Cloud\n","excerpt":"How to interact with the OpenStack providers APIs in the EGI Cloud\n","ref":"/users/compute/cloud-compute/openstack/","tags":"","title":"Using OpenStack Providers"},{"body":"According to AARC-G002 the information about the groups a user is a member of is commonly used by Service Providers in order to authorise user access to protected resources.The entity responsible for disseminating this information is the EGI Check-in AAI proxy and the format used is that of a URN namespace, called eduPersonEntitlement, that is uniformly interpreted across infrastructures.\nThe general form of the eduPersonEntitlement string is:\n\u003cNAMESPACE\u003e:group:\u003cVO\u003e[:\u003cGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e\nAs a result, an eduPersonEntitlement string informing the Service Provider that the user has the role Associate in the vo.example.eu VO (modelled as a COU) is:\nurn:mace:egi.eu:group:vo.example.eu:role=associate#aai.egi.eu\nEntitlement Construction For the case of the CO Person with a profile/canvas, like the one provided above, we expect to get entitlements for all the entries listed under the tab Role Attributes. Additionally, we will get entitlements for all the General Purpose(GP) Groups enlisted under the tab Groups. These GP Groups have no prefix, neither CO: nor CO:COU, and no postfix, neither active nor all.\nVO(COU) For each entry in the table Role Attributes, that is in status Active or Grace Period, we create one eduPersonEntitlement for each different Role and for the Affiliation. For example, the CO Person from above is affiliated as Member to the VO vo.example.eu and has been assigned the role of an Associate. This will generate two entitlements as:\nurn:mace:egi.eu:group:vo.example.eu:role=associate#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.eu:role=member#aai.egi.eu\nVO Groups (sub COUs) There are occasions where we need a VO to be organized in subgroups. For example vo.example.eu contains the sub-COU vo.example-sub.eu.\nThe CO Person is affiliated as member and with the Role of Support in the VO sub-group vo.example-sub.eu:\nIn such occasions the eduPersonEntitlement will have the following structure:\nurn:mace:egi.eu:group:vo.example.eu:vo.example-sub.eu:role=support#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.eu:vo.examples-sub.eu:role=member#aai.egi.eu\n","categories":"","description":"Expressing VO group membership and role information\n","excerpt":"Expressing VO group membership and role information\n","ref":"/users/aai/check-in/vos/expressing-vo-information/","tags":"","title":"VO Membership Information"},{"body":"Overview The EGI Data Transfer web interface (WebFTS) offers an easy way to initiate and monitor file transfers between storages using different protocols. You can check the CERN WebFTS documentation and FAQ for more details.\nCredential Delegation In order to move files between storage endpoints, named also Storage Elements (SE), the authentication is done using an X.509 certificate which needs to be installed on the browser. The Integration with EGI Check-in is under development and will be available at a later date in the production instance.\nA pop-up window appears when you choose the “My jobs” or “Submit a transfer” tabs. There, you will need to paste the private RSA key of your certificate. The private key WILL NOT BE TRANSMITTED ANYWHERE. It is only used locally (within the user’s browser) to generate the proxies needed to have access to the FTS services.\nTo obtain the private key, you can run in the console:\nopenssl pkcs12 -in yourCert.p12 -nocerts -nodes | openssl rsa This command requires openssl.\nIf you need a delegation with VOMS credentials (required to access some types of Storage Elements), you will need to introduce the name of the virtual organization (VO) you belong to.\nFrom the right button where the remaining time for the current delegation is shown, you can remove the current delegation and delegate again (the delegation window will appear).\nSubmit a transfer Open the “Submit a transfer” tab and Load origin and destination storage elements as endpoints. If one endpoint URL is not known, there is an autocompletion once 3 characters have been typed. You should also specify the protocol, the address of the endpoint and the path. (Ex: gsiftp://lxfsra10a01.cern.ch/dpm/cern.ch/home/).\nBrowse the content and select all the files you want to transfer. CTRL and SHIFT keys can be used for selecting multiple entries at once.\nIf you need to filter the list of files and folders, you can use the available filters: name, size and date. Once you have loaded both endpoints and selected the files, the transfer buttons will be enabled. Click the button with the correct direction. A success or error message will appear above the endpoints' containers.\nYou can now check the status of you transfer in the “My jobs” tab.\nListing your transfers Open the “My jobs” tab. If you have done any transfers recently, they will appear there. If you click on an individual transfer, you can see its state and any errors.\nYou can resubmit transfer jobs, but you would need to delegate your credentials if you did not do it before. You can do this at any state by clicking the “Resubmit” button.\nThe transfer states are: YELLOW if still running, GREEN if successfully completed and RED if something went wrong.\nThe transfers that are not completed have a “Cancel” button on the left of the “Resubmit” button. Clicking this button will cancel that transfer.\n","categories":"","description":"The web interface of EGI Data Transfer\n","excerpt":"The web interface of EGI Data Transfer\n","ref":"/users/data/management/data-transfer/webfts/","tags":"","title":"Data Transfer Web Interface"},{"body":"To access the web interface of the EGI Configuration Database (GOCDB), users can either:\nUse EGI Check-in with an institutional account, or Use an X.509 digital certificate installed in the internet browser, or the local machine’s certificate store. Users can access the system as soon as they are authenticated. However, they will only be able to update information based on their roles, and only once they will have registered a new user account.\nMore information about roles and associated permission is available in the Users and roles section of the documentation.\nApplications requesting a specific role have to be validated by parent roles or administrators. Once granted, users can access and/or modify relevant information, according to the roles granted to them.\nUsing institutional account via EGI Check-in In order to be able to access the Configuration Database with their institutional account, users need to:\nHave their Identity Provider (IdP) federated in EGI Check-in (via eduGAIN or directly). Have created an EGI Check-in account. Important In the case the user cannot use an IdP compliant with REFEDS R\u0026S and REFEDS Sirtfi, the user will have to request joining a specific group, by performing the steps below. Using a compliant IdP is the preferable solution.\nUser should ask to join the GOCDB user group. The access request will be managed by the EGI Operations team. Using an X.509 digital certificate To access the Configuration Database using a digital certificate, first obtain a certificate from one of the recognised EU-Grid-PMA Certification Authorities (CAs), then install it in your browser of choice (or import it into the certificate store of your local machine, on Windows).\nNote X.509 certificates do not support single or double quotes in the certificate’s Distinguished Name (DN). The DN below is rejected because of the single quote:\n/C=UK/O=STFC/OU=SomeOrgUnit/CN=David Mc'Donald\nThis is in accordance with RFC1778, which also disallows single quotes in all Relative Distinguished Name (RDN) components, and the OGF Certificate Authority Working Group (CAOPS) who strongly discourage any type of quote in a certificate DN as specified by their Grid Certificate Profile document.\nRegistering a new user account Being authenticated in one of the two ways described above is enough to have read-only access to all the public features of the EGI Configuration Database. If you need to edit data in and request roles, you will need to fill in the registration form.\nTo Register:\nGo to the EGI Configuration Database web portal In the left sidebar, look out for the User status panel click on the “Register” link fill in the form and validate If you were registered but are not recognised anymore (e.g. because your certificate DN changed), do not register again! Instead, follow the steps Lost access to your Configuration Database account section.\n","categories":"","description":"Accessing the Configuration Database","excerpt":"Accessing the Configuration Database","ref":"/internal/configuration-database/access/","tags":"","title":"Access"},{"body":"Managing GGUS account data GGUS users should be able to manage their GGUS account data by themselves.\nInformation about registration and account management is collected on GGUS registration page and could be reached by clicking the “Registration” link on top of GGUS home page.\nFor managing the GGUS account data the user has to click the link “Check your GGUS account” at the bottom of this page.\nUpdating the personal certificate information Users should update their certificate information before their old certificate expires. They can navigate to the registration page, click the link “Check your GGUS account” and update their data. Modifying the DN field will result in an additional DN field: the old DN will be kept and the new DN will be added. Subsequently, the user is identified with the two DNs. If the old DN is no more valid please just clear the DN field and save the change. Once the old certificate has expired before updating the account data, users can request an update via GGUS ticket against Helpdesk(GGUS) support unit.\nLinking the EGI SSO persistent ID to the GGUS account Pre-requisites: the email address of your EGI SSO account must be equal to the email address registered in GGUS!\nFor adding your EGI SSO persistent ID to your account data please do the following:\nlogin to GGUS using your EGI SSO account on GGUS home click the Registration link click the button “Add my Shibboleth ID to my account” at the bottom of the yellow box. Getting supporter privileges In order to process tickets assigned to the support unit you belong to, or in general tickets submitted by other users and assigned to other support units, you need to own supporter privileges. Therefore users need to register an account at GGUS. Registration can be done either using an X509 personal certificate or using the EGI AAI account.\nRegistration with an x509 certificate For registering with an X509 certificate the user should go to GGUS home and click the registration link in the menu bar at the left. This link opens the registration information page which gives some additional information about registration process. Clicking on the registration link guides to the form that the user has to fill in. After filling in the registration form, GGUS team will check whether support privileges can be granted. The user will receive an email from the GGUS team (usually) confirming their supporter privileges.\nRegistration with the EGI AAI account Users who do not have a valid X509 certificate can access GGUS via EGI Check-in. For getting support privileges the user needs to be member of the ggus-supporters group in EGI Check-in. However the user should fill in the registration form for creating an account at GGUS. Support privileges will be granted automatically. The user will receive an automated email from GGUS system confirming their support privileges.\nBoth authentication methods, X509 certificate and EGI Check-in account, will be guaranteed in the future. Since the use of login and password will no longer be guaranteed in the future, it is recommended to access GGUS with a valid digital certificate or EGI Check-in.\nTroubleshooting I’m member of ggus-supporters.egi.eu group. Why don’t I have support privileges in GGUS? Pre-requisites: the email address of your EGI SSO account must be equal to the email address registered in GGUS!\nCase 1: user isn’t registered in GGUS yet. Unregistered users have to register first. On registration page you will see this text: “You are member of EGI AAI CheckIn ggus supporter. You will get automatically support right for the GGUS portal once you will submit this form.“ Support permissions will be granted automatically.\nClick on the “Add my ….” button.\nCase 2: user already registered in GGUS with an X509 DN. You can decide to add the Shibboleth ID to the existing account and check the updated account. In case the email addresses are different you may either harmonize your email address in EGI SSO and GGUS account or create a new account using EGI SSO data.\nGetting in touch wit the GGUS team? The preferred way to get in contact with the GGUS team is by submitting a GGUS ticket against Helpdesk (GGUS) support unit.\nAdditional information GGUS user guide GGUS short guide GGUS support staff guide ","categories":"","description":"Managing account and privileges","excerpt":"Managing account and privileges","ref":"/internal/helpdesk/account-and-privileges/","tags":"","title":"Account and privileges"},{"body":"What is it? EGI Accounting tracks and reports usage of EGI services, offering insights and control over resource consumption. EGI Federation members can use it to account for the resource usage of their own services.\nEGI Accounting consists of two main components:\nThe Accounting Repository is where all accounting data is collected by a network of message brokers that transfer usage data from hosts and services. The Accounting Portal allows filtering and displaying resource usage information. Note Documentation for the Accounting Repository is available in the EGI Wiki, documentation for the Accounting Portal is also available in the EGI Wiki. ","categories":"","description":"Resource usage accounting for EGI services\n","excerpt":"Resource usage accounting for EGI services\n","ref":"/internal/accounting/","tags":"","title":"Accounting"},{"body":"There are two different processes handling the accounting integration:\ncASO, which connects to the OpenStack deployment to get the usage information, and, ssmsend, which sends that usage information to the central EGI accounting repository. They should be run by cron periodically, settings below run cASO every hour and ssmsend every six hours.\nUsing the VM Appliance cASO configuration is stored at /etc/caso/caso.conf. Most default values should be OK, but you must set:\nsite_name (line 12), with the name of your site as defined in GOCDB.\nprojects (line 20), with the list of projects you want to extract accounting from.\ncredentials to access the accounting data (lines 28-47, more options also available). Check the cASO documentation for the expected permissions of the user configured here.\nThe mapping from EGI VOs to your local projects /etc/caso/voms.json, following this format: :\n{ \"vo name\": { \"projects\": [ \"project A that accounts for the vo\", \"project B that accounts for the VO\" ] }, \"another vo\": { \"projects\": [\"project C that accounts for the VO\"] } } cASO will write records to /var/spool/apel from where ssmsend will take them.\nSSM configuration is available at /etc/apel. Defaults should be OK for most cases. The cron file uses /etc/grid-security for the CAs and the host certificate and private keys (/etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem).\nRunning the services Both caso and ssmsend are run via the root user crontab. For convenience there are two scripts /usr/local/bin/caso-extract.sh and /usr/local/bin/ssm-send.sh that run the docker container with the proper volumes.\n","categories":"","description":"Accounting integration\n","excerpt":"Accounting integration\n","ref":"/providers/cloud-compute/openstack/accounting/","tags":"","title":"Accounting"},{"body":"Authentication OpenID Connect is the main authentication protocol used on the EGI Cloud. It replaces the legacy VOMS-based authentication for all OpenStack providers.\nAuthentication to web based services (like the AppDB) will redirect you to the EGI Check-in authentication page. Just select your institution or social login and follow the regular authentication process.\nAccess to APIs or via command-line interfaces (CLI) requires the use of OAuth2.0 tokens and interaction with the OpenStack Keystone OS-FEDERATION API. The process for authentication is as follows:\nObtain a valid OAuth2.0 access token from Check-in. Access tokens are short-lived credentials that can be obtained by recognised Check-in clients once a user has been authenticated. Interchange the Check-in access token for a valid unscoped Keystone token. Discover available projects from Keystone using the unscoped token. Use the unscoped Keystone token to get a scoped token for a valid project. Scoped tokens will allow the user to perform operations on the provider. Authorisation Cloud Compute service is accessed through Virtual Organisations (VOs). Users that are members of a VO will have access to the providers supporting that VO: they will be able to manage VMs, block storage and object storage available to the VO. Resources (VMs and storage) are shared across all members of the VO, please do not interfere with the VMs of other users if you are not entitled to do so (specially do not delete them).\nSome users roles have special consideration in VOs:\nUsers with VO Manager, VO Deputy or VO Expert Role have extra privileges in the AppDB for managing the Virtual Appliances to be available at every provider. Check the Virtual Machine Image Management documentation for more information. Pilot VO The vo.access.egi.eu Virtual Organisation serves as a test ground for users to try the Cloud Compute service and to prototype and validate applications. It can be used for up to 6 month by any new user.\nWarning After the 6-month long membership in the vo.access.egi.eu VO, you will need to move to a production VO, or establish a new VO. The resources are not guaranteed and may be removed without notice by providers. Back-up frequently to avoid losing your work! For joining this VO, please click on the enrollment URL using your EGI account.\nOther VOs Pre-existing VOs of EGI can be also used on IaaS cloud providers. Consult with your VO manager or browse the existing VOs at the EGI Operations Portal.\nCheck-in and access tokens Access tokens can be obtained via several mechanisms, usually involving the use of a web server and a browser. Command-line clients/APIs without access to a browser or interactive prompt for user authentication can use refresh tokens. A refresh token is a special token that is used to generate additional access tokens. This allows you to have short-lived access tokens without having to collect credentials every single time one expires. You can request this token alongside the access and/or ID tokens as part of a user’s initial authentication flow.\nIf you need to obtain these kind of tokens for using it in command-line tools or APIs, you can easily do so with the EGI Check-in Token Portal. You can access the EGI Check-in Token Portal and click on 'Authorise' to log in with your Check-in credentials to obtain:\na client ID (token-portal) a refresh token Refresh tokens Refresh tokens should be treated with care! This is a secret that can be used to impersonate you in the infrastructure. It is recommended not to store them in plain text. Alternatively, you can use the oidc-agent tool that is able to manage your tokens locally, or the fedcloud client executed inside EGI Notebooks.\nDiscovering projects in Keystone The access token will provide you access to a cloud provider, but you may have access to several different projects within that provider (a project can be considered equivalent to a VO allocation). In order to discover which projects are available you can do that using the Keystone API.\nYou can use the fedcloud client to simplify the discovery of projects.\n# Get a list of sites (also available in [AppDB](https://appdb.egi.eu)) fedcloud site list # Get list of projects that you are allowed to access # You can either specify the name of the account in your oidc-agent configuration # or directly a valid access token fedcloud endpoint projects --site=\u003cname of the site\u003e \\ [--oidc-agent-account \u003caccount name\u003e|--oidc-access-token \u003caccess token\u003e] # You can also use environment variables for the configuration export EGI_SITE=\u003cname of the site\u003e export OIDC_ACCESS_TOKEN=\u003cyour access token\u003e fedcloud endpoint projects # or with oidc-agent export OIDC_AGENT_ACCOUNT=\u003caccount name\u003e fedcloud enpoint projects Using the OpenStack API Once you know which project to use, you can use your regular openstack cli commands for performing actual operations in the provider:\nfedcloud openstack image list --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e For third-party tools that can use token based authentication in OpenStack, use the following command:\nexport OS_TOKEN=$(fedcloud openstack --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e \\ token issue -c id -f value) Legacy X.509 AAI Warning OpenID Connect is the preferred federated identity technology on EGI Cloud. Use of X.509 certificates should be limited to legacy applications. VOMS uses X.509 proxies extended with VO information for authentication and authorisation on the providers. You can learn about X.509 certificates and VOMS in the Check-in documentation.\nVOMS configuration Valid configuration for fedcloud.egi.eu is available on the FedCloud client VM as generated by the fedcloud-ui installation script.\nVOMS client expects your certificate and private key to be available at $HOME/.globus/usercert.pem and $HOME/.globus/userkey.pem respectively.\nAccess the providers VOMS authentication differs from one provider to another depending on the technology used. There are 3 different cases handled automatically by the rOCCI-cli. For accessing native OpenStack sites there are two different plugins available for Keystone that are installed with a single library:\npip install openstack-voms-auth-type For Keystone-VOMS based installations (Keystone URL ending on /v2.0), just define the location of your proxy and v2voms as authorisation plugin:\nopenstack --os-auth-url https://\u003ckeystone-url\u003e/v2.0 \\ --os-auth-type v2voms --os-x509-user-proxy /tmp/x509up_u1000 \\ token issue +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-02-04T12:41:25+0000 | | id | gAAAAABcWCTlMoz6Jx9IHF5hj-ZOn-CI17CfX81FTn7yy0ZJ54jkza7QNoQTRU5-KRJkphmes55bcoSaaBRnE3g2clFgY-MR2GVUJZRkCmj9TXsLZ-hVBWXQNENiX9XxUwnavj7KqDn4b9B1K22ijTrjdDVkcdpvMw | | user_id | 9310054c2b6f4fd28789ee08c2351221 | +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+ For those Keystone installations supporting only v3, specify v3voms as authorisation plugin, egi.eu as identity provider, mapped as protocol, and the location of your proxy:\nopenstack --os-auth-url https://\u003ckeystone url\u003e/v3 \\ --os-auth-type v3voms --os-x509-user-proxy /tmp/x509up_u1000 \\ --os-identity-provider egi.eu --os-protocol mapped \\ token issue +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-02-04T12:45:32+0000 | | id | gAAAAABcWCXcXGUDpHUYnI1IDLW3MnEpDzivw_OPaau8DQDYxA7gK9XsmOqZh1pL5Uqqs8aM-tHowdJQnJURww2-UhmQVqk5PxbjdnvLeqtXPYURCLaSsbmhkQg6kB311c_ZA1jfgdT-pG6fZz3toeH66SEFX-H0bThSUy0KFLhcZVkrZIbYgTsAOIzFkTfLjOgTw_tNChS8 | | user_id | 50fa8516b2554daeae652619ba9ebf96 | +---------+---------------------------------------------------------------------------------------------------------------------------------------- ","categories":"","description":"Authentication and Authorisation in EGI Cloud\n","excerpt":"Authentication and Authorisation in EGI Cloud\n","ref":"/users/compute/cloud-compute/auth/","tags":"","title":"Authentication and Authorisation"},{"body":"What is it? Binder allows the re-creation of a custom computing environment for reproducible execution of notebooks (and potentially many other types of applications). Users who create their own notebooks in the EGI Notebooks to analyze data can easily create a shareable link for those notebooks in the form of a GitHub repository. Based on this link, anyone can then reproduce the same data analysis using the link in the EGI Binder service.\nThe service builds on BinderHub, an Open Source tool that allows to build docker images from a Git repository and then makes them available through your browser.\nEGI Binder offers a service similar to the publicly accessible mybinder.org site. However, EGI Binder has the following additional features:\nAccess with academic user accounts: login via Check-in that’s connected to eduGAIN and social media accounts. Access to scalable storage: selected storage spaces of EGI DataHub are directly available under the datahub folder, simplifying the access to shared data from Binder notebooks. Guaranteed capacity: environments have 2GB of RAM guaranteed and can reach 4GB as maximum. Persistent sessions: There is no hard limit on the session time per user, although sessions will be shut down automatically after 1 hour of inactivity (see session limitations at the public mybinder.org service). Access to the rest of EGI services: a personal access token is available in the Binder session to interact with the rest of the EGI infrastructure. Community Binder environments: User communities can have their customized Binder service instance from EGI, with extra features as requested (such as access to GPUs, integration with community specific data repositories and services). EGI offers consultancy and support the setup of these instances, and provides operational oversight for them. Reproducible research Binder facilitates the sharing and reproducibility of digital data analysis:\nUsers can define their computational analysis in the EGI Notebooks service. Once the notebook is ready for publishing, it can be shared in a GitHub repository. Optionally, users can use the Zenodo-GitHub integration for generating DOIs that can be cited in publications and can be discovered by fellow researchers Anyone can use the link to the GitHub repository or Zenodo DOI to reproduce the computational analysis in EGI Binder. Access to the service EGI’s Binder has the same access conditions as the centrally operated Notebooks service from EGI. Before using the service, you need to have an EGI account and be a member of one of the supported resource pools (alias Virtual Organisations). Follow the instructions on the EGI Binder login page for access\nCreating a Binder repository Binder starts from a code repository that contains the code or notebook you’d like to run and a set of configuration files that specify what’s the exact computational environment your code needs to run.\nBinder then creates a reproducible container using repo2docker, and generates a user session to interact with the container in the browser.\nThe configuration for building the container supports specifying conda environments; installing Python, R and Julia environments; installing additional OS packages; and even complete custom Dockerfiles to bring any application to the system. The code repository can be hosted on popular git hosting platforms like GitHub and GitLab and can also be referenced with a DOI from Zenodo, FigShare or Dataverse. You can learn more on the configuration of your repository with Binder at the Binder user documentation\nYou can start by forking the EGI-Federation/binder-example GitHub repository for creating your own reproducible environment. To run this directly on EGI’s Binder click on the button below:\nYou can create such link to share your notebooks from the Binder interface, as shown in the screenshot below, you can copy the URL shown when the building is in progress:\nThe binder examples organisation on GitHub contains more sample repositories for common configurations that can help you getting started.\nAccessing data Your notebooks running in Binder have outgoing internet connectivity, so you can connect to external services to bring data in for analysis or deposing the notebooks output.\nEvery session that you start will also provide access to your spaces in the DataHub under a folder named datahub. Only those spaces configured to be mounted locally will be made available automatically. Check the documentation for the Notebook’s DataHub support for more information.\n","categories":"","description":"Custom reproducible computing environments for notebooks\n","excerpt":"Custom reproducible computing environments for notebooks\n","ref":"/users/dev-env/binder/","tags":"","title":"Binder"},{"body":"EGI Foundation is having access to a GÉANT Trusted Certificate Service subscription.\nSectigo is the current Certificate Authority (CA) providing certificates to GÉANT TCS.\nThis can be used to issue IGTF-trust (AKA eScience certificates) and public-trust certificates, for the domains managed by EGI Foundation, like egi.eu.\nOperators of central services can request two types of certificates:\nhost certificates, with public-trust, and optionally IGTF trust. robot email certificates, to be used as client certificate, to authenticate using X509 as a service. In some specific cases, like for Cloud Compute providers not having access to an IGTF CA, it’s possible for them to request a robot certificate, as an IGTF certificate is required for sending accounting records.\nYou can contact scs-ra@egi.eu (or operations@egi.eu) if you need support.\nRequesting a new certificate Open a ticket to Collaboration Tools SU in EGI Helpdesk, providing:\nJustification of the request. Type of certificate (host or robot). FQDN of the service. Mailing list to be used as contact address that will receive renewal notifications. For host certificates: a Certificate Signing Request, or mentioning the desire to use the ACME protocol. An operator will follow with the request, and help you getting the certificate.\nRetrieving certificates Host certificates will be sent by email, you will receive a notification with links allowing to download it. For robot certificates, you will receive an invitation by email allowing to generate and retrieve it. Renewing certificates Certificates are usually valid for one year. Auto-renewal of certificates is enabled by default, 30 days before expiration, notifications will be sent to the contact address provided when requesting the certificate.\nThe notifications contains links allowing to renew the certificate.\nCreating a Certificate Signing Request In order to get a certificate, Service Providers may be requested to send a Certificate Signing Request (CSR).\nIn the CSR only the Common Name (CN) is important, it should be the (FQDN) of the service Most of other fields will be replaced by the CA while generating the certificate.\nIt can be done via different ways, some are documented below.\nUsing a web application DigiCert provides an online web application.\nUsing CloudFalre cfssl tool It can be done with the cfssl tool from CloudFlare.\n# Replace #FQDN# by the FQDN of the service $ cfssl genkey \u003c(echo '{\"hosts\":[\"#FQDN#\"],\"CN\"#FQDN#\",\"key\":{\"algo\":\"rsa\",\"size\":4096}}') | cfssljson -bare ##FQDN##.rsa Using OpenSSL It can be done using the following OpenSSL command (This will generate a password-protected key.\nYou will be asked for various questions, but the only important ones are the Common Name (CN) and Subject Alternative Names (SAN) (in case you want to request a certificates covering different FQDNs), as other values will be overwritten by the CA.\n$ openssl req -out CSR.csr -new -newkey rsa:4096 -keyout privateKey.key Adding the -nodes option will disable password protection for the key, beware if using it.\nUsing ACME protocol It is possible to automate the certificate request and renewal using the ACME protocol via certbot or similar tools.\nTwo things should be considered:\nit’s not yet possible to get eScience/IGTF-trusted certificates, it’s on the roadmap but without any firm ETA. it’s using a different intermediate CA from Sectigo, not the usual GEANT one used for TCS, but this shouldn’t have much impact for generic/non-eScience public services. The EGI SDIS team will have to create and register an ACME client ID for you.\nOnce you will have the credential it should be possible to request a certificate, using the standard certbot client, as documented below.\nRegistering the client and saving the credentials locally This will interactively register your certbot client. Even if the ACME credentials are shared, only a given client is able to manage the certificates it requested. Email address is used for urgent renewal and security notices. This is preferred way if you need to get notifications on certificate expiration and if you are doing some manual management or testing.\n$ sudo certbot register --no-eff-email \\ --server https://acme.sectigo.com/v2/OV \\ --eab-kid \u003cEAB_KID\u003e \\ --eab-hmac-key \u003cEAB_HMAC_KEY\u003e \\ --email \u003cCONTACT_EMAIL\u003e # Checking existing account $ sudo certbot show_account --server https://acme.sectigo.com/v2/OV # Unregistering an account # Beware: you won't any more be able to revoke certificate issued with the account $ sudo certbot unregister --server https://acme.sectigo.com/v2/OV Requesting a certificate $ sudo certbot certonly --standalone --non-interactive \\ --server https://acme.sectigo.com/v2/OV \\ --domain fakedomaindonotexist.egi.eu Revoking a certificate $ sudo certbot revoke \\ --server https://acme.sectigo.com/v2/OV \\ --cert-name fakedomaindonotexist.egi.eu Registering and requesting a certificate all at once This is useful when you don’t want interactive registration, like for one shot scripts. Email address is used for urgent renewal and security notices.\nApparently the notification to use this email is not visible in cert-manager, and the first option with explicit registration may be safer to get the notifications, if it’s a required feature.\n$ sudo certbot certonly --standalone --non-interactive --agree-tos \\ --server https://acme.sectigo.com/v2/OV \\ --eab-kid \u003cEAB_KID\u003e --eab-hmac-key \u003cEAB_HMAC_KEY\u003e \\ --rsa-key-size 4096 \\ --email \u003cCONTACT_EMAIL\u003e \\ --domain fakedomaindonotexist.egi.eu Other usual certbot options should also work. You may also be able to use other tools that can speak the ACME protocol, it should be standard.\n","categories":"","description":"Certificate provisioning","excerpt":"Certificate provisioning","ref":"/internal/collaboration-tools/certificates/","tags":"","title":"Certificates"},{"body":"","categories":"","description":"Integration with Check-in for IdPs and SPs","excerpt":"Integration with Check-in for IdPs and SPs","ref":"/providers/check-in/","tags":"","title":"Check-in"},{"body":" Overview The FTS3 service offers a command-line client to ease the interaction with the service.\nPrerequisites The client software is available for RHEL 6 and 7 derivatives.\nPlease note that the RHEL 6 support is ending the 30/11/2020 and the implementation for RHEL 8 is on-going.\nUsers from other distributions should refer to the RESTFul API section.\nInstallation The CLI can be installed from the EPEL repositories for RHEL 7 with the following package:\nyum install fts-rest-cli -y Commands This section describes some of the commands that can be issues via the FTS CLI. As per the API, in order to authenticate to the FTS REST server you need an X.509 User certificate, please refer to this section\nfor more information.\nCheck the full documentation about the FTS CLI\nfts-rest-whoami This command can be used to check, as the name suggests, who are we for the server.\nUsage fts-rest-whoami [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example $ fts-rest-whoami -s https://fts3-public.cern.ch:8446 User DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi VO: dteam VO id: 6b10f4e4-8fdc-5555-baa2-7d4850d4f406 Delegation id: 9ab8068853808c6b Base id: 01874efb-4735-4595-bc9c-591aef8240c9 fts-rest-delegate This command can be used to (re)delegate your credentials to the FTS3 server.\nUsage fts-rest-delegate [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) -f/--force : Force the delegation -H/--hours : Duration of the delegation in hours (default: 12) Example $ fts-rest-delegate -s https://fts3-public.cern.ch:8446 Delegation id: 9ab8068853808c6b fts-rest-transfer-submit This command can be used to submit new jobs to FTS3. It supports simple and bulk submissions. The bulk format is as follows:\n{ \"files\": [ { \"sources\": [ \"gsiftp://source.host/file\" ], \"destinations\": [ \"gsiftp://destination.host/file\" ], \"metadata\": \"file-metadata\", \"checksum\": \"ADLER32:1234\", \"filesize\": 1024 }, { \"sources\": [ \"gsiftp://source.host/file2\" ], \"destinations\": [ \"gsiftp://destination.host/file2\" ], \"metadata\": \"file2-metadata\", \"checksum\": \"ADLER32:4321\", \"filesize\": 2048, \"activity\": \"default\" } ] } Usage fts-rest-transfer-submit [options] SOURCE DESTINATION [CHECKSUM] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) -b/--blocking : Blocking mode. Wait until the operation completes. -i/--interval : Interval between two poll operations in blocking mode. -e/--expire : Expiration time of the delegation in minutes. --delegate-when-lifetime-lt : Delegate the proxy when the remote lifetime is less than this value (in minutes) -o/--overwrite : Overwrite files. -r/--reuse : Enable session reuse for the transfer job. --job-metadata : Transfer job metadata. --file-metadata : File metadata. --file-size : File size (in bytes) -g/--gparam : Gridftp parameters. -t/--dest-token : The destination space token or its description. -S/--source-token : The source space token or its description. -K/--compare-checksum : Deprecated: compare checksums between source and destination. -C/--checksum-mode : Compare checksums in source, target, both or none. --copy-pin-lifetime : Pin lifetime of the copy in seconds. --bring-online : Bring online timeout in seconds. --timeout : Transfer timeout in seconds. --fail-nearline : Fail the transfer is the file is nearline. --dry-run : Do not send anything, just print the json message. -f/--file : Name of configuration file --retry : Number of retries. If 0, the server default will be used. If negative, there will be no retries. -m/--multi-hop : Submit a multihop transfer. --cloud-credentials : Use cloud credentials for the job (i. E. Dropbox). --nostreams : Number of streams --ipv4 : Force ipv4 --ipv6 : Force ipv6 Example fts-rest-transfer-submit -s https://fts3-public.cern.ch:8446 \\ gsiftp://source.host/file gsiftp://destination.host/file Job successfully submitted. Job id: 7e02b4fa-d568-11ea-9c80-02163e018681 $ fts-rest-transfer-submit -s https://fts3-public.cern.ch:8446 -f test.json Job successfully submitted. Job id: 9a28d204-d568-11ea-9c80-02163e018681 fts-rest-transfer-status This command can be used to check the current status of a given job.\nUsage fts-rest-transfer-status [options] JOB_ID Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example fts-rest-transfer-status -s https://fts3-public.cern.ch:8446 \\ 7e02b4fa-d568-11ea-9c80-02163e018681 Request ID: 7e02b4fa-d568-11ea-9c80-02163e018681 Status: FAILED Client DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi Reason: One or more files failed. Please have a look at the details for more information Submission time: 2020-08-03T09:05:36 Priority: 3 VO Name: dteam fts-rest-transfer-cancel This command can be used to cancel a running job. It returns the final state of the cancelled job. Please, mind that if the job is already in a final state (FINISHEDDIRTY, FINISHED, FAILED), this command will return this state. You can additionally cancel only a subset appending a comma-separated list of file IDs.\nUsage fts-rest-transfer-cancel [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example fts-rest-transfer-cancel -s https://fts3-public.cern.ch:8446 9a28d204-d568-11ea-9c80-02163e018681 CANCELED ","categories":"","description":"Clients for accessing EGI Data Transfer\n","excerpt":"Clients for accessing EGI Data Transfer\n","ref":"/users/data/management/data-transfer/clients/","tags":"","title":"Data Tranfer Clients"},{"body":"EGI DataHub spaces can be accessed via web interface, the oneclient component or the API.\nThe official documentation for oneclient is hosted on the Onedata homepage, and a specific tutorial on how to install and use it from a Virtual Machine is also available.\nUsing the web interface Using EGI Check-in it's possible to connect with your institute credentials.\nOn this page it’s possible to have an overview of all the spaces and their supporting providers.\nOn this capture, the information about the spaces supported by a specific provider is displayed.\nThe data space can be managed (i.e. uploading/downloading/managing files and metadata, managing space access) using the web browser.\nGenerating tokens for using Oneclient or APIs Important In order to be able to access your spaces using oneclient or the API, it is required to generate an access token. Tokens have to be generated from the EGI DataHub (Onezone) interface.\nThe access tokens can be created and managed using the EGI DataHub web interface.\nEnvironment variables The sections below assume you have defined the following variables in your environment:\nONECLIENT_ACCESS_TOKEN: access token allowing to access all the spaces ONECLIENT_PROVIDER_HOST: name or IP of the Oneprovider the client should connect to. Installing and testing Oneclient in a docker container Important In order to be able to use FUSE, the container should run in privileged mode. A quick and simple solution for testing is to install the client on demand in a container for a supported Operating System flavor (mainly various CentOS and Ubuntu releases).\ndocker run -it --privileged centos:7 /bin/bash root@81dbd7e84438 /]# curl -sS http://get.onedata.org/oneclient.sh | bash # (...) Complete! Installation has been completed successfully. Run 'oneclient --help' for usage info. root@81dbd7e84438 /]# export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e root@81dbd7e84438 /]# export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu root@81dbd7e84438 /]# mkdir /tmp/space root@81dbd7e84438 /]# oneclient /tmp/space root@81dbd7e84438 /]# ls /tmp/space Here the data is mounted in /tmp/space, creating a file into it will push it to the Oneprovider and it will be accessible in the web interface and from other providers supporting the space.\nFor a real production usage it's preferable to use the Oneclient container as a source for a volume mounted into another container.\nTesting Oneclient in a Oneclient docker container with NFS or samba Docker containers for the Oneclient are available, the existing versions can be seen on the Oneclient docker hub.\nIt’s possible to use the most recent version by specifying the latest tag. We also recommend using the same version as shown on the Onezone and Oneprovider pages.\nexport ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu docker run -it --privileged -e ONECLIENT_ACCESS_TOKEN=$ONECLIENT_ACCESS_TOKEN -e ONECLIENT_PROVIDER_HOST=$ONECLIENT_PROVIDER_HOST onedata/oneclient:20.02.7 Connecting to provider 'plg-cyfronet-01.datahub.egi.eu:443' using session ID: '4138963898952098752'... Getting configuration... Oneclient has been successfully mounted in '/mnt/oneclient' Now the client will run in the background and the data will be available through samba/CIFS or nfs protocols:\n# Identifying the IP of the container docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" $(docker ps -ql) 172.17.0.2 So the data can be accessed at\nsmb://172.17.0.2/onedata nfs://172.17.0.2/onedata Testing Oneclient in a Oneclient docker container with local file access Another solution is to mount a local directory as a volume in the container, allowing to access both the working directory as well as the Onedata spaces, thus allowing to easily exchange files between a local directory and a Onedata space.\nIn order to do this we will open a bash shell in the container then we will mount manually the Onedata spaces.\nexport ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu docker run -it --privileged -e ONECLIENT_ACCESS_TOKEN=$ONECLIENT_ACCESS_TOKEN -e ONECLIENT_PROVIDER_HOST=$ONECLIENT_PROVIDER_HOST -v $PWD:/mnt/src --entrypoint bash onedata/oneclient:20.02.7 root@aca612a84fb4:/tmp# oneclient /mnt/oneclient Connecting to provider 'plg-cyfronet-01.datahub.egi.eu:443' using session ID: '1641165171427694510'... Getting configuration... Oneclient has been successfully mounted in '/mnt/oneclient'. root@aca612a84fb4:/tmp# ls /mnt/oneclient (...) root@aca612a84fb4:/tmp# ls /mnt/src (...) Now it's possible to use the following mount points:\n/mnt/oneclient: the Onedata spaces /mnt/src: the local directory (any absolute path could have been used instead of $PWD that points to the working directory) Testing Oneclient in a Virtual Machine The following variables have to be exported:\nONECLIENT_ACCESS_TOKEN: access token allowing to access all the spaces. ONECLIENT_PROVIDER_HOST: name or IP of the Oneprovider the client should connect to. curl -sS http://get.onedata.org/oneclient.sh | bash export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu mkdir /tmp/space oneclient /tmp/space Testing Oneclient in a Vagrant box It's possible to quickly test Oneclient using Vagrant.\nvagrant init ubuntu/xenial64 vagrant up vagrant ssh curl -sS http://get.onedata.org/oneclient.sh | bash export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu mkdir /tmp/space oneclient /tmp/space ","categories":"","description":"Clients for accessing data EGI DataHub\n","excerpt":"Clients for accessing data EGI DataHub\n","ref":"/users/data/management/datahub/clients/","tags":"","title":"DataHub Clients"},{"body":"What is it? Elastic Cloud Compute Cluster (EC3) is a tool to create elastic virtual clusters on top of Infrastructure-as-a-Service (IaaS) providers.\nBeing based on Infrastructure Manager, EC3 supports the same wide choices of backends, either public (such as Amazon Web Services, Google Cloud or Microsoft Azure) or on-premises (such as OpenStack). EC3 can provision clusters running TORQUE, SLURM, HTCondor, Apache Mesos, Nomad, Kubernetes and others, which will be automatically resized to fit the load (e.g. number of jobs at the batch system).\nNote EC3 was presented in one of the EGI Webinars. Please see more details on the Indico page and check out the video recording on YouTube. The following section of the documentation will guide you on how to:\nDeploy a simple EC3 elastic cluster on top of the IaaS providers of the EGI Cloud, either using the web interface or the command-line interface, and Run pre-configured scientific applications in the EC3 elastic cluster. ","categories":"","description":"Working with Elastic Cloud Compute Clusters in the EGI Cloud\n","excerpt":"Working with Elastic Cloud Compute Clusters in the EGI Cloud\n","ref":"/users/compute/orchestration/ec3/","tags":"","title":"Elastic Cloud Compute Clusters"},{"body":"The EGI Cloud Container Compute service allows you to run container-based applications on the providers of the EGI Federated Cloud. There are two main ways of executing containers:\nUsing docker (or a similar container runtime) on a VM, so you can just interact directly with the container runtime to run your applications. This fits simpler applications that can easily fit on one node and are composed by a small number of containers.\nUsing a container orchestration platform, e.g. kubernetes on a set of VMs to manage the applications in an automated way for you. This is usually suited for more complex applications that spawn several nodes and are composed of several containers that need to cooperate to deliver the expected functionality.\nFollow the guides below to learn more about them.\nThe EGI Cloud Container Compute service was presented in one of the EGI Webinars. See more details on the indico page and a video recording on YouTube.\n","categories":"","description":"Run containers on the EGI Cloud\n","excerpt":"Run containers on the EGI Cloud\n","ref":"/users/compute/cloud-container-compute/","tags":"","title":"Cloud Container Compute"},{"body":"What is it? The EGI Collaboration Tools are a set of online services maintained by the EGI Foundation and meant to support collaboration across the EGI Federation, supporting its daily activities (sharing information and documents, organising meetings, communicating in task or project specific groups…).\nNote Component-specific documentation is maintained by the upstream software providers. What are the components? EGI Single Sign On (EGI SSO) The EGI SSO is a central authentication and authorisation service allowing to use a single username and password to access various Collaboration Tools.\nGroup owners can manage group members, and send invitations to new users. Groups are used in Collaboration Tools services for authorisation.\nEGI SSO is now being deprecated in favour of EGI Check-in, providing a federation authentication and authorisation service.\nEGI.eu domain Domain entries can be created for the service as long as they are relevant for the EGI infrastructure. All EGI central services are usually available under the egi.eu domain.\nRequests needs to be discussed with the EGI Operations team.\nCertificate for EGI.eu domain EGI Foundation is having access to a GÉANT Trusted Certificate Service subscription.\nThis can be used to issue IGTF-trust (AKA eScience certificates) and public-trust certificates, for the domains managed by EGI Foundation, like egi.eu.\nPlease refer to documentation on requesting a certificate.\nEGI.eu site Information about the EGI Federation activities is published on the public site.\nWiki A Confluence-based “Wiki” space is available, allowing the EGI members to work collaboratively on documenting many aspects like Service Management, Polices and Procedures or activities from Boards and Groups.\nSeparate instances for EGI and EOSC are operated by EGI Foundation.\nIssues management Jira EGI internal request tracking system, meant to support the Service Management System and projects.\nEGI Foundation is operating dedicated instances for EGI and EOSC.\nRequest Tracking (RT) EGI internal request tracking system, being replaced for many activities by Jira.\nMailing lists Many mailing lists exists to cover specific areas of collaboration. New ones can be crated on request as documented below.\nMembership in mailing lists is usually determined by membership in EGI SSO groups. Only group owners can manage the group membership.\nNote that canonical addresses are list-name@mailman.egi.eu, not list-name@egi.eu.\nSee the dedicated page on mailing list for more information like how to request the creation of a new mailing list.\nDocument server EGI Documentation database, hosting many published document relevant to the EGI Federation.\nDocuments have metadata describing them and can be versioned. EGI SSO groups are used for restricting access to documents.\nAgenda management via Indico Event planner, used for various conferences and meetings, powered by the Indico product.\nAll users with an SSO account can use it.\nIndico also has its own local accounts, which can be used by people who do not want an EGI SSO account, but want to register to some meeting in Indico.\nContact EGI Collaboration Tools support can be contacted via:\nEGI Helpdesk Support Unit “Collaboration Tools” EGI IT Support ","categories":"","description":"Fostering Collaboration across the EGI Federation\n","excerpt":"Fostering Collaboration across the EGI Federation\n","ref":"/internal/collaboration-tools/","tags":"","title":"Collaboration Tools"},{"body":"You can find here documentation on how to deploy a sample SLURM cluster, which you can then adapt to create other kind of clusters easily.\nGetting started We will use docker for running EC3, direct installation is also possible and described at EC3 documentation. First get the docker image:\ndocker pull grycap/ec3 And check that you can run a simple command:\n$ docker run grycap/ec3 list name state IP nodes ------------------------ For convenience we will create a directory to keep the deployment configuration and status together.\nmkdir ec3-test cd ec3-test You can list the available templates for clusters with the templates command:\n$ docker run grycap/ec3 templates name kind summary ---------------------------------------------------------------------------------------------------------------------- blcr component Tool for checkpointing applications. [...] sge main Install and configure a cluster SGE from distribution repositories. slurm main Install and configure a cluster using the grycap.slurm ansible role. slurm-repo main Install and configure a cluster SLURM from distribution repositories. [...] We will use the slurm template for configuring our cluster.\nSite details EC3 needs some information on the site that you are planning to use to deploy your cluster:\nauthentication information network identifiers VM image identifiers We will use the FedCloud client to discover the required information. Set your credentials as shown in the authentication guide and create the autorisation files needed for ec3 (in this case for CESGA with VO vo.access.egi.eu):\nfedcloud ec3 init --site CESGA --vo vo.access.egi.eu This will generate an auth.dat file with your credentials to access the site and a templates/refresh.radl with a refresh token to allow long running clusters to be managed on the infrastructure.\nLet’s get also some needed site information. Start getting the available networks, we will need both a public and private network:\n$ fedcloud openstack --site CESGA --vo vo.access.egi.eu network list +--------------------------------------+----------------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+----------------------+--------------------------------------+ | 12ffb5f7-3e54-433f-86d0-8ffa43b52025 | net-vo.access.egi.eu | 754342b1-92df-4fc8-9499-2ee8b668141f | | 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 | public00 | 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 | +--------------------------------------+----------------------+--------------------------------------+ Then, get the list of images available:\n$ fedcloud openstack --site CESGA --vo vo.access.egi.eu image list +--------------------------------------+----------------------------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+----------------------------------------------------------+--------+ | 9d22cb3b-e6a3-4467-801a-a68214338b22 | Image for CernVM3 [CentOS/6/QEMU-KVM] | active | | b03e8720-d88a-4939-b93d-23289b8eed6c | Image for CernVM4 [CentOS/7/QEMU-KVM] | active | | 06cd7256-de22-4e9d-a1cf-997b5c44d938 | Image for Chipster [Ubuntu/16.04/KVM] | active | | 8c4e2568-67a2-441a-b696-ac1b7c60de9c | Image for EGI CentOS 7 [CentOS/7/VirtualBox] | active | | abc5ebd8-f65c-4af9-8e54-a89e3b5587a3 | Image for EGI Docker [Ubuntu/18.04/VirtualBox] | active | | 22064e93-6af9-430b-94a1-e96473c5a72b | Image for EGI Ubuntu 16.04 LTS [Ubuntu/16.04/VirtualBox] | active | | d5040b3e-ef33-4959-bb88-5505e229f579 | Image for EGI Ubuntu 18.04 [Ubuntu/18.04/VirtualBox] | active | | 79fadf3f-6092-4bb7-ab78-9a322f0aad33 | cirros | active | +--------------------------------------+----------------------------------------------------------+--------+ For our example we will use the EGI CentOS 7 with id 8c4e2568-67a2-441a-b696-ac1b7c60de9c.\nFinally, with all this information we can create the images template for EC3 that specifies the site configuration for our deployment. Save this file as templates/centos.radl:\ndescription centos-cesga ( kind = 'images' and short = 'centos7-cesga' and content = 'CentOS7 image at CESGA' ) network public ( provider_id = 'public00' and outports contains '22/tcp' ) network private (provider_id = 'net-vo.access.egi.eu') system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' and disk.0.os.credentials.username = 'centos' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and ec3_max_instances = 5 and # maximum number of worker nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' and disk.0.os.credentials.username = 'centos' ) Note we have used public00 as public network and opened port 22 to allow ssh access. The private network uses net-vo.access.egi.eu. We have two kind of VMs in almost every deployment: the front, that runs the batch system, and the wn, that will execute the jobs. In our example, both will use the same CentOS image, which is specified with the disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' line: ost refers to OpenStack, fedcloud-osservices.egi.cesga.es is the hostname of the URL obtained above with fedcloud endpoint list and 8c4e2568-67a2-441a-b696-ac1b7c60de9c is the ID of the image in OpenStack. The size of the VM is also specified.\nLaunch cluster We are ready now to deploy the cluster with ec3 (this can take several minutes):\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 launch mycluster slurm ubuntu refresh -a auth.dat Creating infrastructure Infrastructure successfully created with ID: 74fde7be-edee-11ea-a6e9-da8b0bbd7c73 Front-end configured with IP 193.144.46.234 Transferring infrastructure Front-end ready! We can check the status of the deployment:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 list name state IP nodes ---------------------------------------------- mycluster configured 193.144.46.234 0 And once configured, ssh to the front node. The is_cluster_ready command will report whether the cluster is fully configured or not:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 ssh mycluster Warning: Permanently added '193.144.46.234' (ECDSA) to the list of known hosts. Last login: Thu Sep 3 14:07:46 2020 from torito.i3m.upv.es $ bash cloudadm@slurmserver:~$ is_cluster_ready Cluster configured! cloudadm@slurmserver:~$ EC3 will deploy CLUES, a cluster management system that will power on/off nodes as needed depending on the load. Initially all the nodes will be off:\nnode state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn2 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn3 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn4 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn5 off enabled 00h03'55\" 0,0.0 1,1073741824.0 SLURM will also report nodes as down:\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 5 down* wn[1-5] As we submit a first job, some nodes will be powered on to meet the request. You can also start them manually with clues poweron.\ncloudadm@slurmserver:~$ srun hostname srun: Required node not available (down, drained or reserved) srun: job 2 queued and waiting for resources srun: job 2 has been allocated resources wn1.localdomain cloudadm@slurmserver:~$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1 idle enabled 00h07'45\" 0,0.0 1,1073741824.0 wn2 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn3 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn4 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn5 off enabled 00h52'25\" 0,0.0 1,1073741824.0 cloudadm@slurmserver:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 4 down* wn[2-5] debug* up infinite 1 idle wn1 Destroying the cluster Once you are done with the cluster and want to destroy it, you can use the destroy command. If your cluster was created more than one hour ago, your credentials to access the site will be expired and need to refreshed first with fedcloud ec3 refresh:\n$ fedcloud ec3 refresh # refresh your auth.dat $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 list # list your clusters name state IP nodes ---------------------------------------------- mycluster configured 193.144.46.234 0 $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy mycluster -a auth.dat -y WARNING: you are going to delete the infrastructure (including frontend and nodes). Success deleting the cluster! ","categories":"","description":"Getting started with Elastic Cloud Compute Cluster on EGI Cloud with the Command Line Interface\n","excerpt":"Getting started with Elastic Cloud Compute Cluster on EGI Cloud with …","ref":"/users/compute/orchestration/ec3/cli/","tags":"","title":"EC3 Command-Line Interface"},{"body":"You can find here documentation covering getting started with IM Command-Line Interface (CLI) on EGI Cloud Compute sites. Full documentation at IM CLI documentation\nGetting started Install with pip You only have to call the install command of the pip tool with the IM-client package.\n$ pip install IM-client IM-Client Docker image The IM Client has an official Docker container image available on Docker Hub that can be used instead of installing the CLI. You can use it by typing:\n$ docker run --rm -ti -v \"$PWD:/tmp/im\" grycap/im-client \\ -r https://server.com:8800 -a /tmp/im/auth.dat list Configuration To avoid typing the parameters in all the client calls, the user can define a config file im_client.cfg in the current directory or a file .im_client.cfg in their home directory. In the config file the user can specify the following parameters:\n[im_client] restapi_url=https://appsgrycap.i3m.upv.es:31443/im auth_file=auth.dat Authentication data file An authentication file must be created to access the IM service. It must have one line per authentication element. It must have at least one line with the authentication data for the IM service and another one for the Cloud provider(s) the user want to access. Each line of the file is composed by pairs of key and value separated by semicolon, and refers to a single credential. The key and value should be separated by =, that is an equals sign preceded and followed by one whitespace at least. The following lines shows the credentials needed to access an EGI Cloud Compute site:\ntype = InfrastructureManager; token = egi_aai_token_value id = egi; type = EGI; host = CESGA; vo = vo.access.egi.eu; token = egi_aai_token_value The value of egi_aai_token_value must be replaced with a valid EGI Check-in access token. Users of EGI Check-in can get all the information needed to obtain access tokens, by visiting EGI Check-in Token Portal.\noidc-agent can be used to get a valid access token:\nid = im; type = InfrastructureManager; token = command(oidc-token OIDC_ACCOUNT) id = egi; type = EGI; host = SCAI; vo = vo.access.egi.eu; token = command(oidc-token OIDC_ACCOUNT) Create and Manage an infrastructure To create a virtual infrastructure you have to describe a file documenting the required resources. IM supports its native language RADL and the OASIS TOSCA Simple Profile in YAML Version 1.0. You can find some examples in the IM GitHub repository.\nFor example we can use RADL to define a simple VM with 1 CPU, 1 GB of RAM using the EGI Ubuntu 20.04 image.\nnetwork public (outbound = 'yes') system node ( cpu.count\u003e=2 and memory.size\u003e=4g and net_interface.0.connection = 'public' and disk.0.os.name='linux' and disk.0.image.url = 'appdb://SCAI/egi.ubuntu.20.04?vo.access.egi.eu' and ) configure wn ( @begin --- - tasks: - debug: msg=\"Configured!\" @end ) deploy node 1 IM also supports TOSCA. For example this is an equivalent TOSCA document to deploy a single VM:\ntosca_definitions_version: tosca_simple_yaml_1_0 imports: - indigo_custom_types: https://raw.githubusercontent.com/indigo-dc/tosca-types/master/custom_types.yaml topology_template: node_templates: simple_node: type: tosca.nodes.indigo.Compute capabilities: endpoint: properties: network_name: PUBLIC host: properties: num_cpus: 2 mem_size: 4 GB os: properties: image: appdb://SCAI/egi.ubuntu.20.04?vo.access.egi.eu outputs: node_ip: value: { get_attribute: [ simple_node, public_address, 0 ] } node_creds: value: { get_attribute: [ simple_node, endpoint, credential, 0 ] } Then we can call the create operation of the IM client tool using the a radl or a TOSCA yaml file:\n$ im_client.py create infra.radl Secure connection with: https://appsgrycap.i3m.upv.es:31443/im Infrastructure successfully created with ID: 457273ea-85e4-11ec-aa81-faaae69bc911 Then we can call get the currrent state of infrastructure using the getstate operation of the IM client tool:\n$ im_client.py getstate 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://appsgrycap.i3m.upv.es:31443/im The infrastructure is in state: pending VM ID: 0 is in state: pending. The valid VM and infrastructure states are the following:\npending, launched, but still in initialization stage; running, created successfully and running, but still in the configuration stage; configured, running and contextualized; unconfigured, running but not correctly contextualized; stopped, stopped or suspended; off, shutdown or removed from the infrastructure; failed, an error happened during the launching; or unknown, unable to obtain the status. deleting, in the deletion process. Once the configuration step has started we can get the output of the ansible process using the getcontmsg operation:\n$ im_client.py getcontmsg 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://appsgrycap.i3m.upv.es:31443/im Connected with: http://localhost:8800 Msg Contextualizator: 2022-02-11 10:40:12.768523: Copying YAML, hosts and inventory files. VM 0: Contextualization agent output processed successfullyGenerate and copy the ssh key Sleeping 0 secs. Launch task: wait_all_ssh ... Once the VM is booted we can access it via SSH using the ssh operation:\n$ im_client.py ssh 457273ea-85e4-11ec-aa81-faaae69bc911 When using a TOSCA yaml document to create the infrastructure, we can get the TOSCA output values with the getoutputs operation:\n$ im_client.py getoutputs 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://appsgrycap.i3m.upv.es:31443/im The infrastructure outputs: node_ip = 8.8.8.8 node_creds = {'token': '...', 'user': 'cloudadm', 'token_type': 'private_key'} Once we no more need the Infrastructure, we can destroy it using the destroy operation:\n$ im_client.py destroy 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://appsgrycap.i3m.upv.es:31443/im Infrastructure successfully destroyed ","categories":"","description":"Getting started with the command-line interface of Infrastructure Manager\n","excerpt":"Getting started with the command-line interface of Infrastructure …","ref":"/users/compute/orchestration/im/cli/","tags":"","title":"Infrastructure Manager Command-Line Interface"},{"body":"Introduction to Rucio commands There are many commands found within Rucio CLI that you may want to become familar with. In this guide I will provide a few of the common commands wanted by new users.\nTo find more of the commands that you may want to use type rucio into the containerised client will provide all of the arguments for rucio. Typing in the command followed by -h, or --help will provide you with all the options that are available as well as some explaination for each.\nping Is the simplest command that a user can use to ask the Rucio server which version it is using.\n$ rucio ping This checks that there is a connection between the containerised client and the server.\nwhoami Another simple command, which asks the server for the information Rucio has on the current user.\n$ rucio whoami This will return output like the following:\nstatus : ACTIVE account : user1 account_type : USER created_at : YYYY-MM-DDTHH:MM:SS updated_at : YYYY-MM-DDTHH:MM:SS suspended_at : None deleted_at : None email : myemail@domail.country This ensures that you know which user you are interacting with Rucio as, this is very important if you get multiple accounts. But also verifies that the client is set up correctly.\nupload A Rucio command that allows you to upload files from your current environment to any RSE within your VO.\n$ rucio upload [-h] --rse RSE [--lifetime LIFETIME] [--scope SCOPE] [--impl IMPL] [--register-after-upload] [--summary] [--guid GUID] [--protocol PROTOCOL] [--pfn PFN] [--name NAME] [--transfer-timeout TRANSFER_TIMEOUT] [--recursive] args [args ...] Several of the options you will not need to use as they will be set by the Rucio VO Admins when they set up the RSEs. Below are a list of options that you may find useful:\nRSE is the Rucio Storage Element or site that you wish to store the data at, the list of available RSEs can be seen for your VO with the command rucio list-rses. Lifetime is how long you wish the file to exist, not specifyting will make the file permenant until rucio is told to delete it. Scope can be used in many ways, but often can be an experiment name, or a user space, all users have their own scope assigned to them user.\u003cusername\u003e. Register-after-upload allows for files to be uploaded to the destination, and then registered with Rucio, rather than the other way around. This can be useful if your connection is intermittent. Name is the name of the file that it will be registered to Rucio with, if this is not set it will be the name of the file or files provided. Recursive Allows you to set the arg to a directory, and all files within that directory and any sub directories will be uploaded. Args is the path to the file, or files you wish to upload, this can be a single file, directory (with recursive set), or a list of files seperated with a space e.g. rucio upload --rse main-rse file1 file2 file3 file4 get A Rucio command to download files from any RSE in your VO to your local environment.\n$ rucio get [-h] [--dir DIR] [--allow-tape] [--rse RSE] [--rses RSES] [--impl IMPL] [--protocol PROTOCOL] [--nrandom NRANDOM] [--ndownloader NDOWNLOADER] [--no-subdir] [--pfn PFN] [--archive-did ARCHIVE_DID] [--no-resolve-archives] [--ignore-checksum] [--transfer-timeout TRANSFER_TIMEOUT] [--transfer-speed-timeout TRANSFER_SPEED_TIMEOUT] [--aria] [--filter FILTER] [--scope SCOPE] [--metalink METALINK_FILE] [--deactivate-file-download-exceptions] [dids [dids ...]] Dir is the location within the container you wish for the files to be downloaded (if you wish to move these files outside of the container, you may want to mount a volume in the container to allow the files to persist). RSE(s) specifying which RSE(s) you wish to download the files from, leaving this blank will allow Rucio to decide which RSE(s) are best. nrandom allows you to specify a number and if the target is a dataset or container will download n files from that DID. This allows you to check are correct before commiting to download the entire dataset or container. dids is the data identifier for the file, dataset or container you wish to download. add-rule Create a rule which Rucio will work to make true. These are often how files are moved from site to site. Creating a rule that says file x (which is currently at storagesite1), needs to be at storagesite2. Upon creation of the rule, Rucio will ensure that the file is moved from where is closest to the new site.\n$ rucio add-rule [-h] [--weight WEIGHT] [--lifetime LIFETIME] [--grouping {DATASET,ALL,NONE}] [--locked] [--source-replica-expression SOURCE_REPLICA_EXPRESSION] [--notify NOTIFY] [--activity ACTIVITY] [--comment COMMENT] [--ask-approval] [--asynchronous] [--delay-injection DELAY_INJECTION] [--account RULE_ACCOUNT] [--skip-duplicates] dids [dids ...] copies rse_expression lifetime How long you want the file to persist before it can be deleted by Rucio. locked sets the dataset or container to a locked state, that prevents other files from being added or removed. dids the files within Rucio you wish to be replicates. copies How many copies of the data you want to make. rse_expression can either be a specific RSE, or can be a filter. expression, such as tape=True or country=UK and Rucio will place as many copies as was requested in different sites (when possible), to fulfil the rule. delete-rule A command to delete a rule which you have created. Just because you have deleted a rule does not mean the file will be deleted. But it will adjust your quota accordingly. Other people within your VO may also having a rule that states the file needs to be at the same site.\n$ rucio delete-rule [-h] [--purge-replicas] [--all] [--rse_expression RSE_EXPRESSION] [--rses RSES] [--account RULE_ACCOUNT] rule_id all should not be used by users it it will attempt to delete all rules. rse_expression which RSE expression encapsulates the rules you wish to delete, either rse_expression, or RSE needs to be specified. RSES exactly which RSE is the target of the rule deletion. account which account requres the rule to be deleted, this is generally only needed by Rucio Admins and does not need to be specified if you are deleting your own rules. rule_id is a Rucio specific ID for the file that you wish to be deleted, a list of the rules, and their rule_ids that are within your account can be gotten by running rucio list-rules --account youraccountname. list-rules A command to list all the rules related to an account, a DID, or a file.\n$ rucio list-rules [-h] [--id RULE_ID] [--traverse] [--csv] [--file FILE] [--account RULE_ACCOUNT] [--subscription ACCOUNT SUBSCRIPTION] [did] Provide a full list of the files IDs, account, scope, state, RSE/expression copies and expiry.\naccount specify which account you wish to see the replication rules. file If you know the name of a specif file, this allows you to see all the rules are associated with the file. did If a dataset or container are listed, all rules associated with the specific DID will be displayed. ","categories":"","description":"The most common Rucio commands","excerpt":"The most common Rucio commands","ref":"/users/data/management/rucio/commands/","tags":"","title":"Rucio Command-Line Interface"},{"body":"The EGI documentation is a static site built using Hugo from Markdown source files. Hugo uses goldmark to parse and render markdown, which is compliant with CommonMark and GitHub Flavored Markdown (also based on CommonMark).\nThe EGI documentation is organized into sections and pages. Read below to uderstand when to use each of these, and how to create new sections and add new pages to a section.\nImportant Avoid using pages in the documentation for now, create a distinct section for every page, because currently there is no way to automatically validate links in pages. Sections Sections are those pages that can have subpages. They always appear in bold in the left-side navigation tree: Sections are also pages, meaning that selecting them in the navigation tree will show their content.\nCreating sections To create a new section, create a folder under /content/\u003clanguage\u003e/, add a file named _index.md to it, then author content for it as described below.\nNote Sections immediately under /content/\u003clanguage\u003e/ can show up in the top-level navigation bar. See below for details on how to control this. Pages Pages are Markdown files that contain the documentation about a specific topic. They hold the content for a section (in which case are named _index.md and the containing folder is the section), or a stand-alone page that is immediately under a section (the containing folder is the section).\nThis is how stand-alone pages appear in the left-side navigation tree: Creating pages Creating a documentation page is done by creating a Markdown file (with .md extension) under the relevant section (in the section’s folder).\nNote When authoring pages please observe and adhere to the Style Guide. Page metadata Each page needs some metadata that controls where the page appears and how its content will be rendered. The beginning of the Markdown file contains a front matter in YAML, holding the metadata of the page:\n--- title: \"Style\" linkTitle: \"Style Guide\" description: \"Style guide for EGI documentation\" type: docs weight: 30 --- The parameter weight controls the order of the pages on the same level in the left-side navigation tree. Pages will appear in ascending order of their weight.\nThe above metadata produces the following result (relevant elements highlighed): Add page to top navigation bar Pages can be added to the top navigation bar by using a front matter like this:\n--- title: \"About\" description: \"About EGI Documentation\" type: docs menu: main: weight: 50 --- Pages will be added to the top navigation bar in ascending order of their menu weight, from left to right.\nIf you also want to add an icon to the entry in the top navigation bar:\n--- title: \"About\" description: \"About EGI Documentation\" type: docs menu: main: weight: 50 pre: \u003ci class='fa fa-info'\u003e\u003c/i\u003e --- Embedding images (or other content) Hugo organizes content for each page into a subfolder with the same name as the page’s filename. This allows authors to easily keep track of the resources used by each page.\nLet’s assume we have a section named About with a subpage Concepts, using the following hierarchy of files:\n/documentation /content /en /about _index.md concepts.md map.png /concepts metadata.png Embedding an image in the page of the section (the file _index.md) can be done with:\n![Image title](map.png) Embedding an image in a subpage can be done by editing the Markdown file of the page (concepts.md in our case):\n![Image title](metadata.png) ![Image title](../map.png) Note For subpages, the (relative) path to the image already includes a folder with the same name as the file’s base name. Linking to pages You can include hyperlinks in the documentation that will link to any documentation page, or to external resources.\nAssuming we have the same section named About with a subpage Concepts as used above, and the file _index.md contains this:\nThis is a link to a [page](concepts) This is another link to the same [page](concepts.md) while the page concepts.md contains this:\nThis is link to a [section](../) This another link to a [section](../../about) the links in these pages exemplify how to link to a different page, be it a section or a stand-alone page.\nNote When linking to a stand-alone page, if you omit the file extension, .md will be assumed. Linking to documentation headings You can also include hyperlinks in the documentation that will link to any heading (aka chapter) from any documentation page.\nTo link to a heading in a documentation page, you need to point the hyperlink to the anchor created automatically by Markdown for the targeted heading.\nNote You can derive the anchor from the name of the target heading, by converting it to lowercase, removing non-alphanumeric characters, replacing spaces with dashes, and keeping exactly one dash separating each subsequent word pair in the anchor. Below you have an example of a page with sample headings and links to those headings:\nThis is a link to a [chapter](#some-chapter-title) This is a link to another [subchapter](#some-subchapter-title) ## Some chapter title ... ### Some subchapter title ... Assuming we have the same section named About with a subpage Concepts as used above, and the file _index.md contains this:\n## Section chapter This is a link to a [page chapter](concepts#page-chapter) This is another link to the same [page chapter](concepts.md#page-chapter) while the page concepts.md contains this:\n## Page chapter This s link to a [section chapter](../#section-chapter) This s link to a [section chapter](../../about#section-chapter) the links in these pages exemplify how to link to headings in a different page or section.\nGlossary The EGI Glossary is available on the EGI Glossary space.\n","categories":"","description":"Concepts used when writing documentation","excerpt":"Concepts used when writing documentation","ref":"/about/concepts/","tags":"","title":"Concepts"},{"body":"Overview The data management services of EGI comprises two groups of services:\nServices that provide data management capabilities to enhance the raw storage available in the EGI infrastructure Specialized services that offer advanced organisation of data during ongoing research projects, as an integrated environment with data management and digital lab notebook The EGI data management services offer both application programming inferfaces (APIs) and command-line interfaces (CLIs) that are integrated with the advanced EGI services and platforms (such as development environments, machine learning, or cloud orchestrators), and can be accessed from most compute services.\nGeneric data management These higher-level data management services are available to researchers:\nEGI Rucio is tailored to medium/big scientific collaborations, allowing users to organise, manage, and access their data at scale. Data can be distributed across heterogeneous data centers at widely distributed locations. EGI DataHub is a high-performance data management solution that offers unified data access across multiple types of underlying storage, allowing users to share, collaborate and easily perform computations on the stored data. Specialized data management The following specialized data management services are also available:\nEGI Data Transfer is a low-level service to move data from one Grid or Object storage to another. It is used internally by Rucio to schedule transfers based on the data policies defined by the users. openRDM is a combined FAIR data management platform, Electronic Laboratory Notebook (ELN) and Inventory Management System allowing a complete overview of workflows and information, from initial data generation to data analysis and publication. Use-cases for storing and managing research data Depending on the type of the employed compute services and the use-cases being addressed, users might need to choose different data service to store, access, and manage data.\nUser Data storage Data management (optional) Cloud user Block and Object storage DataHub HTC user Grid storage Rucio HPC user High-performance parallel file systems or Object storage DataHub or Rucio The following sections offer detailed descriptions for each data management service.\n","categories":"","description":"Data management services in the EGI infrastructure\n","excerpt":"Data management services in the EGI infrastructure\n","ref":"/users/data/management/","tags":"","title":"Data Management"},{"body":"What is it? EGI DataHub is a high-performance data management solution that offers unified data access across globally distributed environments and multiple types of underlying storage. It allows researchers to share, collaborate and perform computations on the stored data easily.\nUsers can bring data close to their community or to the compute facilities they use, in order to exploit it efficiently. This is as simple as selecting which (subset of the) data should be available at which supporting provider.\nThe main features of DataHub are:\nDiscovery of data spaces via a central portal Policy based data access Replication of data across providers for resiliency and availability purposes Integration with EGI Check-in allows access using comunity credentials, including from other EGI services and components File catalog to track replication of data and manage logical and physical files EGI DataHub supports multiple access policies:\nUnauthenticated, open access Access after user registration or Access restricted to members of a Virtual Organization (VO) Data replication in EGI DataHub may take place either on-­demand or automatically. Replication uses a file catalogue to enable tracking of logical and physical copies of data.\nNote EGI DataHub is based on the Onedata technology. Motivations Putting up a (scalable) distributed data infrastructure needs specific expertise, resources and knowledge No easy way to discover and transfer data No easy way of making data (publicly) accessible without transferring it with a sharing service No easy way of combining multiple datasets from different data providers Users need to access data locally and from compute resources Components and concepts The following concepts (components) will help you undesrtand how EGI DataHub works.\nSpace Virtual volume where users will organize their data. A space is supported by one or more Oneproviders that provide the actual storage resources.\nOneprovider Data management component deployed in the data centres, provisioning data and managing transfers. A Oneprovider is typically deployed at a site near the local storage resources, and can access local storage resources over multiple connectors (e.g. CEPH, POSIX). A default one is operated for EGI by CYFRONET.\nOnezone Central component for federating providers. It takes care of Authentication and Authorization and other management tasks (like space creation). EGI DataHub is a Onezone instance.\nEGI DataHub The central Onezone instance of the EGI Federation. Single Sign On (SSO) with all the connected storage providers (Oneprovider) is guaranteed through EGI Check-in\nOneclient Client application providing access to the spaces through a Linux FUSE mount point (local POSIX access), as if they were part of the local file system. Oneclient can be used from VMs, containers, desktops, etc.\nNote Web interfaces and APIs are also available. Highlighted features Using the EGI DataHub web interface it's possible to manage the space.\nUsing Oneclient it's possible to mount a space locally, and access it over a POSIX interface, using files as they were stored locally. The file's blocks are downloaded on demand.\nIn Onedata the file distribution is done on a block basis, blocks will be replicated on the fly, and it's possible to instrument the replication between Oneproviders.\nThree different formats of metadata can be attached to files: basic (key-value), JSON and RDF. The metadata can be managed using the Web interface and the APIs. It's also possible to create indexes and query them.\nIt's possible to view the popularity of a file and manage smart caching.\n","categories":"","description":"Discover, manage, and replicate data with EGI DataHub","excerpt":"Discover, manage, and replicate data with EGI DataHub","ref":"/users/data/management/datahub/","tags":"","title":"EGI DataHub"},{"body":"The EGI Docker VA is a ready-to-use Virtual Machine Image with docker and docker-compose pre-installed.\nYou can start that image as any other VA available from AppDB:\nGo to the EGI Docker image entry in AppDB.\nCheck the identifiers of the endpoint, image and flavor you want to use at the provider.\nUse a ssh key when, so you can log into the VM once it's instantiated.\nOnce up, just ssh in the VM and start using docker as usual.\nUsing docker from inside the VM You can log in with user ubuntu and your ssh key:\nssh -i \u003cyourprivatekey\u003e ubuntu@\u003cyour VM ip\u003e Once in, you can run any docker command, e.g.:\nubuntu@fedcloud_vm:~$ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b901d36b6f2f: Pull complete 0a6ba66e537a: Pull complete Digest: sha256:8be990ef2aeb16dbcb9271ddfe2610fa6658d13f6dfb8bc72074cc1ca36966a7 Status: Downloaded newer image for hello-world:latest Hello from Docker. This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker Hub account: https://hub.docker.com For more examples and ideas, visit: https://docs.docker.com/userguide/ Docker-compose can be also used to execute applications with more than one container running together, follow this documentation to learn more.\nKnown issues MTU and docker Depending on the cloud provider you may encounter unexpected network connectivity issues when working inside the docker container. If that is the case, please try reconfiguring the MTU for the docker daemon:\n# check current MTU value sudo docker network inspect bridge | grep mtu | awk '{print $2}' # the default 1500 value does not work properly # edit docker configuration vi /etc/docker/daemon.json # ensure MTU value is 1376 { \"mtu\": 1376 } # then restart docker sudo systemctl restart docker We experienced this issue when trying to install a pip dependency using continuumio/miniconda3 container from docker hub.\n","categories":"","description":"Run containers on the EGI Cloud on a single VM with Docker\n","excerpt":"Run containers on the EGI Cloud on a single VM with Docker\n","ref":"/users/compute/cloud-container-compute/docker/","tags":"","title":"Docker VM"},{"body":"Introduction A ROD team’s duties can be split into three main areas: handling alarms and tickets, handling downtimes, and communicating urgent issues to the EGI Operations and CSIRT teams.\nHandling alarms and tickets The main responsibility of ROD is to deal with alarms and tickets issued for sites in the region. This includes making sure that the tickets are created and handled properly.\nThe ROD on duty is required to:\ncheck alarm notifications in the Dashboard at least twice a day; close alarms which are in the OK state; handle non-OK alarms less than 24 hours old (notify the site administrators according to your NGI’s procedures); create tickets for alarms older then 24 hours that are not in an OK state; escalate tickets to NGI Management/EGI Operations if necessary (in the Dashboard); monitor and update any GGUS tickets up to the solved status (preferably via the Dashboard); handle the final state of GGUS tickets not opened from the Dashboard by changing their status to verified. Putting a site in downtime for urgent matters ROD can place a site or a service endpoint (there can be multiple services running on a single host) in downtime in the GOCDB if it is either requested by the site, or if ROD sees an urgent need to do it.\nNote: This is actually optional; an NGI may decide on a different policy if the site admins are not happy with ROD setting downtimes for them. However, it should be considered mandatory in case of urgent security incidents.\nROD may also suspend a site, under exceptional circumstances, without going through all the steps of the escalation procedure. For example, if a security hazard occurs, ROD must suspend a site on the spot in the case of such an emergency. It is important to know that EGI Operations can also suspend a site in the case of an emergency, for example as a result of a security incident or lack of response.\nIn both scenarios, it is important that ROD communicates their actions to all involved parties.\nNotifying EGI Operations and EGI CSIRT about urgent matters ROD should create tickets to EGI Operations in the case of urgent matters. For security related issues, ROD should also notify the CSIRT duty contact.\nROD is also responsible for propagating actions from EGI Operations down to sites (this occurs rather infrequently, though).\n","categories":"","description":"Description of duties of regional operators","excerpt":"Description of duties of regional operators","ref":"/providers/rod/duties/","tags":"","title":"Duties"},{"body":"EGI SSO is a legacy central Identity Provider mainly intended to access the EGI Collaboration Tools.\nFor most services people should look into using federated authentication via EGI Check-in. Still, for some services not yet integrated with Check-in, an EGI SSO account should be used, in that case you can create an EGI SSO account.\nFeatures Central Identity Provider allowing to access EGI Collaboration Tools Group management Synchronisation of groups with other EGI Collaboration Tools Linking of X.509 certificate Distinguished Name (DN) to authenticate using a client certificate Linking an X.509 certificate to an EGI SSO account Open the EGI SSO user management page Select your new certificate to authenticate with it Complete the login process using your username and password Once logged, your new DN will be linked to your user account Updating the DN of a certificate linked to a given EGI SSO account On purpose a user cannot manually edit a certificate DN\nYou usually just need to link your new DN to your EGI SSO account, as documented above.\nYou may have to do this by in a private browser session to ensure that your previous certificate is used.\nOnce you have done this, you can request us to remove the former DN by opening an Helpdesk ticket to the Collaboration Tools Support Unit.\nLegacy DNs shouldn’t prevent you accessing any service with a newer certificate, as long as its DN is linked to your account.\n","categories":"","description":"EGI SSO usage","excerpt":"EGI SSO usage","ref":"/internal/collaboration-tools/sso/","tags":"","title":"EGI SSO"},{"body":"Welcome to the home page of Service Providers. This section provides documentation aimed at Service Providers across the EGI Federation.\nSee the users section for user-centric documentation.\nJoining the EGI Infrastructure Any service provider can join the EGI Federation and in doing so benefit from being part of the major worldwide e-infrastructure that is EGI. From a technical point of view, the Federation may be joined by integrating your resources with federation services such as Check-in, the EGI Federation Authentication and Authorisation Infrastructure (AAI) service. This may be done at no cost, and resources can immediately benefit from EGI federation services.\nIn order to make a difference in helping to define the strategic direction of the EGI Federation and its federation services, countries or international organisations can join the EGI Council. More details may be found on the EGI site.\nProviders have different options to become members of the EGI Federation, and deliver high-quality services for advanced computing within the EGI Infrastructure. The options are documented below, linking to detailed instructions:\nJoining as a federated resource centre, becoming a provider (that we call Resource Centre) delivering one of the following services: Cloud Compute, HTC Compute, Cloud Container Compute, or Online Storage. More than 200 Resource Centres have been integrated in this way. Joining as a software provider, also called Technology Provider, which is providing middleware deployed on the federated resource centres. Joining as a new provider for an existing service in the Compute and Data Federation or the Platform services, such as DataHub, Notebooks, Training Infrastructure, and Workload Manager Joining as Core/Central service provider for services supporting all the other services of the EGI Infrastructure. Contributing a new service, enabling advanced computing in research and education, and not yet present in the EGI service portfolio. The Operations-related documentation will help you starting with EGI Operations duties.\nIntegrating a service with EGI Check-in If you are interested in integrating your service with Check-in, head to the Check-in for service providers! If you are willing to connect your Identity Providers and allowing your users to access services via Check-in, look at the documentation for Check-in for Identity Providers! ","categories":"","description":"Documentation for service providers","excerpt":"Documentation for service providers","ref":"/providers/","tags":"","title":"Service Provider Guides"},{"body":"What is it? Grid storage enables storage of files in a fault-tolerant and scalable environment, and sharing it with distributed teams. Your data can be accessed through multiple protocols, and can be replicated across different providers to increase fault-tolerance. Grid storage gives you complete control over what data you share, and with whom you share the data.\nThe main features of grid storage:\nAccess highly-scalable storage from anywhere Control the data you share Organise your data using a flexible, hierarchical structure Grid storage file access is based on the gridFTP and WebDav/HTTP protocols, together with XRootD and legacy SRM (under deprecation at some of the endpoints).\nSeveral grid storage implementations are available in the EGI Infrastructure, the most common being:\ndCache DPM (N.B. under decommissioning) StoRM EOS Endpoint discovery The grid storage endpoints that are available to a user’s Virtual Organizations are discoverable via the EGI Information System (BDII).\nThe lcg-infosites command can be used to obtain VO-specific information on existing grid storages, using the following syntax:\n$ lcg-infosites --vo voname -[v] -f [site name] [option(s)] [-h| --help] [--is BDII] For example, to list the Storage Elements (SEs) available to the biomed VO, we could issue the following command:\n$ lcg-infosites --vo biomed se Avail Space(kB) Used Space(kB) Type SE ------------------------------------------ 280375465082 n.a SRM ccsrm.ihep.ac.cn 10995116266 11 SRM cirigridse01.univ-bpclermont.fr Access from the command-line Access to grid storage via a command-line interface (CLI) requires users to obtain a valid X.509 user VOMS proxy. Please refer to the Check-in documentation for more information.\nNote Integration via OpenID Connect to the EGI Check-in service is under piloting at some of the endpoints of the EGI Cloud infrastructure , but it has not yet reached the production stage. The CLI widely used to access grid-storage is gfal2, which is available for installation both on RHEL and Debian compatible systems.\nIn particular, gfal2 provides an abstraction layer on top of several storage protocols (XRootD, WebDAV, SRM, gsiftp, etc), offerint a convenient API that can be used over different protocols.\nThe gfal2 CLI can be installed as follows (for RHEL compatible systems):\n$ yum install gfal2-util gfal2-all where gfal2-all will install all the plug-ins (to deal with all the available protocols).\nBelow you can find examples of the usual commands needed to access storage via gfal2. For a complete list of available commands, and the guide on how to use them, please refer to the gfal2-util documentation.\nNote In the examples below, the used gsiftp protocol can be replaced by any other supported protocol. List files on a given endpoint $ gfal-ls gsiftp://dcache-door-doma01.desy.de/dteam 1G.header-1 domatest gb SSE-demo test tpctest Create a folder $ gfal-mkdir gsiftp://dcache-door-doma01.desy.de/dteam/test Copy a local file $ gfal-copy test.json gsiftp://dcache-door-doma01.desy.de/dteam/test Copying file:///root/Documents/test.json [DONE] after 0s Copy files between storages $ gfal-copy gsiftp://prometheus.desy.de/VOs/dteam/public-file gsiftp://dcache-door-doma01.desy.de/dteam/test Copying gsiftp://prometheus.desy.de/VOs/dteam/public-file [DONE] after 3s Download a file to a local folder $ gfal-copy gsiftp://prometheus.desy.de/VOs/dteam/public-file /tmp Copying gsiftp://prometheus.desy.de/VOs/dteam/public-file [DONE] after 0s Delete a file $ gfal-rm gsiftp://dcache-door-doma01.desy.de/dteam/test/public-file gsiftp://dcache-door-doma01.desy.de/dteam/test/public-file DELETED Access via EGI Data Transfer The EGI Data Transfer service provides mechanisms to optimize the transfer of files between EGI Online Storage endpoints. Both a graphical user interface (GUI) and command-line interfaces (CLI) are available to perform bulk movement of data. Please check out the related documentation for more information.\nIntegration with Data Management frameworks Grid storage access, most of the time, is hidden from users by the integration with the Data Management Frameworks (DMFs) used by Collaborations and Experiments.\nFor example, EGI Workload Manager provides a way to efficiently access grid storage endpoints in order to read/store files, and to catalogue the existing file and related metadata.\nWhen running computation via the EGI Workload Manager, users do not actually access the storage directly. However, users can retrieve the output of the computation once it has been stored on the grid.\n","categories":"","description":"Grid Storage offered by EGI HTC providers\n","excerpt":"Grid Storage offered by EGI HTC providers\n","ref":"/users/data/storage/grid-storage/","tags":"","title":"Grid Storage"},{"body":"Templates We will build a torque cluster on one of the EGI Cloud providers using EC3. Create a directory to store EC3 configuration and init it with the FedCloud client:\nmkdir -p torque cd torque fedcloud ec3 init --site \u003cyour site\u003e --vo \u003cyour vo\u003e We will use the following templates:\ntorque (from ec3 default templates) nfs (from ec3 detault templates), ubuntu-1604 (user’s template), cluster_configure (user’s template) You can find the content below (make sure that you adapt them to your needs):\ntemplates/ubuntu-1604.radl specifies the VM image to use in the deployment:\ndescription ubuntu-1604 ( kind = 'images' and short = 'Ubuntu 16.04' and content = 'FEDCLOUD Image for EGI Ubuntu 16.04 LTS [Ubuntu/16.04/VirtualBox]' ) system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 4 and memory.size \u003e= 8196 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://\u003curl\u003e/\u003cimage_id\u003e' and disk.0.os.credentials.username = 'ubuntu' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048m and ec3_max_instances = 10 and # maximum number of working nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://\u003curl\u003e/\u003cimage_id\u003e' and disk.0.os.credentials.username = 'ubuntu' ) templates/cluster_configure.radl customises the torque deployment to match our needs:\nconfigure front ( @begin --- - vars: - USERS: - { name: user01, password: \u003cPASSWORD\u003e } - { name: user02, password: \u003cPASSWORD\u003e } [..] tasks: - user: name: \"{{ item.name }}\" password: \"{{ item.password }}\" shell: /bin/bash append: yes state: present with_items: \"{{ USERS }}\" - name: Install missing dependences in Debian system apt: pkg={{ item }} state=present with_items: - build-essential - mpich - gcc - g++ - vim become: yes when: ansible_os_family == \"Debian\" - name: SSH without password include_role: name: grycap.ssh vars: ssh_type_of_node: front ssh_user: \"{{ user.name }}\" loop: '{{ USERS }}' loop_control: loop_var: user - name: Updating the /etc/hosts.allow file lineinfile: path: /etc/hosts.allow line: 'sshd: XXX.XXX.XXX.*' become: yes - name: Updating the /etc/hosts.deny file lineinfile: path: /etc/hosts.deny line: 'ALL: ALL' become: yes @end ) configure wn ( @begin --- - vars: - USERS: - { name: user01, password: \u003cPASSWORD\u003e } - { name: user02, password: \u003cPASSWORD\u003e } [..] tasks: - user: name: \"{{ item.name }}\" password: \"{{ item.password }}\" shell: /bin/bash append: yes state: present with_items: \"{{ USERS }}\" - name: Install missing dependences in Debian system apt: pkg={{ item }} state=present with_items: - build-essential - mpich - gcc - g++ - vim become: yes when: ansible_os_family == \"Debian\" - name: SSH without password include_role: name: grycap.ssh vars: ssh_type_of_node: wn ssh_user: \"{{ user.name }}\" loop: '{{ USERS }}' loop_control: loop_var: user - name: Updating the /etc/hosts.allow file lineinfile: path: /etc/hosts.allow line: 'sshd: XXX.XXX.XXX.*' become: yes - name: Updating the /etc/hosts.deny file lineinfile: path: /etc/hosts.deny line: 'ALL: ALL' become: yes @end ) Create the cluster Deploy the cluster using ec3 docker image:\n$ docker run -it -v $PWD:/root/ -w /root \\ grycap/ec3 launch torque_cluster \\ torque nfs ubuntu-1604 refresh cluster_configure \\ -a auth.dat Creating infrastructure Infrastructure successfully created with ID: 529c62ec-343e-11e9-8b1d-300000000002 Front-end state: launching Front-end state: pending Front-end state: running IP: 212.189.145.XXX Front-end configured with IP 212.189.145.XXX Transferring infrastructure Front-end ready! To access the cluster, use the command:\n$ docker run -ti -v $PWD:/root/ -w /root grycap/ec3 ssh torque_cluster Warning: Permanently added '212.189.145.140' (ECDSA) to the list of known hosts. Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 3.13.0-164-generic x86_64) * Documentation: https://help.ubuntu.com/ Last login: Tue Feb 19 13:04:45 2019 from servproject.i3m.upv.es Configuration of the cluster Enable Password-based authentication Change settings in /etc/ssh/sshd_config\n# Change to no to disable tunnelled clear text passwords PasswordAuthentication yes and restart the ssh daemon:\nsudo service ssh restart Configure the number of processors of the cluster $ cat /var/spool/torque/server_priv/nodes wn1 np=XX wn2 np=XX [...] To obtain the number of CPU/cores (np) in Linux, use the command:\n$ lscpu | grep -i CPU CPU op-mode(s): 32-bit, 64-bit CPU(s): 16 On-line CPU(s) list: 0-15 CPU family: 6 Model name: Intel(R) Xeon(R) CPU E5520 @ 2.27GHz CPU MHz: 2266.858 NUMA node0 CPU(s): 0-3,8-11 NUMA node1 CPU(s): 4-7,12-15 Test the cluster Create a simple test script:\n$ cat test.sh #!/bin/bash #PBS -N job #PBS -q batch #cd $PBS_O_WORKDIR/ hostname -f sleep 5 Submit to the batch queue:\nqsub -l nodes=2 test.sh Destroy the cluster To destroy the running cluster, use the command:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy torque_cluster WARNING: you are going to delete the infrastructure (including frontend and nodes). Continue [y/N]? y Success deleting the cluster! ","categories":"","description":"Using Elastic Cloud Computing Cluster (EC3) platform to create elastic virtual clusters on the EGI Cloud.\n","excerpt":"Using Elastic Cloud Computing Cluster (EC3) platform to create elastic …","ref":"/users/compute/orchestration/ec3/apps/htc/","tags":"","title":"HTC"},{"body":"Notebooks running on EGI can access other existing computing and storage services from EGI or other e-Infrastructures. For data services, check data section of the documentation\nEGI services: access tokens Most services integrated with EGI Check-in can handle valid access tokens for authorising users. These are short-lived (normally less than 1-hour) and need to be renewed for longer usage. EGI Notebooks provides a ready to use access token that can be accessed from your notebooks and is automatically refreshed so you can always have a valid one.\nThe token is available at /var/run/secrets/egi.eu/access_token and you can use it for example to access cloud providers of the EGI cloud. See the following sample code where a list of VMs is obtained for CESGA:\nfrom keystoneauth1.identity import v3 from keystoneauth1 import session from novaclient import client with open(\"/var/run/secrets/egi.eu/access_token\") as f: access_token = f.read() auth = v3.OidcAccessToken(auth_url=\"https://fedcloud-osservices.egi.cesga.es:5000/v3\", identity_provider=\"egi.eu\", protocol=\"openid\", project_id=\"3a8e9d966e644405bf19b536adf7743d\", access_token=access_token) sess = session.Session(auth=auth) nova = client.Client(session=sess, version=2) nova.servers.list() A valid ID token is also available at /var/run/secrets/egi.eu/id_token.\nfedcloud client A direct benefit of the integration with access tokens in EGI Notebooks is that you can easily work with the fedcloud client. Once logged into the EGI Notebooks open a terminal and run:\nexport OIDC_ACCESS_TOKEN=`cat /var/run/secrets/egi.eu/access_token` fedcloud token check If the fedcloud command is not available, please follow the getting started guide to get it.\nD4Science If you are using a Notebooks instance integrated with D4Science, you can easily invoke DataMiner or any other D4Science functionality as the service will provide the GCUBE_TOKEN environment variable with a valid token.\nThis code will print the list of DataMiner methods available within your VRE:\nimport os from owslib.wps import WebProcessingService # init http header parameter and base folders for gCube REST API gcube_vre_token_header = {'gcube-token': os.environ[\"GCUBE_TOKEN\"]} # init WPS access for DataMiner algorithms dataminer_url = 'http://dataminer-prototypes.d4science.org/wps/WebProcessingService' wps = WebProcessingService(dataminer_url, headers=gcube_vre_token_header) for process in wps.processes: print('- Name: ', process.title) DataMiner algorithms can be invoked also from Notebooks, this code shows a sample:\nfrom owslib.wps import ComplexDataInput, monitorExecution # define processid and inputs processid = 'org.gcube.dataanalysis.wps.statisticalmanager.synchserver.mappedclasses.transducerers.WOFOST_CLOUD_V0_2_1' inputs = [ ('ClassToRun', 'nl.wur.wofostsystem.App'), ('FileInput', ComplexDataInput( 'https://data.d4science.org/shub/E_eVhZTzBWWktOaVJxQjJkdTUxR3FHaTFFdE9BTDYrZkZxQnFWcGMyaVVJbXptejdDOEFpSVNmam82RllkRUJ6cA==', mimeType=\"text/xml\") ) ] # execute the process execution = wps.execute(processid, inputs) monitorExecution(execution, sleepSecs=5, download=True) print(execution.status) Note that inputs that point to a URL should be specified using the ComplextDataInput class as shown above.\nOther third-party services We are open for integration with other services that may be relevant for your research. Please contact support \u003cat\u003e egi.eu with your request so we can investigate the best way to support your needs.\n","categories":"","description":"Access new services from the Notebooks\n","excerpt":"Access new services from the Notebooks\n","ref":"/users/dev-env/notebooks/integration/","tags":"","title":"Notebooks Integration with Other Services"},{"body":"Introduction Technology Providers (TP) develop or deliver technology and software for specific user communities or customisation for specific requirements. In EGI case, they maintain the middleware which the RCs install and allowing the users to exploit the compute, storage, data, and cloud resources.\nIntegration of middleware stack To maintain an appropriate quality of service of the EGI production infrastructure, every middleware stack (Compute, Storage, etc.) installed on and delivered by the RCs must fulfil a number of requirements.\nNote Related procedure: PROC19 Integration of new cloud management framework or middleware stack in the EGI Infrastructure) PROC19 was defined to ensure that any single aspect of the integration of the new piece of middleware with the infrastructure is reviewed and assessed.\nAfter the creation of the request in the EGI Helpdesk, with details about the technology, the contacts, the expected customers, and the motivation, the integration steps cover the following areas (where possible, steps can be done in parallel):\nUnderpinning Agreement (UA) between EGI Foundation and the technology provider it could be either the Corporate-level Technology Provider Underpinning Agreement or a customised version. Configuration Management: mapping of the new technology in the Configuration Management Database (CMDB) Information System: evaluating if the new technology should publish information in the Information System according to the GLUE Schema. Monitoring: the new technology should allow external monitoring. If particular aspects of the technology need to be monitored, specific monitoring probes should be provided by the TPs and deployed on the EGI Monitoring service. Support: the Support Unit where incidents and service requests will be addressed needs to be defined in the EGI Helpdesk, associated to the Quality of Support defined in the UA. Accounting: the need to gather usage data, which depends on the technology itself and on the infrastructure requirements and will be published in the EGI Accounting Portal. Integration in UMD or CMD: the Unified Middleware Distribution (UMD) and Cloud Middleware Distribution (CMD) are the integrated sets of software components contributed by Technology Providers and packaged for deployment as production quality services in EGI. Documentation: exhaustive documentation for RC administrators and users should be provided and may be added to the EGI Documentation. Security: a security assessment of the software is required according to a number of guidelines defined by the EGI Security team. ","categories":"","description":"Guidelines for Technology Providers to join the EGI Infrastructure","excerpt":"Guidelines for Technology Providers to join the EGI Infrastructure","ref":"/providers/joining/technology-provider/","tags":"","title":"Joining as a Technology Provider"},{"body":"There are several container management tools available, on the EGI Cloud we use Kubernetes as the default platform for our service. This guide explains how to get a running scalable Kubernetes deployment for your applications with EC3.\nGetting started Before getting your kubernetes cluster deployed, you need to get access to the Cloud Compute service, check the Authentication and Authorisation guide for more information. You should also get the FedCloud client installed to get EC3 templates needed to start deployment.\nYour kubernetes deployment needs to be performed at an specific provider (site) and project. Discover them using fedcloud as described in the EC3 tutorial.\nEC3 Templates EC3 relies on a set of templates that will determine what will be deployed on the infrastructure. fedcloud helps you to get an initial set of templates for your kubernetes deployment:\nmkdir k8s cd k8s fedcloud ec3 init --site \u003cyour site\u003e --vo \u003cyour vo\u003e You will also need a base image template for the deployment. Please refer to the EC3 tutorial to create such file. Below you can see an example for IFCA-LCG2 site with project related to vo.access.egi.eu:\ndescription ubuntu-ifca ( kind = 'images' and short = 'ubuntu18-ifca' and content = 'Ubuntu 18 image at IFCA-LCG2' ) network public ( provider_id = 'external' and outports contains '22/tcp' ) network private (provider_id = 'provider-2008') system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://api.cloud.ifca.es/723171cb-53b2-4881-ae37-a7400ce0665b' and disk.0.os.credentials.username = 'cloudadm' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and ec3_max_instances = 5 and # maximum number of working nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://api.cloud.ifca.es/723171cb-53b2-4881-ae37-a7400ce0665b' and disk.0.os.credentials.username = 'cloudadm' ) Now you are ready to deploy the cluster using launch command of ec3 with:\nthe name of the deployment, k8s in this case\na list of templates: kubernetes for configuring kubernetes, ubuntu for specifying the image and site details, refresh to enable credential refresh and elasticity\nthe credentials to access the site with -a auth.dat\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 \\ launch k8s kubernetes ubuntu refresh -a auth.dat Creating infrastructure Infrastructure successfully created with ID: b9577c34-f818-11ea-a644-2e0fc3c063db Front-end configured with IP 193.144.46.249 Transferring infrastructure Front-end ready! Your kubernetes deployment is now ready, log in with the ssh command of ec3:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 ssh k8s Warning: Permanently added '193.144.46.249' (ECDSA) to the list of known hosts. Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-109-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage * Kubernetes 1.19 is out! Get it in one command with: sudo snap install microk8s --channel=1.19 --classic https://microk8s.io/ has docs and details. * Canonical Livepatch is available for installation. - Reduce system reboots and improve kernel security. Activate at: https://ubuntu.com/livepatch Last login: Wed Sep 16 14:35:35 2020 from 158.42.104.204 $ bash cloudadm@kubeserver:~$ You can interact with the kubernetes cluster with the kubectl command:\ncloudadm@kubeserver:~$ sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kubeserver.localdomain Ready master 23h v1.18.3 The cluster will only have one node (the master) and will start new nodes as you create pods. Alternatively you can poweron nodes manually:\ncloudadm@kubeserver:~$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn2.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn3.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn4.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn5.localdomain off enabled 00h32'22\" 0,0 1,1073741824 cloudadm@kubeserver:~$ clues poweron wn1.localdomain node wn1.localdomain powered on cloudadm@kubeserver:~$ sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kubeserver.localdomain Ready master 24h v1.18.3 wn1.localdomain Ready \u003cnone\u003e 6m49s v1.18.3 Exposing services outside the cluster Kubernetes uses services for exposing an applications via the network. The services can rely on Load Balancers supported at the underlying cloud provider, which is not always feasible on the EGI Cloud providers. As an alternative solution we use an ingress controller which allows us to expose services using rules based on hostnames.\nHelm allows you to quickly install the nginx based ingress. Add the helm repos:\ncloudadm@kubeserver:~$ sudo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx cloudadm@kubeserver:~$ sudo helm repo add stable https://kubernetes-charts.storage.googleapis.com/ cloudadm@kubeserver:~$ sudo helm repo update Create a configuration file (ingress.yaml), get the externalIP using ip addr:\ncontroller: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master service: type: NodePort externalIPs: - 192.168.10.3 defaultBackend: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master and install:\ncloudadm@kubeserver:~$ sudo helm install ingress -n kube-system -f ingress.yaml ingress-nginx/ingress-nginx Now you are ready to expose your services using a valid hostname. Use the Dynamic DNS service for getting hostnames if you need. Assign as IP the public IP of the master node. Once you have a hostname assigned to the master IP, the ingress will be able to reply to requests already:\n$ curl ingress.test.fedcloud.eu \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e404 Not Found\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e404 Not Found\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003enginx/1.19.2\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e The following example yaml creates a service and exposes at that ingress.test.fedcloud.eu host:\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: my-nginx labels: app: nginx spec: ports: - port: 80 protocol: TCP selector: app: nginx --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test spec: rules: - host: ingress.test.fedcloud.eu http: paths: - pathType: Prefix path: \"/\" backend: serviceName: my-nginx servicePort: 80 Now the ingress will redirect request to the NGINX pod that we have just created:\n$ curl ingress.test.fedcloud.eu \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Volumes Volumes on these deployments can be supported with NFS volume driver. You can either manually configure the server on one of the nodes or use EC3 to deploy it and configure it for you. Create a templates/nfs.radl to do so:\ndescription nfs ( kind = 'component' and short = 'Tool to configure shared directories inside a network.' and content = 'Network File System (NFS) client allows you to access shared directories from Linux client.' ) system front ( ec3_templates contains 'nfs' and disk.0.applications contains (name = 'ansible.modules.grycap.nfs') ) configure front ( @begin - tasks: - name: Create /volume for supporting NFS file: path: /volumes state: directory mode: '0777' - roles: - role: grycap.nfs nfs_mode: 'front' nfs_exports: - path: \"/volumes\" export: \"wn*.localdomain(rw,async,no_root_squash,no_subtree_check,insecure) kubeserver.localdomain(rw,async,no_root_squash,no_subtree_check,insecure)\" @end ) system wn (ec3_templates contains 'nfs') configure wn ( @begin - tasks: - name: Install NFS common apt: name: nfs-common state: present @end ) if you have a running cluster, you can add the NFS support by reconfiguring the cluster:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 \\ reconfigure k8s -a auth.dat -t nfs Reconfiguring infrastructure Front-end configured with IP 193.144.46.249 Transferring infrastructure Front-end ready! And then install the NFS driver in kubernetes with helm:\ncloudadm@kubeserver:~$ sudo helm install nfs-provisioner \\ stable/nfs-client-provisioner \\ --namespace kube-system \\ --set nfs.server=192.168.10.9 \\ --set storageClass.defaultClass=true \\ --set nfs.path=/volumes \\ --set tolerations[0].effect=NoSchedule,tolerations[0].key=node-role.kubernetes.io/master Now you are ready to create a PVC and attach it to a pod, see this example:\n--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: nfs-client accessModes: - ReadWriteOnce resources: requests: storage: 3Gi --- apiVersion: v1 kind: Pod metadata: name: pvc-pod namespace: default spec: restartPolicy: Never volumes: - name: vol persistentVolumeClaim: claimName: test-pvc containers: - name: test image: \"busybox\" command: [\"sleep\", \"1d\"] volumeMounts: - name: vol mountPath: /volume Once you apply the yaml, you will see the new PVC gets bounded to a PV created in NFS:\ncloudadm@kubeserver:~$ sudo kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-39f970de-eaad-44d7-b49f-90dc9de54a14 3Gi RWO nfs-client 9m46s Destroying the cluster Once you don’t need the cluster anymore, you can undeploy with the destroy command of EC3:\n$ fedcloud ec3 refresh # refresh your credentials to interact with the cluster $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy k8s -y -a auth.dat WARNING: you are going to delete the infrastructure (including frontend and nodes). Success deleting the cluster! ","categories":"","description":"Run containers on the EGI Cloud with Kubernetes\n","excerpt":"Run containers on the EGI Cloud with Kubernetes\n","ref":"/users/compute/cloud-container-compute/k8s/","tags":"","title":"Kubernetes"},{"body":" Linking new identities to your EGI Account Identity linking allows you to access EGI resources with your existing personal EGI ID, using any of the login credentials you have linked to your account. You can use any of your organisational or social login credentials for this purpose. To link a new organisational or social identity to your EGI account:\nEnter the following URL in a browser: https://aai.egi.eu/registry\nClick Login and authenticate using any of the login credentials already linked to your EGI account\nNavigate to My EGI User Community Identity page in one of the following ways:\nhover over your display name next to the gear icon on the top right corner of the page; or, alternatively, select EGI User Community from the list of available Collaborations and then click My EGI User Community Identity from the People menu Under the Organisational Identities section of your profile page, expand Actions menu and click Link New Identity.\nOn the introductory page for Identity Linking, click Begin\nYou will need to sign in using the login credentials from the institutional/social identity provider you want to link to your account.\nWarning It is very important to escape the identity provider selection, cached in the discovery page, before picking the new one. After successful authentication, the new Identity Provider will be available under the Organizational Identities tab and you’ll be able to access EGI resources with your existing personal EGI ID using the login credentials of the identity provider you selected in Step 6.\nLinking your certificate to your EGI Account Certificate linking allows you to add the subject DN of your certificate to your existing personal EGI ID. For this you need to import your certificate to your browser.\nTo link a subject DN to your EGI account:\nEnter the following URL in a browser: https://aai.egi.eu/registry\nClick Login and authenticate using any of the login credentials already linked to your EGI account\nNavigate to My EGI User Community Identity page in one of the following ways:\nhover over your display name next to the gear icon on the top right corner of the page; or, alternatively, select EGI User Community from the list of available Collaborations and then click My EGI User Community Identity from the People menu Under the Organisational Identities section of your profile page, expand Actions menu and click Link New Identity.\nOn the introductory page for Identity Linking, click Begin\nContinuously, you will need to sign in using the IGTF Certificate Proxy.\nWarning It is very important to escape the identity provider selection, cached in the discovery page, before picking the new one. Then select the certificate you want to link to your account from the popup window.\nAfter successful authentication you will be redirected back to your EGI Account. Also, you’ll be able to access EGI resources with your existing personal EGI ID using IGTF Certificate Proxy and your certificate.\nTo verify that the subject DN is added to your EGI account scroll down to Organisational Identities and click on view button in the row where the source is https://edugain-proxy.igtf.net/simplesaml/saml2/idp/metadata.php.\nThen scroll down to Certificates and you should see the subject DN of your certificate.\n","categories":"","description":"Linking additional organisational/social identities to your EGI Account\n","excerpt":"Linking additional organisational/social identities to your EGI …","ref":"/users/aai/check-in/linking/","tags":"","title":"Linking Identities"},{"body":"Mailing lists are managed by software called Mailman.\nThe software provides a web interface and a public list of mailing lists is available.\nRequesting the creation of a new mailing list By default all mailing lists are managed using groups in EGI SSO. Only for very specific they may be managed directly in mailman.\nOpen a ticket to Collaboration Tools SU in EGI Helpdesk, providing:\nList name List Description List administrator contact After validation of those information the list will be created.\nName and description of the mailing list will be validated, you may be recommended to change name or description to follow already existing best practices for naming, or to improve description.\nList administrators EGI mailing lists are usually managed by EGI SSO, with some exceptions for lists that include members without SSO accounts.\nList administrators of every SSO-managed list are set to the owners of the corresponding EGI SSO group when a new list is created.\nNote For SSO managed-lists: a change to the list of list administrators through the Mailman interface will not be permanent. Only the EGI IT support can change who are the owners of an SSO group, please contact us if you want to add a new list administrator.\nEach list can be administered over the web interface by following its link from the page Admin links. You can use your EGI SSO password to access the administration pages of the lists for which you are the administrator.\nList members Members of lists are synchronized with members of same named groups in the EGI SSO system.\nNote For SSO managed-lists: Changes made to list membership through the Mailman interface will not be permanent. Managing list members must be done by managing members of corresponding groups in EGI SSO. Log into your EGI SSO account, and you will see a list of groups of which you are the owner. Click on the “»manage” link, it will display a page where you can\nremove members from the group add existing users as members of thew group invite new users to the group If you manage more then one list, and you need to add many new users to several lists at once, it may be helpful to follow the link titled “»Create new users in groups”. It directs to a page where you can enter many addresses at once, and specify the groups to which they should be added. Existing users will be added immediately, non-existing users will be invited to create an account and they will be assigned to the groups when they create the account.\nDocumentation Mailman documentation Mailman Frequently Asked Questions List administrator tasks ","categories":"","description":"Mailing lists usage\n","excerpt":"Mailing lists usage\n","ref":"/internal/collaboration-tools/mailing-lists/","tags":"","title":"Mailing lists"},{"body":"Document control Property Value Title Service Intervention Management Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Managing Service interventions Owner SDIS team Service Intervention A service intervention is defined as an action which will involve or lead to the possibility of a loss, or noticeable degradation of a service. Depending on the planning of the outage, we have two types of intervention:\nScheduled interventions: planned and agreed in advance Unscheduled interventions: unplanned, usually triggered by an unexpected failure How to manage an intervention Interventions are recorded through the Configuration Database. For more information, have a look at the description.\nScheduled interventions Scheduled interventions MUST be declared at least 24 h in advance, specifying reason and duration. Existing scheduled interventions CAN be extended, provided that it’s done 24 hours in advance. Unscheduled interventions Any intervention declared less than 24 h in advance will be considered unscheduled. Sites MUST declare unscheduled interventions as soon as they are detected to inform the users. Unscheduled interventions CAN be declared up to 48 hours in the past (retroactive information to the user community). Required information The required information to fill in when declaring an intervention is:\nSeverity (Outage or Warning) Description Timezone Starting and ending dates Affected site / Affected services and endpoints Recommendations For interventions that impact end users, the downtime SHOULD be declared 5 working days in advance, specifying reason and duration. A post−mortem SHOULD be included in the downtime report. Notifications Intervention notifications (through broadcasts, RSS feeds, etc) as specified in the following procedures are automatically sent when declaring a downtime in Configuration Database: at declaration time, 24 h in advance and 1 h before the intervention.\nSuspension policy Sites on downtime for more than 1 month will be suspended/uncertified. AT_RISK downtime declarations are only for providing warnings to users, and are ignored for calculating site availability (actual status will be used).\n","categories":"","description":"How to service interventions","excerpt":"How to service interventions","ref":"/providers/operations-manuals/man02_service_intervention_management/","tags":"","title":"MAN02 Service intervention management"},{"body":"EGI offers MATLAB on the cloud using the MATLAB Integration for Jupyter and shares data and code with other EGI users. MATLAB is also available directly on compute services offered by the members of the EGI Federation.\nMATLAB on the EGI infrastructure can be used by scientists and engineers for Open Science by enabling them to share their data and code using computational notebooks.\nBefore getting started MATLAB Licenses To use the MATLAB Integration on the EGI Notebooks service, a supported MATLAB license is required. You can also use the toolboxes linked to your MATLAB license.\nIf you are unsure of your MATLAB license type, contact your System Administrator or MathWorks support.\nTutorials If you are not familiar or have limited experience with MATLAB, MATLAB Onramp provides a free, self-paced tutorial. Additional Onramps are available for other topics using MATLAB.\nGetting started Once your server has started up (after selecting the MATLAB environment), click on the MATLAB icon\nA dialog for handling MATLAB licensing will appear.\nIndividual and Campus Wide MATLAB Licenses\nIf you have access to a MATLAB Individual License or a MATLAB Campus Wide License, use the Online License Manager tab to log in using your MathWorks account (does not need to match the email used on the EGI Identity Provider) and click on next.\nConcurrent/Network Licenses\nIf you have Network Licenses, enter the port@hostname of your License Server on the Network License Manager tab of the login screen. Before doing so, please check with your system administrator about allowing the EGI access to your on-prem Network License Manager (License Server).\nFree trial MATLAB Licenses If you do not have a MATLAB license and would like to try it out, you can download a free MATLAB trial license here. Note that the trial license is for MATLAB only and does not have any toolboxes.\nMore information on MATLAB licensing with this integration can be found here.\nFollowing this, the MATLAB IDE will appear in your browser.\nTo facilitate sharing your research output, you can use MATLAB Live Scripts combining rich text, equations, images, code and inline output all in one document. Live Scripts can be accessed from the Live Editor tab. Here is an example of a tutorial Live Script from the EGI webinar showing steps required to analyze some public COVID-19 data with MATLAB. Some other examples of Live Scripts can be found here.\nLimitations Browser based MATLAB has some differences as compared to MATLAB on the desktop. For more information, see the limitations here. Additionally, Simulink is not supported on the EGI at this time.\nRestarting, stopping or signing out of the MATLAB session At any time, clicking on this symbol will bring up a dialog to stop, restart or sign out of the current MATLAB session.\nOpen Science using MATLAB on EGI Notebooks You can access publicly available datasets via the EGI DataHub and analyze them in-the-cloud directly using MATLAB, without the need for time consuming downloads. Your MATLAB code can also be shared with your community in a variety of interoperable and open formats.\nAnalyzing public datasets using MATLAB on EGI This short video and this webinar explains how to access EGI Data Services from MATLAB in detail. Data from different data providers can be accessed from the DataHub. More information on Data Management from Notebooks and persistent storage can be found here.\nMATLAB support for data formats MATLAB supports several scientific and standard data formats including web data and those from specialized hardware.\nSharing data and code You can share your data and analyses by sharing your EGI provided persistent storage space with others. Users belonging to a specific community can request community Notebooks.\nInteroperability with Python and other languages You can collaborate with users of other languages by calling these languages (eg. Python) from MATLAB for conducting specific analyses. You can also save data in formats compatible with other languages.\nHere are some other ways in which MATLAB enables Open Science\nUsing MATLAB on other EGI services In addition to MATLAB on EGI Notebooks, you can also use your MATLAB license to access MATLAB on any of the Institutions participating in the EGI Federation. To run your compute-intensive MATLAB code faster or to run code in parallel, you will need MATLAB Parallel Server and MATLAB Parallel Computing Toolbox on your MATLAB license. If you have access to a Campus Wide License , you can now scale up to use all available workers on the HPC cluster of your choice.\nTo access MATLAB on any site of the EGI Federation, follow these steps.\nCheck if you have access to MATLAB Parallel Server and Parallel Computing Toolbox\nAsk the system administrator of the site about MATLAB as part of EGI\nContact Shubo Chakrabarti at MathWorks or Enol Fernandez at EGI for setting up access to MATLAB at the site.\n","categories":"","description":"Using MATLAB in EGI Notebooks\n","excerpt":"Using MATLAB in EGI Notebooks\n","ref":"/users/dev-env/notebooks/kernels/matlab/","tags":"","title":"MATLAB"},{"body":"This documentation covers how to install and configure a OneData OneProvider in order to join a new or existing EGI DataHub space. In particular two types of installations are available, depending if the provider wants to support the space with an empty storage or if existing data should be exposed via the Oneprovider.\nRequirements for production installation Oneprovider RAM: 32GB CPU: 8 vCPU Disk: 50GB SSD To be adjusted for the dataset and usage scenario For high Input/Output operations per second (IOPS) High performance backend storage (CEPH) Low latency network Network Requirements The following ports need to be open on the local and site firewall: 80, 443, 9443, 6665 (for data transfer) Warning The Oneprovider installation includes also a Memcached server running on port 11211. Please ensure that this port and other unused ports are not open to the internet by setting up proper local firewall rules. Installation and attach empty storage to the EGI DataHub The installation of a new Oneprovider is performed using the onedatify installation script which will setup the components using Docker and Docker-compose.\nThis simple installation script can be generated from the EGI DataHub interface.\nFirstly, you need to login to the EGI DataHub and using the Data menu you either select an existing space or create a new one.\nSecondly, you can select on the space menu the Providers section and click on the Add Support button on the top right corner.\nYou should then select on the page the tab: Deploy your own provider\nIn order to obtain the PROVIDER_REGISTRATION_TOKEN that is part of the preconfigured command, you need to contact the EGI HelpDesk, selecting DataHub as support unit.\nPlease include in the request the following info:\nUse case for the installation of a Oneprovider connected to the EGI DataHub Name and the email of the Oneprovider admin domain name of the Oneprovider Run the command on the host With the obtained PROVIDER_REGISTRATION_TOKEN, paste the copied command in the terminal on the Oneprovider machine as superuser.\nIf necessary, the Onedatify script will ask for permission to install all necessary dependencies including Docker and Docker Compose.\nAfter the dependency installation is complete, the script will ask several questions and suggest default settings:\nThe progress can be monitored on a separate terminal using the following command:\nonedatify logs After the deployment is complete, a message will be shown, including connection details for the administration panel of the Oneprovider:\nThis administration panel at port 9443 can be used to administer the Oneprovider.\nInstallation and expose existing data to the EGI DataHub The installation of a new OneProvider to expose existing datasets to an EGI DataHub space is similar to the installation with an empty storage.\nWhen adding support to an existing or new space you should select from the EGI DataHub user interface the tab : Expose Existing dataset.\nPlease refer to the steps described in the Installation and attach empty storage to the EGI DataHub section to request a PROVIDER_REGISTRATION_TOKEN.\nOnce obtained, you will have to copy the command already configured with the correct parameters for the Onezone (datahub.egi.eu) and the space to join.\nRun the command on the host Paste the copied command in the terminal on the Oneprovider machine as superuser, and follow the instructions as for the case of an empty storage.\nThe only difference is that at the end of the installation and configuration process the existing files will be automatically imported to the OneProvider.\nYou can monitor the import activity from the administration panel at port 9443.\n","categories":"","description":"Documentation for installation/configuration of OneProvider to join EGI DataHub spaces","excerpt":"Documentation for installation/configuration of OneProvider to join …","ref":"/providers/datahub/oneprovider/","tags":"","title":"DataHub OneProvider"},{"body":"This manual provides information on how to set up a Resource Centre providing cloud resources in the EGI infrastructure. Integration with FedCloud requires a working OpenStack installation as a pre-requirement. EGI supports any recent OpenStack version (tested from OpenStack Mitaka).\nEGI expects the following OpenStack services to be available and accessible from outside your site:\nKeystone Nova Cinder Glance Neutron Swift (if providing Object Storage) FedCloud components are distributed through CMD (Cloud Middleware Distribution) These docker containers come pre-packaged and ready to use in the EGI FedCloud Appliance so you do not need to install any extra components on your site but just run a VM and configure it approprietely to interact with your services.\nThe integration is performed by a set of EGI components that interact with the OpenStack services APIs:\nAuthentication of EGI users into your system is performed by configuring the native OpenID Connect support of Keystone. Support for legacy VOs using VOMS requires the installation of the Keystone-VOMS Authorization plugin to allow users with a valid VOMS proxy to obtain tokens to access your OpenStack deployment. cASO collects accounting data from OpenStack and uses SSM to send the records to the central accounting database on the EGI Accounting service (APEL) cloud-info-provider registers the RC configuration and description through the Messaging service to facilitate service discovery cloudkeeper (and cloudkeeper-os) synchronises with EGI AppDB so new or updated images can be provided by the RC to user communities (VO). Not all EGI components need to share the same credentials. They are individually configured, you can use different credentials and permissions if desired.\nInstallation options EGI distributes the integration components as:\nA Virtual Appliance (VA) that uses Docker containers to bundle all of the components in a single VM and just needs minor configuration to get started RPM and DEB Packages in the CMD distribution FedCloud Virtual Appliance The EGI FedCloud Appliance is available at AppDB as an OVA file. You can easily extract the VMDK disk by untaring and optionally converting it to your preferred format with qemu-img:\n# get image and extract VMDK $ curl $(curl \"https://appdb.egi.eu/store/vm/image/fc90d1aa-b0ae-46a0-b457-96f6f7a7d446:7875/json?strict\" \\ | jq -r .url) \\ | tar x \"*.vmdk\" # convert to qcow2 $ qemu-img convert -O qcow2 FedCloud-Appliance.Ubuntu.*.vmdk fedcloud-appliance.qcow2 The appliance running at your OpenStack must:\nHave a host certificate to send the accounting information to the accounting repository. DN of the host certificate must be registered in GOCDB with service type eu.egi.cloud.accounting. The host certificate and key in PEM format are expected in /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem respectively. Have enough disk space for handling the VM image replication (~ 100GB for fedcloud.egi.eu VO). By default these are stored at /image_data. You can mount a volume at that location. Upgrading the OpenStack Appliance From 2018.05.07 or newer to 2021.03.12 Configuration changes:\nRemoves BDII, service is no longer in use A cloud-info-provider cron is added Uses AMS for pushing accounting records. New configuration file for ssmsend is available From 2017.08.09 to 2018.05.07 Configuration changes:\nThis upgrade moves the voms.json file to the respective caso and cloudkeeper-os directories under /etc/ No other changes in configuration are needed From 20160403 to 2017.08.09 There are several major changes between these versions, namely:\natrope has been deprecated and cloudkeeper is used instead. The configuration cannot be reused directly and the new services need to be configured as described above caso is upgraded to version 1.1.1, the configuration file has some incompatible changes. A new bdii.service is available for managing the process is available. CMD Packages The CMD-OS repository provides packages that have gone through a quality assurance process for the supported distributions. Packages are available via the EGI repository.\nOpen Ports The following services must be accessible to allow access to an OpenStack-based FedCloud site (default ports listed below, can be adjusted to your installation)\nPort Application Note 5000/TCP OpenStack/Keystone Authentication to your OpenStack. 8776/TCP OpenStack/cinder Block Storage management. 8774/TCP OpenStack/nova VM management. 9696/TCP OpenStack/neutron Network management. 9292/TCP OpenStack/glance VM Image management. Outgoing ports The EGI Cloud components require the following outgoing connections open:\nPort Host Note 443/TCP msg.argo.grnet.gr ARGO Messaging System (used to send accounting records by SSM). 8443/TCP msg.argo.grnet.gr AMS authentication (used to send accounting records by SSM). 443/TCP vmcaster.appdb.egi.eu AppDB image lists (used by cloudkeeper). 8080/TCP cephrgw01.ifca.es Swift server hosting EGI images (used by cloudkeeper). Images listed in AppDB may be hosted in other servers besides cephrgw01.ifca.es. Check the specific VO-wide image lists for details.\nPermissions This is an overview of the expected account permissions used in an OpenStack site, these accounts can be merged as needed for your deployment:\nComponent Permission cloud-info Member of all projects supporting EGI VOs accounting Member of all projects and able to list users (allowed to identity:list_users in keystone) cloud-keeper Permission to manage the images for all the projects supporting EGI VOs Other users Automatically created by Keystone and permission set as configured in the mappings ","categories":"","description":"Integration of OpenStack providers\n","excerpt":"Integration of OpenStack providers\n","ref":"/providers/cloud-compute/openstack/","tags":"","title":"OpenStack"},{"body":"OpenStack providers in the EGI infrastructure offer services and features via OpenStack APIs, and the command-line interface (CLI), both integrated with EGI Check-in accounts.\nThe extensive OpenStack user documentation includes details on every OpenStack project, but most providers offer:\nKeystone, for identity Nova, for VM management Glance, for VM image management Cinder, for block storage Swift, for object storage Neutron, for network management Horizon, as a web dashboard The Horizon web-dashboard of the OpenStack providers can be used to manage and use services. It can be accessed using EGI Check-in credentials directly. Select EGI Check-In:\nOr OpenID Connect:\nOr egi.eu:\nIn the Authenticate using drop-down menu of the login screen.\nAdditionally you may need to select aai.egi.eu/oidc as well:\nTip You can quickly find the dashboards of all providers in the EGI infrastructure that are accessible to you (use the correct VO) with the FedCloud Client:\n$ fedcloud endpoint list --service-type org.openstack.horizon --site ALL_SITES The same way you can also discover other types of resources, just use the correct resource type:\norg.openstack.horizon for dashboards org.openstack.nova for virtual machines org.openstack.swift for object storage Note For more advanced information discovery, including resources not based on OpenStack deployments, check out the EGI architecture summary. ","categories":"","description":"How to interact with OpenStack providers\n","excerpt":"How to interact with OpenStack providers\n","ref":"/users/getting-started/openstack/","tags":"","title":"Using OpenStack Providers"},{"body":"In this section you can find the common operational activities related to keep the service available to our users.\nInitial set-up Notebooks VO The resources used for the Notebooks deployments are managed with the vo.notebooks.egi.eu VO. Operators of the service should join the VO, check the entry at the operations portal and at AppDB.\nClients installation In order to manage the resources you will need these tools installed on your client machine:\nfedcloudclient for discovering sites and managing tokens, terraform to create the VMs at the providers, ansible to configure the VMs and install kubernetes at the providers, terraform-inventory to get the list of hosts to use from terraform. Get the configuration repository All the configuration of the notebooks is stored at a git repository available in keybase. You'll need to be part of the opslife team in keybase to access. Start by cloning the repo:\ngit clone keybase://team/opslife/egi-notebooks Kubernetes We use terraform and ansible to build the cluster at one of the EGI Cloud providers. If you are building the cluster for the first time, create a new directory on your local git repository from the template, add it to the repo, and get terraform ready:\ncp -a template \u003cnew provider\u003e git add \u003cnew provider\u003e cd \u003cnew provider\u003e/terraform terraform init Using the fedcloud you can get set the right environment for interacting with the OpenStack APIs of a given site:\neval \"$(fedcloud site show-project-id --site CESGA --vo vo.notebooks.egi.eu)\" Whenever you need to get a valid token for the site and VO, you can obtain it with:\nOS_TOKEN=$(fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu \\ token issue -c id -f value) First get the network IDs and pool to use for the site:\n$ fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu network list +--------------------------------------+-------------------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-------------------------+--------------------------------------+ | 1aaf20b6-47a1-47ef-972e-7b36872f678f | net-vo.notebooks.egi.eu | 6465a327-c261-4391-a0f5-d503cc2d43d3 | | 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 | public00 | 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 | +--------------------------------------+-------------------------+--------------------------------------+ In this case we will use public00 as the pool for public IPs and 1aaf20b6-47a1-47ef-972e-7b36872f678f as the network ID. Check with the provider which is the right network to use. Use these values in the terraform.tfvars file:\nip_pool = \"public00\" net_id = \"1aaf20b6-47a1-47ef-972e-7b36872f678f\" You may want to check the right flavors for your VMs and adapt other variables in terraform.tfvars. To get a list of flavors you can use:\n$ fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu flavor list +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ | 26d14547-96f2-4751-a686-f89a9f7cd9cc | cor4mem8hd40 | 8192 | 40 | 0 | 4 | True | | 42eb9c81-e556-4b63-bc19-4c9fb735e344 | cor2mem2hd20 | 2048 | 20 | 0 | 2 | True | | 4787d9fc-3923-4fc9-b770-30966fc3baee | cor4mem4hd40 | 4096 | 40 | 0 | 4 | True | | 58586b06-7b9d-47af-b9d0-e16d49497d09 | cor24mem62hd60 | 63488 | 60 | 0 | 24 | True | | 635c739a-692f-4890-b8fd-d50963bff00e | cor1mem1hd10 | 1024 | 10 | 0 | 1 | True | | 6ba0080d-d71c-4aff-b6f9-b5a9484097f8 | small | 512 | 2 | 0 | 1 | True | | 6e514065-9013-4ce1-908a-0dcc173125e4 | cor2mem4hd20 | 4096 | 20 | 0 | 2 | True | | 85f66ce6-0b66-4889-a0bf-df8dc23ee540 | cor1mem2hd10 | 2048 | 10 | 0 | 1 | True | | c4aa496b-4684-4a86-bd7f-3a67c04b1fa6 | cor24mem50hd50 | 51200 | 50 | 0 | 24 | True | | edac68c3-50ea-42c2-ae1d-76b8beb306b5 | test-bigHD | 4096 | 237 | 0 | 2 | True | +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ Finally ensure your public ssh key is also listed in the cloud-init.yaml file and then you are ready to deploy the cluster with:\nterraform apply Your VMs are up and running, it's time to get kubernetes configured and running with ansible.\nThe following ansible role needs to be installed first:\nansible-galaxy install grycap.kubernetes and then:\ncd .. # you should be now in \u003cnew provider\u003e ANSIBLE_TRANSFORM_INVALID_GROUP_CHARS=silently TF_STATE=./terraform \\ ansible-playbook --inventory-file=$(which terraform-inventory) \\ playbooks/k8s.yaml Interacting with the cluster As the master will be on a private IP, you won't be able to directly interact with it, but you can still ssh into the VM using the ingress node as a gateway host (you can get the different hosts with TF_STATE=./terraform terraform-inventory --inventory)\n$ ssh -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -q egi@\u003cingress ip\u003e\" \\ -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null egi@\u003cmaster ip\u003e egi@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 33m v1.15.7 k8s-nfs Ready \u003cnone\u003e 16m v1.15.7 k8s-w-ingress Ready \u003cnone\u003e 16m v1.15.7 egi@k8s-master:~$ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE certs-man 2 Wed Jan 8 15:56:58 2020 DEPLOYED cert-manager-v0.11.0 v0.11.0 cert-manager cluster-ingress 3 Wed Jan 8 15:56:53 2020 DEPLOYED nginx-ingress-1.7.0 0.24.1 kube-system nfs-provisioner 3 Wed Jan 8 15:56:43 2020 DEPLOYED nfs-client-provisioner-1.2.8 3.1.0 kube-system Modifying/Destroying the cluster You should be able to change the number of workers in the cluster and re-apply terraform to start them and then execute the playbook to get them added to the cluster.\nAny changes in the master, NFS or ingress VMs should be done carfully as those will probably break the configuration of the kubernetes cluster and of any application running on top.\nDestroying the cluster can be done with a single command:\nterraform destroy Notebooks deployments Once the k8s cluster is up and running, you can deploy a notebooks instance. For each deployment you should create a file in the deployments directory following the template provided:\ncp deployments/hub.yaml.template deployments/hub.yaml Each deployment will need a domain name pointing to your ingress host, you can create one at the FedCloud dynamic DNS service.\nThen you will need to create an OpenID Connect client for EGI Check-in to authorise users into the new deployment. You can create a client by going to the EGI Federation Registry. You can find more information about registering an OIDC Client in EGI Check-in guide for SPs Use the following as redirect URL: https://\u003cyour host domain name\u003e/hub/oauth_callback.\nThen add offline_access to the list of scopes and sumbit the request. After the approval of the Service request, save the client and take note of the client ID and client secret for later.\nFinally you will also need 3 different random strings generated with openssl rand -hex 32 that will be used as secrets in the file describing the deployment.\nGo and edit the deployment description file to add this information (search for # FIXME NEEDS INPUT in the file to quickly get there)\nFor deploying the notebooks instance we will also use ansible:\nANSIBLE_TRANSFORM_INVALID_GROUP_CHARS=silently \\ TF_STATE=./terraform ansible-playbook \\ --inventory-file=$(which terraform-inventory) playbooks/notebooks.yaml The first deployment trial may fail due to a timeout caused by the downloading of the container images needed. You can retry after a while to re-deploy.\nIn the master you can check the status of your deployment (the name of the deployment will be the same as the name of your local deployment file):\n$ helm status hub LAST DEPLOYED: Thu Jan 9 08:14:49 2020 NAMESPACE: hub STATUS: DEPLOYED RESOURCES: ==\u003e v1/ServiceAccount NAME SECRETS AGE hub 1 6m46s user-scheduler 1 3m34s ==\u003e v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hub ClusterIP 10.100.77.129 \u003cnone\u003e 8081/TCP 6m46s proxy-public NodePort 10.107.127.44 \u003cnone\u003e 443:32083/TCP,80:30581/TCP 6m45s proxy-api ClusterIP 10.103.195.6 \u003cnone\u003e 8001/TCP 6m45s ==\u003e v1/ConfigMap NAME DATA AGE hub-config 4 6m47s user-scheduler 1 3m35s ==\u003e v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hub-db-dir Pending managed-nfs-storage 6m46s ==\u003e v1/ClusterRole NAME AGE hub-user-scheduler-complementary 3m34s ==\u003e v1/ClusterRoleBinding NAME AGE hub-user-scheduler-base 3m34s hub-user-scheduler-complementary 3m34s ==\u003e v1/RoleBinding NAME AGE hub 6m46s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE continuous-image-puller-flf5t 1/1 Running 0 3m34s continuous-image-puller-scr49 1/1 Running 0 3m34s hub-569596fc54-vjbms 0/1 Pending 0 3m30s proxy-79fb6d57c5-nj8n2 1/1 Running 0 2m22s user-scheduler-9685d654b-9zt5d 1/1 Running 0 3m30s user-scheduler-9685d654b-k8v9p 1/1 Running 0 3m30s ==\u003e v1/Secret NAME TYPE DATA AGE hub-secret Opaque 3 6m47s ==\u003e v1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE continuous-image-puller 2 2 2 2 2 \u003cnone\u003e 3m34s ==\u003e v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hub 1 1 1 0 6m45s proxy 1 1 1 1 6m45s user-scheduler 2 2 2 2 3m32s ==\u003e v1/StatefulSet NAME DESIRED CURRENT AGE user-placeholder 0 0 6m44s ==\u003e v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE jupyterhub notebooktest.fedcloud-tf.fedcloud.eu 80, 443 6m44s ==\u003e v1beta1/PodDisruptionBudget NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE hub 1 N/A 0 6m48s proxy 1 N/A 0 6m48s user-placeholder 0 N/A 0 6m48s user-scheduler 1 N/A 1 6m47s ==\u003e v1/Role NAME AGE hub 6m46s NOTES: Thank you for installing JupyterHub! Your release is named hub and installed into the namespace hub. You can find if the hub and proxy is ready by doing: kubectl --namespace=hub get pod and watching for both those pods to be in status 'Running'. You can find the public IP of the JupyterHub by doing: kubectl --namespace=hub get svc proxy-public It might take a few minutes for it to appear! Note that this is still an alpha release! If you have questions, feel free to 1. Read the guide at https://z2jh.jupyter.org 2. Chat with us at https://gitter.im/jupyterhub/jupyterhub 3. File issues at https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues Updating a deployment Just edit the deployment description file and run ansible again. The helm will be upgraded at the cluster.\n","categories":"","description":"Getting the service up and running","excerpt":"Getting the service up and running","ref":"/providers/notebooks/operations/","tags":"","title":"Service Operations"},{"body":"This page contains information about connecting services to EGI Check-in in order to allow user login through Check-in and to receive users’ attributes. Check-in is connected to a wide range of academic and social Identity Providers that users can choose from in order to access your service.\nServices eligible for integration EGI Operations, as owner of the Check-in service, must approve every request for integration of new services with Check-in. The approval (or non-approval) is based on some prerequisites, the relevance of the service for the EGI community and the available resources to support the integration. The prerequisites are described in the following sections.\nEGI at any time can prevent a service provider to access the Check-in service\nServices federated in EGI All the services that are operated by Resource Providers federated in EGI federation and that abide to the RC OLA, and consequently to the relevant security policies of EGI, can be connected with Check-in. Fulfilling all the relevant EGI policies makes the service eligible in receiving attributes released by Check-in.\nServices not federated in EGI A service not part of the EGI federation can be integrated with Check-in if the organisation providing the service complies with the EGI security requirements relevant to the service providers.\nBy accepting the policies a service provider assures that they will operate the service in good faith, without deliberately exposing the user to security risks, without claiming intellectual property on the data owned by the user, and protecting sensitive data generated by the interaction of the user with the service.\nThe policies that the service provider will have to accept are available in the EGI Policies and procedures page and specifically are:\nEGI Security Policy Service Operations Security Policy Traceability and Logging Policy Security Incident Response Policy Policy on the Processing of Personal Data Service Provider integration workflow To integrate your Service Provider with the EGI Check-in service, you need to create a registration request using the EGI Federation Registry Portal. You can also use the Federation Registry portal to request the reconfiguration or deregistration of an existing deployed service. Service registration requests typically require approval by an administrator. Please refer to the Federation Registry Documentation for more information.\nThe integration follows a two-step process:\nRegister your Service Provider and test integration with the demo instance of EGI Check-in by selecting the “Demo” integration environment during registration through the EGI Federation Registry Portal. Service registration requests require approval by an administrator. The review process for the demo environment involves primarily the technical aspects of the service configuration. However, moving the service to production requires compliance with all the eligibility criteria (see Step 2). The demo instance allows for testing authentication and authorisation through the academic and social Identity Providers connected to Check-in without affecting the production Check-in service. Note that while the demo instance has identical functionality to the production instance, no information is shared between the two systems. You can also test new features of Check-in that are not available in production yet, by registering your Service Provider and testing integration with the development instance of Check-in. In the development instance service requests can be self-reviewed without the need to wait for approval from an administrator. As with the demo instance, the development instance allows for testing authentication and authorisation without affecting the production Check-in service. NB: the list of supported Identity Providers in the development instance is limited. Therefore, we recommend using any of the social identity providers or the EGI SSO to test the login workflow when using the development instance. Register your Service Provider with the production instance of EGI Check-in by selecting the “Production” integration environment during registration through the EGI Federation Registry Portal. The production instance allows access to your service through the academic and social Identity Providers connected to Check-in. This requires that your service meets all the eligibility criteria and that integration has been thoroughly tested during Step 1. General Information EGI Check-in supports two authentication and authorisation protocols that you can choose from:\nSecurity Assertion Markup Language (SAML) 2.0 OpenID Connect - an extension to OAuth 2.0 Service providers should ensure that a proper authorisation model is put in place: if low assurance accounts, like those coming from social media identity providers, are granted access without any vetting, it may lead to an abuse of their service.\nRegardless of which of the two protocols you are going to use, you need to provide the following information to connect your service to EGI Check-in:\nName of the service (in English and optionally in other languages supported by the service) Short description of the service Site (URL) for localised information about the service; the content found at the URL SHOULD provide more complete information than what provided by the description Contact information of the following types: Helpdesk/Support contact information (for redirecting user) Administrative Technical Security/incident response Privacy statement URL: The privacy policy is used to document the data collected and processed by the service. You can use the Privacy Policy template Logo URL (optional for showing in catalogues); if provided, logos SHOULD: use a transparent background where appropriate to facilitate the usage of logos within a user interface use PNG, or GIF (less preferred), images use HTTPS URLs in order to avoid mixed-content warnings within browsers have a size larger than 40000 and smaller than 50000 characters when encoded in base64 Country of the service Compliance with the EGI Policies and the REFEDS Data Protection Code of Conduct The most important URLs for each environment are listed in the table below but more information can be found in the protocol-specific sections that follow.\nProduction Demo Development Protocol Production environment SAML https://aai.egi.eu/proxy/saml2/idp/metadata.php OpenID Connect https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration Protocol Demo environment SAML https://aai-demo.egi.eu/proxy/saml2/idp/metadata.php OpenID Connect https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration Protocol Development environment SAML https://aai-dev.egi.eu/proxy/saml2/idp/metadata.php OpenID Connect https://aai-dev.egi.eu/auth/realms/egi/.well-known/openid-configuration SAML Service Provider To enable federated access to a web-based application, you can connect to the EGI Check-in IdP as a SAML Service Provider (SP). Users of the application will be redirected to Check-in in order to log in, and Check-in can authenticate them using any of the supported backend authentication mechanisms, such as institutional IdPs registered with eduGAIN or Social Providers. Once the user is authenticated, EGI Check-in will return a SAML assertion to the application containing information about the authenticated user.\nMetadata registration SAML authentication relies on the use of metadata. Both parties (you as a SP and the EGI Check-in IdP) need to exchange metadata in order to know and trust each other. The metadata include information such as the location of the service endpoints that need to be invoked, as well as the certificates that will be used to sign SAML messages. The format of the exchanged metadata should be based on the XML-based SAML 2.0 specification. Usually, you will not need to manually create such an XML document, as this is automatically generated by all major SAML 2.0 SP software solutions (e.g., Shibboleth, SimpleSAMLphp, and mod_auth_mellon). It is important that you serve your metadata over HTTPS using a browser-friendly SSL certificate, i.e. issued by a trusted certificate authority.\nYou can get the metadata of the EGI Check-in IdP Proxy on a dedicated URL that depends on the integration environment being used:\nProduction Demo Development Production environment https://aai.egi.eu/proxy/saml2/idp/metadata.php Demo environment https://aai-demo.egi.eu/proxy/saml2/idp/metadata.php Development environment https://aai-dev.egi.eu/proxy/saml2/idp/metadata.php To register your SAML SP, you must submit a service registration request at Federation Registry. Your request should include the general information about your service (see General Information) and the SP’s metadata and entity ID.\nMetadata Metadata provided by your SP should contain a descriptive name of the service that your SP represents in at least English. It is recommended to also provide the name in other languages which are commonly used in the geographic scope of the deployment. The name should be placed in the \u003cmd:ServiceName\u003e in the \u003cmd:AttributeConsumingService\u003e container.\nIt is recommended that the \u003cmd:IDPSSODescriptor\u003e element included in your SP metadata contains both an AuthnRequestsSigned and an WantAssertionsSigned attribute set to true.\nYour SP metadata should also contain contact information for support and for a technical contact. The \u003cmd:EntityDescriptor\u003e element should contain both a \u003cmd:ContactPerson\u003e element with a contactType of \"support\" and a \u003cmd:ContactPerson\u003e element with a contactType of \"technical\". The \u003cmd:ContactPerson\u003e elements should contain at least one \u003cmd:EmailAddress\u003e. The support address may be used for generic support questions about the service, while the technical contact may be contacted regarding technical interoperability problems. The technical contact must be responsible for the technical operation of the service represented by your SP.\nAttributes The EGI Check-in IdP is guaranteed to release a minimal subset of the REFEDS R\u0026S attribute bundle to connected Service Providers without administrative involvement, subject to user consent. The following attributes constitute a minimal subset of the R\u0026S attribute bundle:\nCommunity User Identifier (CUID) which is a globally unique, opaque, persistent and non-reassignable identifier identifying the user (voPersonID). For users whose community identity is managed by Check-in, this identifier is of the form \u003cuniqueID\u003e@egi.eu. The \u003cuniqueID\u003e portion is an opaque identifier issued by Check-in. Email address (mail) Display name (displayName) OR (givenName AND sn) A more extensive list of all the attributes that may be made available to Service Providers is included in the User Attribute section.\nAttribute-based authorisation As mentioned in the General Information, omitting authorisation checks may lead to abuse of the service.\nEGI Check-in provides information about the authenticated user that may be used by Service Providers in order to control user access to resources. This information is provided by the EGI Check-in IdP in the SAML attribute assertion. The table below lists the SAML attributes that are relevant for user authorisation:\nDescription SAML Attribute VO/group membership/roles of the authenticated user eduPersonEntitlement Capabilities eduPersonEntitlement GOCDB roles eduPersonEntitlement Identity Assurance eduPersonAssurance References Shibboleth Service Provider Documentation SimpleSAMLphp Service Provider QuickStart Simple SAML 2.0 service provider with mod_auth_mellon Apache module OpenID Connect Service Provider Service Providers can be integrated with EGI Check-in using OpenID Connect (OIDC) as an alternative to the SAML2 protocol. To allow this, the EGI Check-in IdP provides an OpenID Connect (OAuth2) API based on MITREid Connect, which has been certified by the OpenID Foundation. Interconnection with the EGI Check-in OIDC Provider allows users to sign in using any of the supported backend authentication mechanisms, such as institutional IdPs registered with eduGAIN or Social Providers. Once the user has signed in, EGI Check-in can return OIDC Claims containing information about the authenticated user.\nImportant The EGI Check-in OIDC Provider will be migrated to Keycloak. Please check OIDC Client Migration to Keycloak for more details Client registration Before your service can use the EGI Check-in OIDC Provider for user login, you must submit a service registration request using Federation Registry in order to obtain OAuth 2.0 credentials. The client configuration should include the general information about your service, as described in General Information section.\nObtaining OAuth 2.0 credentials You need OAuth 2.0 credentials, which typically include a client ID and client secret, to authenticate users through the EGI Check-in OIDC Provider.\nYou can specify the client ID and secret when creating/editing your client or let them being automatically generated during registration (recommended).\nTo find the ID and secret of your client, do the following:\nSelect your client from the Manage Services Page. Look for the Client ID in the Protocol tab. Select the Display/edit client secret: option from the Protocol tab. Note You can copy these values using the green copy button next to the desired field. Setting one or more Redirection URIs The Redirection URI(s) that you set when creating/editing your client determine where the EGI Check-in OIDC Provider sends responses to your authentication requests. Note that the Redirection URI MUST use the https scheme; the use of http Redirection URIs is only allowed in the development environment.\nTo find the Redirection URI(s) for your client, do the following:\nOpen the Manage Services Find the redirect URIs for your client listed under the Protocol column of the overview table or Edit the particular client and look for the Redirect URI(s) in the Protocol tab. Setting additional information about the client It is strongly suggested that you add a short description and a logo for the client. Lastly, you need to set the email addresses of one or more contacts.\nClaims The EGI Check-in UserInfo Endpoint is an OAuth 2.0 Protected Resource that returns specific information about the authenticated end user as Claim Values. To obtain the requested Claims about the end user, the Client makes a request to the UserInfo Endpoint using an Access Token obtained through OpenID Connect Authentication. The scopes associated with the Access Token used to access the EGI Check-in UserInfo Endpoint will determine what Claims will be released. These Claims are represented by a JSON object that contains a collection of name and value pairs for the Claims.\nThe following scope values can be used to request Claims from the EGI Check-in UserInfo Endpoint:\nScope Claims openid sub profile namegiven_namefamily_namepreferred_username email emailemail_verifiedvoperson_verified_email aarc namegiven_namefamily_namepreferred_usernameemailemail_verifiedvoperson_verified_emailvoperson_id eduperson_entitlement eduperson_entitlement eduperson_scoped_affiliation eduperson_scoped_affiliation voperson_id voperson_id A more extensive list of all the attributes that may be made available to Service Providers is included in the User Attribute section.\nGrant Types Check-in supports the following OpenID Connect/OAuth2 grant types:\nAuthorization Code: used by Web Apps executing on a server. Token Exchange: used by clients to request and obtain security tokens in support of delegated access to resources. Device Code: used by devices that lack a browser to perform a user-agent based OAuth flow. Endpoints The most important OIDC/OAuth2 endpoints are listed below:\nProduction Demo Development Endpoints Production environment Provider configuration https://aai.egi.eu/oidc/.well-known/openid-configuration (MITREid Connect)https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration (Keycloak) Issuer https://aai.egi.eu/oidc/ (MITREid Connect)https://aai.egi.eu/auth/realms/egi (Keycloak) Authorisation https://aai.egi.eu/oidc/authorize (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/auth (Keycloak) Token https://aai.egi.eu/oidc/token (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token (Keycloak) Device Code https://aai.egi.eu/oidc/devicecode (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/auth/device (Keycloak) JSON Web Key(JWK) https://aai.egi.eu/oidc/jwk (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/certs (Keycloak) User Info https://aai.egi.eu/oidc/userinfo (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo (Keycloak) Introspection https://aai.egi.eu/oidc/introspect (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect(Keycloak) Logout https://aai.egi.eu/oidc/saml/logout (MITREid Connect)https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/logout(Keycloak) Endpoints Demo environment Provider configuration https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration Issuer https://aai-demo.egi.eu/auth/realms/egi Authorisation https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/auth Token https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/token Device Code https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/auth/device JSON Web Key(JWK) https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/certs User Info https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo Introspection https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Logout https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/logout Endpoints Development environment Provider configuration https://aai-dev.egi.eu/auth/realms/egi/.well-known/openid-configuration Issuer https://aai-dev.egi.eu/auth/realms/egi Authorisation https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/auth Token https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/token Device Code https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/auth/device JSON Web Key(JWK) https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/certs User Info https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo Introspection https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Logout https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/logout Authorization Endpoint The Authorization Endpoint performs Authentication of the end user. This is done by sending the User Agent to the Authorization Server's Authorization Endpoint for Authentication and Authorization, using request parameters defined by OAuth 2.0 and additional parameters and parameter values defined by OpenID Connect.\nThe request parameters of the Authorization endpoint are:\nclient_id: ID of the client that ask for authentication to the Authorization Server. redirect_uri: URI to which the response will be sent. scope: A list of attributes that the application requires. state: Opaque value used to maintain state between the request and the callback. response_type: value that determines the authorization processing flow to be used. For Authorization Code grant set response_type=code. This way the response will include an authorization code. Token Endpoint To obtain an Access Token, an ID Token, and optionally a Refresh Token, the Client sends a Token Request to the Token Endpoint.\nDepending on the grant type, the following parameters are required:\nAuthorization Code Parameter Presence Values grant_type Required authorization_code code Required The value of the code in the response from authorization endpoint. redirect_uri Required URI to which the response will be sent (must be the same as the request to authorization endpoint) Proof Key for Code Exchange (PKCE) The Proof Key for Code Exchange (PKCE, pronounced pixie) extension (RFC 7636) describes a technique for public clients (clients without client_secret) to mitigate the threat of having the authorization code intercepted. The technique involves the client first creating a secret, and then using that secret again when exchanging the authorization code for an access token. This way if the code is intercepted, it will not be useful since the token request relies on the initial secret.\nClient configuration To enable PKCE you need to go to the Manage Services Page and create/edit a client. In “Protocol” tab under “Token Endpoint Authentication Method” select “No authentication” and in “Crypto” tab under “Proof Key for Code Exchange (PKCE) Code Challenge Method” select “SHA-256 hash algorithm”.\nProtocol Flow Because the PKCE-enhanced Authorization Code Flow builds upon the standard Authorization Code Flow, the steps are very similar.\nFirst, the client creates and records a secret named the code_verifier. The code_verifier is a high-entropy cryptographic random STRING using the unreserved characters [A-Z] / [a-z] / [0-9] / “-” / “.” / “_” / “~”, with a minimum length of 43 characters and a maximum length of 128 characters. Then the client creates a code_challenge derived from the code_verifier by using one of the following transformations on the code verifier:\nplain code_challenge = code_verifier S256 code_challenge = BASE64URL-ENCODE(SHA256(ASCII(code_verifier))) If the client is capable of using S256, it MUST use S256. Clients are permitted to use plain only if they cannot support S256 for some technical reason.\nNote There are various tools that generate these values such as https://tonyxu-io.github.io/pkce-generator/ Then the code_challenge is sent in the Authorization Request along with the transformation method (code_challenge_method).\nExample request:\nGET \"${AUTHORISATION_ENDPOINT}? client_id=${CLIENT_ID} \u0026scope=openid%20profile%20email \u0026redirect_uri=${REDIRECT_URI} \u0026response_type=code \u0026code_challenge=${CODE_CHALLENGE} \u0026code_challenge_method=S256\" Note You can find the Authorisation Endpoint in the Endpoints table. The Authorization Endpoint responds as usual but records code_challenge and the code_challenge_method.\nExample response:\nHTTP/1.1 302 Found Location: ${REDIRECT_URI}? code=fgtLHT The client then sends the authorization code in the Access Token Request as usual but includes the code_verifier secret generated in the first request.\nExample request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=${CODE}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"redirect_uri=${REDIRECT_URI}\" \\ -d \"code_verifier=${CODE_VERIFIER}\" | python -m json.tool Note You can find the Token Endpoint in the Endpoints table. The authorization server transforms code_verifier and compares it to code_challenge from the first request. Access is denied if they are not equal.\nExample response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUlMyNTYifQ...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUlMyNTYifQ...\", \"scope\": \"openid email profile\", \"token_type\": \"Bearer\" } Refresh request The following request allows obtaining an access token from a refresh token using the grant_type value refresh_token:\nParameter Presence Values client_id Required The identifier of the client. client_secret Required The secret value of the client. grant_type Required refresh_token refresh_token Required The value of the refresh token scope Required This parameter should contain openid at least Example request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\ -u \"${CLIENT_ID}\":\"${CLIENT_SECRET}\" \\ -d \"grant_type=refresh_token\" \\ -d \"refresh_token=${REFRESH_TOKEN}\" \\ -d \"scope=openid%20email%20profile\" | python -m json.tool; Note You can find the Token Endpoint in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWx...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYW...\", \"refresh_token\": \"eyJhbGciOiJub25...\", \"scope\": \"openid profile email\", \"token_type\": \"Bearer\" } Refresh Request with PKCE To combine the refresh token grant type with PKCE you need to make the following request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"grant_type=refresh_token\" \\ -d \"refresh_token=${REFRESH_TOKEN}\" \\ -d \"scope=openid%20email%20profile\" | python -m json.tool; Note You can find the Token Endpoint in the Endpoints table. Token Exchange To get a token from client B using a token issued for client A, the parameters of the request are:\nParameter Presence Values grant_type Required urn:ietf:params:oauth:grant-type:token-exchange audience Optional Define the logical name of the service that the token will be used for subject_token Required The value of the access token subject_token_type Required urn:ietf:params:oauth:token-type:access_token (because this feature accepts access tokens only) scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected Example request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\ -u \"${CLIENT_B_ID}\":\"${CLIENT_B_SECRET}\" \\ -d \"grant_type=urn:ietf:params:oauth:grant-type:token-exchange\" \\ -d \"subject_token=${ACCESS_TOKEN_A}\" \\ -d \"subject_token_type=urn:ietf:params:oauth:token-type:access_token\" \\ -d \"scope=openid%20profile%20offline_access\" | python -m json.tool; Note You can find the Token Endpoint in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"refresh_token\": \"eyJhbGciOiJub25lIn0.eyJleHAiO...\", \"scope\": \"openid profile offline_access\", \"token_type\": \"Bearer\" } Device Code The device code flow enables OAuth clients on (input-constrained) devices to obtain user authorization for accessing protected resources without using an on-device user-agent, provided that they have an internet connection.\n1. Device Authorization Request The client initiates the authorization flow by requesting a set of verification codes from the authorization server by making an HTTP “POST” request to the device authorization endpoint. The client constructs the request with the following parameters:\nParameter Presence Values client_id Required The identifier of the client scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected Example request:\ncurl -X POST \"${DEVICE_CODE_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"scope=openid%20email%20profile\" | python -m json.tool Note You can find the Device Code Endpoint in the Endpoints table. Example response:\n{ \"device_code\": \"HvtHOpSah_Anupq-0dtzvN7cb-wcnwxytiMzpBZBN6E\", \"expires_in\": 600, \"interval\": 5, \"user_code\": \"NMEM-SDPK\", \"verification_uri\": \"https://aai.egi.eu/auth/realms/egi/device\", \"verification_uri_complete\": \"https://aai.egi.eu/auth/realms/egi/device?user_code=NMEM-SDPK\" } 2. User Interaction After receiving a successful Authorization Response, the client displays or otherwise communicates the user_code and the verification_uri to the end user and instructs them to visit the URI in a user agent on a secondary device (for example, in a browser on their mobile phone), and enter the user code.\n3. Device Access Token Request After displaying instructions to the user, the client makes an Access Token Request to the token endpoint. The request contains the following parameters:\nParameter Presence Values grant_type Required urn:ietf:params:oauth:grant-type:device_code device_code Required The device verification code, device_code from the Device Authorization Response client_id Required The identifier of the client client_secret Required The secret value of the client scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected Example request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Adevice_code\" \\ -d \"device_code=${DEVICE_CODE}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"scope=openid%20profile\" | python -m json.tool Note You can find the Token Endpoint in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJhZG1pbiIs...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiI5MDM0Mi...\", \"scope\": \"openid profile\", \"token_type\": \"Bearer\" } Device Code with PKCE To combine Device Code flow with PKCE you need to make the following requests:\n1 - Device Authorization Request:\ncurl -X POST \"${DEVICE_CODE_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"scope=openid%20email%20profile\" \\ -d \"code_challenge=${CODE_CHALLENGE}\" \\ -d \"code_challenge_method=S256\" | python -m json.tool Note You can find the Device Code Endpoint in the Endpoints table. 2 - Device Access Token Request\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Adevice_code\" \\ -d \"device_code=${DEVICE_CODE}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"code_verifier=${CODE_VERIFIER}\" | python -m json.tool Note You can find the Token Endpoint in the Endpoints table. Claims-based authorisation As mentioned in the General Information, omitting authorisation checks may lead to abuse of the service.\nEGI Check-in provides information about the authenticated user that may be used by Service Providers in order to control user access to resources. This information is provided by the EGI Check-in OIDC Provider in the form of OIDC claims. The table below lists the claims that are relevant for user authorisation:\nDescription OIDC Claim VO/group membership/roles of the authenticated user eduperson_entitlement Capabilities eduperson_entitlement GOCDB roles eduperson_entitlement Identity Assurance eduperson_assurance Example OIDC Client In this guide we will demonstrate how to install and configure a Simple OIDC Client.\nInstall simple-oidc-client-php This guide assumes the Apache HTTP server has been installed and the document root is /var/www/html\nMove to the apache document root and download and extract simple-oidc-client-php.tar.gz.\nConfigure Client Login to the EGI Federation Registry\nThen create a new service or edit your existing service. In General tab fill all the required fields. For Integration Environment select Demo. In Protocol Specific tab select as Protocol the OIDC Service and then in the Redirect URI(s) insert your simple-oidc-client-php URL (e.g. http://localhost/simple-oidc-client-php/refreshtoken.php). This URL must link to refreshtoken.php which is located in simple-oidc-client-php directory. Next, in Scope select the scopes that your service needs. Then, submit the form and and self approve it. Finally you should get a pair of Client ID and Client Secret.\nConfigure simple-oidc-client-php Now that you have everything you need, you can configure your login settings. Go to your terminal and open config.php with your favourite text editor.\nExample:\nvi simple-oidc-client-php/config.php Let’s go quickly through the settings:\ntitle required, the title on the navigation bar img required, the source of the logo scope_info optional, a message that informs the user for the application requirements issuer required, the base URL of our IdentityServer instance. This will allow oidc-client to query the metadata endpoint so it can validate the tokens client_id required, the ID of the client we want to use when hitting the authorization endpoint client_secret optional, a value the offers better security to the message flow pkceCodeChallengeMethod optional, a string that defines the code challenge method for PKCE. Choose between plain or S256. redirect_url required, the redirect URL where the client and the browser agree to send and receive correspondingly the code scopesDefine required, defines the scopes the client supports refresh_token_note optional, info for the refresh token access_token_note optional, info for the access token manage_token_note optional, message the informs the user where can manage his tokens manageTokens optional, URL of the manage tokens service sessionName required, define the name of the cookie session sessionLifetime required, define the duration of the session. This must be equal to the validity time of the access token. You must change the followings options based on your Service configuration you setup earlier:\nissuer client_id client_secret redirect_url scopesDefine sessionName (based on the installation path of the portal) An example configuration follows:\n\u003c?php // index.php interface configuration $title = \"Generate Tokens\"; $img = \"https://clickhelp.co/images/feeds/blog/2016.05/keys.jpg\"; $scope_info = \"This service requires the following permissions for your account:\"; // Client configuration $issuer = \"https://aai-demo.egi.eu/oidc/\"; $client_id = \"CHANGE_ME\"; $client_secret = \"CHANGE_ME\"; // comment if you are using PKCE // $pkceCodeChallengeMethod = \"S256\"; // uncomment to use PKCE $redirect_url = \"http://localhost/simple-oidc-client-php/refreshtoken.php\"; // add scopes as keys and a friendly message of the scope as value $scopesDefine = array( 'openid' =\u003e 'log in using your identity', 'email' =\u003e 'read your email address', 'profile' =\u003e 'read your basic profile info', ); // refreshtoken.php interface configuration $refresh_token_note = \"NOTE: New refresh tokens expire in 12 months.\"; $access_token_note = \"NOTE: New access tokens expire in 1 hour.\"; $manage_token_note = \"You can manage your refresh tokens in the following link: \"; $manageTokens = $issuer . \"manage/user/services\"; $sessionName = \"simple-oidc-client-php\"; $sessionLifetime = 60*60; // must be equal to access token validation time in seconds Client Migration to Keycloak The migration guide below applies to OIDC clients registered in the Development, Demo and Production environments of Check-in.\nDevelopment and Demo: Beginning June 24, 2022, clients using the legacy Check-in OIDC endpoints will no longer be supported.\nProduction: Beginning September 16, 2022, clients using the legacy Check-in OIDC endpoints will no longer be supported.\nNote For OpenStack Services please read the OpenStack specific migration guide on Cloud Compute documentation. How to Migrate your Service to Keycloak All the clients that were registered in MITREid Connect have been moved to Keycloak preserving all the options (Client ID, Client Secret, Redirect URIs etc.), so you do not need to re-register your Service.\nEndpoints The first thing you need to do is to update the OIDC endpoints according to the Endpoints table. If the Application/Library supports Dynamic Discovery, then you need to update on the issuer. Otherwise, you need to update all the Endpoints separately.\nSize of the Tokens The size of the Access/Refresh Tokens that are issued by Keycloak is larger of the respective Tokens created by MITREid Connect. For example, the size of an Access Token is around 1400 characters, depending on the information that are included in the payload of the JWT. So make sure that your OIDC implementation can handle larger Tokens.\nLogout The Redirect URI query parameter in the logout request has been changed from redirect to post_logout_redirect_uri and must be URL encoded. Also, the value of the post_logout_redirect_uri must be defined in the Valid Redirect URIs of the Service configuration in the EGI Federation Registry.\nToken Introspection The Token Introspection is available to all the clients that are using any authentication method (client_secret_basic, client_secret_post, client_secret_jwt or private_key_jwt) (Confidential Clients) to the Token Endpoint. Public Clients (clients that do not use any authentication method) will not be able to get a successful response from the Introspection Endpoint. Saying that, the “Introspection” option in the EGI Federation Registry will be removed.\nPKCE If you are not using PKCE (Proof Key for Code Exchange), please make sure to disable the “PKCE Code Challenge Method” in the Service configuration in EGI Federation Registry, otherwise you will get the following HTTP response during the authentication flow:\nerror=invalid_request\u0026error_description=Missing parameter: code_challenge_method Device Code If you are using a confidential client with the Device Code grant, please make sure that the client_secret is present in the request to the Device Code Endpoint either as HTTP Basic or HTTP POST parameter (see Device Authorization Request).\nToken Exchange If you are using the Token Exchange grant, please make sure that the audience (Optional) defines the logical name of the service that the token will be used for; when specified, it must match the client ID of a client registered in Check-in otherwise an invalid_client error is returned (\"description\": \"audience not found\")\nClient Credentials If you are using the Client Credentials grant, there is a minor change in the responses from userinfo and introspection endpoints. The Client ID of the client is not released as the sub claim any more and has replaced with by the client_id claim. The sub contains the identifier of the client which is unique, non-reassignable and scoped @egi.eu.\nObtain Refresh Tokens If you have obtained an Refresh Token from EGI Check-in Token Portal or oidc-agent issued by the MITREid Connect instance, you will need to replace them by creating new Refresh Tokens issued by Keycloak.\nIf you have obtained Refresh Tokens using the EGI Check-in Token Portal, please check the following table:\nIssuer Production environment Keycloak https://aai.egi.eu/token-keycloak MITREid Connect (Legacy) https://aai.egi.eu/token If you have obtained Refresh Tokens using the oidc-agent, please use the following command:\noidc-gen --pub --issuer \u003cISSUER\u003e --scope ... Note You can find the ISSUER in the Endpoints table. Common issues Error messages referring to missing code_challenge, code_challenge_method or code_verifier HTTP parameter If you get error messages containing the PKCE HTTP parameters, probably the PKCE mode is enabled in your Service Configuration but the Application is not performing the PKCE mode.\nTo solve this, you need to follow the steps below:\nLogin to Federation Registry Open your Service Configuration Click on the “Protocol Specific” tab and scroll down to “Proof Key for Code Exchange (PKCE) Code Challenge Method” and select “PKCE will not be used for this service” Click on “Submit” to apply the reconfiguration request Error messages referring to invalid_code If you try to perform the Authorization Code flow and you get an invalid_code error message, probably the Application sends the authorization request to the Authorization Endpoint of the Keycloak based EGI Check-in OP and then sends the code to the Token Endpoint of the MITREid Connect based EGI Check-in OP or vice versa.\nTo fix this you need to verify that you have updated all the OIDC Endpoints with the Keycloak ones. You can find all the OIDC Endpoints of Keycloak in the Endpoint table.\nError messages referring to the redirect_uri If you try to perform the Authorization Code flow and you get an invalid_redirect_uri error, probably the redirect_uri in the Authorization Request mismatches with the Allowed Redirect URIs in the Service Configuration.\nTo solve this, you need to follow the steps below:\nLogin to Federation Registry Open your Service Configuration Click on the “Protocol Specific” tab and in the “Redirect URI(s)” edit the URI. Click on “Submit” to apply the reconfiguration request UserInfo invalid_token or 401 Unauthorized error response If you are trying to make a request to the UserInfo Endpoint and the response contains the invalid_token error message, probably you are using an invalid Token or the UserInfo endpoint is wrong.\nTo solve this, please make sure the that:\nYou have obtained an Keycloak issued Access Token and you make a request to the Keycloak based UserInfo Endpoint You have added the Access Token to the Authorization header of the request Integrating Science Gateways with RCauth for obtaining (proxy) certificates In order for Science Gateways (VO portals) to obtain RFC proxy certificates derived from personal end-entity certificates, an EGI Science Gateway can make use of the IGTF-approved IOTA-type RCauth.eu online CA. The actual integration goes via an intermediary service, called a Master Portal. EGI is running two Master Portal instances, one development, one production instance.\nProduction Development Endpoint Production environment Provider configuration https://aai.egi.eu/mp-oa2-server/.well-known/openid-configuration Client registration https://aai.egi.eu/mp-oa2-server/register Authorisation https://aai.egi.eu/mp-oa2-server/authorize Token https://aai.egi.eu/mp-oa2-server/token JSON Web Key(jwt) https://aai.egi.eu/mp-oa2-server/certs User Info https://aai.egi.eu/mp-oa2-server/userinfo Endpoint Development environment Provider configuration https://masterportal-pilot.aai.egi.eu/mp-oa2-server/.well-known/openid-configuration Client registration https://masterportal-pilot.aai.egi.eu/mp-oa2-server/register Authorisation https://masterportal-pilot.aai.egi.eu/mp-oa2-server/authorize Token https://masterportal-pilot.aai.egi.eu/mp-oa2-server/token JSON Web Key(jwt) https://masterportal-pilot.aai.egi.eu/mp-oa2-server/certs User Info https://masterportal-pilot.aai.egi.eu/mp-oa2-server/userinfo Registering a client at the Master Portal In order to register a new client for your VO portal go to:\nEGI Development instance: https://masterportal-pilot.aai.egi.eu/mp-oa2-server/register EGI Production instance: https://aai.egi.eu/mp-oa2-server/register Note Make sure to store the client_id and client_secret in a secure place In order to get the client approved, send an email to the administrator of the EGI Master Portal using checkin-support \u003cAT\u003e mailman.egi.eu.\nDetailed information For further and detailed instructions on the integration flow, see the generic RCAuth.eu MasterPortal VOPortal integration guide\nSSH key authentication for proxy retrieval The EGI MasterPortal also allows users to authenticate using SSH key pair, in order to retrieve proxy certificates from the MasterPortal. Users need to first upload the public key via a self-service portal, https://aai.egi.eu/sshkeys/. About once a week they need to follow a web-flow to ensure a long-lived proxy certificate is present in MasterPortal, e.g. by going to https://aai.egi.eu/vo-portal/. They can then obtain a proxy certificate by doing\nssh proxy@ssh.aai.egi.eu and storing the output in /tmp/x509up_u$(id -u)\nGeneric information for users on how to do this can be found at Instructions for end users on how to use the SSH key authN for proxy retrieval. Alternatively VO portals could implement such functionality themselves by using the API described at the Master Portal sshkey endpoint description.\nUser attributes This section defines the attributes that can be made available to services connected to Check-in.\n1. Community User Identifier attribute name Community User Identifier description The User’s Community Identifier is a globally unique, opaque, persistent and non-reassignable identifier identifying the user. For users whose community identity is managed by Check-in, this identifier is of the form \u003cuniqueID\u003e@egi.eu. The \u003cuniqueID\u003e portion is an opaque identifier issued by Check-in SAML Attribute(s) urn:oid:1.3.6.1.4.1.25178.4.1.6 (voPersonID)urn:oid:1.3.6.1.4.1.5923.1.1.1.13 (eduPersonUniqueId) OIDC scope voperson_idaarcopenid OIDC claim(s) voperson_idsub OIDC claim location ID tokenUserinfo endpointIntrospection endpoint origin The Community User Identifier is assigned by Check-in or an external AAI service managing the community identity of the user changes No multiplicity No availability Always example ef72285491ffe53c39b75bdcef46689f5d26ddfa00312365cc4fb5ce97e9ca87@egi.eu notes Use Community User Identifier within your application as the unique-identifier key for the user. Obtaining the Community User Identifier from the sub claim using the openid scope for OIDC Relying Parties or from eduPersonUniqueId for SAML Relying Parties will be deprecated. OIDC RPs should request either the voperson_id or aarc scope to obtain the Community User Identifier. SAML PRs should request the voPersonID attribute to obtain the Community User Identifier. status Stable 2. Display Name attribute name Display Name description The user’s full name, in a displayable form SAML Attribute(s) urn:oid:2.16.840.1.113730.3.1.241 (displayName) OIDC scope profileaarc OIDC claim(s) name OIDC claim location Userinfo endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example John Doe notes - status Stable 3. Given Name attribute name Given Name description The user’s first name SAML Attribute(s) urn:oid:2.5.4.42 (givenName) OIDC scope profileaarc OIDC claim(s) given_name OIDC claim location Userinfo endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example John notes - status Stable 4. Family Name attribute name Family Name description The user’s last name SAML Attribute(s) urn:oid:2.5.4.4 (sn) OIDC scope profileaarc OIDC claim(s) family_name OIDC claim location Userinfo endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example Doe notes - status Stable 5. Username attribute name Username description The username by which the user wishes to be referred to SAML Attribute(s) urn:oid:0.9.2342.19200300.100.1.1 (uid) OIDC scope profileaarc OIDC claim(s) preferred_username OIDC claim location ID tokenUserinfo endpointIntrospection endpoint origin Check-in assigns this attribute on user registration changes No multiplicity Single-valued availability Always example jdoe notes The Service Provider MUST NOT rely upon this value being unique status Stable 6. Email Address attribute name Email Address description The user’s email address SAML Attribute(s) urn:oid:0.9.2342.19200300.100.1.3 (mail) OIDC scope emailaarc OIDC claim(s) email OIDC claim location Userinfo endpointIntrospection endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example john.doe@example.org notes This MAY NOT be unique and is NOT suitable for use as a primary key status Stable 7. Verified email flag attribute name Verified email flag description True if the user’s email address has been verified; otherwise false SAML Attribute(s) See Verified email list OIDC scope emailaarc OIDC claim(s) email_verified OIDC claim location Userinfo endpointIntrospection endpoint origin Check-in assigns this attribute on user registration changes Yes multiplicity Single-valued availability Always example true notes This claim is available only in OpenID Connect status Stable 8. Verified email list attribute name Verified email list description A list of user’s email addresses that have been verified SAML Attribute(s) urn:oid:1.3.6.1.4.1.25178.4.1.14 (voPersonVerifiedEmail) OIDC scope emailaarc OIDC claim(s) voperson_verified_email OIDC claim location Userinfo endpointIntrospection endpoint origin Check-in or the user’s Identity Provider changes Yes multiplicity Multi-valued availability Not always example john.doe@example.orgjdoe@example.com notes - status Experimental 9. Affiliation attribute name Affiliation description The user’s affiliation within a particular security domain (scope) SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.9 (eduPersonScopedAffiliation) OIDC scope eduperson_scoped_affiliation OIDC claim(s) eduperson_scoped_affiliation OIDC claim location Userinfo endpointIntrospection endpoint origin Check-in assigns this attribute on user registration changes Yes multiplicity Multi-valued availability Always example member@example.orgfaculty@example.org notes Service Providers are encouraged to validate the scope of this attribute status Stable 10. Groups attribute name Groups description The user’s group/VO membership/role information expressed as entitlements SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement) OIDC scope eduperson_entitlement OIDC claim(s) eduperson_entitlement OIDC claim location Userinfo endpointIntrospection endpoint origin Group memberships are managed by group administrators changes Yes multiplicity Multi-valued availability Not always example urn:mace:egi.eu:aai.egi.eu:member@fedcloud.egi.euurn:mace:egi.eu:aai.egi.eu:vm_operator@fedcloud.egi.eu notes - status Stable 11. Capabilities attribute name Capabilities description This attribute describes the resource or child-resource a user is allowed to access, optionally specifying certain actions the user is entitled to perform, as described in AARC-G027 SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement) OIDC scope eduperson_entitlement OIDC claim(s) eduperson_entitlement OIDC claim location Userinfo endpointIntrospection endpoint origin Capabilities are managed by Check-in changes Yes multiplicity Multi-valued availability Not always example urn:mace:egi.eu:res:rcauth#aai.egi.euurn:mace:egi.eu:res:gocdb#aai.egi.eu notes - status Stable 12. GOCDB Roles attribute name GOCDB Roles description The user’s GOCDB role information expressed as entitlements SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement) OIDC scope eduperson_entitlement OIDC claim(s) eduperson_entitlement OIDC claim location Userinfo endpointIntrospection endpoint origin The roles are managed in GOCDB changes Yes multiplicity Multi-valued availability Not always example urn:mace:egi.eu:goc.egi.eu:100453G0:GRIDOPS-CheckIn:Site+Administrator@egi.euurn:mace:egi.eu:goc.egi.eu:92503G08:GRIDOPS-MON:Site+Operations+Manager@egi.eu notes - status Stable 13. Assurance attribute name Assurance description Assurance of the identity of the user, following REFEDS Assurance Framework (RAF) and the EGI AAI Assurance Profiles. The following RAF values are qualified and automatically set for all Community identities:$PREFIX$$PREFIX$/ID/unique$PREFIX$/ID/eppn-unique-no-reassign$PREFIX$/IAP/low$PREFIX$/ATP/ePA-1m$PREFIX$/ATP/ePA-1dFollowing RAF values are set if the currently used authentication provider asserts (or otherwise qualifies to) them:$PREFIX$/IAP/medium$PREFIX$/IAP/highThe following compound profiles are asserted if the user qualifies to them$PREFIX$/profile/cappuccino$PREFIX$/profile/espresso SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.11 (eduPersonAssurance) OIDC scope eduperson_assurance OIDC claim(s) eduperson_assurance OIDC claim location Userinfo endpointIntrospection endpoint origin Check-in assigns this attribute on user registration changes Yes multiplicity Multi-valued availability Not always example https://aai.egi.eu/LoA#Lowhttps://refeds.org/assurance/IAP/low notes - status Stable 14. CertEntitlement attribute name CertEntitlement description Provides information about the user’s certificate subject(s) and the associated VO(s) SAML Attribute(s) Not available OIDC scope cert_entitlement OIDC claim(s) cert_entitlement OIDC claim location Userinfo endpointIntrospection endpoint origin VO/group management tools integrated with Check-in changes Yes multiplicity Multi-valued availability Not always example [{\"cert_subject_dn\": \"/C=GR/O=HellasGrid/...\",\"cert_iss\": \"/C=GR/O=HellasGrid/...\",\"eduperson_entitlement\": \"urn:mace:egi.eu:group:checkin-integration:role=VO-Admin#aai.egi.eu\"}] notes This is available only for DIRAC status Stable 15. SSH Public Key attribute name SSH Public Key description Provides information about the user’s SSH public key(s) SAML Attribute(s) urn:oid:1.3.6.1.4.1.24552.500.1.1.1.13 (sshPublicKey) OIDC scope ssh_public_key OIDC claim(s) ssh_public_key OIDC claim location Userinfo endpoint origin Added SSH public key(s) in user’s Check-in Profile changes Yes multiplicity Multi-valued availability Not always example ssh-rsa AAAAB3NzaC...qxxEEipdnZ nikosev@grnet-hq.admin.grnet.grssh-rsa AAAA4xzdIf...fxgsRDfgAt nikosev@example.org notes - status Experimental 16. ORCID iD attribute name ORCID iD description Provides information about the user’s ORCID iD SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.16 (eduPersonOrcid) OIDC scope orcid OIDC claim(s) orcid OIDC claim location Userinfo endpoint origin ORCID Identity Provider changes No multiplicity Single-valued availability Not always example https://orcid.org/XXXX-XXXX-XXXX-XXXX notes The attribute is available when logging in using ORCID status Experimental User authorisation The following information about the authenticated user can be provided by EGI Check-in in order to control user access to resources:\nVO/group membership and role information about the authenticated user Capabilities Identity Assurance GOCDB roles VO/group membership and role information Background VO/group membership and role information is encoded in entitlements (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN.\nSyntax An entitlement value expressing group membership and role information has the following syntax (components enclosed in square brackets are OPTIONAL):\nurn:mace:egi.eu:group:\u003cGROUP\u003e[:\u003cSUBGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e where:\n\u003cGROUP\u003e is the name of a VO, research collaboration or a top level arbitrary group. \u003cGROUP\u003e names are unique within the urn:mace:egi.eu:group namespace; zero or more \u003cSUBGROUP\u003e components represent the hierarchy of subgroups in the \u003cGROUP\u003e; specifying sub-groups is optional the optional \u003cROLE\u003e component is scoped to the rightmost (sub)group; if no group information is specified, the role applies to the VO \u003cGROUP-AUTHORITY\u003e is a non-empty string that indicates the authoritative source for the entitlement value. For example, it can be the FQDN of the group management system that is responsible for the identified group membership information Example:\nurn:mace:egi.eu:group:fedcloud.egi.eu:role=vm_operator#aai.egi.eu Capabilities Background The user’s capability information is encoded in entitlements (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN following the syntax defined in AARC-G027.\nSyntax An entitlement value expressing a capability has the following syntax (components enclosed in square brackets are OPTIONAL):\n\u003cNAMESPACE\u003e:res:\u003cRESOURCE\u003e[:\u003cCHILD-RESOURCE\u003e]...[:act:\u003cACTION\u003e[,\u003cACTION\u003e]...]#\u003cAUTHORITY\u003e where:\n\u003cNAMESPACE\u003e is controlled by the e-infrastructure, research infrastructure or research collaboration that manages the capability. The \u003cNAMESPACE\u003e of capabilities managed by Check-in is set to urn:mace:egi.eu, while, generally, it is in the form of urn:\u003cNID\u003e:\u003cDELEGATED-NAMESPACE\u003e[:\u003cSUBNAMESPACE\u003e]... where:\n\u003cNID\u003e is the namespace identifier associated with a URN namespace registered with IANA2, ensuring global uniqueness. Implementers SHOULD use one of the existing registered URN namespaces, such as urn:mace[MACE].\n\u003cDELEGATED-NAMESPACE\u003e is a URN sub-namespace delegated from one of the IANA registered NIDs to an organisation representing the e-infrastructure, research infrastructure or research collaboration. It is RECOMMENDED that a publicly accessible URN value registry for each delegated namespace be provided.\nThe literal string \"res\" indicates that this is a resource-specific entitlement as opposed to, for example, an entitlement used for expressing group membership AARC-G002.\n\u003cRESOURCE\u003e is the name of the resource. Whether the name should be unique is an implementation decision.\nAn optional list of colon-separated \u003cCHILD-RESOURCE\u003e components represents a specific branch of the hierarchy of resources under the identified \u003cRESOURCE\u003e.\nAn optional list of comma-separated \u003cACTION\u003es MAY be included, which, if present, MUST be prefixed with the literal string “act”. This component MAY be used for further specifying the actions a user is entitled to do at a given resource. Note that the list of \u003cACTION\u003es is scoped to the rightmost child-resource; if no child-resource information is specified, actions apply to the top level resource. The interpretation of a capability without actions specified is an implementation detail.\n\u003cAUTHORITY\u003e is a mandatory and non-empty string that indicates the authoritative source of the capability. This SHOULD be used to further specify the exact issuing instance. For example, it MAY be the FQDN of the service that issued that specific capability. The \u003cAUTHORITY\u003e is specified in the f-component RFC8141 of the URN; thus, it is introduced by the number sign (\"#\") character and terminated by the end of the URN. All characters must be encoded according to RFC8141. Hence, the \u003cAUTHORITY\u003e MUST NOT be considered when determining equivalence (Section 3 in RFC8141) of URN-formatted capabilities. The \u003cAUTHORITY\u003e of capabilities managed by Check-in is typically set to aai.egi.eu.\nExample:\nurn:mace:egi.eu:res:rcauth#aai.egi.eu Identity Assurance Based on the authentication method selected by the user, the EGI proxy assigns a Identity Assurance, which is conveyed to the SP through both the eduPersonAssurance attribute and the Authentication Context Class (AuthnContextClassRef) of the SAML authentication response. EGI Check-in uses Assurance Profiles which distinguish between three Identity Assurance levels, similarly to the eID Assurance Framework (eIDAF). Each level is represented by a URI as follows:\nLow: Authentication through a social identity provider or other low identity assurance provider: https://aai.egi.eu/LoA#Low Substantial: Password/X.509 authentication at the user's home IdP: https://aai.egi.eu/LoA#Substantial High: Substantial + multi-factor authn (not yet supported, TBD): https://aai.egi.eu/LoA#High Moreover, EGI Check-in follows the REFEDS Assurance framework (RAF). The EGI Check-in conveys any RAF values provided by the IdP directly to the SP, through the aforementioned methods. Furthermore, Check-in will append into the User’s profile any additional LoA, if the user is eligible for it. For example, a user having a Verified Email is eligible for the RAF value https://refeds.org/assurance/IAP/low\nSome EGI SPs have been configured to provide limited access (or not to accept at all) credentials with the Low LoA.\nNote: When logging in through the EGI SSO IdP, the LoA is determined based on the selected authentication mechanism as follows:\nUsername/password credentials → Low X.509 certification → Substantial GOCDB Roles Background GOCDB roles, as per GOCDB documentations, are encoded (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN.\nSyntax An entitlement value expressing GOCDB roles has the following syntax (components enclosed in square brackets are OPTIONAL):\nurn:mace:egi.eu:goc.egi.eu:\u003cPRIMARY_KEY\u003e:\u003cON_ENTITY\u003e:\u003cUSER_ROLE\u003e@egi.eu where:\n\u003cPRIMARY_KEY\u003e is the primary key for the user role, e.g. “123G0” \u003cON_ENTITY\u003e is the name of the entity on which the user role applies to, e.g. “GRIDOPS-GOCDB” \u003cUSER_ROLE\u003e is the user’s role, e.g. “Site Operations Manager” Example:\nurn:mace:egi.eu:goc.egi.eu:100453G0:GRIDOPS-CheckIn:Site+Administrator@egi.eu ","categories":"","description":"Check-in guide for Service Providers","excerpt":"Check-in guide for Service Providers","ref":"/providers/check-in/sp/","tags":"","title":"Service Providers"},{"body":"General recommendations All files and folders should be lower case EGI Services should be named exactly as in the EGI Services Portfolio Acronyms should be used only when it makes sense Service names should never be replaced by acronyms When introducing services, link to the public page of the service, if any: [EGI Cloud Compute](https://www.egi.eu/services/cloud-compute/) Markdown writing guidelines Documentation pages have to be written in Markdown, compliant with CommonMark and GitHub Flavored Markdown.\nBasic rules Headings must start at level 2 (##), as level 1 (#) is the title of the page Lines should be wrapped at 80 characters Sentences must be separated by one space only Indent is made with spaces, not with tabs Bullet lists should be using - not * Numbered lists should be using 1. for each line (automatic numbering) Indent secondary (and following) level lists with 2 spaces Lines must end with a Line Feed character (\\n) Files must end with an empty line Shell examples should include a prompt ($ or \u003e) in front of commands, to make it easy to understand which is the command and which is the output Commands in shell examples should be broken into multiple lines of 80 characters or less, using a trailing backslash character (\\) on each line that continues on the next Never break command output in shell examples to multiple lines, instead use style exceptions when necessary Tip Syntax examples that can be used in the files are documented in the shortcodes section. Automating formatting and checking Style should be enforced via the usage of Prettier. Prettier can be integrated with various editors.\nWith VIM/neovim it can be used via a plugin like ALE as described in the official documentation. With VisualStudio Code please see the official documentation Configuration is provided in .prettierrc, options can be set as follows:\n--print-width 80 --tab-width 2 --prose-wrap always When a contribution is received (via a pull request), the proposed changes are checked using various linters.\nGeneral writing guidelines Follow the guidelines below to ensure readability and consistency of the EGI documentation. These are based on the OpenStack documentation writing style guidelines, released under a Creative Commons license.\nTip Short and simple sentences are easier to read and understand. Use standard English Use standard British English (UK) throughout all technical publications. When in doubt about the spelling of a word, consult the Merriam-Webster’s Collegiate Dictionary and the IBM developerWorks editorial style guide.\nBe clear and concise Follow the principles of minimalism. If you can describe an idea in one word, do not use two words. Eliminate all redundant modifiers, such as adjectives and adverbs.\nWrite objectively Do not use humor, jargon, exclamation marks, idioms, metaphors, and other colloquialisms.\nDescribe the most common use case first Put the most common case in the main clause and at the beginning of a paragraph or section. You can introduce additional use cases by starting a sentence with “however” or “if”.\nWrite in active voice In general, write in active voice rather than passive voice. Active voice identifies the agent of action as the subject of the verb, usually the user. Passive voice identifies the recipient (not the source) of the action as the subject of the verb.\nActive-voice sentences clarify the performer of an action and are easier to understand than passive-voice sentences. Passive voice is usually less engaging and more complicated than active voice. When you use passive voice, the actions and responses of the software can be difficult to distinguish from those of the user. In addition, passive voice usually requires more words than active voice.\nExamples Do not use Use After the software has been installed, the computer can be started. After you install the software, start the computer. The Configuration is saved when you click OK. Click OK to save the configuration. A server is created by you. Create a server. However, passive voice is acceptable in the following situations:\nUsing active voice sounds like you are blaming the user. For example, you can use passive voice in an error message or troubleshooting content when the active subject is the user. Example Do not use Use If the build fails, you probably omitted the flavor. If the build fails, the flavor might have been omitted. The agent of action is unknown, or you want to de-emphasize the agent of action and emphasize the object on which the action is performed. Example Do not use Use The product, OS, or database returns the messages. The messages are returned [by the database]. Recasting the sentence in active voice is wordy or awkward. Example Do not use Use In 2009, engineers developed a software that simplifies the installation. A software that simplifies the installation was developed in 2009. Write in second person Users are more engaged with documentation when you use second person (that is, you address the user as “you”).\nWriting in second person has the following advantages:\nSecond person promotes a friendly tone by addressing users directly. Using second person with the imperative mood (in which the subject you is understood) and active voice helps to eliminate wordiness and confusion about who or what initiates an action, especially in procedural steps. Using second person also avoids the use of gender-specific, third-person pronouns such as he, she, his, and hers. If you must use third person, use the pronouns they and their, but ensure that the pronoun matches the referenced noun in number. Use first person plural pronouns (we, our) judiciously. These pronouns emphasize the writer or EGI rather than the user, so before you use them, consider whether second person or imperative mood is more “user-friendly.” However, use “we recommend” rather than “it is recommended” or “EGI recommends”. Tip You can use “we” in the place of EGI if necessary. Do not use first person to avoid naming the product or to avoid using passive voice. If the product is performing the action, use third person (the product as an actor). If you want to de-emphasize the agent of action and emphasize the object on which the action is performed, use passive voice.\nThe first-person singular pronoun “I” is acceptable in the question part of FAQs and when authors of blogs or signed articles are describing their own actions or opinions.\nImportant Do not switch person (point of view) in the same guide or on the same page. Examples Do not use Use Creating a server involves specifying a name, flavor, and image. To create a server, specify a name, a flavor, and image. To create a server, the user specifies a name, flavor, and image. To create a server, you specify a name, flavor, and image. Use the present simple tense Users read documentation to perform tasks or gather information. For users, these activities take place in their present, so the present tense is appropriate in most cases. Additionally, the present tense is easier to read than the past or future tense.\nExample Do not use Use The product will prompt you to verify the deletion. After you log in, your account will then begin the verification process. The product prompts you to verify the deletion. After you log in, your account begins the verification process. Tip Use the future tense only when you need to emphasize that something will occur later (from the users’ perspective). Do not humanize inanimate objects Do not give human characteristics to non-human subjects or objects.\nExample Do not use Use This guide assumes… This guide describes… Avoid personification Do not express your fears or feelings in technical writing. Avoid the adverbs such as “probably”, “hopefully”, “basically”, and so on.\nAvoid ambiguous titles Each title should include a clear description of the page’s or chapter’s subject.\nExample Do not use Use Update metadata Update object metadata Eliminate needless politeness Do not use “please” and “thank you” in technical documentation.\nWrite positively Write in a positive tone. Positive sentences improve readability. Try to avoid the following words as much as possible:\nExamples Do not use Use damage affect catastrophic serious bad serious (or add explanation) fail unable to kill cancel or stop fatal serious destroy remove or delete wrong incorrect or inconsistent Do not use contractions Generally, do not contract the words.\nExamples Do not use Use can’t cannot don’t do not Do not overuse this, that, these, and it Use these pronouns sparingly. Overuse contributes to readers’ confusion. To fix the ambiguity, rephrase the sentence.\nExample Do not use Use The monitoring system should perform regular checks to verify that the Ceph cluster is healthy. This can be achieved using the Ceph health command. The monitoring system performs regular checks to ensure the Ceph cluster is functioning correctly. Use the Ceph health command to run a health check. Tip You can also fix the ambiguity by placing a noun modifier immediately after the pronoun. Do not split infinitives Do not place modifiers between “to” and the verb. Typically, placing an adverb or an adjective between “to” and a verb adds ambiguity to a sentence.\nAvoid prepositions at the end of sentences As much as possible, avoid trailing prepositions in sentences by avoiding phrasal verbs.\nExample Do not use Use The image registration window will open up. The image registration window opens. To fix the verb-preposition constructions, replace them with active verbs.\nExamples Do not use Use written up composed pop up appear Use consistent terminology Use consistent terms across all content. Avoid multiple variations or spellings to refer to the same service, function, UI element, and so on.\nUse spelling and grammar checking tools Run text through spelling and grammar checking tools, if available. Correcting mistakes, especially to larger sections of new content, helps eliminate rework later.\nLists When reading a document for the first time, users scan through pages stopping only on the content that stands out, such as titles, lists, links, diagrams, and so on. Lists help to organize options, as well as help readers to find information easily.\nWhen listing items, follow these guidelines:\nUse a bulleted list for options. Create a bulleted list when you need to describe more than three options. Use a numbered list for steps. Use a definition list to explain terms or describe command-line parameters, options, or arguments. Use a colon at the end of the sentence that introduces a list. Use the same grammatical structure (aka parallel structure) for all items in a list. Start each option with a capital letter. When listing options in a paragraph, add and or or before the last item in a list. Use a serial (Oxford) comma before these conjunctions if they connect three or more items.\nPunctuation in lists In bulleted lists:\nIf you list individual words or phrases, do not add a period at the end of each list item. If you use full sentences, add a period at the end of each sentence. If your list includes both individual words or phrases and full sentences, be consistent and either add or do not add periods to all items. In numbered lists:\nAdd periods at the end of steps. If an item of a numbered list is followed by a code block that illustrates how to perform the step, use a colon at the end. Adding exceptions for style violations Successfully passing the checks is a firm requirement, but for the following cases it is possible to add exceptions and bypass some checks in Markdown files:\nWhen in-line HTML must be used (e.g. in tables, or when no other proper solution is available) When the same procedure needs to be described for multiple platforms, and the automatic checker flags it as duplicate content Important Exceptions should only be used when there are no other choices, and should be confined to the smallest possible block of Markdown code. Dealing with in-line HTML tags In some specific cases it is impossible to use anything but in-line HTML tags:\nPresentation page leveraging bootstrap CSS classes or other advanced features Using special formatting for the information presented (e.g. a list in a table cell) Blocks with in-line HTML tags should be preceded by a HTML comment starting with markdownlint-disable to disable the no-inline-html check, as in the following example:\nTip When having a table is not absolutely necessary, use a different construct to present the information. \u003c!-- markdownlint-disable no-inline-html --\u003e | Action | OCCI | OpenStack | This is a very long column with important data | | ----------- | ------------------------ | ---------------------- | ---------------------------------------------- | | List images | `occi -a list -r os_tpl` | `openstack image list` | \u003cul\u003e\u003cli\u003eLorem\u003c/li\u003e\u003cli\u003eipsum\u003c/li\u003e\u003c/ul\u003e | \u003c!-- markdownlint-enable no-inline-html --\u003e Tip Do not forget to follow up with a HTML comment starting with markdownlint-enable to re-enable the no-inline-html check. Important Always use the tag that is providing the proper semantic: e.g. for a list use \u003cul\u003e and \u003cli\u003e, not \u003cbr /\u003e. Dealing with duplicate content When the same procedure needs to be described for multiple platforms, or when the same code has to be exemplified for multiple languages, it is possible that the automatic checkers will flag these as duplicates.\nFor example, describing the following procedure will result in duplicates being reported:\n{{\u003c tabpanex \u003e}} {{\u003c tabx header=\"Linux\" \u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/engine/install/#server), then run the following commands: ```shell $ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Mac\" \u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/desktop/mac/install/), then run the following commands: ```shell $ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Windows\" \u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/desktop/windows/install/), then run the following commands: ```shell \u003e docker pull tdviet/fedcloudclient \u003e docker run -it tdviet/fedcloudclient bash ``` {{\u003c /tabx \u003e}} {{\u003c /tabpanex \u003e}} This type of content should be preceded by a HTML comment that disables the check for duplicates, and followed by another HTML comment that enables it again.\n\u003c!-- // jscpd:ignore-start --\u003e ...content with duplicates here... \u003c!-- // jscpd:ignore-end --\u003e ","categories":"","description":"Style guide for EGI documentation","excerpt":"Style guide for EGI documentation","ref":"/about/contributing/style/","tags":"","title":"Style Guide"},{"body":"Overview The following guides and tutorials show you how to perform common tasks in the EGI infrastructure. Each of the following sections addresses a specific use case: it describes how to set up the EGI services needed for that use case, how to connect or operate them together, and will walk you step-by-step towards achieving a task or setup that can be immediately used by researchers and scientists.\n","categories":"","description":"Tutorials for common uses cases in the EGI infrastructure\n","excerpt":"Tutorials for common uses cases in the EGI infrastructure\n","ref":"/users/tutorials/","tags":"","title":"Tutorials \u0026 Guides"},{"body":"Virtual Organisations (VOs) in Check-in are represented as Collaborative Organisation Units (COUs).\nIn order to join a Virtual Organisation you must have an EGI account. If you don’t have, then first sign up for an EGI account.\nLogin to Check-in registry with your EGI account.\nExpand the People drop down menu and click Enroll.\nClick the Begin link of the Enrollment flow of the VO you want to join\nClick the Begin button to start the Enrollment flow\na. If there are no pending petitions the enrollment flow will continue as usual.\nb. If there is one pending petition:\nb1. If its status is in Pending Approval you will see a page similar to this:\nWhere you can:\nClick the Notify Approver(s) Again button and a reminder email will be sent to approver(s)\nClick Proceed with new enrollment, so a new enrollment flow will start\nClick Delete and proceed with new enrollment. In this case, the pending petition will be deleted and a new enrollment flow will be created.\nb2. If its status is not in Pending Approval, Pending Confirmation or Finalized you will see a page similar to this:\nWhere you can:\nClick the Resume button and continue the enrollment flow\nClick Proceed with new enrollment, so a new enrollment flow will start\nClick Delete and proceed with new enrollment. In this case, the pending petition will be deleted and a new enrollment flow will start.\nc. If there is more than one pending petition related to the enrollment flow:\nWhere you can:\nClick the View Button of each petition and review it. (see (b) use case above)\nClick Proceed with new enrollment and a new enrollment flow will start\n","categories":"","description":"Joining a Virtual Organisation (VO) in Check-in\n","excerpt":"Joining a Virtual Organisation (VO) in Check-in\n","ref":"/users/aai/check-in/joining-virtual-organisation/","tags":"","title":"Joining a Virtual Organisation"},{"body":"For monitoring purposes, each service endpoints registered into the Configuration Database, and having the flags production and monitored should include the endpoint URL information in order to be contacted by the corresponding service-specific nagios probe.\nThe information needed for service type are:\nSRM: the value of the attribute GlueServiceEndpoint published in the Configuration Database or BDII (e.g. httpg://se.egi.eu:8444/srm/managerv2) Cloud: org.openstack.nova: The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3 org.openstack.swift:The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3 eu.egi.cloud.accounting: for the host sending the records to the accounting repositority Other service types: the value of the attribute GlueServiceEndpoint published in the BDII It is also possible to register additional endpoints for every services, they will also be monitored if the “Monitored” flag is set.\nFor having more information about managing the Service endpoints in the Configuration Database, please consult the service endpoints documentation.\nRetrieving the information For retrieving the queue URL from the BDII, you can use the command lcg-infosites, to be executed from an UI. Be sure to query a production Top BDII: you can either use the one provided by your Operations Centre or choose one from the Configuration Database\nFor example:\n$ export LCG_GFAL_INFOSYS=egee-bdii.cnaf.infn.it:2170 $ lcg-infosites --vo ops ce | grep nikhef 5680 15 0 0 0 dissel.nikhef.nl:2119/jobmanager-pbs-infra 5680 17 1 1 0 gazon.nikhef.nl:8443/cream-pbs-infra 5680 15 0 0 0 juk.nikhef.nl:8443/cream-pbs-infra 5680 15 0 0 0 klomp.nikhef.nl:8443/cream-pbs-infra 5680 16 0 0 0 stremsel.nikhef.nl:8443/cream-pbs-infra In order to find the GlueServiceEndpoint URL of your SRM service, you can launch a LDAP query to your Site BDII (or directly to the SRM service):\n$ ldapsearch -x -LLL -H ldap://sbdii01.ncg.ingrid.pt:2170 \\ -b \"mds-vo-name=NCG-INGRID-PT,o=grid\" \\ '(\u0026(objectClass=GlueService)(GlueServiceType=SRM))' \\ GlueServiceEndpoint dn: GlueServiceUniqueID=httpg://srm01.ncg.ingrid.pt:8444/srm/managerv2,Mds-Vo-name=NCG-INGRID-PT,o=grid GlueServiceEndpoint: httpg://srm01.ncg.ingrid.pt:8444/srm/managerv2 In a similar way, by just changing the value of GlueServiceType, you can retrieve the endpoint URLs of other services.\nAn alternative way for retrieving the GlueServiceEndpoint URL is using the GLUE2 information browser provided by VAPOR: select your NGI, then your site and hence the Storage service; click on the endpoint details button for finding the URL associated to the SRM interface.\nFilling the information in URLs information are completely missing Editing the services information Site overview This is the home page regarding your site. You need to fill in the URL information.\nClick on a service for displaying its page (e.g. the CREAM-CE).\nEditing a service Click on the EDIT button on the top right corner\nAdding a Service URL fill in the Service URL field with the queue URL\nReviewing the site Now the CREAM-CE service endpoint contains the required queue information.\nProceed in a similar way for the other services.\nAdditional endpoints information In case you need to register an additional endpoint for a service, go on the service summary page and add the proper information. In the example below it is shown the case of a computing element.\nService summary page This is the service summary page.\nYou need to click on the Add endpoint button for registering additional endpoint URLs.\nAdding an endpoint Fill in the proper information and don’t forget to select the “Monitored” flag for making nagios to detect the new endpoint.\nReviewing the endpoint description The summary page of the endpoint just added should look like this one.\nReviewing the service description And this is the summary page of the service reporting the information about all its endpoints registered: the first one in the Grid Information section and the additional ones in the Service Endpoints section.\nExamples webdav In order to properly monitor your webdav endpoint:\nyou should register a new service endpoint with the webdav service type, separated from the SRM one; the endpoint URL information used for monitoring purposes should be set in the extension properties section. Create the following: Name: ARGO_WEBDAV_OPS_URL Value: webdav URL containing also the VO ops folder, for example: https://darkstorm.cnaf.infn.it:8443/webdav/ops or https://hepgrid11.ph.liv.ac.uk/dpm/ph.liv.ac.uk/home/ops/ it corresponds to the value of GLUE2 attribute GLUE2EndpointURL (containing the used port and without the VO folder); verify that the webdav URL (for example: https://darkstorm.cnaf.infn.it:8443/webdav) is properly accessible. EOS and XrootD service endpoints The EOS service endpoints expose an XrootD interface, so in order to properly monitor them, even in case you provide a plain XrootD endpoint, please do the following:\nyou should register a new service endpoint with the XrootD service type; the endpoint URL information used for monitoring purposes should be set in the extension properties section. Create the following: Name: ARGO_XROOTD_OPS_URL Value: XRootD base SURL to test (the path where ops VO has write access), for example: root://eosatlas.cern.ch//eos/atlas/opstest/egi/, root://recas-se-01.cs.infn.it:1094/dpm/cs.infn.it/home/ops/, root://dcache-atlas-xrootd-ops.desy.de:2811/pnfs/desy.de/ops or similar). Pay attention to the port configured for the interface. GridFTP In order to properly monitor your gridftp endpoint for ops VO\nregister a new service endpoint, associating the storage element hostname to the service type globus-GRIDFTP, with the “production” flag disabled; in the “Extension Properties” section of the service endpoint page, fill in the following fields: Name: SE_PATH Value: /dpm/ui.savba.sk/home/ops (this is an example, set the proper path) check if the tests are OK (it might take some hours for detecting the new service endpoint) and then switch the production flag to “yes” SURL value for SRM The SURL value needed by the SRM monitoring probes is the following structure:\nsrm://\u003chostname\u003e:\u003cport\u003e/srm/managerv2?SFN=\u003cGlueSAPath or GlueVOInfoPath\u003e\nFor example:\nsrm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/pnfs/in2p3.fr/data/dteam/\nAs explained in previous sections, you can retrieve the port number from the GlueServiceEndpoint URL information. If your SE provides GlueSAPath information, use that. To retrieve it: $ ldapsearch -x -LLL -H \u003cldap://sbdii01.ncg.ingrid.pt:2170\u003e \\ -b \"mds-vo-name=NCG-INGRID-PT,o=grid\" \\ '(\u0026(objectClass=GlueSA)(GlueSAAccessControlBaseRule=VO:ops))' \\ GlueSAPath GlueChunkKey dn: GlueSALocalID=opsdisk:replica:online,GlueSEUniqueID=srm01.ncg.ingrid.pt,Mds-Vo-name=NCG-INGRID-PT,o=grid GlueChunkKey: GlueSEUniqueID=srm01.ncg.ingrid.pt GlueSAPath: /gstore/t2others/ops if your SE doesn’t provide GlueSAPath information, use instead the GlueVOInfoPath one: $ ldapsearch -x -LLL -H \u003cldap://ntugrid5.phys.ntu.edu.tw:2170\u003e \\ -b \"Mds-Vo-name=TW-NTU-HEP,o=grid\" \\ (\u0026(objectClass=GlueVOInfo)(GlueVOInfoAccessControlBaseRule=VO:ops)) \\ GlueVOInfoLocalID GlueChunkKey dn: GlueVOInfoLocalID=ops:SRR,GlueSALocalID=SRR:SR:replica:*****,GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw,Mds-Vo-name=TW-NTU-HEP,o=grid GlueVOInfoPath: /dpm/phys.ntu.edu.tw/home/ops GlueChunkKey: GlueSALocalID=SRR:SR:replica:***** GlueChunkKey: GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw GlueVOInfoLocalID: ops:SRR dn: GlueVOInfoLocalID=ops:data01,GlueSALocalID=data01:replica:online,GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw,Mds-Vo-name=TW-NTU-HEP,o=grid GlueVOInfoPath: /dpm/phys.ntu.edu.tw/home/ops GlueChunkKey: GlueSALocalID=data01:replica:online GlueChunkKey: GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw GlueVOInfoLocalID: ops:data01 Pay attention to use the storage path for the ops VO On GOCDB, in the “Extension Properties” section of the SRM service endpoint page, fill in the following fields: Name: SURL Value: the actual SURL value, for example: srm://srm01.ncg.ingrid.pt:8444/srm/managerv2?SFN=/gstore/t2others/ops ","categories":"","description":"Adding service endpoints information into Configuration Database","excerpt":"Adding service endpoints information into Configuration Database","ref":"/internal/configuration-database/adding-service-endpoint/","tags":"","title":"Adding service endpoints"},{"body":"Within Rucio there are several levels of administrators. There are the super admins, which are the staff that runs Multi-VO Rucio. Then there are virtual organisation (VO) specific admins that will look after the day-to-day operations of thier VO. Below are some of the tasks that VO admins will need to do to set up and maintain their VO.\nCreating Accounts, Identities, and Quotas To add new users within your VO, you will need to communicate with Rucio as the VO admin. Then using the rucio-admin commands, create a new account and add identities to the account. The account is a username with no permissions, or authentication methods. The identities bind authentication methods and permissions to the account. The account you want to create identities for is input as an argument. Accounts will have different permissions and access (such as how much data they can store on a particular RSE).\nCLI Example $ rucio-admin account add \\ --type USER \\ --email jdoe@email.com jdoe Added new account: jdoe $ rucio-admin identity add \\ --account jdoe \\ --type USER \\ --id userjdoe \\ --email jdoe@email.com \\ --password jdoepass Added new identity to account: userjdoe-jdoe $ rucio-admin account set-limits jdoe storagesite1 100GB Set account limit for account jdoe on RSE storagesite1: 100.000 GB Python Client Example \u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e CLIENT.add_account('jdoe', 'USER', 'jdoe@email.com') True \u003e\u003e\u003e CLIENT.add_identity('jdoe', 'USER', 'jdoe@email.com') True \u003e\u003e\u003e CLIENT.set_account_limit('jdoe', 'storagesite1', 107374182400, 'global') True Creating RSE(s) Rucio Storage Elements (RSEs) are how Rucio represents the physical storage available to your VO. As with many aspects of Rucio there are a lot of optional attributes that can be set for an RSE, but as a minimum a protocol for transfers need to be added before it can be used.\nCreating RSE(s) CLI Example $ rucio-admin rse add NEW_RSE Added new deterministic RSE: NEW_RSE $ rucio-admin rse add-protocol \\ --hostname test.org \\ --scheme gsiftp \\ --prefix '/filepath/rucio/' \\ --port 8443 NEW_RSE \\ --domain-json '{ \"wan\": { \"read\": 1, \"write\": 1, \"third_party_copy\": 0, \"delete\": 1 }, \"lan\": { \"read\": 1, \"write\": 1, \"third_party_copy\": 0, \"delete\": 1 } }' Creating RSE(s) Python Client Example \u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e CLIENT.add_rse('NEW_RSE') True \u003e\u003e\u003e CLIENT.add_protocol( 'NEW_RSE', { 'hostname': 'test.org', 'scheme': 'gsiftp', 'prefix': '/filepath/rucio/', 'port': 8443, 'impl': 'rucio.rse.protocols.gfalv2.Default', 'domain': { \"wan\": { \"read\": 1, \"write\": 1, \"third_party_copy\": 0, \"delete\": 1 }, \"lan\": { \"read\": 1, \"write\": 1, \"third_party_copy\": 0, \"delete\": 1 } } } ) True Updating RSE Protocols On occasion, it may be necessary to change or update an RSE protocol. Unlike settings (rucio-admin rse update) or attributes (rucio-admin rse set-attribute), there isn’t a direct CLI function for changing a protocol. It would therefore be necessary to remove (rucio-admin rse delete-protocol) and then add (rucio-admin rse add-protocol) it again using different information. Alternatively, the Python client has additional functionality to directly update or swap the priority of RSE protocols. For example to update the impl without changing anything else (the data argument is used to update the protocol, with the other settings used to specify the protocol to change):\n\u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e CLIENT.update_protocols( rse='NEW_RSE', scheme='gsiftp', data={ 'impl': rucio.rse.protocols.gfal.Default' }, hostname='test.org', port=8433 ) True To swap the priority of two protocols for the third party copy operation:\n\u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e CLIENT.swap_protocols( rse='NEW_RSE', domain='wan', operation='third_party_copy', scheme_a='gsiftp', scheme_b='root' ) True It’s also worth noting that when an RSE is deleted using rucio-admin rse delete, the entry remains in the database. This “soft” deletion means that attempting to add a new RSE with the same name as a deleted RSE will fail. This is due to the RSE not having a unique name/VO combination. In practice it is therefore better to update a badly configured RSE rather than attempting to delete and re-add it. However, if the latter method is preferred, it is possible manually rename the deleted RSE in the database (as there are no foreign key constraints on its name, just the ID and VO) so that the old name can be re-used.\nBasic Usage This section covers some of the basic Rucio functions that can be run once the VO has accounts and RSEs set up. As with the setup, there are many options that won’t be covered here. For more information refer to either the main documentation or the help for the function in question.\nDaemons Most operations in Rucio (such as transfers, deletions, rule evaluation) require one or more of the daemons to be running in order to take effect. For a multi-VO instance, these should be running for all VOs already. However, on new VO’s joining Rucio some updating of the daemons will be neccessary.\nIf it seems like it is not quite right please contact the Rucio team through GGUS.\nUploading Data In Rucio files and their replicas are represented by Data IDentifiers (DIDs), which are composed of a scope and name. Furthermore, multiple files can be attached to a dataset, which in turn can be attached to a container (which can be attached to another container and so on). Datasets and containers are also represented by DIDs.\nScopes are always associated with a particular Rucio account, and must be added to Rucio using an admin account. If no scope is provided when uploading, Rucio will default to user.\u003caccount\u003e, but this still needs to have been added by an admin.\nOnce a file has been uploaded via the CLI or Python client, it can then be attached to a dataset. It’s worth noting that by default, some Rucio commands will not list files, only datasets.\nUploading Data CLI Example Assuming the file test.txt exists locally:\n$ rucio-admin scope add --account root --scope user.root Added new scope to account: user.root-root $ rucio upload --rse NEW_RSE test.txt 2020-08-14 15:28:15,059 INFO Preparing upload for file test.txt 2020-08-14 15:28:15,235 INFO Successfully added replica in Rucio catalogue at NEW_RSE 2020-08-14 15:28:15,334 INFO Successfully added replication rule at NEW_RSE 2020-08-14 15:28:15,579 INFO Trying upload with mock to NEW_RSE 2020-08-14 15:28:15,579 295 INFO Trying upload with mock to NEW_RSE 2020-08-14 15:28:15,580 INFO Successful upload of temporary file. mock://test.org:123/filepath/rucio/user/root/46/6b/test.txt.rucio.upload 2020-08-14 15:28:15,580 295 INFO Successful upload of temporary file. mock://test.org:123/filepath/rucio/user/root/46/6b/test.txt.rucio.upload 2020-08-14 15:28:15,580 INFO Successfully uploaded file test.txt 2020-08-14 15:28:15,580 295 INFO Successfully uploaded file test.txt 2020-08-14 15:28:15,583 295 DEBUG Starting new HTTPS connection (1): rucio:443 2020-08-14 15:28:15,598 295 DEBUG https://rucio:443 \"POST /traces/ HTTP/1.1\" 201 7 2020-08-14 15:28:15,662 295 DEBUG https://rucio:443 \"PUT /replicas HTTP/1.1\" 200 0 $ rucio list-dids user.root:test.txt --filter type=ALL +--------------------+--------------+ | SCOPE:NAME | [DID TYPE] | |--------------------+--------------| | user.root:test.txt | FILE | +--------------------+--------------+ $ rucio add-dataset user.root:test_dataset Added user.root:test_dataset $ rucio attach user.root:test_dataset user.root:test.txt DIDs successfully attached to user.root:test_dataset $ rucio list-content user.root:test_dataset +--------------------+--------------+ | SCOPE:NAME | [DID TYPE] | |--------------------+--------------| | user.root:test.txt | FILE | +--------------------+--------------+ Uploading Data Python Client Example Assuming the file test.txt exists locally:\n\u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e CLIENT.add_scope('root', 'user.root') True \u003e\u003e\u003e from rucio.client.uploadclient import UploadClient \u003e\u003e\u003e UPLOAD_CLIENT = UploadClient() \u003e\u003e\u003e UPLOAD_CLIENT.upload([{'path': 'test.txt', 'rse': 'NEW_RSE'}]) 2020-08-14 14:47:31,147 8431 DEBUG Starting new HTTPS connection (1): rucio:443 2020-08-14 14:47:31,166 8431 DEBUG https://rucio:443 \"POST /traces/ HTTP/1.1\" 201 7 2020-08-14 14:47:31,224 8431 DEBUG https://rucio:443 \"PUT /replicas HTTP/1.1\" 200 None 0 \u003e\u003e\u003e list(CLIENT.list_dids('user.root', {}, type='all')) [u'test.txt'] \u003e\u003e\u003e CLIENT.add_dataset('user.root', 'test_dataset') True \u003e\u003e\u003e CLIENT.attach_dids('user.root', 'test_dataset', [{'scope': 'user.root', 'name': 'test.txt'}]) \u003e\u003e\u003e list(CLIENT.list_content('user.root', 'test_dataset')) [{u'adler32': u'00000001', u'name': u'test.txt', u'bytes': 0, u'scope': u'user.root', u'type': u'FILE', u'md5': u'd41d8cd98f00b204e9800998ecf8427e'}] Adding Replication Rules Once a DID exists within the Rucio catalogue, replicas of that file, dataset or collection are created and maintained by Replication rules. By uploading a file to a particular RSE, a replication rule is created for that file, however rules can also be added for existing DIDs. As a minimum an RSE and number of copies must be specified, but further options such as lifetime of the rule and selecting RSEs based on user set attributes are also possible.\nAdding Replication Rules CLI Example $ rucio list-rules --account root ID ACCOUNT SCOPE:NAME STATE[OK/REPL/STUCK] RSE_EXPRESSION COPIES EXPIRES (UTC) CREATED (UTC) -------------------------------- --------- ------------------ ---------------------- ---------------- -------- --------------- ------------------- 991f9ace7ed74cad989efde90b6a23c5 root user.root:test.txt OK[1/0/0] NEW_RSE 1 2020-08-14 15:28:15 $ rucio add-rule user.root:test_dataset 1 NEW_RSE bd51b767ef524878bb3cc68db16d2374 $ rucio list-rules --account root ID ACCOUNT SCOPE:NAME STATE[OK/REPL/STUCK] RSE_EXPRESSION COPIES EXPIRES (UTC) CREATED (UTC) -------------------------------- --------- ---------------------- ---------------------- ---------------- -------- --------------- ------------------- 991f9ace7ed74cad989efde90b6a23c5 root user.root:test.txt OK[1/0/0] NEW_RSE 1 2020-08-14 15:28:15 bd51b767ef524878bb3cc68db16d2374 root user.root:test_dataset OK[1/0/0] NEW_RSE 1 2020-08-14 15:47:15 Adding Replication Rules Python Client Example \u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e list(CLIENT.list_account_rules('root')) [{u'locks_ok_cnt': 1, u'source_replica_expression': None, u'locks_stuck_cnt': 0, u'purge_replicas': False, u'rse_expression': u'NEW_RSE', u'updated_at': datetime.datetime(2020, 8, 14, 15, 28, 15), u'meta': None, u'child_rule_id': None, u'id': u'991f9ace7ed74cad989efde90b6a23c5', u'ignore_account_limit': False, u'error': None, u'weight': None, u'locks_replicating_cnt': 0, u'notification': u'NO', u'copies': 1, u'comments': None, u'split_container': False, u'priority': 3, u'state': u'OK', u'scope': u'user.root', u'subscription_id': None, u'stuck_at': None, u'ignore_availability': False, u'eol_at': None, u'expires_at': None, u'did_type': u'FILE', u'account': u'root', u'locked': False, u'name': u'test.txt', u'created_at': datetime.datetime(2020, 8, 14, 15, 28, 15), u'activity': u'User Subscriptions', u'grouping': u'DATASET'}] \u003e\u003e\u003e CLIENT.add_replication_rule([{'scope': 'user.root', 'name': 'test_dataset'}], 1, 'NEW_RSE') [u'76b262b45dca4e769221224e1ccf5c7a'] \u003e\u003e\u003e list(CLIENT.list_account_rules('root')) [{u'locks_ok_cnt': 1, u'source_replica_expression': None, u'locks_stuck_cnt': 0, u'purge_replicas': False, u'rse_expression': u'NEW_RSE', u'updated_at': datetime.datetime(2020, 8, 14, 15, 28, 15), u'meta': None, u'child_rule_id': None, u'id': u'991f9ace7ed74cad989efde90b6a23c5', u'ignore_account_limit': False, u'error': None, u'weight': None, u'locks_replicating_cnt': 0, u'notification': u'NO', u'copies': 1, u'comments': None, u'split_container': False, u'priority': 3, u'state': u'OK', u'scope': u'user.root', u'subscription_id': None, u'stuck_at': None, u'ignore_availability': False, u'eol_at': None, u'expires_at': None, u'did_type': u'FILE', u'account': u'root', u'locked': False, u'name': u'test.txt', u'created_at': datetime.datetime(2020, 8, 14, 15, 28, 15), u'activity': u'User Subscriptions', u'grouping': u'DATASET'}, {u'locks_ok_cnt': 1, u'source_replica_expression': None, u'locks_stuck_cnt': 0, u'purge_replicas': False, u'rse_expression': u'NEW_RSE', u'updated_at': datetime.datetime(2020, 8, 14, 15, 47, 15), u'meta': None, u'child_rule_id': None, u'id': u'bd51b767ef524878bb3cc68db16d2374', u'ignore_account_limit': False, u'error': None, u'weight': None, u'locks_replicating_cnt': 0, u'notification': u'NO', u'copies': 1, u'comments': None, u'split_container': False, u'priority': 3, u'state': u'OK', u'scope': u'user.root', u'subscription_id': None, u'stuck_at': None, u'ignore_availability': False, u'eol_at': None, u'expires_at': None, u'did_type': u'DATASET', u'account': u'root', u'locked': False, u'name': u'test_dataset', u'created_at': datetime.datetime(2020, 8, 14, 15, 47, 15), u'activity': u'User Subscriptions', u'grouping': u'DATASET'}] Multi-VO Features From a users perspective, whether the instance is multi or single VO should not change any functionality. Furthermore, depending on the client setup, VO does not need to be provided. There are however, some occasions when an optional argument for the VO can be given in a multi-VO instance.\nSwapping VOs Just like how an identity can be associated with (and used to authenticate against) multiple accounts, the same identity can be used for accounts at more than one VO. Account and identity can be retrieved from the config file if present, and the VO set there will be used (unless the environment variable RUCIO_VO is also set, in which case the latter takes precedent). Both will be ignored however if the VO is passed as an optional argument in the CLI or Python client. Using this optional argument allows a user to quickly run commands on a different VO they have access to.\nSwapping VOs CLI Example $ rucio whoami status : ACTIVE account : jdoes_abc_account account_type : USER created_at : 2020-08-07T08:27:29 updated_at : 2020-08-07T08:27:29 suspended_at : None deleted_at : None email : N/A $ rucio --vo xyz whoami status : ACTIVE account : jdoes_xyz_account account_type : USER created_at : 2020-08-11T12:13:58 updated_at : 2020-08-11T12:13:58 suspended_at : None deleted_at : None email : N/A Swapping VOs Python Client Example \u003e\u003e\u003e from rucio.client import Client \u003e\u003e\u003e CLIENT = Client() \u003e\u003e\u003e CLIENT.whoami() {u'status': u'ACTIVE', u'account': u'jdoes_abc_account', u'account_type': u'USER', u'created_at': u'2020-08-07T08:27:29', u'updated_at': u'2020-08-07T08:27:29', u'suspended_at': None, u'deleted_at': None, u'email': u'N/A'} \u003e\u003e\u003e CLIENT_XYX = Client(vo='xyz') \u003e\u003e\u003e CLIENT_XYZ.whoami() {u'status': u'ACTIVE', u'account': u'jdoes_xyz_account', u'account_type': u'USER', u'created_at': u'2020-08-11T12:13:58', u'updated_at': u'2020-08-11T12:13:58', u'suspended_at': None, u'deleted_at': None, u'email': u'N/A'} ","categories":"","description":"Help Rucio admins understand and perform actions for their VO","excerpt":"Help Rucio admins understand and perform actions for their VO","ref":"/users/data/management/rucio/admin/","tags":"","title":"Administering Rucio"},{"body":"Overview EGI Data Transfer offers Application Programming Interfaces (APIs) for both regular users and administrator. This page focuses on the user APIs, available in two favors:\nREST API Python Easy Bindings Note Please check out the documentation for more details about the avilable APIs, their parameters and return values. Authentication \u0026 Authorisation Users have to authenticate before they can call the API.\nImportant Authentication requires an X.509 user certificate. Integration with EGI Check-in, which will allow authentication using OIDC tokens is under development. During the authentication phase, credentials are delegated to the FTS service, which will contact the storages to steer the data transfers on behalf of the users.\nThe FTS service supports both plain X.509 proxies and X.509 proxies extended with VO information (VOMS) for authentication and authorisation. You can learn more about VOMS configuration and proxy creation.\nRESTFul API The User RESTFul APIs can be used to submit transfers jobs (collections of single transfers), monitor and cancel existing transfers. Please check the CERN documentation for the full API details. Here we will provide some examples usage using the Curl client.\nChecking how the server sees the identity of the user curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY https://fts3-public.cern.ch:8446/whoami { \"dn\": [ \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi\", \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi/CN=proxy\" ], \"vos_id\": [ \"6b10f4e4-8fdc-5555-baa2-7d4850d4f406\" ], \"roles\": [], \"delegation_id\": \"9ab8068853808c6b\", \"user_dn\": \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi\", \"level\": { \"transfer\": \"vo\" }, \"is_root\": false, \"base_id\": \"01874efb-4735-4595-bc9c-591aef8240c9\", \"vos\": [ \"dteam\" ], \"voms_cred\": [ \"/dteam/Role=NULL/Capability=NULL\" ], \"method\": \"certificate\" } Getting a list of jobs running Filtered by VO\ncurl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY https://fts3-public.cern.ch:8446/jobs?vo_name=dteam [ { \"cred_id\": \"1426115d1660de4d\", \"user_dn\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=ftssuite/CN=737188/CN=Robot: fts3 testsuite\", \"job_type\": \"N\", \"retry\": -1, \"job_id\": \"94560e74-7ca3-11e9-97dd-02163e00d613\", \"cancel_job\": false, \"job_state\": \"FINISHED\", \"submit_host\": \"fts604.cern.ch\", \"priority\": 3, \"source_space_token\": \"\", \"max_time_in_queue\": null, \"job_metadata\": { \"test\": \"test_bring_online\", \"label\": \"fts3-tests\" }, \"source_se\": \"mock://somewhere.uk\", \"bring_online\": 120, \"reason\": null, \"space_token\": \"\", \"submit_time\": \"2019-05-22T15:09:22\", \"retry_delay\": 0, \"dest_se\": \"mock://somewhere.uk\", \"internal_job_params\": \"nostreams:1\", \"overwrite_flag\": false, \"copy_pin_lifetime\": -1, \"verify_checksum\": \"n\", \"job_finished\": null, \"vo_name\": \"dteam\" } ] Cancelling a job curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY \\ -X DELETE \\ https://fts3-pilot.cern.ch:8446/jobs/a40b82b7-1132-459f-a641-f8b49137a713 Getting expiration time of delegated credentials curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY \\ https://fts3-public.cern.ch:8446/delegation/9ab8068853808c6b { \"voms_attrs\": [ \"/dteam/Role=NULL/Capability=NULL\" ], \"termination_time\": \"2020-07-31T22:50:28\" } Python Bindings The Python bindings for FTS can be installed from the EPEL package repository (EL6 and EL7 packages are available) with Python 2.7 being supported.\nyum install python-fts -y For using the bindings, you need to import fts3.rest.client.easy, although for convenience it can be renamed as something else:\nimport fts3.rest.client.easy as fts3 In the following code snippets, an import as above is assumed.\nIn order to be able to do any operation, information about the state of the user credentials and remote endpoint needs to be kept. That’s the purpose of a Context.\ncontext = fts3.Context(endpoint, ucert, ukey, verify=True) The endpoint to use corresponds to the FTS instance REST server and it must have the following format:\nhttps://\\\u003chost\u003e:\\\u003cport\u003e\nfor instance https://fts3-public.cern.ch:8446\nIf you are using a proxy certificate, you can either specify only user_certificate, or point both parameters to the proxy. user_certificate and user_key can be safely omitted, and the program will use the values defined in the environment variables X509_USER_PROXY or X509_USER_CERT + X509_USER_KEY.\nIf verify is False, the server certificate will not be verified.\nHere are some examples about creating a context, submitting a job with a single transfer and getting the job status:\n# pretty print the json outputs \u003e\u003e\u003e import pprint \u003e\u003e\u003e pp = pprint.PrettyPrinter(indent=4) # creating the context \u003e\u003e\u003e context = fts3.Context(\"https://fts3-public.cern.ch:8446\") # printing the whoami info \u003e\u003e\u003e pp.pprint (fts3.whoami(context)) { u'base_id': u'01874efb-4735-4595-bc9c-591aef8240c9', u'delegation_id': u'9ab8068853808c6b', u'dn': [ u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi', u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi/CN=proxy'], u'is_root': False, u'level': { u'transfer': u'vo'}, u'method': u'certificate', u'roles': [], u'user_dn': u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi', u'voms_cred': [u'/dteam/Role=NULL/Capability=NULL'], u'vos': [u'dteam'], u'vos_id': [u'6b10f4e4-8fdc-5555-baa2-7d4850d4f406']} # creating a new transfer and submiting a job \u003e\u003e\u003e transfer = fts3.new_transfer( ... 'gsiftp://source/path', 'gsiftp://destination/path', ... checksum='ADLER32:1234', filesize=1024, ... metadata='Submission example' ... ) \u003e\u003e\u003e job = fts3.new_job([transfer]) \u003e\u003e\u003e job_id = fts3.submit(context, job) \u003e\u003e\u003e print job_id b6191212-d347-11ea-8a47-fa163e45cbc4 # get the job status \u003e\u003e\u003e pp.pprint(fts3.get_job_status(context, job_id)) { u'bring_online': -1, u'cancel_job': False, u'copy_pin_lifetime': -1, u'cred_id': u'9ab8068853808c6b', u'dest_se': u'gsiftp://destination', u'http_status': u'200 Ok', u'internal_job_params': u'nostreams:1', u'job_finished': u'2020-07-31T16:05:55', u'job_id': u'b6191212-d347-11ea-8a47-fa163e45cbc4', u'job_metadata': None, u'job_state': u'FAILED', u'job_type': u'N', u'max_time_in_queue': None, u'overwrite_flag': False, u'priority': 3, u'reason': u'One or more files failed. Please have a look at the details for more information', u'retry': -1, u'retry_delay': 0, u'source_se': u'gsiftp://source', u'source_space_token': u'', u'space_token': u'', u'submit_host': u'fts-public-03.cern.ch', u'submit_time': u'2020-07-31T16:05:54', u'user_dn': u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi', u'verify_checksum': u't', u'vo_name': u'dteam'} ","categories":"","description":"The Application Programming Interface of EGI Data Transfer\n","excerpt":"The Application Programming Interface of EGI Data Transfer\n","ref":"/users/data/management/data-transfer/api/","tags":"","title":"Data Transfer API"},{"body":"Most operations in EGI DataHub can be performed using one of the OneData Application Programming Interfaces (APIs).\nImportant In order to be able to access the Onedata APIs, an access token is required. See below for instructions on how to generate one. Getting an API access token Tokens have to be generated from the EGI DataHub (Onezone) interface as documented in Generating tokens for using Oneclient or APIs or using a command-line call as documented hereafter.\nBear in mind that a single API token can be used with both Onezone, Oneprovider and other Onedata APIs.\nIt’s possible to retrieve the CLIENT_ID and REFRESH_TOKEN using the EGI Check-in Token Portal. See Check-in documentation for more information.\n$ CLIENT_ID=\u003cCLIENT_ID\u003e $ REFRESH_TOKEN=\u003cREFRESH_TOKEN\u003e # Retrieving an OIDC token from Check-in $ curl -X POST \\ -d \"client_id=$CLIENT_ID\u0026grant_type=refresh_token\u0026refresh_token=$REFRESH_TOKEN\u0026scope=openid%20email%20profile%20eduperson_entitlement\" \\ 'https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token' | python -m json.tool; # Token is in the access_token field of the response The following variables should be set:\nOIDC_TOKEN: OpenID Connect Access token. ONEZONE_HOST: name or IP of the Onezone host (to use Onezone API). $ ONEZONE_HOST=https://datahub.egi.eu $ OIDC_TOKEN=\u003cOIDC_ACCESS_TOKEN\u003e $ curl -H \"X-Auth-Token: egi:$OIDC_TOKEN\" -X POST \\ -H 'Content-type: application/json' \\ \"$ONEZONE_HOST/api/v3/onezone/user/tokens/named\" \\ -d '{ \"name\": \"REST and CDMI access token\", \"type\": { \"accessToken\": {} }, \"caveats\": [ { \"type\": \"interface\", \"interface\": \"rest\" } ] }' Data access via CDMI and REST API Below are example commands to learn how to access DataHub files and folders via CDMI and REST API using the command-line interface.\nFor more information please check the Onedata CDMI documentation and the Onedata Oneprovider REST API\nCommon configuration Follow instructions above to get an API access token, and configure environment variables:\n$ export DATAHUB_TOKEN=\u003cDATAHUB_ACCESS_TOKEN\u003e $ export ONEPROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu Having jq installed is useful for better formatting of the JSON output.\nCDMI Configure a header to be passed in some operations.\n$ export CDMI_VSN_HEADER='X-CDMI-Specification-Version: 1.1.1' See examples on how to list a folder, and file download/upload using CDMI:\n# List files in a folder $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -H \"$CDMI_VSN_HEADER\" \\ \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/?children\" | jq . # Download \"helloworld.txt\" from DataHub to \"downloadtest.txt\" on your computer $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/helloworld.txt\" \\ -o downloadtest.txt # Upload \"helloworld.txt\" from your computer to \"uploadtest.txt\" on DataHub $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -H \"$CDMI_VSN_HEADER\" \\ -X PUT \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/uploadtest.txt\" \\ -T helloworld.txt REST API See examples on how to list a folder, and file download/upload using REST API:\n# Get base folder ID $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X POST \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/lookup-file-id/PLAYGROUND\" # Add the folder ID to an environment variable $ export DIR_ID=\u003cID_FROM_PREVIOUS_COMMAND\u003e # List files inside the folder with DIR_ID $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X GET \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$DIR_ID/children\" \\ | jq . # Add the ID of the file that you want to download $ export FILE_ID=\u003cID_FROM_PREVIOUS_COMMAND\u003e # Download file with FILE_ID from DataHub to \"helloworld.txt\" on your computer $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X GET \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$FILE_ID/content\" \\ -o helloworld.txt # Upload \"helloworld.txt\" on your local computer to \"uploadtest.txt\" on DataHub $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X POST \\ \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$DIR_ID/children?name=uploadtest.txt\" \\ -H \"Content-Type: application/octet-stream\" -d \"@helloworld.txt\" Data access from Python If your application is written in Python please check the documentation for the OnedataFS Python library\nTesting the API with the REST client A docker container with clients acting as wrappers around the API calls is available: onedata/rest-cli. It's very convenient for discovering and testing the Onezone and Oneprovider API.\n$ docker run -it onedata/rest-cli # Exporting env for Onezone API $ export ONEZONE_HOST=https://datahub.egi.eu $ export ONEZONE_API_KEY=\u003cACCESS_TOKEN\u003e # Checking current user $ onezone-rest-cli getCurrentUSer | jq '.' # Listing all accessible spaces $ onezone-rest-cli listEffectiveUserSpaces | jq '.' $ docker run -it onedata/rest-cli # Exporting env for Oneprovider API $ export ONEPROVIDER_HOST=https://plg-cyfronet-01.datahub.egi.eu $ export ONEPROVIDER_API_KEY=\u003cACCESS_TOKEN\u003e # Listing all spaces supported by the Oneprovider $ oneprovider-rest-cli getAllSpaces | jq '.' # Listing content of a space $ oneprovider-rest-cli listFiles path='EGI Foundation/' $ oneprovider-rest-cli listFiles path='EGI Foundation/CS3_dataset' Printing the raw REST calls of a wrapped command Raw REST calls (used with curl) can be printed using the --dry-run switch.\n$ docker run -it onedata/rest-cli $ export ONEZONE_HOST=https://datahub.egi.eu $ export ONEZONE_API_KEY=\u003cACCESS_TOKEN\u003e # Listing all accessible spaces $ onezone-rest-cli listEffectiveUserSpaces | jq '.' # Printing the curl command without running it $ onezone-rest-cli listEffectiveUserSpaces --dry-run Working with PID / Handle It’s possible to mint a Permanent Identifier (PID) for a space or a subdirectory of a space using a handle service (like Handle.net) that is registered in the Onezone (EGI DataHub).\nOnce done, accessing the PID using its URL will redirect to the Onedata share allowing to retrieve the files.\nPrerequisites: access to a Handle service registered in the Onezone. See the Handle Service API documentation for documentation on registering a new Handle service or ask a Onezone administrator to authorize you to use an existing Handle service already registered in the Onezone.\nThe following variables should be set:\nAPI_ACCESS_TOKEN: Onedata API access token ONEZONE_HOST: name or IP of the Onezone host (to use Onezone API). ONEPROVIDER_HOST: name or IP of the Oneprovider host (to use Oneprovider API) # Getting the IDs of the available Handle Services $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handle_services\" HANDLE_SERVICE=\u003cHANDLE_SERVICE_ID\u003e # Getting details about a specific Handle service $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handle_services/$HANDLE_SERVICE\" # Listing all spaces $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/effective_spaces/\" | jq '.' # Displaying details of a space $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/spaces/$SPACE_ID\" | jq '.' # Listing content of a space $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEPROVIDER_HOST/api/v3/oneprovider/files/EGI%20Foundation/\" | jq '.' # Creating a share of a subdirectory of a space $ DIR_ID_TO_SHARE=\u003cDIR_ID\u003e $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ -X POST -H 'Content-Type: application/json' \\ -d '{\"name\": \"input\"}' \"$ONEPROVIDER_HOST/api/v3/oneprovider/shares-id/$DIR_ID_TO_SHARE\" | jq '.' # Displaying the share $ SHARE_ID=\u003cSHARED_ID\u003e $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/shares/$SHARE_ID\" | jq '.' # Registering a handle # Proper Dublin Core metadata is required # It can be created using https://nsteffel.github.io/dublin_core_generator/generator_nq.html $ cat metadata.xml # Escape double quotes and drop line return $ METADATA=$(cat metadata.xml | sed 's/\"/\\\\\"/g' | tr '\\n' ' ') # On handle creation the created handles is provided in the Location header $ curl -D - --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ -H \"Content-type: application/json\" -X POST \\ -d '{\"handleServiceId\": \"'\"$HANDLE_SERVICE_ID\"'\", \"resourceType\": \"Share\", \"resourceId\": \"'\"$SHARE_ID\"'\", \"metadata\": \"'\"$METADATA\"'\"}' \\ \"$ONEZONE_HOST/api/v3/onezone/user/handles\" # Listing handles $ curl --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handles\" # Displaying a handle $ HANDLE_ID=\u003cHANDLE_ID\u003e $ curl --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handles/$HANDLE_ID\" Subscribe to file events Following is an example of how to subscribe to DataHub to receive notification on file events which is described in details in the official documentation Subscribe to file events:\n$ curl -N -H \"X-Auth-Token: $TOKEN\" \\ -X POST \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/changes/metadata/$SPACE_ID\" \\ -H \"Content-Type: application/json\" -d \"@./changes_req.json\" This requires the permission set as following:\nFor groups or single users. For single users, one way to add one is to select Effective members\" -\u003e in the user list search for the required user and “Make an owner”. In this case the user will have admins privileges in addition to the one required. As this might not be the desired configuration it will be enough to remove all the unwanted permissions, e.g.: make it the same as the VO to which the user belongs to, and leave only, as extra, the permission shown in the screenshot.\n","categories":"","description":"The Application Programming Interfaces of EGI DataHub\n","excerpt":"The Application Programming Interfaces of EGI DataHub\n","ref":"/users/data/management/datahub/api/","tags":"","title":"DataHub API"},{"body":"This section documents how to run some applications and use the existing tools with EC3.\nHow to run scientific applications in EC3 NAMD cluster To deploy NAMD clusters, please select one of the available LRMS (Local Resource Management System) and choose NAMD from the list of applications.\nHow to use generic tools/practices in EC3 ECAS cluster Check the dedicated ECAS documentation.\nKubernetes Check the Cloud Container Compute documentation.\nMesos + Marathon + Chronos To deploy a virtual cluster with Marathon, Mesos, and Chronos as an orchestration, please select Mesos + Marathon + Chronos from the list of available LRMS.\nOSCAR cluster To deploy Serverless computing for data-processing applications in EGI, please select OSCAR from the list of LRMS (Local Resource Management System). OSCAR supports data-driven serverless computing for file-processing applications. A file upload, to the object storage backend MinIO, will trigger the execution of a chosen shell script running inside a user-defined container. These will be orchestrated as Kubernetes batch jobs. The output data will be uploaded to any object storage backends support. Synchronous invocations are also available.\nAs external object storage providers, the following services can be used:\nExternal MinIO servers, which may be in clusters other than the platform. Amazon S3, the Amazon’s object storage service that offers industry-leading scalability, data availability, security, and performance in the public Cloud. Onedata, the global data access solution for science used in the EGI DataHub. See the documentation to deploy an elastic Kubernetes cluster with the OSCAR platform with EC3: Deploy OSCAR with EC3\nSee some use cases of applications that use the OSCAR framework for event-driven high-throughput processing of files (you can found it in the GitHub repository examples folder):\nInference of a machine learning model: See full description at OSCAR Blog entry. Mask detection: See full description at OSCAR Blog entry. Plants Classification, an application that performs plant classification using Lasagne/Theano. ImageMagick, a tool to manipulate images. Radiomics, a use case about the handling of Rheumatic Heart Disease (RHD) through image computing and Artificial Intelligence (AI). More information in the OSCAR web page\nSLURM cluster To deploy SLURM clusters, please select SLURM from the list of available LRMS. See also the dedicated guide on HTC clusters\n","categories":"","description":"How to run scientific applications and tools with EC3\n","excerpt":"How to run scientific applications and tools with EC3\n","ref":"/users/compute/orchestration/ec3/apps/","tags":"","title":"EC3 Applications and Tools"},{"body":"Approving role and change requests When a registered user applies for a role, the request has to be validated by someone who has the proper permissions to grant such a role. If you request a role on a given entity, any user with a valid role on that entity or above will be able to approve your request.\nExample - If you request a “site administrator” role on site X, then the following users can approve your request:\nsite administrators and security officers of site X regional operations staff, managers and deputies of the Operations Centre to which site X belongs GOCDB admins Role requests you can approve are listed on the Manage roles page (accessible by clicking the Manage roles link in the user status panel in the sidebar).\nIn order to approve or decline role requests, simply click on the accept or deny links in front of each role request.\nRevoking roles If a user within your scope has a role that needs to be revoked, you can do this from the user’s page, where user’s details are listed along with his/her current roles. To revoke a role, simply click on the role name then on the revoke link at the top right of the role’s details page.\nNote: This works for other users within your scope but also for yourself. However just note that if you revoke your own roles you may not have proper permissions to recover them afterwards.\n","categories":"","description":"Approving/revoking accounts, roles and other actions","excerpt":"Approving/revoking accounts, roles and other actions","ref":"/internal/configuration-database/users-roles/approve-revoke/","tags":"","title":"Approving/revoking accounts, roles and other actions"},{"body":"Working with federated resources EGI is a federation of compute and storage resource providers.\nIn order for the providers to be able to contribute their services to the federation and to help scientific user communities take advantage of compute and storage solutions that could potentially span across multiple of these providers, a common Authentication and Authorisation Infrastructure (AAI) is needed.\n","categories":"","description":"Identity and access management in the EGI Cloud\n","excerpt":"Identity and access management in the EGI Cloud\n","ref":"/users/aai/","tags":"","title":"Authentication \u0026 Authorization"},{"body":"What is it? EGI Check-in is a proxy service that allows scientific communities to securely access and control access to resources in the EGI Federated infrastructure. It operates as a central hub that connects federated Identity Providers (IdPs) with EGI service providers.\nUsers can authenticate with their preferred IdP (e.g an eduGAIN account, institutional account, social media account, etc.) to access and use EGI services in a uniform and easy way.\nThe main features of Check-in:\nEnables multiple federated authentication sources using different technologies Has an user registration portal that also allows linking accounts Increases productivity and security by simplifying user authentication and authorization Federated in eduGAIN as a service provider Combines user attributes originating from various authoritative sources (IdPs and attribute provider services) and delivers them to EGI service providers in a transparent way Note EGI Check-In is based on the AARC Blueprint Architecture. The following sections cover how to sign up for and EGI account, and how to use it to access resources.\n","categories":"","description":"Control access to resources in the EGI Infrastructure\n","excerpt":"Control access to resources in the EGI Infrastructure\n","ref":"/users/aai/check-in/","tags":"","title":"EGI Check-in"},{"body":"This documentation covers how to join the EGI Cloud federation as a provider. If you are interested in joining please first contact EGI operations team at operations@egi.eu, expressing interest and providing few details about:\nthe projects you may be involved in as cloud provider.\nthe user communities you want to support (AKA Virtual Organisations or VOs). You can also support the ’long-tail of science’ through the vo.access.egi.eu VO.\nthe technologies (Cloud Management Framework) you want to provide.\ndetails on the current status of your deployment (to be installed or already installed, already used or not, how it is used, who uses the services…)\nIntegration of cloud stacks into EGI FedCloud follows a well-defined path, with certain steps which need to be taken, depending on the cloud stack in question. By integration here, we refer to the proper interoperation with EGI infrastructure services such as accounting, monitoring, authentication and authorisation, etc. These configurations make your site discoverable and usable by the communities you wish to support, and allow EGI to support you in operational and technical matters.\nIntegration of these services implies specific configuration actions which you need to take on your site. These aim to be unintrusive and are mostly to facilitate access to your site by the communities you wish to support, without interfering with normal operations. This can be summarised essentially as:\nNetwork configuration Permissions configuration AAI configuration Accounting configuration Information system integration VM and appliance repository configuration If at any time you experience technical difficulties or need support, please open a ticket or discuss the matter with us on the forum\nYou can follow find integration guides for your cloud management in this documentation.\n","categories":"","description":"IaaS Service providers documentation","excerpt":"IaaS Service providers documentation","ref":"/providers/cloud-compute/","tags":"","title":"Cloud Compute"},{"body":" Command-line tools The various public EGI services can be managed and used/accessed with a wide variety of command-line interface (CLI) tools. The documentation of each service contains a summary of the CLIs that can be used with that service, together with recommendations on which one to use in what context.\nThe FedCloud client The FedCloud client is a high-level Python package for a command-line client designed for interaction with the OpenStack services in the EGI infrastructure.\nTip The FedCloud client is the recommended command-line interface to use with most EGI services. FedCloud client has the following modules (features):\nCheck-in allows checking validity of access tokens and listing Virtual Organisations (VOs) of a token Endpoint can search endpoints in the Configuration Database and extract site-specific information from unscoped/scoped tokens Sites allows management of site configurations OpenStack can perform commands on OpenStack services deployed to sites EC3 allows deploying elastic cloud compute clusters Installation The FedCloud client can be installed with the pip3 Python package manager (without root or administrator privileges).\nLinux / Mac Windows To install the FedCloud client:\n$ pip3 install fedcloudclient This installs the latest version of the FedCloud client, together with its required packages (like openstackclient). It will also create executables fedcloud and openstack, adding them to the bin folder corresponding to your current Python execution environment ($VIRTUAL_ENV/bin for executing pip3 in a Python virtual environment, ~/.local/bin for executing pip3 as user (with --user option), and /usr/local/bin when executing pip3 as root).\nAs there are non-pure Python packages needed for installation, the Microsoft C++ Build Tools is a prerequisite, make sure it’s installed with the following options selected:\nC++ CMake tools for Windows C++ ATL for latest v142 build tools (x86 \u0026 x64) Testing tools core features - Build Tools Windows 10 SDK (\u003clatest\u003e) In case you prefer to use non-Microsoft alternatives for building non-pure packages, please see here.\nTo install the FedCloud client:\n\u003e pip3 install fedcloudclient This installs the latest version of the FedCloud client, together with its required packages (like openstackclient). It will also create executables fedcloud and openstack, adding them to the bin folder corresponding to your current Python execution environment.\nCheck if the installation is correct by executing the client:\n$ fedcloud --version Installing EGI Core Trust Anchor certificates Some sites in the EGI infrastructure use certificates issued by Certificate Authorities (CAs) that are not included in the default OS distribution. If you receive error message “SSL exception connecting to…”, install the EGI Core Trust Anchor Certificates by running the following commands:\n$ wget https://raw.githubusercontent.com/tdviet/python-requests-bundle-certs/main/scripts/install_certs.sh $ bash install_certs.sh Note The above script does not work on all Linux distributions. Change python to python3 in the script if needed, see the README for more details, or follow the official instructions for installing EGI Core Trust Anchor certificates in production environments. Using via Docker container The FedCloud client can also be used without installation, by running it in a Docker container. In this case, the EGI Core Trust Anchor certificates are preinstalled.\nLinux Mac Windows To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n$ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n$ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n\u003e docker pull tdviet/fedcloudclient \u003e docker run -it tdviet/fedcloudclient bash Once you have a shell running in the container with the FedCloud client, usage is the same as from the command-line.\nUsing from EGI Notebooks EGI Notebooks are integrated with access tokens so it simplifies using the FedCloud client. First make sure that you follow the installation steps above. Then, below are the commands that you need to run inside a terminal in JupyterLab:\nexport OIDC_ACCESS_TOKEN=`cat /var/run/secrets/egi.eu/access_token` fedcloud token check Please follow instructions below to learn how to use the fedcloud command.\nUsing from the command-line The FedCloud client has these subcommands:\nfedcloud token for checking access tokens (see token subcommands) fedcloud endpoint for querying the Configuration Database (see endpoint subcommands) fedcloud site for manipulating site configurations (see site subcommands) fedcloud openstack or fedcloud openstack-int for performing OpenStack commands on sites (see openstack subcommands) fedcloud ec3 for provisioning elastic cloud compute clusters (see cluster subcommands) Note See also the complete documentation or read and contribute to the source code. Performing any OpenStack command on any site requires only three options: the site, the VO and the command. For example, to list virtual machine (VM) images available to members of VO fedcloud.egi.eu on the site CYFRONET-CLOUD, run the following command:\n$ fedcloud openstack image list --vo fedcloud.egi.eu --site CYFRONET-CLOUD Authentication Many of the FedCloud client commands need access tokens for authentication. Users can choose whether to provide access tokens directly (via option --oidc-access-token), or generate them on the fly with oidc-agent (via option --oidc-agent-account) or from refresh tokens (via option --oidc-refresh-token, which must be provided together with option --oidc-client-id and option --oidc-client-secret).\nTip Users of EGI Check-in can get a Check-in client ID and refresh token, as well as all the information needed to obtain access tokens for their FedCloud client, by visiting EGI Check-in Token Portal. Tip To provide access tokens automatically via oidc-agent, follow these instructions to register a client, then pass the client name (account name used during client registration) to the FedCloud client via option --oidc-agent-account. Important Refresh tokens have long lifetime (one year in EGI Check-in), so they must be properly protected. Exposing refresh tokens via environment variables or command-line options is considered insecure and will be disabled in the near future in favor of using oidc-agent. If multiple methods of getting access tokens are given at the same time, the FedCloud client will try to get an access token from the oidc-agent first, then obtain one using the refresh token.\nThe default authentication protocol is openid. Users can change the default protocol via the option --openstack-auth-protocol. However, sites may have the protocol fixed in the site configuration (e.g. oidc for the site INFN-CLOUD-BARI).\nThe default OIDC identity provider is EGI Check-in (https://aai.egi.eu/auth/realms/egi). Users can set another OIDC identity provider via option --oidc-url.\nNote Remember to also set the identity provider’s name accordingly for OpenStack commands, by using the option --openstack-auth-provider. Environment variables Most of the FedCloud client options can be set via environment variables:\nTip To save a lot of time, set the frequently used options like site, VO, etc. using environment variables. Tip When you want commands to work on all sites in the EGI infrastructure, use ALL_SITES for the --site parameter (pass it directly or via an environment variable). Environment variable Command-line option Default value OIDC_AGENT_ACCOUNT --oidc-agent-account OIDC_ACCESS_TOKEN --oidc-access-token OIDC_REFRESH_TOKEN --oidc-refresh-token OIDC_CLIENT_ID --oidc-client-id OIDC_CLIENT_SECRET --oidc-client-secret OIDC_URL --oidc-url https://aai.egi.eu/auth/realms/egi OPENSTACK_AUTH_PROTOCOL --openstack-auth-protocol openid OPENSTACK_AUTH_PROVIDER --openstack-auth-provider egi.eu OPENSTACK_AUTH_TYPE --openstack-auth-type v3oidcaccesstoken EGI_SITE --site EGI_VO --vo Getting help The FedCloud client can display help for the commands and subcommands it supports. Try running the following command to see the commands supported by the FedCloud client:\n$ fedcloud --help Usage: fedcloud [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. --help Show this message and exit. Commands: ec3 EC3 related commands endpoint endpoint command group for interaction with GOCDB and... openstack Executing OpenStack commands on site and VO openstack-int Interactive OpenStack client on site and VO site Site command group for manipulation with site... token Token command group for manipulation with tokens Similarly, you can see help for e.g. the openstack subcommand by running the command below:\n$ fedcloud openstack --help Usage: fedcloud openstack [OPTIONS] OPENSTACK_COMMAND... Executing OpenStack commands on site and VO Options: --oidc-client-id TEXT OIDC client id --oidc-client-secret TEXT OIDC client secret --oidc-refresh-token TEXT OIDC refresh token --oidc-access-token TEXT OIDC access token --oidc-url TEXT OIDC URL [default: \u003chttps://aai.egi.eu/auth/realms/egi\u003e] --oidc-agent-account TEXT short account name in oidc-agent --openstack-auth-protocol TEXT Check-in protocol [default: openid] --openstack-auth-type TEXT Check-in authentication type [default: v3oidcaccesstoken] --openstack-auth-provider TEXT Check-in identity provider [default: egi.eu] --site TEXT Name of the site or ALL_SITES [required] --vo TEXT Name of the VO [required] -i, --ignore-missing-vo Ignore sites that do not support the VO -j, --json-output Print output as a big JSON object --help Show this message and exit. Note Most commands support multiple levels of subcommands, you can get help for all of them using the same principle as above. Using from Python The FedCloud client can be used as a library for developing other services and tools for EGI services. Most of the functionalities can be called directly from Python code without side effects.\nAn usage example is available on GitHub. Just copy/download the code, add your access token and execute python demo.py to see how it works.\nUsing in scripts The FedCloud client can also be used in scripts for simple automation, either for setting environment variables for other tools, or to process outputs from OpenStack commands.\nSetting environment variables for external tools Some FedCloud commands generate output that contains shell commands to set environment variables with the returned result, as exemplified below.\nLinux / Mac Windows PowerShell Run a command to get details of a project:\n$ export EGI_SITE=IISAS-FedCloud $ export EGI_VO=eosc-synergy.eu $ fedcloud site show-project-id export OS_AUTH_URL=\"https://cloud.ui.savba.sk:5000/v3/\"; export OS_PROJECT_ID=\"51f736d36ce34b9ebdf196cfcabd24ee\"; Run the same command but set environment variables with the returned values:\n$ eval $(fedcloud site show-project-id) The environment variables will have their values set to what the command returned:\n$ echo $OS_AUTH_URL https://cloud.ui.savba.sk:5000/v3/ $ echo $OS_PROJECT_ID 51f736d36ce34b9ebdf196cfcabd24ee Run a command to get details of a project:\n\u003e set EGI_SITE=IISAS-FedCloud \u003e set EGI_VO=eosc-synergy.eu \u003e fedcloud site show-project-id set OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ set OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee If you copy the returned output and execute it as commands in a command prompt:\n\u003e set OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ \u003e set OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee The environment variables will have their values set to what the command returned:\n\u003e set OS_AUTH_URL OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ \u003e set OS_PROJECT_ID OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee Run a command to get details of a project:\n\u003e $Env:EGI_SITE = \"IISAS-FedCloud\" \u003e $Env:EGI_VO = \"eosc-synergy.eu\" \u003e fedcloud site show-project-id $Env:OS_AUTH_URL=\"https://cloud.ui.savba.sk:5000/v3/\"; $Env:OS_PROJECT_ID=\"51f736d36ce34b9ebdf196cfcabd24ee\"; Run the same command but set environment variables with the returned values:\n\u003e fedcloud site show-project-id | Out-String | Invoke-Expression The environment variables will have their values set to what the command returned:\n\u003e $Env:OS_AUTH_URL https://cloud.ui.savba.sk:5000/v3/ \u003e $Env:OS_PROJECT_ID 51f736d36ce34b9ebdf196cfcabd24ee Processing output from OpenStack commands The fedcloud openstack subcommand’s output can be converted to JavaScript Object Notation (JSON) format by using the --json-output option. This is useful for further machine processing of the command output.\nTip JSON output can be processed with a tool like jq, which can slice, filter, map, and transform structured data. It acts as a filter: it takes an input and produces an output. Check out the tutorial for using it to extract data from JSON sources. $ export EGI_SITE=IISAS-FedCloud $ export EGI_VO=eosc-synergy.eu $ fedcloud openstack flavor list --json-output [ { \"Site\": \"IISAS-FedCloud\", \"VO\": \"eosc-synergy.eu\", \"command\": \"flavor list\", \"Exception\": null, \"Error code\": 0, \"Result\": [ { \"ID\": \"0\", \"Name\": \"m1.nano\", \"RAM\": 64, \"Disk\": 1, \"Ephemeral\": 0, \"VCPUs\": 1, \"Is Public\": true }, { \"ID\": \"2e562a51-8861-40d5-8fc9-2638bab4662c\", \"Name\": \"m1.xlarge\", \"RAM\": 16384, \"Disk\": 40, \"Ephemeral\": 0, \"VCPUs\": 8, \"Is Public\": true }, ... ] } ] # The following jq command selects flavors with VCPUs=2 and prints their names $ fedcloud openstack flavor list--json-output | \\ jq -r '.[].Result[] | select(.VCPUs == 2) | .Name' m1.medium Note Note that --json-output option can be used only with those OpenStack commands that have outputs. Using this parameter with commands with no output (e.g. setting properties) will generate an unsupported parameter error. ","categories":"","description":"EGI command line interface\n","excerpt":"EGI command line interface\n","ref":"/users/getting-started/cli/","tags":"","title":"Command Line Interface"},{"body":"ROD intra-team communication At the end of a shift the current ROD team should prepare the handover for internal ROD matters. Each ROD can decide independently on what the handover should look like and how it should be passed on to the next team. The following list provides mere suggestions for what should be included:\na list of tickets which will continue into the next week. Each item should contain the name of the site in question, EGI Helpdesk ticket number, an optional ROD ticket ID if your NGI uses an internal ticket system, and the current status of the ticket; any tickets opened that are not related to a particular alarm; a summary of problems encountered with core grid services; a report of any problems with operational tools that occurred during the shift; anything else the new team should be aware of. For internal communication ROD can use mailing list(s), instant messengers, etc. Each ROD team is free to choose how the internal communication is established.\nCommunication with EGI Operations and site administrators ROD should provide an email contact to where all ticket information should be sent and register this address into the EGI Helpdesk. Another address (or possibly the same) should be made available to make it possible for EGI Operations, site administrators, or other bodies to contact them directly.\nROD should communicate with EGI Operations through the mailing list “operations (AT) egi.eu”. Urgent matters should be communicated via a Helpdesk ticket assigned to the EGI Operations support unit to make tracking of the case possible.\nThere is also the handover section in the ROD dashboard which allows for EGI Operations team and RODs to intercommunicate.\n","categories":"","description":"Communication channels for RODs.","excerpt":"Communication channels for RODs.","ref":"/providers/rod/communication/","tags":"","title":"Communication channels"},{"body":"What is it? The EGI Configuration Database (GOCDB) is a central registry that records topology information about all sites participating in the EGI infrastructure.\nThe configuration database also provides different rules and grouping mechanisms for filtering and managing the information associated to resources. This can include entities such as operations and resource centres, service endpoints and their downtimes, contact information and roles of staff responsible for operations at different levels.\nThe configuration database is used by all the actors (end users, site managers, NGI managers, support teams, VO managers), by other tools, and by third party middleware to discover information about the infrastructure topology.\n","categories":"","description":"Topology and configuration registry for sites in EGI infrastructure\n","excerpt":"Topology and configuration registry for sites in EGI infrastructure\n","ref":"/internal/configuration-database/","tags":"","title":"Configuration Database"},{"body":"Thank you for taking the time to contribute to this project. The maintainers greatly appreciate the interest of contributors and rely on continued engagement with the community to ensure this project remains useful. We would like to take steps to put contributors in the best possible position to have their contributions accepted. Please take a few moments to read this short guide on how to contribute.\nNote Before you start contributing to the EGI documentation, please familiarize yourself with the concepts used by documentation authors. When authoring pages, please observe and adhere to the Style Guide. Tip We also welcome contributions regarding how to contribute easier and more efficiently. Feedback and questions If you wish to discuss anything related to the project, please open a GitHub issue or start a topic on the EGI Community Forum.\nNote The maintainers will move issues from GitHub to the community forum when longer, more open-ended discussion would be beneficial, including a wider community scope. Contribution process All contributions have to go through a review process, and contributions can be made in two ways:\nFor simple contributions navigate to the documentation page you want to improve, and click the Edit this page link in the top-right corner (see also the GitHub documentation). You will be guided through the required steps. Be sure to save your changes quickly as the repository may be updated by someone else in the meantime. For more complex contributions or when you want to preview and test changes locally you should fork the repository as documented on the Using Git and GitHub page. Contributing via PRs Note If you need to discuss your changes beforehand (e.g. adding a new section or if you have any doubts), please consult the maintainers by creating a GitHub issue.\nYou can also create an issue by navigating to a documentation page, and clicking the Create documentation issue link in the top-right corner.\nBefore proposing a contribution via the so-called Pull Request (PR) workflow, there should be an open issue describing the need for your contribution (refer to this issue number when you submit the PR). We have a three-step process for contributions:\nFork the project if you have not done so yet, and commit changes to a feature branch. Building the documentation locally is described in the README. Create a GitHub PR from your feature branch, following the instructions in the PR template. Perform a code review with the maintainers on the PR. Tip Rebase your fork’s main branch on the EGI documentation repository’s main branch, before you create new feature branches from it. PR requirements If the PR is not finalised mark it as draft using the GitHub web interface, so it is clear it should not be reviewed yet. Explain your contribution in plain language. To assist the maintainers in understanding and appreciating your PR, please use the template to explain why you are making this contribution, rather than just what the contribution entails. Code review process Code review takes place in GitHub pull requests (PRs). See this article if you’re not familiar with GitHub PRs.\nOnce you open a PR, automated checks will verify the style and syntax of your changes and maintainers will review your code using the built-in code review process in GitHub PRs.\nThe process at this point is as follows:\nAutomated syntax and formatting checks are run using GitHub Actions, successful checks are a hard requirement, but maintainers will help you address reported issues. Maintainers will review your changes and merge it if no changes are necessary. Your change will be merged into the repository’s main branch. If a maintainer has feedback or questions on your changes, they will set request changes in the review and provide an explanation. Release cycle The documentation is using a rolling release model, all changes merged to the main branch are directly deployed to the live production environment.\nThe main branch is always available. Tagged versions may be created as needed following semantic versioning when applicable.\nCommunity EGI benefits from a strong community of developers and system administrators, and vice-versa. If you have any questions or if you would like to get involved in the wider EGI community you can check out:\nEGI Community Forum EGI site ","categories":"","description":"Contributing to EGI documentation","excerpt":"Contributing to EGI documentation","ref":"/about/contributing/","tags":"","title":"Contributing"},{"body":"The IM Dashboard is a graphical interface for the IM Server specially developed for EOSC users to access EGI Cloud Compute resources.\nFunctionalities:\nOIDC authentication Display user’s infrastructures Display infrastructure details, template and log Delete infrastructure Create new infrastructure Add nodes to an infrastructure Resize VMs Tip More details about installing and configuring the dashboard are available in the IM Dashboard documentation. Usage The dashboard of the IM enables non advanced users to manage their infrastructures by launching a set of predefined TOSCA templates on top of EGI Cloud Compute resources. The dashboard does not provide all the features provided by the IM service. In case you need more advanced features use the IM Web interface or the IM-CLI.\nLogin Users must use EGI Check-in to log into the dashboard. Once authenticated, they will be redirected to the portfolio of available TOSCA templates.\nMain menu bar The main menu bar is located at the top of the pages:\nThe first button IM Dashboard enables the user to go to the portfolio of available TOSCA templates. Second item Infrastructures redirects to the list of current user deployed infrastructures. In the Advanced item the Settings sub-item displays some configuration settings as the URL of the IM service or the OIDC issuer. External Links show a set of configurable information links (documentation, video tutorials, etc.) Finally, on the right top corner, appears the User menu item. This item shows the full name of the logged user, and an avatar obtained from Gravatar. In this menu the user can access their Cloud Credentials with the cloud providers or log out from the application. Cloud Credentials To be able to access any Cloud site the user must specify the credentials to access them. This page allows the user to specify the credentials for accessing any cloud provider. In the list the user can edit, delete and enable or disable the selected cloud credentials.\nEditing or adding the credentials will show a modal form where the user has the ability to specify all the parameters needed to access the supported cloud providers. In particular, for Cloud Compute sites the user only has to select one of the VOs he is member of and one of sites that supports that VO. These drop-down fields are generated using the information available from the sites and the list of VOs the user is member of.\nTOSCA Templates The list of available TOSCA templates enables the user to select the required topology to deploy. Each TOSCA template can be labelled by the TOSCA developer with any tag that will show a ribbon displayed on the right bottom corner. The special elastic tag is used to mark templates that are configured to automatically manage the elasticity of the deployed cluster.\nThe user must click on the Configure button to set the input values of the TOSCA template and select the VO, Site and Image to use for deploying the infrastructure.\nInitially the user can set a name to describe the infrastructure to be deployed. It will simplify identifying infrastructures. In the firsts tabs, the user can introduce the set of input values of the topology. By default there is only one tab called Input Values, but the TOSCA developer can add or rename them to simplify the selection of input values.\nThe final tab will be the Cloud Provider Selection. In this tab the user first has to select one of the Cloud providers that has been previously added (and not disabled) in the Cloud Credentials page, then has to select the base image used to deploy the VMs. In case of EGI Cloud Compute sites, the user has two options: he can select an image from the list of images provided by the EGI AppDB information system or from the list provided directly by the Cloud site.\nOther providers will only show a drop-down list with the available images. Only in the case of AWS Cloud provider the user has to specify manually the AMI ID of the image.\nInfrastructures This page lists the infrastructures deployed by the user. The first column shows the name set by the user on infrastructure creation, then shows the ID assigned by the IM service, the third column shows the current status of the infrastructure, the fourth show the list of VMs with their IDs and finally a button with a set of actions appears.\nList of Actions The following figure shows the list of actions available to manage the existing infrastructures:\nAdd nodes: The Add nodes action enables to add new VMs to the users' deployment. It will show the list of the different types of nodes currently deployed in the infrastructure and the user must set the number of nodes of each type he wants to deploy. It will also show a dropdown list with the available base images. It will enable changing the base image used to deploy the new nodes. In some cases it will be necessary because the original one has been removed.\nShow template: This action shows the original TOSCA template submitted to create the infrastructure.\nLog: Shows the error/contextualization log of the infrastructure.\nStop: Stops/Suspends all the VMs of the infrastructure.\nStart: Starts/Resumes a previously stopped infrastructure.\nOutputs: Shows the outputs of the TOSCA template. Private key of credentials can be downloaded as a file or copied to the clipboard.\nDelete: Delete this infrastructure and all the associated resources. It also has the option to force the deletion. In this case the infrastructure will be removed from the IM service even if some cloud resources cannot be deleted. Only use this option if you know what you are doing.\nDelete \u0026 Recreate: Delete this infrastructure as the previous option, but once it is deleted it will redirect to the infrastructure creation form, with all the input fields filled with the same set of values used to create the deleted infrastructure.\nReconfigure: Starts the reconfiguration of the infrastructure.\nChange User: Add or change the ownership of the infrastructure at IM level. Providing a valid Access Token of another user, the infrastructure can be shared or transferred to them. If Overwrite is checked, the new user will be the unique owner of the infrastructure (transferring), otherwise it will be added to the list of current users (sharing). The new user must be member of the VO used to create the resources, otherwise he will not be able to manage them.\nVM Info page The VM Info page shows all the information about the selected VM and enables to manage the lifecycle of it. On the top right corner the Manage VM drop-down menu allows to Stop/Start, Reboot, Resize, Reconfigure and Terminate the VM. Furthermore the user can check the error or contextualisation log of this particular VM.\nThe VM information is split in two different tables, the first one with the main information: State, IPs, HW features and the SSH credentials needed to access it. The second table shows additional fields.\nWhen resizing the VM the user must provide the new size of the VM in terms of number of CPUs and amount of memory as shown in the figure below:\n","categories":"","description":"The dashboard of Infrastructure Manager\n","excerpt":"The dashboard of Infrastructure Manager\n","ref":"/users/compute/orchestration/im/dashboard/","tags":"","title":"Infrastructure Manager Dashboard"},{"body":"What is it? EGI Data Transfer allows scientists to move any type of data files asynchronously from one storage to another. The service includes dedicated interfaces to display statistics of on-going transfers and manage storage resource parameters.\nEGI Data Transfer is ideal to move large amounts of files or very large files as the service has mechanisms to verify checksums and ensure automatic retry in case of failures.\nThe main features of EGI Data Transfer are:\nSimplicity. Easy user interfaces for submitting transfers (command-line, Python bindings, WebFTS, Web Monitoring). Reliability. Checksums are automatically calculated for each transfer and failed transfers are retried. Flexibility. Multi-protocol support (WebDAV/HTTTPS, GridFTP, xrootd, SRM, S3, GCloud). Intelligence. Parallel transfer optimization ensures users get the most from network without burning the storages. Transfers can be classified by priority and activity. Tip Eager to test this service? Have a look at our tutorial on how to transfer data in the grid. Note EGI Data Transfer is based on the FTS3 service, developed at CERN. Components FTS3 Server The service is responsible of the asynchronous execution of the file transfer, checksumming and retries in case of errors\nFTS3 REST The RESTFul server which is contacted by clients via REST APIs, CLI and Python bindings\nFTS3 Monitoring A Web interface to monitor transfers activity and server parameters\nWebFTS A web interface that provides a file transfer and management solution in order to allow users to invoke reliable, managed data transfers on distributed infrastructures\nService Instances EGI has signed OLAs with 2 Providers, CERN and STFC, in order to access their FTS3 Service instances.\nThe following endpoints are available:\nCERN FTS REST FTS Mon WebFTS - N.B. Needs personal X.509 certificate installed in your Browser STFC FTS REST FTS Mon N.B. if you access the endpoints via Browser the following CA certificates need to be installed:\nCERN CA certificates UK eScience CA certificates ","categories":"","description":"Very large data transfers in the EGI infrastructure","excerpt":"Very large data transfers in the EGI infrastructure","ref":"/users/data/management/data-transfer/","tags":"","title":"Data Transfer"},{"body":"Here you can find documentation about the EGI DataHub for service providers\n","categories":"","description":"Documentation for EGI DataHub Service Providers","excerpt":"Documentation for EGI DataHub Service Providers","ref":"/providers/datahub/","tags":"","title":"DataHub"},{"body":"Definition A downtime is a period of time for which a service is declared to be inoperable. Downtimes may be scheduled (e.g. for software/hardware upgrades), or unscheduled (e.g. power outages). The Configuration Database stores the following information about downtimes (non exhaustive list):\nThe downtime classification (Scheduled or unscheduled) The severity of the downtime The date at which the downtime was added The start and end of the downtime period A description of the downtime The entities affected by the downtime Manipulating downtimes Viewing downtimes There are different pages in the Configuration Database where downtimes are listed:\nActive \u0026 Imminent, linked from the main menu, that allows users to see currently active downtimes and downtimes planned in the coming weeks. Downtime Calendar, linked from the main menu, that allows users to view and filter all downtimes. Site details, where all the downtimes associated to the site are listed Service endpoint details, where all the downtimes associated to the service endpoint are listed. Service group details, where all the downtimes associated to the service group are listed. Each downtime has its own page providing details, accessible by clicking on the Downtime Id link or similar in downtime listing pages.\nSubscribing to downtimes The EGI Operations Portal provides a publicly-accessible page allowing to view and filter downtimes: Operations Portal.\nAuthenticated users can subscribe to downtimes affecting sites selected using a filter. The downtimes notifications can be sent by email, RSS and iCal, allowing to easily integrate with your calendar.\nAdding downtimes Provided you have proper permissions (check the permissions matrix section), you can add a downtime by clicking on the Add Downtime link in the sidebar.\nThis is done in 2 steps:\nEnter downtime information Specify the full list of impacted services in case there is more than one or select an site to select all the sites associated services. Please note All dates have to be entered in UTC or using the Site Timezone. A downtime can be retrospectively added if its start date is less than 48h in the past (giving a 2 day window to add). downtime classification (scheduled/unscheduled) is determined automatically (see Scheduled or unscheduled section) Editing downtime information To edit a downtime, simply click the edit link on top of the downtime’s details page. A downtime can be retrospectively updated if its start date is less than 48h in the past (giving a 2 day window to modify). Note there are limitations to downtime editing, especially if it has already started, or is due to start in the next 24hrs or is finished. See downtime shortening and extension section for more details. Removing downtimes To delete a downtime, simply click the delete link on top of the downtime’s details page. For integrity reasons, it is only possible to remove downtimes that have not started.\nGood practices and further understanding Scheduled or unscheduled Depending on the planning of the intervention, downtimes can be:\nScheduled: planned and agreed in advance Unscheduled: planned or unplanned, usually triggered by an unexpected failure or at a short term notice EGI defines precise rules about what should be declared as scheduled or unscheduled, based on how long in advance the downtime is declared. These rules are described in MAN02 Service intervention management and are enforced as follows:\nAll downtimes declared less than 24h in advance will be automatically classified as UNSCHEDULED All other downtimes will be classified as SCHEDULED Notes A downtime can be retrospectively declared and/or updated if its start date is less than 48h in the past (giving a 2 day window to add/modify). Although 24h in advance is enough for the downtime to be classified as SCHEDULED, it is good practice to declare it at least 5 working days before it starts. WARNING or OUTAGE? When declaring a downtime, you will be presented the choice of a “severity”, which can be either WARNING or OUTAGE. Please consider the following definitions:\nWARNING means the resource is considered available, but the quality of service might be degraded. Such downtimes generate notifications, but are not taken into account by monitoring and availability calculation tools. In case of a service failure during the WARNING period an OUTAGE downtime has to be declared, cancelling the rest of the WARNING downtime.\nOUTAGE means the resource is considered as unavailable. Such downtimes will be considered as in maintenance by monitoring and availability calculation tools.\nDowntime shortening and extension Limitation rules to downtime extensions are enforced as follows:\nScheduled downtimes due to start in 24 hours cannot be edited in any way, but can be deleted. Other downtimes that have not yet started can be edit and deleted. They can be shortened or moved, i.e. They can be edited such that: Both start and end time are still in the future The duration remains the same or is decreased Ongoing downtimes can not be deleted. A downtime cannot be edited once it has finished, nor can a new downtime be added more than 48 hours into the past. If for any reason a downtime already declared needs to be extended, the procedure is to add another adjacent downtime, before or after.\n","categories":"","description":"Managing and consulting downtimes","excerpt":"Managing and consulting downtimes","ref":"/internal/configuration-database/downtimes/","tags":"","title":"Downtimes"},{"body":"Introduction Sites, Services, service endpoints, and Service Groups can be extended by adding custom key-value pairs (this follows the GLUE2 extensibility mechanism). Extension properties address a number of use cases, such as filtering Sites and/or Services that define particular properties. Selected methods in the Configuration Database API support the ’extensions' URL parameter. This parameter is used to filter resources according to the extensions they define (described below). Properties are rendered in the XML results of the Site/Service/ServiceGroup using the “EXTENSIONS” XML element, for an example see a sample output from get_service_endpoint(link to old EGI Wiki) Note, anyone with permissions over the target entity can add extension properties to that object. This allows ‘Folksonomy’ building: ‘a user-generated system of classifying and organizing content into different categories by the use of metadata such as electronic tags’ A number of use cases can be addressed; e.g. filtering Sites that support a specific property, e.g. ‘P4U_Pilot_Cloud_Wall’ Key-value pairs prevent certain characters from being used in their values. This includes the equals and opening/closing parenthesis chars ‘=()’. This is to simplify lexical parsing of the query. In addition, to guard against cross-site scripting attacks, the quote, double quote, semi-colon and back tick chars are also not allowed. Keys must be unique for a given site, service, or service endpoint, or service group. Extension Properties in the PI Selected PI methods allow results to be filtered by extension properties via the ’extensions’ PI parameter. Supported methods include: get_site, get_site_list, get_service_endpoints and get_service_group, get_downtime, get_downtime_nested, get_site_list. For individual method support please refer to the PI documentation: GOCDB/PI/Technical Documentation (link to old EGI Wiki) The format of the ’extensions’ PI parameter is one or more (key=value) pairs enclosed in brackets. The value part of a (k=v) pair can be ommitted if filtering by value is not required (i.e. ‘(somekey=)’ means select all resources that define the ‘somekey’ property with any value. (k=v) pairs can be optionally prefixed with one of following operators: AND, OR, NOT. If no operator is specified before the FIRST (k=v) pair, then AND is assumed. A single operator applies to ALL the (k=v) pairs to the right of the operator until another operator is encountered. An AND forms a logical conjunction with any previously specified conditions. An OR forms a logical disjunction with any previously specified conditions. A NOT forms a logical conjunction with any previously specified conditions (it can be read as ‘AND NOT’) Because an OR always forms a logical disjunction with any previously specified conditions, you can’t OR against a group occurring to the right that contains multiple k=v pairs e.g. the following is not supported (if there is sufficient demand, it could be considered for a future enhancement): - ((k=v1)AND(k=v2)) OR ((k=v3)AND(k=v4)) Examples:\nEg (note no leading AND): (key1=val)(key2=va2)OR(key3=val3)(key4=val4)NOT(key5=val5)(key6=val6) is expanded to: AND(key1=val)AND(key2=va2)OR(key3=val3)OR(key4=val4)NOT(key5=val5)NOT(key6=val6) which is interpreted as: (((key1=val)AND(key2=va2))OR(key3=val3)) OR(key4=val4) NOT(key5=val5) NOT(key6=val6) Eg: (VObing=true)AND(VObaz=true)AND(VObar=true)OR(s1p1=v1) is equal to: ((VObing=true)AND(VObaz=true)AND(VObar=true))OR(s1p1=v1) Eg: (VO=food)OR(VO2=bar)AND(s4p1=v1) is equal to: ((VO=food)OR(VO2=bar))AND(s4p1=v1) Eg: (VO=food)(s4p1=v1)OR(VObar=true)(VObaz=true) is equal to: ((VO=food)AND(s4p1=v1))OR(VObar=true)OR(VObaz=true) Eg: (VO=food)(s4p1=v1)OR(VObaz=true)AND(VObling=true) is equal to: (((VO=food)AND(s4p1=v1))OR(VObaz=true))AND(VObling=true) To return all sites that define VO with a value of Alice:\n?method=get_site\u0026extensions=(VO=Alice) Use no value to define a wildcard search, i.e. all sites that define the VO property regardless of value:\n?method=get_site\u0026extensions=(VO=) NOTE: From version 5.7 (Autumn/Winter 2016) keys must be unique for a given site, service, or service endpoint, or service group. The following section of documentation has not yet been changed to reflect this.\nExtensions also supports OR/AND/NOT operators. This can be used to search against multiple key values eg:\n?method=get_site\u0026extensions=AND(VO=Alice)(VO=Atlas)(VO=LHCB) These can be used together:\n?method=get_site\u0026extensions=AND(VO=Alice)(VO=Atlas)NOT(VO=LHCB) ?method= get_service_endpoint\u0026extensions=(CPU_HS01_HOUR=1)OR(CPU_HS02_HOUR=2) When no operator is specified the default is AND, therefore the following:\n?method= get_service_endpoint\u0026extensions=(CPU_HS01_HOUR=1)(CPU_HS02_HOUR=2) Is the same as:\n?method= get_service_endpoint\u0026extensions=AND(CPU_HS01_HOUR=1)(CPU_HS02_HOUR=2) The extensions parameter can also be used in conjunction with the existing parameters previously supported:\n?method=get_site\u0026extensions=(VO=Alice)NOT(VO=LHCB)\u0026scope=EGI\u0026roc=NGI_UK The site_extensions and service_extensions can also be used on the get_downtime and get_downtime_nested_services methods using same logic described above. Note, the EXTENSIONS element is not rendered in the XML output for these queries.\n?method=get_downtime_nested_services\u0026site_extensions=(eg.2=val.2)\u0026service_extensions=(eg.2=) ?method=get_downtime\u0026site_extensions=(eg.2=val.2)\u0026service_extensions=(eg.2=) Standard Extension Properties HostDN For EGI Services, the Standard Extension property “HostDN” has been defined to allow the fetching the DNs of multiple hosts behind a load balanced service from the endpoint properties of a single GOCDB Service, rather than creating multiple GOCDB Services with different host DNs.\nRecommended Use To supply multiple or alternate DN(s) for a service, for example of the multiple hosts supporting a single service entry, the Service Extension Property (hereafter Ext) “HostDN” SHOULD be used. If Ext “HostDN” is present it MUST contain one or more x.509 DN values. Multiple values MUST be delimited by enclosing each within “\u003c\u003e” characters. If Ext “HostDN” is present, the Service “Host DN” SHOULD contain the x.509 SubjectAltName used in the X509 certificate(s) presented by the hosts identified by the Ext “HostDN” values.\n","categories":"","description":"Extension Properties\n","excerpt":"Extension Properties\n","ref":"/internal/configuration-database/extension-properties/","tags":"","title":"Extension Properties"},{"body":"The pages of this section detail multiple features of the EGI Helpdesk.\n","categories":"","description":"Helpdesk features","excerpt":"Helpdesk features","ref":"/internal/helpdesk/features/","tags":"","title":"Helpdesk features"},{"body":"What is it? High Throughput Compute (HTC) is a computing paradigm that focuses on the efficient execution of a large number of loosely-coupled tasks (e.g. data analysis jobs). HTC systems execute independent tasks that can be individually scheduled on many different computing resources, across multiple administrative boundaries. Users submit these tasks to the infrastructure as jobs. After a job have been scheduled and executed, the output can be collected from the service(s) that executed the job.\nTarget users The target customers for EGI High Throughput Compute are research communities that need to share, store, process, and produce large sets of data. Typically, their research collaborations involve organizations across Europe and the World.Some may already have local resources (e.g. universities, research institutions) that can only be accessed by local users in accordance to the respective organisation’s access policies.\nIn case of local compute resources researchers can request access to the local compute cluster from their IT department. However, when researchers join collaborations that need to share their research activities, data collections, and repositories, they need a homogenous and coordinated operation of the compute resources, which are not uniformly accessible. In addition, nowadays many research collaborations generate large amounts of data, and managing such data volumes is time consuming and error-prone.\nThe EGI High Throughput Compute service provides access to compute resources, and offers a set of high-level tools that allow managing large amounts of data in a collaborative way (e.g authorization and access control tools can be regulated by the research collaboration in a central manner, data can be uniformly distributed in the EGI Cloud, etc.).\nFeatures EGI High Throughput Compute provides easy, uniform access to shared computating and data services of EGI service providers. Most software deployed in the distributed resource centers is based on open standards and open source middleware services.\nThe main features of the EGI High Throughput Compute are:\nAccess to high-quality computing resources Integrated monitoring and accounting tools provide information about the availability and resource consumption Workload and data management tools to manage all computational tasks Large amounts of processing capacity over long periods of time Faster results for your research Shared resources enable collaborative research The EGI HTC infrastructure The EGI High Throughput Compute infrastructure is the federation of GRID resources provided by EGI providers. Its aim is to share in a secure way the distributed IT resources that are part of the EGI Cloud. It comprises of:\nCompute Resources – execution environment for computing tasks, organized into clusters distributed across multiple resource centers in Europe and the World. Data Infrastructure – storage servers from different resource centers where users can store their data/files in a distributed manner. Federated Operations – global operational tasks (e.g., AAI, accounting, helpdesk) needed to federate the heterogeneous resources of resource centers and their operational activities. User Support – EGI provides the central user support and coordinates support activities of EGI providers, who offer user support for the services/resources they contribute to the EGI ecosystem. Architecture and service components The key components of the EGI High Throughput Compute architecture are:\nData Transfer service (FTS) Online Storage services Computing Elements (CEs) are compute resources made available through GRID interfaces. The most common implementations of CEs in the EGI infrastructure are HTCondor-CE and ARC-CE. Access model Access to HTC resources in the EGI infrastructure is based on X.509 certificates and Virtual Organisations (VOs).\nVOs are fully managed by research communities, allowing communitites to manage their users and grant access to their services and resources. This means communities can either own their resources and use EGI services to share (federate) them, or can use the resources available in the EGI infrastructure for their scientific needs.\nBefore users can access EGI HTC services, they have to:\nObtain an X.509 certficate. The certificates are issued by Certification Authorities (CAs) part of the European Policy Management Authority for Grid Authentication (EUGridPMA), which is also part of the International Global Trust Federation (IGTF). Enroll into a VO before they can use the services, as users are not individually granted access to resources. Add the certificate to their internet browser of choice, or import it into the appropriate certificate store of their local machine (on Windows). Proceed to Workload Manager to submit HTC jobs or retrieve job results, login using EGI Check-in when prompted. ","categories":"","description":"EGI High Throughput Compute service\n","excerpt":"EGI High Throughput Compute service\n","ref":"/users/compute/high-throughput-compute/","tags":"","title":"High Throughput Compute"},{"body":"Users of the EGI Cloud create Virtual Machines (VMs) on the providers. Those VMs are started from images: templates for the root volume of the running instances, i.e. operating system and applications available initially on a VM. The AppDB collects the Virtual Machine Images available on the service as Virtual Appliances (VA).\nAny user can register new Virtual Appliances at the AppDB, these are then managed by special VO members that curate which appliances are available to their VO.\nAppDB Cloud Marketplace The AppDB is a browsable catalogue of Virtual Appliances that users can start at the providers. You can find below a set of reference guides for the catalogue:\nHow to register a VA?: any registered user can register VAs in AppDB for anyone to download or for making them available at the EGI Cloud providers once a VO adds it to the VO-wide image list. Once registered, VAs can be managed as described in the VA management guide. VO managers select VAs to be available at the providers following the VO-wide image list management. Check the full list of Cloud marketplace guides and Cloud marketplace FAQ for more information about the AppDB features.\nCustom images Packaging your application in a custom VM image is a suggested solution in one of the following cases:\nyour particular OS flavor is not available at AppDB; installation of your application is very complex and time-consuming for being performed during contextualization; or you want to reduce the number of 'moving-parts' of your software stack and follow an immutable infrastructure approach for deploying your application. Custom VM images can be crafted in different ways. The two main possibilities are:\nstart from scratch, creating a virtual machine, installing an OS and the software on top of it, then taking the virtual machine OS disk as custom image; or dump an existing disk from a running VM or physical server and modify it, if needed, to run on a virtualisation platform. In this guide we will focus on the first option, because it tends to produce cleaner images and reduces the risks of hardware conflicts. Snapshotting may be also restricted by the cloud providers or by security policies.\nAdvantages:\nPossibility to build the virtual disk directly from a legacy machine, dumping the contents of the disk. Possibility to speed-up the deployment for applications with complex and big installation packages. This because you do not need to install the application at startup, but the application is already included in the machine. Disadvantages:\nBuilding a virtual disk directly from a legacy machine poses a set of compatibility issues with hardware drivers, which usually differs from a virtual and physical environment and even between different virtual environments. You need to keep your machine updated. Outdated VM disk images may take a long time to startup due to the need to download and install the latest OS updates. If you are using special drivers or you are not packaging correctly the disk, your custom VM image may not run (or run slowly) on different cloud providers based on different virtualisation technologies. VM images on public clouds are sometimes public, thus be aware of installing proprietary software on custom images, since other users may be able to run the image or download it. In general, the effort to implement this solution is higher than the basic contextualization. Image size and layout The larger the VM image, the longer it will take to be distributed to the providers and the longer it will take to be started on the infrastructure. As a general rule, always try to make images as smaller as possible following these guidelines:\nDO NOT include (big) data in your image. There are other mechanisms for accessing data from your VM (block/object storage, CVMFS)\nDO NOT include (big) empty space or swap in your image. Extra space for your computation or swap can be added with block storage once the VM is booted or using VM flavors that have extra disk allocated for your VM.\nDO NOT install un-needed software. Tools like GUI are of no-use in most cases since you will have no access to the graphical console of the VM.\nDO adjust the size of the images as much as possible. As stated above, empty space can be allocated on runtime easily.\nDO use compressed image formats, like qcow2 or vmdk (used in OVA) to minimize the size of the image. Preferred format for images in EGI is OVA as it's standardised.\nDO fill with 0 the empty disk space of your image so when compressed it can be significantly reduced, e.g. using:\ndd if=/dev/zero of=/bigemptyfile bs=4096k rm -rf /bigemptyfile DO use a single partition (no /boot, no swap) for the disk layout and avoid LVM. This will allow the cloud provider to easily resize your partition when instantiated and to modify files in it if needed.\nContextualization and credentials Danger Do NOT include any credentials on your images. You should never include any kind of credentials on your images, instead you should use contextualization. cloud-init is a tool that will simplify the contextualization process for you. This is widely available as packages in major OS distributions and is supported by all the providers of the EGI Cloud and most of the commercial providers.\ncloud-init documentation contains detailed examples on how to create users, run scripts, install packages and several other actions supported by the tool.\nFor complex setups, especially when applications involve multiple VMs it may be useful to use cloud-init to bootstrap some Configuration Management Software that will manage the configuration of the VMs during runtime.\nSecurity Always remove all default passwords and accounts from your VM. Disable all services unless necessary for the intended tasks. Make sure the firewall configuration (iptables for Linux, also on IPv6) is minimally open. Put no shared credentials (passwords) in any image. You should also follow the best practice guides for each service that's exposed to the outside world. See for example guides for:\nssh tomcat See also AWS security Best Practices\nTools Whenever possible, automate the process of creating your images. This will allow you to:\nGet reproducible results Avoid tedious manual installation steps Quickly produce updated versions of your images EGI uses packer as a tool for automating the creation of our base images. This tool can use VirtualBox as a hypervisor for the creation of the images and guarantees identical results under different platforms and providers.\nCheck out the fedcloud-vmi-templates GitHub repository for all the packer recipes used to build our images, re-use them as needed for your images.\n","categories":"","description":"Managing VM images in the EGI Cloud\n","excerpt":"Managing VM images in the EGI Cloud\n","ref":"/users/compute/cloud-compute/images/","tags":"","title":"Virtual Machine Images"},{"body":"Information discovery provides a real-time view about the actual images and flavors available at the OpenStack for the federation users. It runs as a single python application cloud-info-provider that pushes information through the Messaging Service.\nBDII is deprecated Cloud providers no longer need to provide BDII as the Argo Messaging Service is used instead for transferring information You can either run the service by yourself or rely on central operations of the cloud-info-provider. In both cases you must register your host DN in the GOCDB entry for the org.openstack.nova.\nCatch-all operations EGI can manage the operation of the cloud-info-provider for the site so you don’t need to do it. In order for your site to be included in the centrally operated cloud-info-provider, you need to create a Pull Request at the EGI-Federation/fedcloud-catchall-operations repository adding your site configuration in the sites directory with a file like this:\nendpoint: \u003cyour endpoint as declared in GOCDB\u003e gocdb: \u003cyour site name as declared in GOCDB\u003e vos: # a list of VOs you support in your deployment as follows - auth: project_id: \u003clocal OpenStack project identifier\u003e name: \u003cname of the vo\u003e - auth: project_id: \u003clocal OpenStack project identifier for second VO\u003e name: \u003cname of another vo\u003e Once PR is merged, the service will be reconfigured and your site should start publishing information.\nLocal operations You can operate by yourself the cloud-info-provider. The software can be obtained as RPMs, debs and python packages from the GitHub releases page.\nThe cloud-info-provider needs a configuration file where your site is described, see the sample OpenStack configuration for the required information. The authentication parameters for your local OpenStack and the AMS are passed as command-line options:\ncloud-info-provider-service --yaml-file \u003cyour site description.yaml\u003e \\ --middleware openstack \\ --os-auth-url \u003cyour keystone URL\u003e \\ [ any other options for authentication ] --format glue21 \\ --publisher ams \\ --ams-cert \u003cyour host certificate\u003e \\ --ams-key \u003cyour host secret key\u003e \\ --ams-topic \u003cyour endpoint topic\u003e For authentication, you should be able to use any authentication method supported by keystoneauth, for username and password use: --os-password and --os-username. Check the complete list of options with cloud-info-provider-service --help.\nThe AMS topic has the format: SITE_\u003cSITE_NAME\u003e_ENDPOINT_\u003cGOCDB_ID\u003e, where \u003cSITE_NAME\u003e is the name of the site as declared in GOCDB and \u003cGOCDB_ID\u003e is the ID of the endpoint in GOCDB. For example, this endpoint would have a topic like: SITE_IFCA-LCG2_ENDPOINT_7513G0.\nYou should periodically run the cloud-info-provider (e.g. with a cron every 5 minutes) to push the information for consumption by clients.\nUsing the EGI FedCloud Appliance The appliance provides a ready-to-use cloud-info-provider configuration if you want to operate it by yourself. Once you have downloaded the appliance check the following files:\n/etc/cloud-info-provider/openstack.rc: the configuration of the account used to log into your OpenStack and the location of the host certificate that will be used to authenticate to the AMS.\n/etc/cloud-info-provider/openstack.yaml: the cloud-info-provider configuration. You need to enter the details about the VOs/projects that the site is supporting.\nThe appliance has a cron job that will connect to the configured OpenStack API and send messages every 5 minutes.\n","categories":"","description":"cloud info provider configuration\n","excerpt":"cloud info provider configuration\n","ref":"/providers/cloud-compute/openstack/cloud-info/","tags":"","title":"Information System"},{"body":"This section documents how to integrate a new application in EC3.\nAbout The process to integrate a new application in EC3 is described by the following process:\nDescribe the application to be integrated with Ansible, the open-source automation engine that automates software provisioning, configuration management, and application deployment. Use the Ansible receipt to create a new RADL template Documentations Ansible documentation EC3 documentation RADL Contacts Miguel Caballer at: micafer1 \u003cat\u003e upv \u003cdot\u003e es Amanda Calatrava at: amcaar \u003cat\u003e i3m \u003cdot\u003e upv \u003cdot\u003e es ","categories":"","description":"How to integrate a new application in EC3\n","excerpt":"How to integrate a new application in EC3\n","ref":"/users/compute/orchestration/ec3/developers/","tags":"","title":"Instructions for Developers"},{"body":"This section contains documentation about the internal EGI services. These services are being operated centrally on behalf of EGI, and are supporting the coordination of the EGI Federation.\nNote See the User Guides section for documentation about public EGI services, and the Service Providers section for details on how to integrate providers into the EGI Federation. Guidelines for software development We provide Guidelines for software development to be considered when developing a product for the EGI Federation.\nRequest for information You can ask for more information about the internal EGI services on our site.\n","categories":"","description":"Documentation for internal EGI services","excerpt":"Documentation for internal EGI services","ref":"/internal/","tags":"","title":"Internal Services"},{"body":"USER tickets // jscpd:ignore-start\nFor generic user tickets selectable issue types are explained in the table below.\nIssue type Description Accounting All kind of issues related to accounting tools (APEL, Accounting Portal and so on). AppDB All kind of issues related to the AppDB or to the interactions with the AppDB. Authorization/Authentication All issues related to authorization and authentication e.g. certificates COD Operations WLCG Coordinator On Duty Operations Catalogue Issues related to file catalogues, like LFC Computing Services All issues related to execution of jobs. It combines two other type of problems (‘Workload management’ and ‘Local batch system’). The ‘Computing Services’ should be used when it is not clear to which subcategory the ticket refersConfiguration Issues of system configuration. Data Management - generic All kind of issues related to data management tools (GFAL, LCG_Util and so on). Databases Issues related to Databases. Deployment - other Issues of software deployment that do not fit any other category. Documentation Issues of missing, wrong, outdated documentation. File Access File access issues. File Transfer File transfer issues, e.g. related to FTS GGUS Bugs and feature requests related to GGUS system. IaaS Operations Issues related to VMs and clients (OCCI, rOCCI, etc.), block storage, networking. Information System Issues related to the BDII. Installation Installation issues. Local Batch System Issues related to the local batch systems. Middleware Issues of middleware stacks like gLite, globus and others. Monitoring Issues of infrastructure and service monitoring. Network problem Issues of network connectivity. Operations Issues of general processes, procedures, information and so on of the entire infrastructure. Other Requests that do not match to any other issue type. Security Issues of infrastructure and operation security. Storage Systems Issues concerning storage systems, like EOS, dCache, DPM VO Specific Software Issues of VO specific software tools and packages. Virtual Appliance Management Issues related to vmcatcher/vmcaster tools, and VA management in general. Workload Management Issues related to workload management system and tools. TEAM tickets Team and alarm tickets have a reduced number of issue types in the drop-down menu.\nIssue type Description Databases Issues related to Databases. File Access File access issues. File Transfer File transfer issues, e.g. related to FTS Local Batch System Issues related to the local batch systems. Middleware Issues of middleware stacks like gLite, globus and others. Monitoring Issues of infrastructure and service monitoring. Network problem Issues of network connectivity. Other Requests that do not match to any other issue type Storage Systems Issues related to storage systems, like EOS, dCache or DPM ALARM tickets Issue type Description Databases Issues related to Databases. File Access File access issues. File Transfer File transfer issues, e.g. related to FTS Local Batch System Issues related to the local batch systems. Middleware Issues of middleware stacks like gLite, globus and others. Monitoring Issues of infrastructure and service monitoring. Network problem Issues of network connectivity. Other Requests that do not match to any other issue type Storage Systems Issues related to storage systems, like EOS, dCache or DPM CMS tickets The CMS VO has an own ticket submit form in the GGUS system. Although this form provides CMS specific issue types no special privilege is required to use it. This does not apply for the TEAM ticket submit form: Here the CMS specific issue types are only selectable for registered CMS TEAM members.\nIssue type Description CMS_AAA WAN Access Issues around WAN CMS_CAF Operations Issues around CAF operations at CERN, incl EOS space requests CMS_Central Workflows Issues around centrally managed MC production and processing CMS_Data Transfers Issues around data transfers, e.g. via Phedex or ASO CMS_Facilities Typically issues at CMS sites CMS_HammerCloud Issues around CMS HammerCloud CMS_Register New CMS Site Chosen when a new site gets registered with CMS CMS_SAM tests Issues around CMS SAM tests CMS_Submission Infrastructure Issues around CMS_Submission Infrastructure CMS_Tier-1 Tape Families For creation of Tape families/groups at Tier-1 archives ATLAS tickets The ATLAS specific issue types are only selectable for registered ATLAS TEAM members.\nIssue type Description ATLAS_ADC Central Issues around ADC in general ATLAS_Databases Issues around databases ATLAS_Deletion Issues around file deletions ATLAS_File Access/Transfer Issues around file access or file transfers ATLAS_Frontier-Squid Issues around frontier or squid ATLAS_Local Batch System Issues around batch systems ATLAS_Middleware Issues related to middleware ATLAS_Monitoring Issues around monitoring ATLAS_Network Problem Issues with network infrastructure ATLAS_Staging Issues around file staging ATLAS_Storage Systems Issues around storage // jscpd:ignore-end\n","categories":"","description":"List of the values of the Issue Type field for the several ticket types\n","excerpt":"List of the values of the Issue Type field for the several ticket …","ref":"/internal/helpdesk/features/issue-type-values/","tags":"","title":"Issue type values"},{"body":"To become a provider of one of the Services for Research already included in the EGI Portfolio, you first have to become provider of a federated resource centre. This first step typically requires you to become either an EGI High Throughput Compute (HTC) provider, by deploying Compute and Storage capabilities, or a Cloud service provider, by deploying OpenStack and federating it into the EGI Cloud Compute service.\nOnce such a foundational role is fulfilled you can configure/deploy the additional service within your resource centre.\n","categories":"","description":"Guidelines for new providers for existing EGI services","excerpt":"Guidelines for new providers for existing EGI services","ref":"/providers/joining/new-provider/","tags":"","title":"Joining as a provider for existing EGI services"},{"body":" Property Value Title Tool Intervention Management Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement How to manage central operational tool unscheduled downtimes Owner SDIS team The purpose of this document is to describe the intervention in case of unscheduled failure of central operational tool.\nScope This manual only applies to unscheduled downtimes of central operational tools. The list of central operational tool is available here.\nNote: Scheduled downtimes are management according to existing procedures (MAN02).\nAnnouncements All announcements should be sent with the Operations Portal Broadcast tool.\nWhen using Operations Portal Broadcast tool the following groups should be included:\nLCG Rollout Mailing List Operators Mailing lists OSG Mailing list Tool Admins Mailing List WLCG Tier 1 contacts NGI managers VO managers VO users Site administrators Operation tools Notice: Individual notification templates together with targets are predefined in Operations Portal Broadcast tool. Administrators are advised to use such predefined templates.\nProcedure In the following sections several relevant scenarios are covered.\nCase 1: short “undetected” downtime Description: Service fails and recovers before administrator manages to react (e.g. short power or network outage).\nAction: Administrator announces the failure by using the following template:\nSubject: [SERVICE_NAME] unscheduled downtime Message: Dear all, [SERVICE_NAME] experienced unscheduled downtime between [START] and [END]. [DETAILED_FAILURE_DESCRIPTION] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] Case 2: long “detected” outage Service fails and administrator detects the problem. The problem takes at least 1 hour time to recover. In the sections below individual situations are described.\n1. Outage Description: Service failure is detected.\nAction: Administrator announces the failure by using the following template:\nSubject: [SERVICE_NAME] outage Message: Dear all, [SERVICE_NAME] is experiencing unscheduled downtime. [ADDITIONAL_INFO] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] 2. Extended downtime Description: Service recovery is delayed. Update should be sent at least every 24h.\nAction: The administrator announces that recovery is taking longer by using the following template:\nSubject: [SERVICE_NAME] extended outage Message: Dear all, Outage of [SERVICE_NAME] is extended. [ADDITIONAL_INFO] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] Note: In this template [ADDITIONAL_INFO] should indicate the estimated time of recovery.\n3. Recovery Description: Service is recovered.\nActions: At the time of recovery the administrator announces the recovery by using the following template:\nSubject: [SERVICE_NAME] recovery Message: Dear all, [SERVICE_NAME] is back online. [ADDITIONAL_INFO] Best Regards [SERVICE_TEAM] 4. Post mortem analysis Description: Service failure required further time to investigate the source of the problem. This action is required only if the post mortem analysis is needed.\nActions: The administrator announces the post mortem analysis of failure by using the following notification template:\nSubject: [SERVICE_NAME] outage analysis Message: Dear all, [SERVICE_NAME] experienced unscheduled downtime between [START] and [END]. [DETAILED_FAILURE_DESCRIPTION] Best Regards [SERVICE_TEAM] ","categories":"","description":"How to manage central operational tool unscheduled downtimes","excerpt":"How to manage central operational tool unscheduled downtimes","ref":"/providers/operations-manuals/man04_tool_intervention_management/","tags":"","title":"MAN04 Tool Intervention Management"},{"body":"This page contains information about using Check-in for managing your Virtual Organisation (VO). For joining a VO please look at Joining Virtual Organisation.\nBackground In simple terms a Virtual Organisation (VO) is just a group of users. In EGI VOs are created to group researchers who aim to share resources across the EGI Federation to achieve a common goal as part of a scientific collaboration. For a more formal definition of VO please look at the EGI Glossary.\nYou can browse existing VOs in the EGI Operations Portal. For each VO you can click on the Details link to get more information. You can join an existing VO either using the enrollment URL or emailing VO managers.\nIf you are interested in creating your own VO, please see instructions in the section below.\nVO management VOs in Check-in are represented as Collaborative Organisation Units (COUs). A COU is more than just a group. It is the concept of groups combined with membership management and advanced enrolment workflows. COUs can also be organised in a hierarchical structure for creating groups or subgroups within a VO.\nIt is assumed that VO managers and members have already registered their EGI Check-in account (A step-by-step guide is provided in this link.\nRegistering your VO Any person who can authenticate to the Operations Portal using their EGI Check-in account can register a new VO.\nThe person initiating the registration is called the VO manager. After the VO is set up and operational, the VO manager is the person who is primarily responsible for the operation of the VO and for providing sufficient information about VO activities for EGI and for VO members (to both people and sites).\nA step-by-step guide for the VO registration process is provided in the procedure PROC14 VO Registration.\nViewing VO members Login to Check-in registry using any of the login credentials already linked to your EGI account.\nTo view the existing members, expand the People drop down menu and click on VO-NAME Population (for example, vo.example.org Population)\nThen you are able to see all the VO members.\nVO enrollment URLs Vo enrollments URLs are essential for someone who wants to enroll to a VO.\nUsers can request membership in your VO by following the VO enrollment URL. The enrollment URL has the following form:\nhttps://aai.egi.eu/registry/co_petitions/start/coef:## where ## is the unique numeric identifier for the enrollment flow of your VO.\nThe VO Manager can insert this URL in an invitation email and send it directly to the users who want to join the VO. Also, it can be used in any registration web page as an href of a button or image etc.\nThe VO enrollment URL can be found both at the Operations Portal and EGI Check-in Registry. At the Operations Portal VO List, by quickly searching for the VO name and clicking on the Details.\nAt EGI Check-in Registry:\nLogin to Check-in registry using any of the login credentials already linked to your EGI account.\nExpand the People drop down menu and click Enroll.\nCopy the Begin link of the Enrollment flow of the VO you want the user to join and send it to the user\nOnce the user submits the VO membership request, the Role Attributes section of their profile page will include the new VO membership role in Pending Approval status\nOnce the VO manager accepts the new member, the Role Attributes section of the user’s profile page will include the VO membership role in Active status\nReviewing VO membership requests Once a user submits a VO membership petition, all VO managers are notified with an email containing a link to the petition. Any of the VO managers can then review the petition and either approve or deny the request.\nWhen a user requests membership to a VO, all VO managers will receive an email, containing the petition URL.\nExcept from email, they will receive a notification on their COmanage profile. At the top right corner clicking the bell icon someone can see all the notifications.\nClicking a notification message, you can find among others, the petition URL inside the Notification Email Body field.\nClicking the petition URL either at mail you received either at the notification message, you can review the request.\nYou can accept or deny the membership, and also provide a justification at the textarea, below the two buttons. This field is optional. In case is filled, it will also be sent to the user that made the request.\nIn the petition view you can also view user’s assurance per linked identity under Assurance section. More information about identity assurance can be found here.\nThe complete user profile can be found by clicking the enrollee name under Attached Identities (in Petition View).\nAt user profile you can find also information about Organisation Identities. An Organisation Identity, among others, includes information about Certificates and Assurances.\nManaging VO groups VO groups can only be created by Check-in platform administrators. Please contact checkin-support \u003cAT\u003e mailman.egi.eu indicating the following information for every (sub)group that you need to add/remove to/from your VO:\nVO name Group name Group description Optional, Group manager(s), i.e. the Check-in identifiers (in the form of \"xxxxxxx@egi.eu\") of one or more users responsible for managing the VO group members. Group managers can also appoint other users as (sub)group managers. The manager(s) of the VO (or any parent group) are implicitly managers of the group. You can provide additional Check-in user identifiers to extend the list of group managers. Optional, Parent VO group name (in the case of a hierarchical group, e.g. \u003cVO\u003e –\u003e \u003cPARENT_GROUP\u003e –\u003e \u003cGROUP\u003e) Known limitation: Group names must be unique so the names you suggest may need to be adjusted by the Check-in administrators to guarantee their uniqueness.\nAdding members to VO groups Login to Check-in registry using any of the login credentials already linked to your EGI account.\nThen expand the People drop down menu and click \u003cVO-NAME\u003e Population (for example, vo.example.org Population)\nFind the user you want to add to the VO Group and click Edit.\nClick Add at the Role Attributes section of the user profile\nFill in the fields in the form and click Add. The user now is a member of the new VO group. For more information about Affiliation and Role fields you can see below at section Managing Affiliation and Role of VO Member\nRemoving members From the VO members list (see Viewing VO members above):\nClick Edit on the person that is going to be removed.\nUnder Role Attributes click Delete on the right of the COU entry of interest (for example, vo.example.com). On success the selected row will be removed. In this example we removed the vo.geoss.eu that we previously added.\nManaging Affiliation and Role of VO Member User’s Affiliation to a VO, as defined in RFC4512, has eight permissible values. These are faculty, student, staff, alum, member, affiliate, employee, library-walk-in. EGI Check-in assigns to all user’s the affiliation Member by default, during the VO(COU) enrollment process. This value is immutable for the user but editable for the VO administrator. As a result, if there is a change of status the administrator can always step in and change it appropriately. Additionally, the user’s Role in a VO is the EGI User Community Title column, in Co Person Role’s View. This column can be either a custom text value; or a value chosen from a drop down list. The drop down list administration is an EGI Check-in CO administrator task and can not be managed by any VO admin.\nUpdate User’s VO affiliation Navigate to Co Person Role view Choose Affiliation from drop down list Update User’s VO Role Navigate to Co Person Role view Choose Role from drop down list, if available, or add custom text if no list is present. Subsequently, EGI Check-in uses the CO Person’s group membership and role information in order to construct the eduPersonEntitlement values, in short entitlements. These URN-formatted attributes can be used for representing group membership, as well as to indicate rights to resources. According to the AARC-G002 specification, a user that is a member of the VO vo.example.org, and has the role supervisor, obtains the following entitlements:\nurn:mace:egi.eu:group:vo.example.org:role=member#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.org:role=supervisor#aai.egi.eu\nManaging COU Admin members COU Admin Groups are used to determine COU Administrators. Admin Groups are automatically created when a COU is created. The default name for COU admin groups is\nCO:COU:\u003cCOU_Name\u003e:admins\nFor example CO:COU:vo.example.org:admins\nA CO Person can be a member, an owner, both, or neither. Specifically: COU Admin Group members can manage COU members Approve or decline membership petitions A CO Person is added to a COU Admin Group if the following requirements are met:\nCO Person is declared as VO administrator in the VO’s ID Card, under the Operations Portal\nA request is made to the Check-in Support Unit via a EGI Helpdesk ticket.\nExpiration Policy VO membership expires within a given time, typically a year after the VO member joins the VO. VO members receive a notification from EGI Check-in Notifications with the subject “vo.example.org membership will expire soon” warning them that their membership will expire four weeks before expiration. The notification email is sent on a weekly basis and includes all the instructions needed by VO members in order to reapply for a membership. If the VO member does not take any action to renew their membership, a final notification email is sent when the VO membership expires. Please note that a user with expired membership is not eligible for VO membership entitlements and as a result the user will not have access to VO resources relying on these entitlements.\nVO API API v2 Check-in provides a REST API that allows clients to:\nmanage membership information only for the VOs they are authoritative for get VO group information only for the VOs they are authoritative for Membership Features:\nMembers of the VO are identified via their EGI Check-in Community User Identifier (CUID) Membership can be limited to a specified period All REFEDS membership affiliations are supported Role titles are supported Different membership status values are supported, namely Active, Expired, Deleted, Suspended Check-in automatically changes the membership status from Active to Expired beyond the validity period Endpoints for VO Groups Production environment Demo environment Development environment # Export VO API Base URL parameter $ export VO_API_BASE_URL=https://aai.egi.eu/registry/cous.json # Export CO ID parameter. # The CO ID is the number part of the API username prefix. # e.g. for the username `co_2.test`, the CO_ID is `2` $ export CO_ID=2 # Export VO API Base URL parameter $ export VO_API_BASE_URL=https://aai-demo.egi.eu/registry/cous.json # Export CO ID parameter. # The CO ID is the number part of the API username prefix. # e.g. for the username `co_2.test`, the CO_ID is `2` $ export CO_ID=2 # Export VO API Base URL parameter $ export VO_API_BASE_URL=https://aai-dev.egi.eu/api/cous.json # Export CO ID parameter. # The CO ID is the number part of the API username prefix. # e.g. for the username `co_2.test`, the CO_ID is `2` $ export CO_ID=2 Endpoints for VO Memberships Production environment Demo environment Development environment # Export VO API Base URL parameter $ export VO_API_BASE_URL=https://aai.egi.eu/api/v2/VoMembers # Export CO ID parameter. # The CO ID is the number part of the API username prefix. # e.g. for the username `co_2.test`, the CO_ID is `2` $ export CO_ID=2 # Export VO API Base URL parameter $ export VO_API_BASE_URL=https://aai-demo.egi.eu/api/v2/VoMembers # Export CO ID parameter. # The CO ID is the number part of the API username prefix. # e.g. for the username `co_2.test`, the CO_ID is `2` $ export CO_ID=2 # Export VO API Base URL parameter $ export VO_API_BASE_URL=https://aai-dev.egi.eu/api/v2/VoMembers # Export CO ID parameter. # The CO ID is the number part of the API username prefix. # e.g. for the username `co_2.test`, the CO_ID is `2` $ export CO_ID=2 Connection Parameters The API username and API password will be assigned by the EGI Check-in team when you request REST API access. In order to obtain REST API credentials you need to email your request to EGI Check-in Support with Subject Request REST API access for \u003cvo_name\u003e VO. Make sure you indicate the instance of EGI Check-in (production, demo or development) hosting the VO you request access to. Only VO managers can request REST API credentials for a given VO.\nAuthentication The REST client is authenticated via username/password credentials transmitted over HTTPS using the Basic Authentication scheme. More sophisticated authentication mechanisms, such as OpenID Connect/OAuth 2.0 access tokens, may be supported in the future.\nVO Groups Methods Retrieving all VO groups:\n$ curl -vX GET $VO_API_BASE_URL?coid=$CO_ID \\ --user \"example-client\":\"veryverysecret\" output:\n{ \"ResponseType\": \"Cous\", \"Version\": \"1.0\", \"Cous\": [ { \"Version\": \"1.0\", \"Id\": 123, \"CoId\": 2, \"Name\": \"vo.example.org\", \"Description\": \"Example Virtual Organisation\", \"Lft\": 109, \"Rght\": 112, \"Created\": \"2020-09-09 08:17:51\", \"Modified\": \"2020-09-09 08:17:51\", \"Revision\": 0, \"Deleted\": false, \"ActorIdentifier\": \"actoridentifier@egi.eu\", \"Metadata\": [ { \"Type\": \"mailman\" } ] }, {...}, {...} ] } The JSON object Metadata encapsulates additional VO group information like the type.\nResponse Format:\nHTTP Status Description Response Body 200 OK Role Returned JSON response 400 Bad Request CO ID unknown 401 Unauthorized Authentication Required 500 Other error Unknown Error Retrieving all VO groups filtered by Group type:\n$ curl -vX GET $VO_API_BASE_URL?coid=$CO_ID\u0026dept=mailman \\ --user \"example-client\":\"veryverysecret\" output:\n{ \"ResponseType\": \"Cous\", \"Version\": \"1.0\", \"Cous\": [ { \"Version\": \"1.0\", \"Id\": 123, \"CoId\": 2, \"Name\": \"vo.example.org\", \"Description\": \"Example Virtual Organisation\", \"Lft\": 109, \"Rght\": 112, \"Created\": \"2020-09-09 08:17:51\", \"Modified\": \"2020-09-09 08:17:51\", \"Revision\": 0, \"Deleted\": false, \"ActorIdentifier\": \"actoridentifier@egi.eu\", }, ] } Response Format:\nHTTP Status Description Response Body 200 OK Role Returned JSON response 400 Bad Request CO ID unknown 401 Unauthorized Authentication Required 500 Other error Unknown Error Retrieving VO Group by name:\n$ curl -vX GET $VO_API_BASE_URL?coid=$CO_ID\u0026name=vo.example.org \\ --user \"example-client\":\"veryverysecret\" output:\n{ \"ResponseType\": \"Cous\", \"Version\": \"1.0\", \"Cous\": [ { \"Version\": \"1.0\", \"Id\": 123, \"CoId\": 2, \"Name\": \"vo.example.org\", \"Description\": \"Example Virtual Organisation\", \"Lft\": 109, \"Rght\": 112, \"Created\": \"2020-09-09 08:17:51\", \"Modified\": \"2020-09-09 08:17:51\", \"Revision\": 0, \"Deleted\": false, \"ActorIdentifier\": \"actoridentifier@egi.eu\", \"Metadata\": [ { \"Type\": \"mailman\" } ] }, ] } Response Format:\nHTTP Status Description Response Body 200 OK Role Returned JSON response 400 Bad Request CO ID unknown 401 Unauthorized Authentication Required 404 COU/VO unknown COU/CO name not found 500 Other error Unknown Error VO Memberships Methods Adding a user to a VO requires specifying the user’s EGI Check-in CUID, the name of the VO (e.g. vo.example.org), the status (Active) and the valid from/through dates. All these parameters are mandatory. Here is an example using curl (see example add.json file below):\n$ curl -vX POST $VO_API_BASE_URL.json \\ --user \"example-client\":\"veryverysecret\" \\ --data @add.json \\ --header \"Content-Type: application/json\" add.json:\n{ \"RequestType\": \"CoPersonRoles\", \"Version\": \"1.0\", \"CoPersonRoles\": [ { \"Version\": \"1.0\", \"Person\": { \"Type\": \"CO\", \"Identifier\": { \"Type\": \"epuid\", \"Id\": \"01234567890123456789@egi.eu\" } }, \"Cou\": { \"CoId\": \"2\", \"Name\": \"vo.example.org\" }, \"Affiliation\": \"member\", \"Title\": \"Engineer\", \"Status\": \"Active\", \"ValidFrom\": \"2022-02-16 11:19:38\", \"ValidThrough\": \"2022-05-16 11:19:38\" } ] } Response Format:\nHTTP Status Description Response Body 201 Added Role Added add.json response 400 Bad Request Role Request not provided in post body 400 Invalid Fields An error in one more fields Response with failing field(s) 401 Unauthorized Authentication Required 403 COU Does not exist The specified COU does not exists 500 Other error Unknown Error Retrieving the VO membership information for a given EGI Check-in CUID:\n$ curl -vX GET $VO_API_BASE_URL/co/$CO_ID/cou/vo.example.org/identifier/01234567890123456789@egi.eu.json \\ --user \"example-client\":\"veryverysecret\" output:\n{ \"RequestType\": \"CoPersonRoles\", \"Version\": \"1.0\", \"CoPersonRoles\": [ { \"Version\": \"1.0\", \"Person\": { \"Type\": \"CO\", \"Id\": 1111 }, \"CouId\": 13, \"Affiliation\": \"member\", \"Title\": \"Pilot\", \"Status\": \"Active\", \"Created\": \"2023-02-16 11:19:38\", \"Modified\": \"2023-02-16 11:20:27\", \"Revision\": 3, \"Deleted\": false, \"ActorIdentifier\": \"co_2.test\" } ] } Beyond the valid_through date, the status will be automatically changed to Expired. So, when querying for VO membership information, it’s important to check that the status is actually set to Active for each of the identified VOs.\nResponse Format:\nHTTP Status Description Response Body 200 OK Role Returned json response 400 Bad Request CO ID unknown 401 Unauthorized Authentication Required 404 COU/VO unknown COU/CO name not found 404 Person unknown Person Identifier not found 404 Identifier type unknown Identifier type is not valid 500 Other error Unknown Error Retrieving all VO members:\n$ curl -vX GET $VO_API_BASE_URL/co/$CO_ID/cou/vo.example.org.json \\ --user \"example-client\":\"veryverysecret\" output:\n{ \"RequestType\": \"CoPersonRoles\", \"Version\": \"1.0\", \"CoPersonRoles\": [ { \"Version\": \"1.0\", \"Person\": { \"Type\": \"CO\", \"Id\": 1111 }, \"CouId\": 13, \"Affiliation\": \"member\", \"Title\": \"Pilot\", \"Status\": \"Active\", \"Created\": \"2022-02-16 11:19:38\", \"Modified\": \"2022-02-16 11:20:27\", \"Revision\": 2, \"Deleted\": false, \"ActorIdentifier\": \"co_2.test\" }, {...}, {...} ] } Beyond the valid_through date, the status will be automatically changed to Expired. So, when querying for VO membership information, it’s important to check that the status is actually set to Active for each of the identified VOs.\nResponse Format:\nHTTP Status Description Response Body 200 OK Role Returned json response 400 Bad Request CO ID unknown 401 Unauthorized Authentication Required 404 COU/VO unknown COU/CO name not found 500 Other error Unknown Error Updating existing VO membership record:\n$ curl -vX PUT $VO_API_BASE_URL/15.json \\ --user \"example-client\":\"veryverysecret\" \\ --data @update.json \\ --header \"Content-Type: application/json\" The request body is the same as the one used for adding new members but update requires:\nusing PUT instead of POST. provide the Role ID as part of the request URL Response Format:\nHTTP Status Description Response Body 200 OK Role Updated 400 Bad Request Role Request not provided in post body 400 Invalid Fields An error in one more fields Response with failing field(s) 401 Unauthorized Authentication Required 403 COU Does not exist The specified COU does not exists 500 Other error Unknown Error Removing VO member: Same as the update but requires setting the membership status to Deleted and do not include a body.\nResponse Format:\nHTTP Status Description Response Body 200 OK Role Returned json response 401 Unauthorized Authentication Required 404 Role unknown Role name not found 500 Other error Unknown Error API v1 (DEPRECATED) Check-in provides a REST API that allows clients to manage membership information only for the VOs they are authoritative for.\nFeatures:\nMembers of the VO are identified via their EGI Check-in Community User Identifier (CUID) Membership can be limited to a specified period Different membership status values are supported, namely Active, Expired, Deleted Check-in automatically changes the membership status from Active to Expired beyond the validity period Authentication The REST client is authenticated via username/password credentials transmitted over HTTPS using the Basic Authentication scheme. More sophisticated authentication mechanisms, such as OpenID Connect/OAuth 2.0 access tokens, may be supported in the future.\nMethods Adding a user to a VO requires specifying the user’s EGI Check-in CUID, the name of the VO (e.g. vo.example.org in the case of LToS), the status (Active) and the valid from/through dates. All these parameters are mandatory. Here is an example using curl (see example add.json file below):\n$ curl -vX POST https://aai.egi.eu/api/v1/VoMembers \\ --user \"example-client\":\"veryverysecret\" \\ --data @add.json \\ --header \"Content-Type: application/json\" add.json:\n{ \"RequestType\": \"VoMembers\", \"Version\": \"1.0\", \"VoMembers\": [ { \"Version\": \"1.0\", \"VoId\": \"vo.example.org\", \"Person\": { \"Type\": \"CO\", \"Id\": \"01234567890123456789@egi.eu\" }, \"Status\": \"Active\", \"ValidFrom\": \"2017-05-21\", \"ValidThrough\": \"2017-06-21\" } ] } Retrieving the VO membership information for a given EGI Check-in CUID:\n$ curl -vX GET https://aai.egi.eu/api/v1/VoMembers/01234567890123456789@egi.eu \\ --user \"example-client\":\"veryverysecret\" output:\n[ { \"id\": 85, \"epuid\": \"01234567890123456789@egi.eu\", \"vo_id\": \"vo.example.org\", \"valid_from\": \"2017-05-20T22:00:00.000Z\", \"valid_through\": \"2017-06-21T22:00:00.000Z\", \"status\": \"Active\" } ] Beyond the valid_through date, the status will be automatically changed to Expired. So, when querying for VO membership information, it’s important to check that the status is actually set to Active for each of the identified VOs (see the vo_id attribute)\nUpdating existing VO membership record:\n$ curl -vX PUT https://aai.egi.eu/api/v1/VoMembers \\ --user \"example-client\":\"veryverysecret\" \\ --data @update.json \\ --header \"Content-Type: application/json\" The request body is the same as the one used for adding new members but update requires using PUT instead of POST.\nRemoving VO member:\nSame as the update but requires setting the membership status to Deleted\nLDAP EGI Check-in provides read access to the members of the VO(s) through the Lightweight Directory Access Protocol (LDAP).\nThere are two entity types in the LDAP:\nPeople: Users of EGI Check-in, expressed as inetOrgPerson (with additional attributes/schemas).\nGroups of collaboration members, expressed as groupOfMembers (with additional attributes/schema).\nConnection Parameters Production environment Demo environment Development environment LDAP address ldaps://ldap.aai.egi.eu:636/ ldaps://ldap.aai-demo.egi.eu:636/ ldaps://ldap.aai-dev.egi.eu:636/ Base DN dc=\u003cvo_name\u003e,dc=ldap,dc=aai,dc=egi,dc=eu dc=\u003cvo_name\u003e,dc=ldap,dc=aai-demo,dc=egi,dc=eu dc=\u003cvo_name\u003e,dc=ldap,dc=aai-dev,dc=egi,dc=eu Bind DN: \u003cprovided by EGI Check-in support\u003e\nThe Bind DN and Bind Password will be assigned by the EGI Check-in team when you request LDAP access. In order to obtain LDAP credentials you need to email your request to EGI Check-in Support with Subject Request LDAP access for \u003cvo_name\u003e VO. Make sure you indicate the instance of EGI Check-in (production, demo or development) hosting the VO you request access to. Only VO managers can request LDAP credentials for a given VO.\nEntities User Users are present in the ou=people subtree.\nAttribute Description Example objectClass inetOrgPerson, eduPerson, voPerson, eduMember, ldapPublicKey voPersonId Community User Identifier (voPersonID) befd2b9ed8878c542555829cb21da3e25ad91a0f9cg54gsdcs35htf@egi.eu uid user ID john.doe cn Full name John Doe displayName Full name John Doe givenName First name John sn Last name Doe mail Email address john.doe@mail.com edupersonUniqueID Community User Identifier (see also voPersonId) befd2b9ed8878c542555829cb21da3e25ad91a0f9cg54gsdcs35htf@egi.eu eduPersonPrincipalName A scoped identifier for a person. The value is the same as the befd2b9ed8878c542555829cb21da3e25ad91a0f9cg54gsdcs35htf@egi.eu eduPersonEntitlement URN that indicates a set of rights to specific resources urn:mace:egi.eu:group:vo.example.org:role=member#aai.egi.eu sshPublicKey SSH public key isMemberOf Group memberships CO:COU:vo.example.org:members voPersonCertificateDN The Subject Distinguished Name of an X.509 certificate held by the person voPersonCertificateDN;scope-cert1: CN=John Doe A251,O=Example,C=US,DC=cilogon,DC=org voPersonCertificateIssuerDN The Subject Distinguished Name of the X.509 certificate issuer voPersonCertificateIssuerDN;scope-cert1: CN=CILogon Basic CA 1, O=CILogon, C=US, DC=cilogon, DC=org Group Groups are present in the ou=groups subtree.\nAttribute Description Example objectClass groupofNames, eduMember cn Common name CO:COU:vo.example.org:members description The description of group CO:COU:vo.example.org Members member The members of this group (multivalued) voPersonID=befd2b9ed8878c542555829cb21da3e25ad91a0f9cg54gsdcs35htf@egi.eu businessCategory The VO/group type (multivalued, optional) mailman ","categories":"","description":"Managing a Virtual Organisation (VO) in Check-in\n","excerpt":"Managing a Virtual Organisation (VO) in Check-in\n","ref":"/users/aai/check-in/vos/","tags":"","title":"Managing a Virtual Organisation"},{"body":"Introduction An NGI forms a grouping of Sites in EGI Configuration Database. The Configuration Database stores the following information about these groups. The main page listing groups actually shows NGIs/ROCs, and is available from “List of NGIs/ROCs and associated contacts”, linked from the main menu.\nEach NGI has its own listing page, accessible by clicking on the “view” link in group listing pages. A group details page shows users with a role on that group, as well as member sites and associated contacts and roles.\nAdding NGIs Adding groups is not possible through the Input System web interface. If you want to start the registration process of a new NGI, please follow the procedure described on:\nPROC02: Operations Centre creation\nIntegration of the new group in the EGI Configuration Database is part of the procedure but has to be done by the Configuration Database admins.\nEditing Groups To edit a group, simply click on the “edit” link at the top of the group’s details page.\nDeleting Groups This operation is not allowed.\n","categories":"","description":"Managing NGIs entities","excerpt":"Managing NGIs entities","ref":"/internal/configuration-database/ngis/","tags":"","title":"Managing NGIs entities"},{"body":"NGI Core Services NGIs can register a number of ‘NGI-Core’ services in the Configuration Database. A core NGI service is one that is used to calculate the availability and reliability of the NGI. These services fall under the responsibility of the NGI and provide production quality (no testing instances). NGIs can distinguish/flag their core services from their other (non-core) services using one of two ways (see A and B below).\nCore Service Requirements The service instance MUST:\nBe flagged as ‘Production’ (see Production Flag) Not be flagged as ‘Beta’ (see Beta Flag) Monitored flag set to true (see Monitored Flag) Be hosted under a ‘NGI’ scoped Site that has a certification status of ‘Certified’ Required Service Types The following service types are mandatory to support the central operations and all NGIs in the EGI scope should define instances of these services:\nemi.ARGUS (Mandatory) (NGI ARGUS) Top-BDII (Mandatory) Other Mandatory services, depending on middleware deployed by sites under NGI responsibility, are the following:\nMyProxy VOMS NGIs should also register their custom core services like accounting, helpdesk if provided.\nRegistering NGI Core Services NGI core services can be grouped/flagged in one of two ways:\nA) By creating a ‘NGI_XX_SERVICES’ Site and adding their core services under this site. This site must be scoped as ‘NGI’ and define a certification status of ‘Certified’. B) By creating a ‘NGI_XX_SERVICES’ ServiceGroup and adding their core services to this ServiceGroup. It is important that these core service Sites/ServiceGroups adhere to the ‘NGI_XX_SERVICES’ naming scheme. The list of existing Service Groups is available on GOCDB.\n","categories":"","description":"Managing the NGI Core Services entries in Configuration Database","excerpt":"Managing the NGI Core Services entries in Configuration Database","ref":"/internal/configuration-database/ngi-core-services/","tags":"","title":"NGI Core Services"},{"body":" What is it? Object storage is a standalone service that stores data as individual objects, organized into containers. It is a highly scalable, reliable, fast, and inexpensive data storage. It has a simple web services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web.\nThe main features of object storage:\nStorage containers and objects have unique URLs, which can be used to access, manage, and share them. Data can be accessed from anywhere, using standard HTTP requests to a REST API (e.g. VMs running in the EGI Cloud or in other cloud provider’s cloud, from any browser/laptop, etc.) Access can be public or can be restricted using access control lists. There is virtually no limit to the amount of data you can store, only the space used is accounted for. Concepts To use object storage effectively, you need to understand the following key concepts and terminology:\nStorage containers Storage containers (aka buckets) are the fundamental holders of data. Every object is stored in a storage container. You can store any number of objects in a storage container.\nStorage containers have an unique name and act as the root folders of the storage space.\nEach storage container has a unique URL (that includes the name) by which anyone can refer to it.\nObjects Objects are the fundamental entities stored in object storage. Objects consist of object data and metadata. The data portion is opaque to object storage. The metadata is a set of name-value pairs that describe the object. These include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type. You can also specify custom metadata at the time the object is stored.\nAn object is uniquely identified within a storage container by a key (name) and a version.\nEach object has a unique URL, based on the storage container’s URL (that includes the key, and optionally the version) by which anyone can refer to it.\nPermissions Storage containers and objects can be shared by sharing their URLs. However, access to a storage container or to an object is controlled by access control lists (ACLs). When a request is received against a resource, object storage checks the corresponding ACL to verify that the requester has the necessary access permissions.\nNote It is possible to set permissions so that the storage container or object can be accessed publicly. Usage from your application The object storage in the EGI Cloud is offered via OpenStack deployments that implement the Swift service.\nUsers can manage object storage using the OpenStack Horizon dashboard of a provider or from the command-line (CLI). More advanced usage include access via the S3 protocol, via the OpenStack Object Store API, or using the EGI Data Transfer service.\nNote Available object storage resources can be discovered in the Configuration Database (GOCDB). Access from the command-line Multiple command-line interfaces (CLIs) are available to manage object storage:\nThe OpenStack CLI The FedCloud Client is a high-level CLI for interaction with the EGI Federated Cloud (recommended) The Swift CLI has some advanced features that are not available through the OpenStack CLI Access with the FedCloud CLI The main FedCloud commands for managing storage containers and storage objects are described below.\nNote See here for documentation on all storage container-related commands, and here for all object-related commands. List storage containers For example, to access to the SWIFT endpoint at IFCA-LCG2 via the Pilot VO (vo.access.egi.eu), and list the available storage containers, use the FedCloud command below:\nLinux / Mac Windows PowerShell To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n$ export EGI_SITE=IFCA-LCG2 $ export EGI_VO=vo.access.egi.eu $ fedcloud openstack container list +------------------+ | Name | +------------------+ | test-egi | +------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e set EGI_SITE=IN2P3-IRES \u003e set EGI_VO=vo.access.egi.eu \u003e fedcloud openstack container list +------------------+ | Name | +------------------+ | test-egi | +------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e $Env:EGI_SITE=\"IN2P3-IRES\" \u003e $Env:EGI_VO=\"vo.access.egi.eu\" \u003e fedcloud openstack container list +------------------+ | Name | +------------------+ | test-egi | +------------------+ Create new storage container To create a new storage container named test-egi, use the follwoing FedCloud command:\n$ fedcloud openstack container create test-egi +---------+-----------+------------------------------------------------------+ | account | container | x-trans-id | +---------+-----------+------------------------------------------------------+ | v1 | test-egi | tx000000000000000000afc-005f845160-2bb3ed4-RegionOne | +---------+-----------+------------------------------------------------------+ Create new object by uploading a file To upload a file as a new object into a storage container named test-egi, use the following FedCloud command:\nTip The newly created object can have a different name than the file being uploaded, use the --name command flag for this. Tip Multiple files can be uploaded at once, but in that case the resulting objects will have the same names as the uploaded files. $ fedcloud openstack object create test-egi file1.txt +-----------+-----------+----------------------------------+ | object | container | etag | +-----------+-----------+----------------------------------+ | file1.txt | test-egi | 5bbf5a52328e7439ae6e719dfe712200 | +-----------+-----------+----------------------------------+ List objects in a storage container To list the objects in a storage container use the FedCloud command below:\n$ fedcloud openstack object list test-egi +-----------+ | Name | +-----------+ | file1.txt | +-----------+ Download (the content of) an object To download an object named file1.txt located in storage container test-egi, and save its content to a file use the FedCloud command below:\nTip The object can be saved into a file named differently than the object itselft, by using the --filename command flag. Tip Multiple files can be downloaded at once, but in that case the resulting files will have the same names as the downloaded objects. $ fedcloud openstack object save test-egi file1.txt Add metadata to an object You can add/update object metadata, stored as key-value pairs among the object properties. E.g. to add a property named key1 with the value value2 to an object named file1.txt located in the storage container named test-egi, you can use the FedCloud command below:\n$ fedcloud openstack object set \\ --property key1=value2 test-egi file1.txt Remove metadata from an object You can also remove metadata from objects. E.g. to remove the property named key1 from the object named file1.txt located in the storage container named test-egi, you can use the FedCloud command below:\nNote Only metadata added by users can be removed (system properties cannot be removed). $ fedcloud openstack object unset \\ --property key test-egi file1.txt Remove an object from a storage container To delete an object named file1.txt from the storage container test-egi, use the following FedCloud command:\nCaution Deleting object from storage containers is final, there is no way to recover deleted objects. Unlike in AWS S3, objects in OpenStack storage containers cannot be protected against deletion. $ fedcloud openstack object delete test-egi file1.txt Removing an entire container To delete a storage container, including all objects in it, use the FedCloud command below.\nTip You can add the -r option to recursively remove sub-containers. Caution Deleting all objects from a storage container is final, there is no way to recover deleted objects. Unlike in AWS S3, objects in OpenStack storage containers cannot be protected against deletion. $ fedcloud openstack container delete test-egi Access via Rclone Rclone is a command-line program to manage files on cloud storage. This section explains how to use rclone to interact with OpenStack Swift available in the EGI Federated Cloud.\nAs a prerequisite, we need to configure the following environment variables: OS_AUTH_URL, OS_AUTH_TOKEN, OS_STORAGE_URL. Use the FedCloud Client to get their values:\n# explore sites with swift storage $ fedcloud endpoint list --service-type org.openstack.swift --site ALL_SITES # get OS_AUTH_URL $ fedcloud openstack --site \u003csite\u003e --vo \u003cvirtual-organisation\u003e catalog show keystone # get OS_AUTH_TOKEN $ fedcloud openstack --site \u003csite\u003e --vo \u003cvirtual-organisation\u003e token issue \\ -c id \\ -f value # get OS_STORAGE_URL for your site and Virtual Organisation $ fedcloud openstack --site \u003csite\u003e --vo \u003cvirtual-organisation\u003e catalog show swift Now configure rclone to work with the environment variables:\n$ rclone config create egiswift swift env_auth true Finally, check that you have access to swift:\n$ export OS_AUTH_TOKEN=\u003ctoken\u003e $ export OS_AUTH_URL=\u003ckeystone-url\u003e $ export OS_STORAGE_URL=\u003cswift-url\u003e $ rclone lsd egiswift: For more information, please see Rclone documentation for Swift.\nAccess via the S3 protocol The OpenStack Swift service is compatible with the S3 protocol, therefore when properly configured, it can be accessed as any other S3-compatible object store.\nNote The S3 protocol was created by Amazon Web Services (AWS) for their object storage, called Simple Storage Service (S3), but it was adopted as the de-facto standard to access object storage offered by other providers. In order to access the storage via S3, an EGI Federated Cloud site admin needs to create and associate to your EGI credentials both access and secret keys which could then be used by clients to have access to the storage.\nAWS CLI The AWS CLI can be used to manage object storages having S3 interface.\nFirst of all the configuration of the access and secret keys need to be done:\n$ aws configure then it offers many commands to list, create buckets, objects, e.g.:\n$ aws s3 ls --no-sign-request \\ --endpoint-url https://object-store.cloud.muni.cz \\ s3://test-egi-public Note In order to access public buckets the --no-sign-request is needed Minio Client The MinIO CLI supports filesystems and Amazon S3 compatible cloud storage services.\nIt offers a modern alternative to UNIX commands like ls, cat, e.g.:\n# key and secret are not mandatory in case of public buckets $ ./mc alias set cesnet https://object-store.cloud.muni.cz $ ./mc ls cesnet/test-egi-public $ ./mc cat cesnet/test-egi-public/file1.txt Davix The Davix Client, developed at CERN for RHEL and Debian environments, is another alternative for working with S3-compatible object storage.\nFor example, to list containers/objects via the S3 protocol, use the command:\n$ davix-ls --s3accesskey 'access' --s3secretkey 'secret' \\ --s3alternate s3s://s3.cl2.du.cesnet.cz/\u003cbucket-name\u003e davix-get, davix-put and davix-del are also available to download, store and delete objects from the storage.\nNote The Davix Client does not support access to public buckets Access via Python The possibility to access progammatically via S3 object storage is also quite important, for instance in the case of interactive computing via EGI Notebooks.\nWhen using Python for instance, S3Fs is a practical Pythonic file interface to S3.\nThe top-level class S3FileSystem holds connection information and allows typical file-system style operations like cp, mv, ls, du, glob, etc., as well as put/get of local files to/from S3.\nimport s3fs fs = s3fs.S3FileSystem(anon=True, client_kwargs={ 'endpoint_url': 'https://object-store.cloud.muni.cz' }) print(fs.ls('s3://test-egi-public')) s3path = 's3://test-egi-public/file1.txt' with fs.open(s3path, 'rb') as f: print(f.read()) There is a good collection of examples on the S3Fs GitHub repository.\nAccess via EGI Data Transfer The EGI Data Transfer service can move files to and from object storages that are compatible with the S3 protocol. You will have to upload the access keys to the EGI Data Transfer service, which will be able to generate properly signed URLs for the objects in the storage.\nNote Please contact support at support \u003cat\u003e egi.eu for more details. You can then refer to this tutorial to see how to transfer to/from an Object storage endpoint.\n","categories":"","description":"Object Storage offered by EGI Cloud providers\n","excerpt":"Object Storage offered by EGI Cloud providers\n","ref":"/users/data/storage/object-storage/","tags":"","title":"Object Storage"},{"body":"Overview This page includes all the available ways to obtain OAuth tokens from EGI Check-in.\n","categories":"","description":"Obtaining Access/Refresh Tokens from Check-in\n","excerpt":"Obtaining Access/Refresh Tokens from Check-in\n","ref":"/users/aai/check-in/obtaining-tokens/","tags":"","title":"Obtaining Access/Refresh Tokens"},{"body":"Pakiti Pakiti is a client-server tool to collect and evaluate data about packages installed on Linux machines, primarily meant to identify vulnerable SW that have not been properly updated. The EGI CSIRT operates the EGI Pakiti instance that is used to monitor the state of the EGI sites.\nPakiti client The pakiti-client can be used to send package informations to pakiti.egi.eu.\nIf you have the proper credentials in the Configuration Database and submit your report with the correct SITE_NAME, you, your NGI-CSIRT and the EGI-CSIRT will be able to monitor the packages installed on your hosts and potentially vulnerabilities. The results can be accessed on the EGI Pakiti central instance.\nRunning the Pakiti client from CVMFS for EGI If you have CVMFS installed and configured to mount grid.cern.ch, you can run pakiti by simply running:\n$ /cvmfs/grid.cern.ch/pakiti/bin/pakiti-client \\ --url \"https://pakiti.egi.eu/feed/\" \\ --site SITE_NAME Please remember to replace SITE_NAME by your actual site name\nManual installation Installing the Pakiti client The pakiti-client is now available from EPEL. If your machine already has EPEL enabled, the following command is enough to install it:\n$ yum install pakiti-client Running the Pakiti client for EGI With the package and the configuration, the following commands will run the\npakiti-client and transmit all its data to the EGI CSIRT pakiti instance!\n$ pakiti-client --url \"https://pakiti.egi.eu/feed/\" --site SITE_NAME Please remember to replace SITE_NAME by your actual site name\nPuppet Installation The simplest way to configure and run the pakiti-client on a cluster is to use puppet: You just need to create a file and a manifest.\npackage { 'pakiti-client': ensure =\u003e 'present', } cron { 'pakiti-egi': ensure =\u003e 'present', command =\u003e 'pakiti-client --url \"https://pakiti.egi.eu/feed/\" --site SITE_NAME', user =\u003e 'nobody', hour =\u003e fqdn_rand(24), minute =\u003e fqdn_rand(60), } ","categories":"","description":"Monitoring patch status","excerpt":"Monitoring patch status","ref":"/internal/security-coordination/monitoring/pakiti/","tags":"","title":"Pakiti"},{"body":"Introduction Scope tags are used to group entities such as Sites, Services and ServiceGroups into flexible categories. A single entity can define multiple scope tags, allowing the resource to be associated with different categories without duplication of information. This is essential to maintain the integrity of topology information across different infrastructures and projects. The Configuration Database admins control which scope tags are made available to avoid proliferation of tags (user defined tags are reserved for the extensibility mechanism). As an example, a site’s scope list could aggregate all of the scopes defined by its child services. In doing this, the site scope list becomes a union of its service scopes plus any other site specific tags defined by the site. By defining scope tags, resources can be ‘filtered-by-scope-tag’ when querying for data in the PI using the ‘scope’ and ‘scope_match’ parameters, see GOCDB Programmatic Interface (GOCDB-PI)(link to old EGI Wiki) for details. Clear Separation of Concerns It is important to understand that scopes and Projects are distinct:\nProjects are used to cascade roles and permissions over child objects Scope tags are used to filter resources into flexible categories/groupings Scope tags can be created to mirror the projects. For example, assuming two projects (e.g. EGI.eu and EUDAT), two corresponding tags may be defined. In addition, it is also possible define additional scopes for finer grained resource filtering e.g. ‘SubGroupX’ and ‘EGI_TEST’. The key benefit: A clear separation of concerns between cascading permissions and resource filtering. EGI Scopes To make a Site, Service or ServiceGroup visible to EGI, the resource’s ‘EGI’ scope tag checkbox must be ticked. EGI scoped resources are exposed to the central operational tools for monitoring and will appear in the central operations portal. Un-ticking the EGI checkbox and selecting the ‘Local’ scope makes the selected object invisible to EGI; it will be hidden from the central operation tools (it will not show in the central dashboard and it will not be monitored centrally). This can be useful if you wish to hide certain parts of your infrastructure from EGI but still have the information stored and accessed from the same Configuration Database instance. A use-case for non-EGI sites/services is to hide those entities from central EGI tools, but to include those sites/services for use by regional versions of the operational tools (such as regional monitoring). Note that exposing a site / service endpoint as EGI does not override the production status or certification status fields. For example if a site isn’t marked as production it won’t be monitored centrally even if it’s marked as visible to EGI. You can submit your request for new scope tags via EGI Helpdesk to the “Configuration and Topology Database (GOCDB)” support unit. Reserved Scope Tags Some tags may be ‘Reserved’ which means they are protected - they are used to restrict tag usage and prevent non-authorised sites/services from using tags not intended for them. Reserved tags are initially assigned to resources by the Configuration Database admins, and can then be optionally inherited by child resources (tags can be initially assigned to NGIs, Sites, Services and ServiceGroups). When creating a new child resource (e.g. a child Site or child Service), the scopes that are assigned to the parent are automatically inherited and assigned to the child. Reserved tags assigned to a resource are optional and can be de-selected if required. Users can reapply Reserved tags to a resource ONLY if the tag can be inherited from the parent Scoped Entity (parents include NGIs/Sites). For Sites: If a Reserved tag is removed from a Site, then the same tag is also removed from all the child Services - a Service can’t have a reserved tag that is not supported by its parent Site. For NGIs: If a Reserved tag is removed from an NGI, then the same tag is NOT removed from all the child Sites - this is intentionally different from the Site-\u003eService relationship. To request a reserved scope tag, an approval is required from the operators of the relevant resources. Details on who to contact are listed below. Once authorisation is given, please contact the Configuration Database admins with details of the approval (e.g. link to an EGI Helpdesk ticket that approves the tag assignment). FedCloud Reserved Tag Tag for resources that contribute to the EGI Federated Cloud. To request this tag, please contact the FedCloud operators / EGI Operations. Elixir Reserved Tag Tag for resources that contribute to the EGI Federated Cloud. To request this tag, please contact the operators of the ‘ELIXIR’ NGI in the EGI Configuration Database. WLCG Reserved Tags A number of reserved scope tags have been defined for the WLCG: The ‘tierN’ tags should be requested for WLCG sites that are defined in REBUS (a management view of the WLCG infrastructure/sites). To request a ‘tierN’ tag, raise a ticket against the REBUS support unit in GGUS. For the experiment VO tags (alice, atlas, cms, lhcb), raise a ticket with the relevant VO support unit. The wlcg tag is a generic catch-all tag for sites/services with either tierN and VO tags and is used to gain an overall view of the WLCG infrastructure. SLA Reserved Tag Entities covered by an EGI VO SLA This Tag will only be applied at the request of EGI operations EOSCCore Tag Tag for resources that contribute to core services of the EOSC. To request this tag, please raise an EGI Helpdesk ticket against the Operations SU. EGICore Tag Tag for resources that are part of the EGI Core services. To request this tag, please raise an EGI Helpdesk ticket against the Operations SU.\n","categories":"","description":"Understanding and manipulating scopes","excerpt":"Understanding and manipulating scopes","ref":"/internal/configuration-database/scopes/","tags":"","title":"Scoping"},{"body":"What is it? EGI is an interconnected federation where a single vulnerable place may have a huge impact on the whole infrastructure. In order to recognise the risks and to address potential vulnerabilities in a timely manner, the EGI Security Monitoring provides an oversight of the infrastructure from the security standpoint.\nAlso, sites connected to EGI differ significantly in the level of security and detecting weaknesses exposed by the sites allows the EGI security operations to contact the sites before the issue leads to an incident.\nInformation produced by security monitoring is also important during assessment of new risks and vulnerabilities since it enables to identify the scope and impact of a potential security incident.\nTechnical description This service includes the following components.\nSecmon A Nagios-based service provided to monitor a range of assets like CRLs, file system permissions, vulnerable file permissions etc.\nAd-hoc probes are deployed to support incident management, to assess the vulnerability of the infrastructure with regards to specific security issues and for proactive security management.\nThe results produced are available to the EGI Security dashboard of the Operations Portal for visualisation.\nPakiti Pakiti is the monitoring and notification service which is responsible for checking the patching status of systems.\nThe results produced are available to the EGI Security dashboard of the Operations Portal for visualisation.\nIncident reporting tool Ticketing system for tracking of incident.\nTools for Security Service Challenge support Security challenges are a mechanism to check the compliance of sites/NGIs/EGI with security requirements. Runs of Security Service Challenges need a set of tools that are used during various stages of the runs.\n","categories":"","description":"Security Monitoring for EGI Resources Providers and Services","excerpt":"Security Monitoring for EGI Resources Providers and Services","ref":"/internal/security-coordination/monitoring/","tags":"","title":"Security Monitoring"},{"body":"Definition A Service entity is formed by a hostname, a hosted service and a URL.\nThe EGI Configuration Database stores the following information about Service entities (non exhaustive list):\nThe fully qualified hostname of the machine The hosted service (see ServiceTypes below) The URL to reach the entities The IP address of the machine The machine’s host certificate DN A description of the node As a machine can host many services, there can be many Service entities per machine.\nExample The machine myhost.domain.org runs a CE, an UI and a UnicoreX service. This will show up in the EGI Configuration Database as 3 Service entities:\nfully qualified hostname ServiceType myhost.domain.org CE myhost.domain.org UI myhost.domain.org unicore6.Gateway Note that a single host can also specify multiple services of the same ServiceType.\nManipulating Service entities Viewing Service entities There are different pages in GOCDB where Service entities are listed:\nA full Service entities listing page, that shows a listing of all the entities in the database, with controls to page through the listing. The table headers can be clicked to set the ordering. Site details page, see GRIDOPS-GOCDB for an example, where all the Service entities belonging to this site are listed Each Service entity also has its own listing page. By clicking the link to view it, you can see all associated information.\nAdding Service entities Provided you have proper permissions (check the permissions matrix in the Permissions_associated_to_roles section), you can add a Service entity by:\nclicking on the Add a New Service link in the sidebar. Simply select parent site, fill the form and validate. By clicking on the Add Service link from a given site’s details page (the link will only appear if you have proper permissions). This will lead you to the same form as above. Editing Service entities information The editing process will show you the same form as the adding process. To edit Service entities, simply click the “edit” link on top of the entities' details page.\nRemoving a Service entity from a site To delete a Service entity you have permissions on, simply click on the “delete” link on top of the entities’ details page. The interface asks for confirmation before proceeding.\nService Endpoint entities A Service entity may optionally define Service Endpoint entities which model network locations for different service functionalities that can’t be described by the main ServiceType and URL alone.\nFor example: The Service entity goc.egi.eu (of ServiceType egi.GOCDB) defines the following Service Endpoint entities:\nName URL Interface Name ProductionPortalInstance https://goc.egi.eu/portal egi.GOCDB.Portal Production PI base URL https://goc.egi.eu/gocdbpi egi.GOCDB.PI Specific Service entities fields and their impact “beta” flag (t/f) This indicates whether the Service entity is a beta service or not (part of the staged rollout process).\nHost DN This is the DN of the host certificate for the service. The format of the DN follows that defined by the OGF Interoperable Certificate Profile which restricts allowed chars to a PrintableString that does NOT contain characters that cannot be expressed in printable 7-bit ASCII. For a list of allowed chars.\nTo supply multiple or alternate DN(s) for a service, for example of the multiple hosts supporting a single Service entity, see strandard extension properties.\n“production” flag (t/f) The Service entities’ Production flag indicates if this service delivers a production quality service to the infrastructure it belongs to (EGI).\nNon-production Service entities can be either Monitored or Not Monitored, depending on the Administrator’s choice. Even if this flag is false, the service is still considered part of the EGI and so shows up in the ROD dashboard. If true, then the Monitored flag must also be true: All production resources MUST be monitored (except if the ServiceType is a VOMS or emi.ARGUS) This flag is not to be confused with PRODUCTION_STATUS, which is a Site level flag that shows if the site delivers to the production or Test infrastructure. “monitoring” flag (t/f) This flag is taken into account by monitoring tools.\nCan only be set to “N” (false) if Production flag is also false. If set to “N” the entities won’t be tested. Usage of PRODUCTION and MONITORED flags for EGI Service entities All production Service entities MUST be monitored (except for emi.ARGUS and VOMS ServiceTypes).\nProduction and Monitored Operations Dashboard: A failing test of production Service entities generates an alarm in the ROD Operations Dashboard. Availability calculation: The Service entities test results are considered for Availability computation (if and only if the ServiceType associated to the entities is one of those included in Availability computation) Non-Production and Monitored: YES/NO Availability calculation: If Monitored is set to YES, the Monitoring Service will test the Service entity, but the test results are ignored by the Availability Computation Engine (ACE). Availability calculation: Non-production Service entities are not considered for site availability calculations. Operations Dashboard: If Monitored is set NO, the Service entities is ignored by the Monitoring Service, and no alarms are raised in the Operations Dashboard in case of CRITICAL failure. Monitoring tests for non-production Service entities generate alarms into the ROD Operations Dashboard in case of CRITICAL failure of the test. These alarms are visible in the Operations Dashboard and are tagged as “non production”. ","categories":"","description":"Managing Service entities","excerpt":"Managing Service entities","ref":"/internal/configuration-database/service-entities/","tags":"","title":"Service Entities"},{"body":"Service Groups A service group is an arbitrary grouping of existing service endpoints that can be distributed across different physical sites and users that belong to the SG (SGs were previously known as ‘Virtual Sites’):\nEach service that appears in a group must already exist and be hosted by a physical site. A service group role does not extend any permissions over its child services. This means that you cannot declare a downtime on the services that you group together or modify the service attributes. Any GOCDB user can create their own service group and as the ‘Service Group Administator’ you can control subsequent user membership requests to the SG (everything is logged, including who created the service group). GOCDB users can request to join an existing service group by finding the target SG and requesting a role on that SG. Service groups are typically used for monitoring a particular collection of services and/or users using the GOCDB ‘get_service_group’ and ‘get_service_group_role’ PI methods. SG memebers can be listed using the get_service_group_role PI method. PI doc: get_service_group(link to old EGI Wiki) get_service_group_role(link to old EGI Wiki) If you have any further use-cases or suggestions, please submit a GGUS ticket. ","categories":"","description":"Understanding and manipulating service groups","excerpt":"Understanding and manipulating service groups","ref":"/internal/configuration-database/service-groups/","tags":"","title":"Service groups"},{"body":"Introduction In the EGI Configuration Database, a service type is a technology used to provide a service. Each service endpoint is associated with a service type. Service types are pieces of software while service endpoints are a particular instance of that software running in a certain context.\nService Type Naming Scheme Service types include grid and cloud middleware, and operational services. This attribute corresponds to the Glue2 Service.Type attribute and is defined as the “Type of service according to a namespace based classification (the namespace MAY be related to a middleware name, an organisation or other concepts)”. The naming scheme for new service types therefore follow a reverse DNS style syntax, usually naming the technology provider/project followed by technology type in lowercase, i.e. ‘provider.type’ (e.g. org.openstack.swift). Please note, this syntax does not necessarily indicate ownership, the main objective is to avoid name clashes between services. For example, different projects may have similar services but these may be modified/customised just enough to merit a different prefix or service type name. Glue2 defines a service type list at: Glue2 Enums Glue2 service types. The Glue2 and GOCDB recommendation is to use lowercase (legacy enum values do exist that use camelCase). These service types are used at some grid sites within EGI but aren’t EGI operational tools or a part of the core middleware distributions.\nService Type List To request a new service type, please submit a request for a new service type (see the section “Adding a new service type”).\nIn the following section there is the list of “middleware agnostic” service types. You can obtain the whole list of service types by browsing Poem, or by launching the following query to the GOCDBPI interface:\nget_service_type Operational Components (middleware agnostic) Site-BDII: (Site service) This service collects and publishes site’s data for the Information System. All grid sites MUST install one Site-BDII. For cloud sites eu.egi.cloud.information.bdii MUST be installed. Top-BDII: (Central service) The “top-level BDII”. These collect and publish the data from site-BDIIs. Only a few instances per region are required. MyProxy: [Central service] MyProxy is part of the authentication and authorization system. Often installed by sites installing the WMS service. egi.APELRepository: (Central service) The central APEL repository egi.AccountingPortal: (Central service) The central accounting portal egi.GGUS: (Central service) The central GGUS egi.GOCDB: (Central service) The central GOCDB egi.MSGBroker: (Central service) The central message broker egi.Portal: (Central Service) for monitoring generic web portals who dont have a specific service type (deprecated) MSG-Broker: (Central service) A broker for the backbone messaging system. egi.MetricsPortal: (Central service) The central metrics portal egi.OpsPortal: (Central service) The central operations portal egi.GRIDVIEW: (Central service) The central gridview portal egi.GSTAT: (Central service) The central GStat portal egi.SAM: (Central service) The central SAM monitoring ngi.SAM: (Regional Service) NGI-level SAM monitoring box vo.SAM: (Regional Service) VO-level SAM monitoring box site.SAM: (Regional Service) Site-level SAM monitoring box ngi.OpsPortal: (Regional service) NGI-level regional operations portal instance argo.poem: POEM is system for managing profiles of probes and metrics in ARGO system. argo.mon: ARGO Monitoring Engine gathers monitoring metrics and publishes to messaging service. argo.consumer: ARGO Consumer collects monitoring metrics from monitoring engines. argo.computeengine: ARGO Compute Engine computes availability and reliability of services. argo.api: ARGO API service for retrieving status and A/R results. argo.webui: ARGO web user interface for metric A/R visualization and recalculation management. egi.aai.saml: EGI Check-in SAML interface. Enables federated access to EGI services and resources using Security Assertion Markup Language (SAML). Provided by GRNET. egi.aai.oidc: EGI Check-in OpenID Connect interface. Enables federated access to EGI services and resources using OpenID Connect (OIDC). Provided by GRNET. egi.aai.tts: EGI Check-in token translation service. Enables the translation between different authentication and authorisation protocols. Provided by GRNET. Adding new services types Please feel free to make a request for a new service type. All new service type requests need to be assessed by EGI via lightweight review process (by EGI OMB and EGI Operations) so that only suitable types are added, and to prevent duplication.\nYou can submit your request via EGI Helpdesk to the “Configuration and Topology Database (GOCDB)” support unit.\nPlease specify the following information as part of your request:\nname of service type (lowercase): high-level description of the service functionality (255 characters max): project/community/organization maintaining the software: scale of deployment (number of instances and by which organizations): contact point (name/email address): Note: please provide a suggested service type name following the naming scheme described above (technology provider’s reversed domain . software name) and a brief sentence to describe the service type.\n","categories":"","description":"Description of Service types.","excerpt":"Description of Service types.","ref":"/internal/configuration-database/service-types/","tags":"","title":"Service Types"},{"body":"Definition A site (also known as a Resource Centre) is a grouping of grid resources collating multiple Service Endpoints (SEs). Downtimes are recorded on selected SEs of a site. GOCDB stores the following information about sites (non exhaustive list). Note, when editing values in the portal, mandatory fields are marked with ‘*’:\nA unique (short) name - case sensitive (GOCDB and GoCDB are considered different) An official (long) name A domain name for the Site/Resource Centre The home web URL of the Site/Resource Centre A contact email address and telephone number Emergency email for a fast response time in case of urgent problem Alarm email is WLCG Tier1 site specific (used as part of a WLCG workflow for dealing with specific monitoring alarms) A security contact email address and telephone number The site timezone The site’s GIIS URL (Case Sensitive - Please ensure you enter your Site name which is usually encoded in the URL in the correct case!). e.g. ldap://bdii-rc.some-site.uk:2170/mds-vo-name=SITE-NAME,o=grid (if your GOCDB site name site name is upper case) A mandatory human readable description of the site The site’s latitude, longitude and location Production Infrastructure: The site’s intended target infrastructure. This specifies the infrastructure that the site’s services deliver to. This has one of the following values: Production (with this target infrastructure, the EGI site certification transition rules apply) Test (in future, if the site delivers to this infrastructure, then its Certification status will be fixed to ‘Candidate’). ROC [GROUP] - The NGI or Region of the site Country IP address range within which the Site/Resource Centre’s services run IP/netmask (x.x.x.x/x.x.x.x). To specify multiple IP/netmask values, use a comma or semi-colon separated list with no spaces, e.g. 1.2.3.4/255.255.255.0, 1.2.3.5/255.255.255.0 Manipulating sites Viewing sites A site listing page shows a listing of all the sites in the database, with controls to page through the listing. The table headers can be clicked to set the ordering (ascending or descending).\nEach site also has its own listing page. By clicking the link to view a site, you can see all of the site’s information\nSite listing page is available from the sidebar by clicking on the Browse Sites link. sites belonging to a given Operations Centre are also listed from the group details pages (see below) Adding a site Provided you have proper permissions (check the permissions matrix in the related section , you can add a site by clicking on the Add a New Site link in the sidebar. Simply fill the form and validate.\nNote: If you just registered as site admin and want your new site to be registered in GOCDB, please contact your NGI representative.\nEditing site information The editing process will show you the same form as the adding process. To edit a site, simply click the “edit” link on top of the site’s details page.\nRenaming a site Provided you have permissios, you can change the Short Name, Official Name and GIIS URL to the new Resource Center details. For more information regarding the site renaming procedure please see PROC15\nRemoving a site Site deletion is not allowed in GOCDB. If a site stops operation, its certification status should be set to “closed” (see the next section for more information).\nChanging Site Certification Status For each site that delivers to the ‘Production’ Target Infrastructure, GOCDB stores and shows information about its certification status. This reflects the different steps of the official SA1 site certification procedure which typically follows:\nCandidate -\u003e Uncertified -\u003e Certified.\nThe different possible certification statuses are:\nCandidate: the Resource Centre is in under registration according to the registration process described in the RC registration certification procedure. A site will have CANDIDATE status only during certification. Uncertified: site information has been validated by the Operations Centre and is ready to be moved to certified status (again). The certification status of a site can only be changed by a user with a higher level ‘Regional’ (or EGI ‘Project’) level role. This usually means that only regional managers/deputies/staff can update the status of a site that belongs to that region, see the permissions associated to the roles in the related section. Certified: the Operations Centre has verified that the site has all middleware installed, passes the tests and appears stable. Suspended: Site does temporarily not conform to production requirements (e.g. minimum service targets - see the Resource Centre OLA, security matters) and requires Operations Centre attention. A site can be suspended for a maximum of 4 months after which it must be re-certified or closed. Closed: Site is definitely no longer operated by EGI and is only shown for historic reasons. Clarifications:\nThe uncertified status would generally be an information that a site is ready to start certification procedure (again). “uncertified” can also be used as a timewise unlimited state for sites having to keep an old version of the middleware for the absolute needs of an important international VO or to flag a site coping with Operations Centre requirements but not with EGI availability/reliability thresholds. Suspended is always having a temporary meaning. It is used to flag a site temporarily not coping with with EGI availability/reliability thresholds or security requirements, and which should be closed or uncertified by its Operations Centre within 4 months. When being suspended, sites can express that they want to pass certification again. The suspened status is useful to EGI and to the Operations Centre themselves to flag the sites that require attention by the Operations Centre. The closed status should be the terminal one. Suspended is not a terminal state. The following site state transitions are allowed:\ncandidate -\u003e uncertified candidate -\u003e closed uncertified -\u003e certified certified -\u003e suspended certified -\u003e closed (on site request) suspended -\u003e uncertified suspended -\u003e closed The following transitions are explicitly forbidden:\nsuspended -\u003e certified candidate -\u003e something else but uncertified and closed closed -\u003e anything else Going with the definition of the suspended status, Operations Centre managers have to regularly give their attention to all their suspended sites, so that they are processed within the given maximum time of four months. Sites being in suspended should either be set to closed or brought back in production via the uncertified status.\nMore information about site certification statuses can be found in the EGI Federation Procedures:\nPROC09 RC Registration and Certification Procedure PROC11 Resource Centre Decommissioning Procedure PROC12 Production Service Decommissioning Procedure Note: Site certification status cannot be changed by site administrators, and requires intervention of Operations Centre staff.\n","categories":"","description":"Managing Sites entities","excerpt":"Managing Sites entities","ref":"/internal/configuration-database/sites/","tags":"","title":"Sites"},{"body":"EGI provides a training instance of the Notebooks service for training events.\nTo get started:\nGo to https://training.notebooks.egi.eu.\nNote This instance may not use the same software version as in production and may not be always available, as it is typically configured only for specific training events. Start the authentication process by clicking on Start your notebooks! button\nSelect the Identity Provider you belong to from the discovery page. If this is the first time you access an EGI service, Check-in will guide you through a registration process.\nYou will see the Jupyter interface once your personal server is started\nLaunching a notebook Click on the New \u003e Python 3 option to launch your notebook with Python 3 kernel. When you create this notebook, a new tab will be presented with a notebook named Untitled.ipynb. You can easily rename it by right-clicking on the current name.\nStructure of a notebook The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking either the “Play” button in the toolbar, or Cell -\u003e Run in the menu bar.\nThe execution behaviour of a cell is determined by the cell’s type.\nThere are three types of cells: cells, Markdown, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be “Code”, initially).\nCode cells A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel.\nWhen a code cell is executed, its content is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell’s output. The output is not limited to text, with many other possible forms of output are also possible, including figures and HTML tables.\nMarkdown cells You can document the computational process in a literate way, alternating descriptive text with code, using rich text. This is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc.\nIf you want to provide structure for your document, you can also use markdown headings. Markdown headings consist of 1 to 6 hash # signs followed by a space and the title of your section. The Markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.\nWhen a Markdown cell is executed, the Markdown code is converted into the corresponding formatted rich text. Markdown allows arbitrary HTML code for formatting.\nRaw cells Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook.\nKeyboard shortcuts All actions in the notebook can be performed with the mouse, but keyboard shortcuts are also available for the most common ones. These are some of the most common:\nShift-Enter: run cell. Execute the current cell, show any output, and jump to the next cell below. If Shift-Enter is invoked on the last cell, it creates a new cell below. This is equivalent to clicking the Cell -\u003e Run menu item, or the Play button in the toolbar. Esc: Command mode. In command mode, you can navigate around the notebook using keyboard shortcuts. Enter : Edit mode. In edit mode, you can edit text in cells. Hands-on We pre-populate your home directory with some sample notebooks to get started, below you can find links to other notebooks that we have used in past trainings that may be useful to explore the system:\nA very basic notebook to get started Getting data and doing a simple plot. Connect to NOAA's GrADS Data Server to plot wind speed. Installing new libraries. Interact with Check-in ","categories":"","description":"Notebooks for training events\n","excerpt":"Notebooks for training events\n","ref":"/users/dev-env/notebooks/training/","tags":"","title":"Training Instance"},{"body":"Authentication The EGI Configuration Database UI attempts to authenticate you in one of two ways (the REST style API applies X.509 only):\nFirst, by requesting an IGTF accredited user certificate from your browser. If a suitable certificate is detected, you will be asked to confirm selection of your certificate in your browser. Note: if a client certificate has been provided, it will take precedence over any IdP based authentication. Second, if you do not have a user certificate or you hide your certificate from (e.g. by starting a new/anonymous private browser session or pressing ‘Cancel’ when prompted for a certificate), you will be redirected to the landing page where you can authenticate with the EGI Identity Provider Service (IdP) and your chosen institution (if available). If authentication is successful, you will be redirected back to the Configuration Database. Please note, not all logins available in the EGI IdP provide a sufficient level of assurance (LoA) to login (the LoA must be ‘Substantial’). Editing your user account The editing process is the same as the registration process. To edit your user account, simply follow these steps:\nclick on the “view details” link in the “User Status” panel on the sidebar. You should get a page showing your user account information. Click on the “edit” link on top of it. Viewing users Each user account has its own user details page which is accessible to anyone with a valid certificate.\nThere is currently no facility for listing all users in the database. List of users that have a role on a given site appears on site details pages (see section about sites). It is also possible to search for a user’s account using the search feature on the sidebar.\nDeleting your user account If you wish to unregister from the Configuration Database, follow these steps:\nclick on the view details link in the “User Status” panel on the sidebar. You should get a page showing your user account information. Click on the “delete” link on top of it. Confirm your choice. Your account will then be deleted along with any roles the account has.\nLost access to your account Under the following circumstances it is possible to lose access to an account:\nYou use your IGTF X.509 certificate to access and renew or change certificate, it is possible that the certificate’s distinguished name (DN) has also changed. This is what the Configuration Database uses to identify your account. You have authenticated with EGI Check-in, but via a different underlying IdP (i.e. You usually log in with your institutional credentials, but today you logged in with EGI SSO). You have changed the way you log in (i.e. X.509 to EGI Check-in) In these situations, it is usually possible to regain access using to your certificate based account by following one of the following procedures:\nIf for any reason you were unable to complete the relevant procedure (e.g. mail confirmations problems) please open an EGI Helpdesk ticket addressed to the “Configuration and Topology Database (GOCDB)” support unit.\nYou have a new certificate and have lost access to your account Install your new certificate in your browser. Go to EGI Configuration Database. If you are already logged in, then clear your caches and restart your browser or start a new private browser session. When prompted, select your new certificate. You should be able to access, but since you are authenticated with your new certificate, it is as if you had no user account. In the User Status panel in the sidebar, click on the Link Identity/Recover Account link. Specify in the form: Authentication type: X.509 The DN of your old certificate previously used to authenticate to your X.509 based account. The email address associated to your account. Submit and, upon validation, an email will be sent to the specified address, which has to match the one registered with your account. This is to avoid identity theft. The email contains a validation link. Click on the validation link or copy/paste in your browser. Once validated, changes are immediate. You can only associate one X.509 DN with your account at any given time.\nYou have authenticated with EGI Check-in, but via a different Identity Provider It’s possible to link this identity with your “other” EGI Check-in identity at the level of the EGI Check-in, see account linking documentation.\nOnce your identity is linked at the EGI Check-in level, if you are still having problems accessing, please reassign the ticket to “Configuration and Topology Database (GOCDB)”\nYou have changed the way you log in (i.e. X.509 to EGI Check-in) You can link these identities at the EGI Configuration Database level by following these steps. The steps assume an existing X.509 based account and that you are currently authenticated via EGI Check-in, though the steps should hold for any pair of supported authentication methods.\nIn the User Status panel in the sidebar, click on the Link Identity/Recover Account link. Specify in the form: Authentication type: X.509 The DN of your certificate used to authenticate to your X.509 based account The email address associated to your X.509 based account. Submit and, upon validation, an email will be sent to the specified address, which has to match the one registered with your X.509 based account. This is to avoid identity theft. The email contains a validation link. Click on the validation link or copy/paste in your browser. Authenticate with your EGI Check-in identity. Once authenticated/validated, changes are immediate and you will be able to access your account with both your X.509 and EGI Check-in identities. ","categories":"","description":"Understanding and manipulating user accounts","excerpt":"Understanding and manipulating user accounts","ref":"/internal/configuration-database/users-roles/managing-accounts/","tags":"","title":"Understanding and manipulating user accounts"},{"body":"Introduction In the sub-pages there is an explanation over the EGI Configuration Database user accounts, how to manage them, and the roles defined.\n","categories":"","description":"Guide about user accounts and the roles","excerpt":"Guide about user accounts and the roles","ref":"/internal/configuration-database/users-roles/","tags":"","title":"Users and roles"},{"body":"Roles definition Registered users with a user account will need at least one role in order to perform any useful tasks.\nRole Types A role: Unregistered users B role: Registered users with no role C role: Users with a role at site level (site admin) C’ role: Users with a management role at site level (site operations manager, site security officer…) D role: Users with a role at regional level (regional staff support staff, ROD, 1st Line Support) D’ role: Users with a management role at regional level (NGI manager or deputy, security officer) E role: Users with a role at project level The only difference between C and C’ users is that:\nC can NOT approve/reject role requests. C’ can only approve/reject role requests for their SITE. The difference between D and D’ users is that:\nD can NOT add/delete sites to/from their NGI. D can NOT update the certification status of member sites. D can NOT approve or reject role requests. Roles At Site level Site Administrator - person responsible of maintaining a site and associated information in the EGI Configuration Database (C Level) Site Security officer - official security contact point at site level (C' Level) Site Operations Deputy Manager - The deputy manager of operations at a site (C’ Level) Site Operations Manager - The manager of site operations (C’ Level) At NGI/Regional level Regional First Line Support - Staff providing first line support for an NGI (D Level) Regional Staff (ROD) - staff involved in Operations Centre activities such as user/operations support (D Level) NGI Security officer - official security contact point at regional level (D' Level) NGI Operations Deputy Manager - Deputy manager of NGI operations (D’ Level) NGI Operations Manager - Manager of NGI operations (D’ Level) At Project level COD staff - COD staff (E Level) COD administrator - People administrating Central COD roles (E Level) EGI CSIRT Officer - official security contact point at project level (E Level) Chief Operations Officer (COO) - The EGI Chief Operations Officer (E Level) Permissions associated to roles Roles and permissions are based on whether the considered object is owned or not. In the table below the following definitions apply:\nOwned group: a group on which the role applies (ROC, NGI, project) Owned site: a site on which the role applies, or belonging to an owned group Owned service endpoint: a service endpoint belonging to an owned site Each role has a set of associated permissions which apply on the role’s scope (site, region or project). Main permissions are summarised in the table below\nAction A) Unregistered users B) Registered users with no role C) Site level users C’ ) Site Management Level Users D) NGI level users D’ ) NGI Management Level Users E) Project level users Add a site to an owned group irr. irr. irr. irr. no yes irr. Add a site to a non owned group no no no no no no no Add a service endpoint to an owned site irr. irr. yes yes yes yes irr. Add a service endpoint to a non owned site no no no no no no no Add a downtime to an owned service endpoint irr. irr. yes yes yes yes irr. Add downtime to a non owned service endpoint no no no no no no no Update information of an owned site irr. irr. yes yes yes yes irr. Update information of a non owned site no no no no no no no Update certification status of an owned site irr. irr. no no no yes yes Update certification status of a non owned site no no no no no no yes Update information of a owned service endpoint irr. irr. yes yes yes yes irr. Update information of a non owned service endpoint no no no no no no no Update information of an owned group irr. irr. irr. irr. yes yes irr. Update information of a non owned group no no no no no no no Update own user account details irr. yes yes yes yes yes yes Update other user’s account no no no no no no no Update a downtime on an owned service endpoint irr. irr. yes yes yes yes irr. Update a downtime on a non owned service endpoint no no no no no no no Delete an owned site irr. irr. no no no no no Delete a non owned site no no no no no no no Delete an owned service endpoint irr. irr. yes yes yes yes irr. Delete a non owned service endpoint no no no no no no no Delete an owned group irr. irr. irr. no no no irr. Delete a non owned group no no no no no no no Delete a downtime on an owned service endpoint irr. irr. yes yes yes yes irr. Delete a downtime on a non owned service endpoint no no no no no no no Delete your own user account irr. yes yes yes yes yes yes Delete other user’s account no no no no no no no Register a new user account yes irr. irr. irr. irr. irr. irr. Request a new role no yes yes yes yes yes yes Approve a role request on an owned group irr. irr. no no no yes yes Approve a role request on an owned site no no no yes no yes irr Approve a role request on a non owned site or group no no no no no no no Reject a role request on an owned group no no no no no yes irr. Reject a role request on an owned site no no no yes no yes irr Reject a role request on a non owned site or group no no no no no no no Revoke an existing role on an owned object irr. irr. no yes no yes irr. Revoke an existing role on a non owned object no no no no no no no Retrieve an existing account/ change certificate DN yes yes yes yes yes yes yes Requesting roles for your account There are 2 ways to request new roles.\nBy clicking on the manage role link (sidebar, user status panel) the first form allows you to choose the entity (site or group) on which you want to request a role the second form lets you choose the role you want to apply for By clicking on the request role link from site detail pages or group detail pages. displayed form lets you choose the role you want to apply for Once made, role requests have to be validated before the role is granted to you. This part of the process is described in the next section.\n","categories":"","description":"Understanding and manipulating roles","excerpt":"Understanding and manipulating roles","ref":"/internal/configuration-database/users-roles/managing-roles/","tags":"","title":"Using roles"},{"body":"With High-Throughput Compute you can run computational jobs at scale on the EGI infrastructure. It allows you to analyse large datasets and execute thousands of parallel computing tasks.\nHigh-Throughput Compute is provided by a distributed network of computing centres, accessible via a standard interface and membership of a virtual organisation. EGI offers more than 1,000,000 cores of installed capacity, supporting over 1.6 million computing jobs per day.\nThis service supports research and innovation at all scales: from individuals to large collaborations.\nMain characteristics of the service:\nAccess to high-quality computing resources Integrated monitoring and accounting tools to provide information about the availability and resource consumption Workload and data management tools to manage all computational tasks Large amounts of processing capacity over long periods of time Faster results for your research Shared resources among users, enabling collaborative research Service management Miscellaneous collection of documentation related to High-Throughput Compute.\nChanging the Site BDII ","categories":"","description":"Execute thousands of computational tasks to analyse large datasets","excerpt":"Execute thousands of computational tasks to analyse large datasets","ref":"/providers/high-throughput-compute/","tags":"","title":"High-Throughput Compute"},{"body":"Notify Site field The list of sites included in the drop-down menu of the “Notify site” field is taken from GOCDB (EGI sites) and from OIM DB (OSG sites). Sites registered in OIM are only visible if they have status “enabled”. Sites registered in GOC DB are only visible if they have status “certified”. Sites in other states (e.g. “suspended”) are not visible in the “Notify sites” drop-down list. the sites information is synchronized once per night. In case a site’s status changes, the site disappears from the “Notify sites” drop-down list. Existing GGUS tickets related to this site get the “Notify sites” field flushed. The NGI to which the site belongs inherits the ticket from the site and is in charge of further processing this ticket. Notify multiple sites option Notifying multiple sites is a feature for submitting tickets with the same topic against an unlimited number of sites in GGUS. This feature can be used only by users owning a dedicated privileges. There is a specific submit form linked from GGUS ticket submit area. In the “Notify SITE” drop-down menu sites can be checked for receiving a ticket. All sites registered in GGUS can be notified using this option. ","categories":"","description":"Seletting a Resource Centre as recipient and submitting tickets to multiple RCs\n","excerpt":"Seletting a Resource Centre as recipient and submitting tickets to …","ref":"/internal/helpdesk/features/tickets-to-multiple-sites/","tags":"","title":"'Notify Site' field and tickets to multiple sites"},{"body":"Overview This tutorial describes the access to EGI DataHub spaces from a virtual machine. In the following paragraphs you will learn how to access data remotely stored in EGI DataHub like if they were local, using traditional POSIX command-line commands, by:\ninstalling the oneclient component configuring access to an EGI DataHub Oneprovider via oneclient Prerequisites In order to access the EGI DataHub data you need an EGI Check-in account. If you don’t have one yet you can Sign up for an EGI account.\nOneclient installation The installation of oneclient package is currently supported for:\nUbuntu 18.04 LTS (Bionic Beaver) Ubuntu 20.04 LTS (Focal Fossa) CentOS 7 CentOS 8 Stream Alternatively a docker based installation is also provided.\nOneclient installation via packages Use the following command in order to install the oneclient package in a supported OS:\n$ curl -sS http://get.onedata.org/oneclient.sh | bash This will also install the needed dependencies.\nOneclient installation via docker In order to use the Dockerized version of oneclient (provided that you have docker installed), you can run the following command:\n$ docker run -it --privileged -v $PWD:/mnt/src --entrypoint bash onedata/oneclient:20.02.15 This command will also expose the current folder to the container (as /mnt/src) to ease the transfer of data.\nGetting the token to access data In order to access data stored in EGI DataHub via oneclient, you need to get an API access token.\nUsing oneclient Once you have acquired a token valid for oneclient you can configure it on the environment as follows:\n$ export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e You must also configure in the environment the provider you would like to connect to. The EGI DataHub offers a PLAYGROUND space hosted by the Oneprovider plg-cyfronet-01.datahub.egi.eu which is accessible for testing by anyone with a valid EGI Check-in account.\nTherefore the access to that particular space can be configured as follows:\n$ export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu Now in order to access data from a local folder you need to run the following commands:\n$ mkdir /tmp/space $ oneclient /tmp/space and then all usual file and folder operations (POSIX) will be available:\n$ root@222d3ceb86df:/tmp/space# ls -l total 0 drwxrwxr-x 1 root root 0 Jan 28 16:56 PLAYGROUND Creating a file into the folder will push it to the Oneprovider and it will be accessible in the web interface and from other providers supporting the space.\nBy using the default settings you can see all the spaces you have access to, but it will also be possible to specify a specific space to access using the option --space \u003cspace\u003e.\nOneclient offers a lot of other options for configuration (e.g. buffer size, direct I/O, etc) which are listed when you type the oneclient command without any argument.\n","categories":"","description":"Use data in EGI DataHub from a virtual machine\n","excerpt":"Use data in EGI DataHub from a virtual machine\n","ref":"/users/tutorials/vm-datahub/","tags":"","title":"Access DataHub from a Virtual machine"},{"body":"Introduction: purpose and conditions The purpose of ALARM tickets is to notify WLCG Tier-0 and Tier-1 administrators about serious problems of the site at any time, independent from usual office hours. Only experts, nominated by the WLCG VOs are allowed to submit alarm tickets. They need to have the appropriate permissions in GGUS user database. The involved VOs are: Alice Atlas Cms Lhcb Only the Tier-0 \u0026 Tier-1 sites are involved in the alarm tickets process. The WLCG Tier-0/Tier-1 site names can be used as well as the relevant GOC DB site names. Alarm tickets are routed to the NGI/ROC the tier site belongs to automatically. They do not need a routing by the TPM. The NGI/ROC is notified about the ticket in the usual way. In parallel the site receives an alarm email signed with a GGUS certificate. This alarm email is processed at the Tier-0/Tier-1 and notifies the relevant people at any time. Alarm email addresses are taken from GOC DB for the EGI sites and from OIM for the OSG sites. VOMS is used by GGUS as the information source for authorised alarmers. Becoming an Alarm-ticket member People who want to become an alarmer have to\nregister in GGUS first be added to the appropriate group in a VOMS server. GGUS system synchronizes its user database once per night with the VOMS servers. The synchronization is based on the DN string. Please make sure the DN of your GGUS account is the same than the one registered in VOMS.\nTier-1 site admins can become alarmers for testing the alarm process for their site. They have to fill in the registration form for supporters. Please add an appropriate comment (e.g. “I’m a xxx tier-1 site admins and I want to become an alarmer for testing purposes.”) in the registration form.\nTechnical description This section describes the workflows of alarm tickets from a technical point of view.\nALARM ticket submission Alarm tickets can be submitted using the GGUS web portal. On top of the ticket submit form in GGUS web portal there is a link to the submit form for alarm tickets.\nAs alarm ticket submitters are experts who will hopefully provide all necessary information, the number of fields on the alarm ticket submit form is reduced to a minimum compared to the number of fields on the user ticket submit form.\nThree fields on this form are mandatory:\nSubject MoU Area Notified Site All other fields are optional.\nALARM ticket processing The processing of a team ticket consists of two main parts: the notification of the notified site and the routing of the ticket to the NGI/ROC the site belongs to.\nTier-1 site notification In parallel to the creation of an alarm ticket, the GGUS system sends an alarm email directly to the tier 1 site specified in field “Notify SITE”. This email is sent to a specific site alarm mail address and signed with the GGUS certificate. The tier-0 alarm mail address is based on the VO name. It is “voname”-operator-alarm\"atnospam\"cern.ch. Tier-1 site alarm mail addresses are taken from the “Emergency Email” field in GOC DB. For tier-1 sites registered in the OSG OIM DB the alarm email address is taken from field “SMSAddress” of the “Administrative Contact” in OIM DB. The DN of the GGUS certificate is /C=DE/O=GermanGrid/OU=KIT/CN=ggusmail/ggus.eu. The alarm mail looks like the following figure.\nTicket routing Alarm tickets are bypassing the TPMs and routed to the appropriate NGI/ROC automatically. The decision to which NGI/ROC a ticket has to be routed is done automatically, based on the value of the “Notify SITE” field. The “Notify SITE” drop-down menu shows both the tier 1 site names from GOC DB and the tier 1 site names used by WLCG.\nALARM confirmation Once the tier 1 site received the alarm email the receipt should be confirmed. Sending a reply mail containing the typical GGUS identifier and the ticket ID “GGUS-Ticket-ID: #00000000” in the subject is sufficient. Such a reply will be added to the alarm ticket.\nWorking on ALARM tickets For working on alarm tickets and resolving them please use the GGUS portal. A reference link to the alarm ticket is given in the alarm email notification.\nPeriodic ALARM ticket testing rules Alarm ticket testing is documented in WLCG twiki.\nSchema of the Alarm tickets process ","categories":"","description":"Definition and usage of the Alarm tickets\n","excerpt":"Definition and usage of the Alarm tickets\n","ref":"/internal/helpdesk/features/alarm-tickets/","tags":"","title":"Alarm tickets"},{"body":"Introduction After discussions in the GGUS-AB and the OMB meetings, members agreed on implementing a work flow for closing tickets waiting for submitter input after a reasonable amount of time. This work flow was also presented at the UCB meeting.\nStatus value “waiting for reply” The status value “waiting for reply” should only be used when input from the ticket submitter is required. Setting the ticket status to “waiting for reply” in GGUS triggers an email notification to the submitter’s email address registered in the ticket. After the submitter has replied either by\nsending an email using the same sender mail address as registered in the ticket or by updating the ticket in web portal using the same credentials/certificate used for submitting the ticket, the ticket status changes to “in progress” automatically. Updates on the ticket done using different credentials/certificates or email addresses than registered in the ticket will not be recognized as reply by the system and hence have no impact on the work flow. The status value “waiting for reply” must not be used in case of waiting for input of any other person involved in the solving process.\nWorkflow First action 5 working days after setting the ticket status to “waiting for reply” the first reminder is sent to the ticket submitter requesting input. The ticket will also be added to the ticket monitoring dashboard. The ticket monitoring team will make sure that the status value “waiting for reply” is used in a correct sense.\nSecond action In case the submitter does not reply, the second reminder is sent to the ticket submitter requesting input after 5 more working days.\nThird action In case the submitter does not reply after another 5 more working days the ticket monitoring team gets notified. The monitoring team will check the ticket for any updates by the submitter not recognized by the system and set the ticket status to “unsolved” if none. The ticket will follow the usual process for “solved”/“unsolved” tickets and be closed after 10 working days without re-opening the ticket again.\nAction Ticket Status Working Days Work Flow 1 waiting for reply 5 first reminder to submitter; adding ticket to monitoring dashboard 2 waiting for reply 5 second reminder to submitter 3 waiting for reply 5 notification to monitoring team; the monitoring team sets status “unsolved” 4 unsolved 10 manuel status change to “closed” by ticket monitoring team Summary In this workflow there is always a human intervention before closing a ticket. The submitter has 15 working days in total for replying to a ticket. Additionally, they have 10 more working days for re-opening the ticket in case they do not agree with setting the ticket to “unsolved”.\nWorkflow graph ","categories":"","description":"Process for closing tickets lacking of submitter's input\n","excerpt":"Process for closing tickets lacking of submitter's input\n","ref":"/internal/helpdesk/workflows/waiting-for-submitter/","tags":"","title":"Closing tickets lacking of submitter input"},{"body":"Working with compute Most scientific work researchers do includes computation. Could be the need to run a small analysis. Or a data reduction job that has to be repeated a million times on every measurement coming in from an ionosphere observation facility. Or a handful of cutting-edge simulations of climate models that each demand a huge number of compute cores.\nEGI provides a variety of compute services for each of these use cases. Researchers can choose the best compute tool for each application, from customisable and elastic Virtual Machine (VM) based cloud to fully managed distributed platforms that will run their jobs using CPUs, GPUs or both.\nThe compute services are summarized below:\nCloud Compute means VM-based computing with associated storage. It delivers customisable resources where users have complete control over the software, the supporting compute type and capacity. Typical use-cases are user gateways or portals, interactive computing platforms and almost any kind of data- and/or compute-intensive workloads. Container Compute supports running container-based applications with either Docker or Kubernetes on top of Cloud Compute. Typical use-cases are multi-tenant, microservices-based applications that must easily scale horizontally. High Throughput Compute provides access to large, shared grid computing systems for running computational jobs at scale. Typical use-cases include analysis of large datasets in an “embarrassingly parallel” fashion (i.e. by splitting the data into small pieces), and executing thousands, or even more independent computing tasks simultaneously, each processing one piece of data. When researchers encounter the need for large amounts of computing power, need hybrid solutions based on the above mentioned services, or simply do not care to manage the underlying compute facilities that will run their workloads, cloud orchestrators can help provision, manage, and monitor compute resources, which then run scientific workloads.\nThe following sections describe each compute service that is available in the EGI infrastructure.\n","categories":"","description":"Compute services in the EGI Cloud\n","excerpt":"Compute services in the EGI Cloud\n","ref":"/users/compute/","tags":"","title":"Compute Services"},{"body":"Working with data One of the main daily activities of researchers is to collect or create data that needs to be processed, analyzed, or shared. The EGI offering in this area is quite vast and includes both low level storage services and data management frameworks, as well as specialized data transfer services.\nExplore the areas below for further details.\n","categories":"","description":"Data services in the EGI Cloud\n","excerpt":"Data services in the EGI Cloud\n","ref":"/users/data/","tags":"","title":"Data Services"},{"body":"What is it? Built on more than a decade of experience in LHC experiments, Rucio serves the data needs of any modern scientific experiments. Rucio can manage large amounts of data, countless numbers of files, heterogeneous storage systems, globally distributed data centres, with monitoring and analytics.\nRucio allows management of data with expressive statements. You to say what you want, and Rucio will figure out the details of how to do it. For example, three copies of my file on different continents with a backup on tape. You can also automatically remove copies of data after a set period or once its access popularity drops.\nWhile Rucio is extremely scalable, the STFC Rucio Data Management Service is designed for smaller communities, with expected data needs up to tens of Petabytes. The fact that the underlying Rucio infrastructure is managed by STFC, allows communities to easily start using and/or test Rucio with little setup cost.\nOfficial Rucio Pages Rucio Homepage Rucio Documentation Multi-VO specific pages Multi-VO Rucio at RAL as a service Privacy Policy Acceptable Use Policy ","categories":"","description":"Organise and access data at scale with Rucio","excerpt":"Organise and access data at scale with Rucio","ref":"/users/data/management/rucio/","tags":"","title":"Rucio"},{"body":"Some procedures to deal with security events Sites facing or suspecting a security incident on their resources have to follow Incident Handling Procedure\nNew vulnerability issues of the middleware should be handled as defined in the related procedure with a guide what to do when you find a vulnerability.\nSites having critical vulnerabilities are handled according to SEC03 EGI-CSIRT Critical Vulnerability Handling Procedure, and if they do not respond properly, they may face suspension.\n","categories":"","description":"Information about procedures to deal with security incidents.","excerpt":"Information about procedures to deal with security incidents.","ref":"/providers/rod/security/","tags":"","title":"Dealing with security incidents"},{"body":"What is it? Dynamic On-Demand Analysis Software (DODAS) enables the execution of user analysis code both in batch mode and interactively via the Jupyter interface. DODAS is highly customizable and offers several building blocks that can be combined together in order to create the best service composition for a given use case. The currently available blocks allow to combine Jupyter and HTCondor as well as Jupyter and Spark or simply a jupyter interface. In addition, they allow the management of data via caches to optimize the processing of remote data. This can be done either via XCache or MinIO S3 object storage capabilities. DODAS is based on docker containers and the related orchestration relies on Kubernetes that enables the possibility to compose the building blocks via a web-based user interface thanks to Kubeapps.\nIn order to deploy it services over various backend, DODAS relies on Infrastructure Manager and INDIGO PaaS\nTip If you want to give it a try and deploy your cluster in the EGI Federation: DODAS. Note For detailed information about DODAS please see its documentation. It was also presented in one of the EGI Webinars, more details are available on the indico page and the video recording is available on YouTube. ","categories":"","description":"Instantiate on-demand customizable infrastructures for data analysis\n","excerpt":"Instantiate on-demand customizable infrastructures for data analysis\n","ref":"/users/compute/orchestration/dodas/","tags":"","title":"Dynamic On-Demand Analysis Software"},{"body":"Connect to Check-in an IdP federated in an hub and spoke federations I get an error similar to: “Error - No connection between institution and service” (SURFconext example) In case of a “hub and spoke” federation the federation coordinator may require that the IdP administrators explicitly request to connect to a SP and let their users to authenticate on these SP.\nIn most of the cases this is not a configuration problem neither for the Check-in service nor for the Identity provider. The connection needs to be implemented in the hub and spoke IdP Proxy.\nOne example of such federation is SURFconext, the national IdP federation for research and education in the Netherlands operated by SURFnet. If you are using credentials from a Dutch IdP in eduGAIN, you or your IdP administrators need to request the connection. The following steps will lead you to perform the connection:\nConnect to SURFconext dashboard Search for “EGI AAI Service provider proxy” If the service does not show in the search, you need to ask SURFnet to add it in the dashboard, please write to support at surfconext dot nl In the dashboard, near the “EGI AAI Service provider proxy” there should be a “Connect” button, this will create a service ticket and the SURFconext team will make the connection active. After you received confirmation that the “EGI AAI Service provider proxy” is accessible, you will be able to login in Check-in Authentication error with ADFS-based Identity Providers Why do I get the error below after successfully authenticating at my Home IdP? opensaml::FatalProfileException at (https://aai.egi.eu/registry.sso/SAML2/POST) SAML response reported an IdP error. Error from identity provider: Status: urn:oasis:names:tc:SAML:2.0:status:Responder The Responder error status is typically returned from ADFS-based IdP implementations (notably Microsoft ADFS 2.0 and ADFS 3.0) that cannot properly handle Scoping elements. Check-in can be configured to omit the scoping element from the authentication requests sent to such IdPs in order to allow successful logins. Please send an email to the Check-in Support team using checkin-support \u003cAT\u003e mailman.egi.eu and include a screenshot of your error.\nI have linked an IGTF X.509 certificate to my Check-in identity but the information is inaccurate or incomplete What can I do? To update your certificate information, follow these steps to log into your Check-in profile page using your IGTF certificate:\nClick here to access your profile page Warning This may log you out of any service you have accessed with Check-in on this browser! 2. On the Check-in identity provider discovery page, select IGTF\nWarning If prompted to log in with a different identity provider, click CHOOSE ANOTHER ACCOUNT and then select IGTF. Alternatively, you can click here for your convenience ","categories":"","description":"Most frequent questions about EGI Check-in\n","excerpt":"Most frequent questions about EGI Check-in\n","ref":"/users/aai/check-in/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"How do I install library X? You can install new software easily on the notebooks using conda or pip. The %conda and %pip magics can be used in a cell of your notebooks to do so, e.g. installing rdkit:\n%conda install rdkit Once installed you can import the library as usual.\nWarning Any modifications to the libraries/software of your notebooks will be lost when your notebook server is stopped (automatically after 1 hour of inactivity)! Can I request library X to be installed permanently? Yes! Just let us know what are your needs. You can contact us via:\nOpening a ticket in the EGI Helpdesk, or Creating a GitHub Issue We will analyse your request and get back to you.\n","categories":"","description":"Most frequent questions asled about EGI Notebooks\n","excerpt":"Most frequent questions asled about EGI Notebooks\n","ref":"/users/dev-env/notebooks/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"Site endpoints must be registered in EGI Configuration Management Database (GOCDB). If you are creating a new site for your cloud services, check the PROC09 Resource Centre Registration and Certification procedure. Services can also coexist within an existing (grid) site.\nExpected Services These are the expected services for a working site:\nIf offering native OpenStack access (nova), register: org.openstack.nova for the Nova endpoint of the site. The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3. Set the Host DN so the cloud-info-provider can be enabled in the AMS.\nIf offering native OpenStack access (swift), register: org.openstack.swift for the swift endpoint of the site. The endpoint URL field must contain the Keystone v3 URL: https://hostname:port/url/v3.\neu.egi.cloud.accounting for the host sending the records to the accounting repository (executing SSM send).\nDeprecated services Deprecated services for cloud providers:\nSite-BDII. This service collects and publishes site's data for the Information System. Existing sites should already have this registered.\neu.egi.cloud.vm-metadata.vmcatcher for the VMI replication mechanism. Register here the host providing the replication (i.e. the host with cloudkeeper installation)\nIf offering OCCI interface, eu.egi.cloud.vm-management.occi for the OCCI endpoint offered by the site. The endpoint URL must follow this syntax:\nhttps://hostname:port/?image=\u003cimage_name\u003e\u0026resource=\u003cresource_name\u003e\nwhere \u003cimage_name\u003e and \u003cresource_name\u003e cannot contain spaces. These attributes map to os_tpl and resource_tpl respectively and will be the ones used for monitoring purposes.\n","categories":"","description":"Registration of service endpoints in GOCDB\n","excerpt":"Registration of service endpoints in GOCDB\n","ref":"/providers/cloud-compute/registration/","tags":"","title":"GOCDB Registration"},{"body":"What is it? Support for EGI services is available through the EGI Helpdesk.\nThe EGI Helpdesk is a distributed tool with central coordination, which provides the information and support needed to troubleshoot product and service problems. Users can report incidents, bugs or request changes.\nThe support activities are grouped into first and second level support.\n","categories":"","description":"EGI Helpdesk\n","excerpt":"EGI Helpdesk\n","ref":"/internal/helpdesk/","tags":"","title":"Helpdesk"},{"body":"Introduction With the term “Central” or “Core” Service we refer to a category of services in the EGI Infrastructure providing capabilities that support and complement the other services of the infrastructure and the related activities. A list of those services is available in the section Services for Federation of our site. The EGI Core Services are co-funded by EGI Foundation and the providers are selected through a bidding process: the technical details of the services that should be delivered are advertised to the EGI Council, and only the providers with the EGI Participant role in the Council can apply to the bid. How to join the EGI Federation as a member of the Council. Current members of the EGI Council Differently from services with a distributed nature such as HTC, Cloud, and Storage, they cannot be ordered through the Marketplace, but they become available as soon as a user joins the infrastructure (e.g., the access to the EGI Helpdesk service). Selection of the providers and registration With the selected providers, EGI Foundation negotiates and signs an OLA defining the terms and conditions for the delivery of the services; at the same time, the process to add the services to the EGI Service portfolio is started, if the service is not already included.\nAt this point, steps, similar to the ones for Resource Centers and Technology Providers, follow in order to guarantee the regular day-to-day operation of the service, such as:\nregistration in the Configuration Database and certification; definition of the Support Unit in the Helpdesk system to handle incidents and service requests; enabling of the monitoring; periodic performance reports as defined in the given OLA to verify that the Service Level Targets are achieved. In addition to this, the providers are also requested to create a capacity plan, an availability and continuity plan, and to interact with Change Management (CHM) and Release and Deployment Management (RDM) processes for managing changes and new releases of their services.\nCapacity plan The capacity plan is important to assess if the capacity of the service is sufficient to respond to the current and to the future demand of the service.\nAny capacity aspect of the service delivery is analysed (human, technical, and financial) with the definition of quantitative parameters to measure the usage and the load of the service. The approach to adjust the capacity of the service in relation to a change in the demand is defined, and recommendations on capacity requirements for the next reporting period are provided as well.\nEGI Foundation defined a template for the Capacity plans and contacts and supports the service providers for the creation of a new Capacity plan or for their regular reviews (the reviews are usually performed at least twice per year).\nAvailability and continuity plan In the Availability and Continuity plan, a number of risks affecting the availability and continuity of the service is identified and assessed: each risk is rated in terms of likelihood and impact with the definition of countermeasures to implement that should avoid the occurrence of the given risk. Any remaining vulnerability is identified as well, and in case the rating of a risk is considered to be too high in relation to the risk acceptance criteria, a plan either to improve the existing countermeasures or to implement new ones is created, with the aim either to reduce the likelihood of a risk or to mitigate the impact in case a risk occurs.\nThe plan is completed by a continuity and recovery test, where the continuity of the service and its recovery capacity are tested against a simulated disruption scenario: the performance of this test is useful to spot any issue in the recovery procedures of the service.\nAlso in this case, the discussion of the Availability and Continuity plan is started and overseen by EGI Foundation who shares with the providers a template that will be filled in with the details of the given service. Availability and Continuity plans are reviewed on a yearly basis.\nManaging changes and new releases All changes to the services should follow the EGI Change Management (CHM) process according to the Change Policy, in order to evaluate the potential impact that a change can have on the service and on the infrastructure as a whole.\nWhen registering a Request for Change, besides a general description of the change, the providers are expected to provide:\nthe risk level as a result of assessing the impact and the likelihood of things going wrong, the type of change, the eventual list of other services potentially affected by the change, if it is possible to revert the change, the proposed date for the implementation of the change, if it is needed to schedule a downtime of the service. Requests for Changes are assessed by the Change Advisory Board (CAB), deciding whether the Change is going to be approved or rejected.\nFor recurrent changes with a relatively low risk level, the providers can request to classify them as Standard Changes, so that invoking the CAB won’t be necessary and they can undergo the release process after the release plan is agreed with the EGI CHM staff.\nThe Emergency Changes are created when there is an urgent need to fix either a newly discovered security vulnerability or a critical issue: in this case the formal approval of the CAB is not required, and it is enough that the release plan is accepted by CHM staff.\nIn all of the other situations the changes are classified as Normal and depending on their risk level they might need to be assessed by the CAB.\nBefore the release in the production environment, the providers may invite the users or other impacted service providers, to test the new changes on a pre-production instance in order to gather feedback and to find out possible issues that might be overlooked during the preparation of the new release. If no problems are found, the release can go live, and a Post Implementation Review is conducted after a few days to close the case.\nSecurity aspects The providers of central services are subject to the same policies, procedures and requirements applying to the federated service providers, as documented at this page.\nNevertheless, they are also subject to the following requirement that is documented in the service OLA that is being agreed with them:\nThey should immediately report suspected security incidents to the EGI Foundation. This is not exempting them to follow the SEC01 EGI CSIRT Security Incident Handling Procedure and inform EGI CSIRT within 4 hours. It’s also important to understand that when processing of personal data is taking place, EGI Foundation holds the role of Data Controller and the provider is a Data Processor, as defined in the Global Data Protection Regulation (GDPR).\nData Processing Agreements, regulating the conditions and constraints of the data processing activities conducted by the Data Processor on behalf of the Data controller, will be put in place on the initiative of the EGI Foundation staff. EGI Foundation will also prepare, together with the service provider, adequate Privacy Notice and Acceptable Use policy that have to be presented and made available to the users of the service. EGI Foundation is also complying with all the principles set out by the REFEDS Data Protection Code of Conduct in its most recent version, implying that the central service provider should also comply with them. The central service provider should also follow requirements relating to the software and covering the usage of a proper licence, the access to and management of the source code, implementation of best practices; as well as requirements relating to the IT Service management and covering the need for having key staff properly trained about IT Service Management, committing to continual improvement and having their Service Management System (SMS) interfacing with the EGI SMS, especially for important processes like the Change Management process.\n","categories":"","description":"Guidelines for joining the EGI Infrastructure to provide a Core Service supporting the other services and activities","excerpt":"Guidelines for joining the EGI Infrastructure to provide a Core …","ref":"/providers/joining/core-service/","tags":"","title":"Joining as a provider of a Core Service"},{"body":" EGI DataHub service Overview slides Community Forum EGI Webinar and YouTube video System requirements Official Onedata documentation Onedata homepage Getting started Source code ","categories":"","description":"Links to additonal DataHub resources\n","excerpt":"Links to additonal DataHub resources\n","ref":"/users/data/management/datahub/links/","tags":"","title":"DataHub Links"},{"body":"Overview Each instance of the FTS3 service, offers a Web monitoring interface, that can be accessed by end users in order to monitor their submitted transfers and obtain statistics.\nFeatures The Web monitoring can be accessed without user authentication, only access to the transfer log files needs an X.509 user certificate installed on the browser.\nOverview page The Overview page offers a way to access the information about the transfers submitted and executed in the last 6 hours. Users can filter transfers per Virtual Organization, source or destination storage or JobId.\nJob details page By selecting a specific job the information about the job details are displayed. Each transfer part of the job is listed with his own information. From this page it’s also possible to access the transfer logs (upon authentication).\nOptimizer page The Optimizer page shows Optimizer information about a specific link, detailing the throughput evolution and the parallel transfer/stream per link at a given time.\n","categories":"","description":"Monitoring file transfers of EGI Data Transfer\n","excerpt":"Monitoring file transfers of EGI Data Transfer\n","ref":"/users/data/management/data-transfer/monitoring/","tags":"","title":"Monitoring Data Transfer"},{"body":"Here you can find documentation about the internal architecture and operations of the EGI Notebooks service.\n","categories":"","description":"Architecture and Operations of EGI Notebooks","excerpt":"Architecture and Operations of EGI Notebooks","ref":"/providers/notebooks/","tags":"","title":"Notebooks"},{"body":"The oidc-agent is a command-line tool for managing OpenID Connect tokens developed by Karlsruhe Institute of Technology (KIT).\nObtain a Token If you haven’t installed oidc-agent in your system, please read the installation guide.\nNote Please make sure that you have installed the version 4.3.0 or later Make sure that the oidc-agent-service is started by executing the command:\n$ eval `oidc-agent-service start` To obtain a Token you need to execute the following command:\nProduction Demo Development $ oidc-gen --pub --issuer https://aai.egi.eu/auth/realms/egi $ oidc-gen --pub --issuer https://aai-demo.egi.eu/auth/realms/egi $ oidc-gen --pub --issuer https://aai-dev.egi.eu/auth/realms/egi Then enter a short name for the account to configure, and the scopes that the Access Token should contain.\nNote To get a Refresh Token you need to specify the offline_access scope in the requested scopes After that, you need to log in either by visiting the provided URL or by scanning the displayed QR code, and then provide the user code displayed in the oidc-agent.\nLast but not least, you will need to provide an encryption password to protect the obtained Access/Refresh Token.\nFor more information, please read the oidc-agent documentation.\n","categories":"","description":"Usage guide of oidc-agent\n","excerpt":"Usage guide of oidc-agent\n","ref":"/users/aai/check-in/obtaining-tokens/oidc-agent/","tags":"","title":"oidc-agent"},{"body":"Introduction QoS stands for Quality of Support. It describes the level of support provided by Support Units in GGUS system.\nIt has an impact on the ticket priority colour in GGUS and the warnings are sent to SUs if 75% of the maximum response time of the QoS level are over.\nQoS levels There are three different QoS levels, each defining different response times for given Ticket Priority.\nBase Medium Advanced The defaut QoS level, if not declared differently, is Base.\nBase Level Base QoS level defines a response time of 5 working days regardless of the ticket priority.\nMedium Level Ticket Priority Response time less urgent 5 working days urgent 5 working days very urgent 1 working day top priority 1 working day Advanced service Ticket Priority Response time less urgent 5 working days urgent 1 working day very urgent 1 working day top priority 4 working hours QoS level declaration The QoS level for all of the SUs are available here\nFor the several EGI services, the QoS levels are defined in the specific Operational Level Agreements linked in the EGI OLA SLA framework page\n","categories":"","description":"Quality of Support (QoS) levels\n","excerpt":"Quality of Support (QoS) levels\n","ref":"/internal/helpdesk/features/quality-of-support-levels/","tags":"","title":"Quality of Support (QoS) levels"},{"body":"Definitions and prerequisites The GGUS Report Generator is available from the support section.\nThe implementation of the report generator started in October 2011. Hence the report generator does not provide data for tickets submitted before December 2011!\nTimestamps and metrics submit timestamp: timestamp when the ticket is submitted assign timestamp: timestamp when a ticket gets assigned to a support unit response timestamp: timestamp when the ticket status changes from “assigned” to any other status value or the ticket gets re-assigned to another support unit expected response timestamp: timestamp when a ticket should have changed the status to any other status value than “assigned” or be re-assigned to another support unit at latest solution timestamp: timestamp when the status changes to either “solved” or “unsolved” Response time The response time is a performance figure calculated from the support unit’s point of view. It describes how quick a support unit is reacting on tickets. Response time is the time from\neither assigning a ticket to a support unit and the support unit is kept until the first status changes to any other value than “assigned”, or assigning a ticket to a support unit and the status value “assigned” is kept until the support unit changes to any other support unit (re-assign). The response time is calculated as difference between the timestamp changing the status or re-assigning the ticket and the assign timestamp. While assigning a ticket to a support unit the expected response timestamp is calculated by adding an amount of time to the assign timestamp. The amount of time added depends on the ticket priority and the kind of support unit. For support units that have declared a quality of service level the response times are defined by the QoS level. For all the other support units a medium QoS is assumed for calculating the expected response timestamp. In case the actual response timestamp is greater than the “expected response” timestamp for middleware support units the “violate” flag is set.\nResponse times are based on office hours. Hence the results unit is working days.\nSolution time The solution time is a performance figure also calculated from the support unit’s point of view. It describes how long it took the support unit for providing a solution. Solution time is the time from assigning a ticket to a support unit until it provides a solution for the problem described. “Providing a solution” means setting the ticket status to “solved” or “unsolved”. The solution time is calculated as the difference between the solution timestamp and the assign timestamp.\nSolution times are based on office hours. Hence the results unit is working days.\nWaiting time Waiting time is the sum of all time slots the ticket was set to “waiting for reply”. Calculating the waiting time has started in July 2012. For tickets submitted before July 2012 no waiting time calculation was done. The waiting time for these tickets may be zero. The waiting time can be excluded when calculation solution times by ticking the checkbox “exclude waiting time”.\nTicket lifetime The ticket lifetime is calculated from the user’s point of view. It describes how long it takes to provide a meaningful solution for a problem reported by the user. Ticket lifetime is the time from ticket submission to ticket solution (status “solved/unsolved”).\nThe ticket lifetime is based on calendar days.\nTime zones GGUS support units are spread over a wide range of time zones. Some of the support units themselves are spread over several time zones. However most support units are located in European time zones. Support units and their time zones are listed on a dedicated page. For middleware product teams GGUS assumes time zone “UTC +1” for all support units. For all the other support units the system uses the relevant time zone for calculating timestamps as far as possible.\nOffice hours The systems assumes usual office hours from 09:00 to 17:00 Monday to Friday for all support units. National holidays are not taken into account. For middleware product teams the timezone UTC+1 is used as default timezone.\nTicket priorities For the calculation of performance figures the original priority set during ticket submission is used. This priority value is kept as long as the support unit is in charge of the ticket. Updating the priority value during ticket lifetime doesn’t affect the calculation of performance figures.\nUnit abbreviations [wd] means working days [d] means calendar days Reports description Although some of the drop-down lists request the selection of values a query can be started anyway. In this case the query is equivalent to a query over all tickets!\nThe “Reset” button resets all fields to their default settings besides the time frame.\nTickets submitted This metric gives the number of tickets submitted within the specified time frame. Major criteria is the submit timestamp. The result lists shows the current status of the tickets.\nOpen tickets time The open tickets time report calculates the time from ticket submission until now in calendar days. The submit timestamp must match the specified time frame for the report.\nTickets closed This metric is focused on the ticket life time. It gives the number of tickets that reached the status “solved” or “unsolved” within the specified time frame. Major criteria is the solution timestamp which is the timestamp setting a ticket to “solved” or “unsolved”. Tickets in other terminal status like “verified” or “closed” appear in the result list as long as they have been set to “solved/unsolved” in the given time frame. Besides date, status and the number of closed tickets the results list displays the average ticket lifetime and the median ticket lifetime. The result lists shows the current status of the tickets.\nResponse time This metric focuses on the responsiveness of support units. Major criteria for this report is the submit timestamp which must match the selected time frame. The result list shows:\nthe number of tickets responded the number of responses average response time and median response time for the specified time frame. The result list shows all support units that have ever been in charge of a ticket. Hence the same ticket ID may appear several times. The number of responses may be greater than the number of tickets.\nResponse times are based on office hours. Hence an average response time of 1d 3h 23min means 13 hours and 23 minutes in total.\nViolated response time Specific privileges are required for this report. Only people belonging to a dedicated technology provider (TP) are able doing reports for this TP. Major criteria for this report are the expected response time defined in SLAs and the TP.\nSolution time Major criteria for this report is the solution timestamp. The result list shows:\nthe number of solutions average response time and median response time for the specified time frame. The result lists shows the current status of the tickets. Solution times are based on office hours. Hence an average solution time of 12d 3h 5min means 99 hours and 5 minutes in total. The waiting time can be excluded by ticking the checkbox “exclude waiting time”. In case a ticket gets re-opened the metrics calculation starts again from scratch. Hence the same ticket can appear several times in the solution times calculation.\nInput parameters and results Input parameters The input parameters vary depending on the report type chosen. Possible input parameters are:\nTime frame Responsible Unit Status Priority Concerned VO Ticket type Ticket category Ticket Scope Notified site Technology provider Date aggregation Time frame The time frame defines begin and end date of the report. The begin date starts at 00:00:00. The end date ends at 00:00:00. As the implementation of the report generator started in October 2011 the report generator does not provide data for tickets submitted before December 2011!\nResponsible Units The drop-down list offers all responsible units integrated in GGUS system. They can be filtered by keywords. Responsible Units can be either selected all, one by one or by checking the boxes in front of the responsible unit groups. In case no responsible units are selected all responsible units are considered in the reports.\nStatus The drop-down list offers all status values available in GGUS system. Multiple selections are possible.\nPriority The drop-down list offers all priority values available in GGUS system. Multiple selections are possible.\nConcerned VO “Concerned VO” provides a drop-down list of all VOs supported by GGUS. Multiple selections are possible.\nTicket type This drop-down list lists all ticket types in GGUS. Multiple selections are possible.\nTicket category Selectable categories are “Incident”, “Change request”, “Documentation”, “Test”. Multiple selections are possible. The category “Test” should not be considered for all reports and therefore, if necessary, be excluded from the selection.\nTicket scope To distinguish between tickets under the responsibility of EGI or WLCG.\nNotified site “Notified site” lists all sites integrated in GGUS. They are derived from GOC DB and OIM DB. In case of reporting exclusively on tickets without any site value specified please ticket the “blank” value in the drop-down list.\nTechnology Provider This input parameter is only visible if choosing the “violated response time” report. For accessing this report specific privileges are required. The drop-down list offers all technology providers currently integrated in GGUS and the “DMSU”. Multiple selections are possible. In case no technology provider is selected the reports will be done for tickets without any technology provider specified.\nDate aggregation The results aggregation level can be chosen from the drop down list “Choose date aggregation”.\nGroup by Results can be grouped by one or more of the input parameters.\nResults The results are displayed below the input parameter area. They can be sorted in different ways by clicking on the column labels. A drill-down is possible by clicking on any row of the results list. The detail results open in a new window. They can be sorted by clicking the column labels too. Clicking on the ticket ID opens the ticket in GGUS system. At the bottom of the result panel there are various icons offering features like:\nSearch Refresh Export Export what you see Doing a simple “Export” saves the data in a csv file stripping all column headers. For exporting the data including the column headers please use “Export what you see”. The “Search” feature allows searching in the result list. Possible search parameters are the columns of the result list.\n","categories":"","description":"Features and usage of the Report generator\n","excerpt":"Features and usage of the Report generator\n","ref":"/internal/helpdesk/features/report-generator/","tags":"","title":"Report generator"},{"body":"Rucio-client setup The setup for the container is the same as that found in the Getting Started section. But is repeated here for ease.\nTo get the Rucio client that is set up for dteam please use this Rucio Client. This would be done by running the command:\n$ docker run \\ -v \u003cpath/to/the/rucio.cfg\u003e:/opt/rucio/etc/rucio.cfg \\ -v \u003cpath/to/your/usercert.pem\u003e:/opt/rucio/etc/usercert \\ -v \u003cpath/to/your/userkey.pem\u003e:/opt/rucio/etc/usercert \\ -e RUCIO_CFG_CLIENT_CERT=/opt/rucio/etc/usercert.pem \\ -e RUCIO_CFG_CLIENT_KEY=/opt/rucio/etc/userkey.pem \\ -e RUCIO_CFG_CA_CERT=/opt/rucio/etc/web/ca-first.pem \\ --name=rucio-client \\ -it \\ -d egifedcloud/rucioclient:1.23.17 Once the container is running you will need to copy some files, to have them owned by the container user, rather then root, and then change the permissions on those files so that they are appropriate for voms-proxy creation. To start with step into the container by running:\n$ docker exec -it rucio-client bash Once inside the container you can then copy and edit file permissions with the following:\n$ cp /opt/rucio/etc/usercert /opt/rucio/etc/usercert.pem $ cp /opt/rucio/etc/userkey /opt/rucio/etc/userkey.pem $ chmod 600 /opt/rucio/etc/usercert.pem $ chmod 400 /opt/rucio/etc/userkey.pem You should now be able to generate a VOMS proxy using the credentials loaded into the container, this is done by running the following command within the container:\n$ voms-proxy-init --voms dteam Rucio configuration setup Inside your docker container edit the rucio.cfg file to include your 3 character VO name, and account name. This will then be loaded into the Rucio client.\n[common] logdir = /var/log/rucio multi_vo = True loglevel = INFO [client] rucio_host = https://rucio-server.gridpp.rl.ac.uk:443 auth_host = https://rucio-server.gridpp.rl.ac.uk:443 vo = dtm account = \u003cyour_account\u003e ca_cert = /opt/rucio/etc/web/ca-first.pem auth_type = x509_proxy client_cert = /opt/rucio/etc/usercert.pem client_key = /opt/rucio/etc/userkey.pem client_x509_proxy = /tmp/x509up_u1000 request_retries = 5 Confirmation of Client setup Once this is complete you should now have access to Rucio. This can be confirmed with a ping and a whoami commands to verify one, the connection to the Rucio host and two, that you are authenticating successfully as your user.\n$ rucio ping 1.23.17 $ rucio whoami status : ACTIVE account : user account_type : USER created_at : YYYY-MM-DDThh:mm:ss suspended_at : None updated_at : YYYY-MM-DDThh:mm:ss deleted_at : None email : user@email.co.uk Once these messages have been displayed with the relevent information, as a user you should now have access to the Dteam VO, and can create rules, upload and download files from the various RSEs.\nIf you have any issues please do contact the Multi-VO admin / dteam VO admins.\n","categories":"","description":"How to get set up with the dteam VO","excerpt":"How to get set up with the dteam VO","ref":"/users/data/management/rucio/dteam-vo/","tags":"","title":"Dteam Specific Documentation"},{"body":"In addition to the formatting support provided by Markdown, Hugo adds support for shortcodes, which are Go templates for easily including or displaying content (images, notes, tips, advanced display blocks, etc.).\nFor reference, the following shortcodes are available:\nHugo’s shortcodes Docsy theme shortcodes Highighted paragraphs This is achieved using Docsy shortcodes.\nPlaceholders The following code:\n{{% pageinfo %}} This is a placeholder. {{% /pageinfo %}} Will render as:\nThis is a placeholder. Information messages The following code:\n{{% alert title=\"Note\" color=\"info\" %}} This is a Note. {{% /alert %}} Will render as:\nNote This is a Note. Warning messages The following code:\n{{% alert title=\"Important\" color=\"warning\" %}} This is a warning. {{% /alert %}} Will render as:\nImportant This is a warning. Code or shell snippets The code or instructions should be surrounded with three backticks, followed by an optional highlighting type parameter.\nThe supported languages are dependant on the syntax highlighter, which depends itself on the mardkown parser.\nNote Hugo uses the goldmark parser, which relies on Prism syntax highlighting. The following Markdown creates a shell excerpt:\n```shell $ ssh-keygen -f fedcloud $ echo $HOME ``` Will render as:\n$ ssh-keygen -f fedcloud $ echo $HOME Tip If you click the Copy button in the top-right corner of a shell example, all commands in that block are copied to the clipboard. The prompt in front of each command, and any command output is not copied. Note In case the command(s) in your shell example cause the introduction of a horizontal scroll bar, consider breaking the command(s) into multiple lines with trailing backslashes (\\). However, you should never break command output to multiple lines, as that makes understanding the output, and recognizing it in real life, very difficult. Code in multiple languages This is also achieved using Docsy shortcodes.\nWhen you need to include code snippets, and you want to provide the same code in multiple programming languages, you can use a tabbed pane for code snippets:\n{{\u003c tabpane \u003e}} {{\u003c tab header=\"C++\" lang=\"C++\" \u003e}} #include \u003ciostream\u003e int main() { std::cout \u003c\u003c \"Hello World!\" \u003c\u003c std::endl; } {{\u003c /tab \u003e}} {{\u003c tab header=\"Java\" lang=\"Java\" \u003e}} class HelloWorld { static public void main( String args[] ) { System.out.println( \"Hello World!\" ); } } {{\u003c /tab \u003e}} {{\u003c tab header=\"Kotlin\" lang=\"Kotlin\" \u003e}} fun main(args : Array\u003cString\u003e) { println(\"Hello, world!\") } {{\u003c /tab \u003e}} {{\u003c tab header=\"Go\" lang=\"Go\" \u003e}} import \"fmt\" func main() { fmt.Printf(\"Hello World!\\n\") } {{\u003c /tab \u003e}} {{\u003c /tabpane \u003e}} Will render as:\nC++ Java Kotlin Go #include \u003ciostream\u003e int main() { std::cout \u003c\u003c \"Hello World!\" \u003c\u003c std::endl; } class HelloWorld { static public void main( String args[] ) { System.out.println( \"Hello World!\" ); } } fun main(args : Array\u003cString\u003e) { println(\"Hello, world!\") } import \"fmt\" func main() { fmt.Printf(\"Hello World!\\n\") } Content with multiple variants When you need to include multiple variants of the same content, other than code snippets in multiple programing languages, you can use the follwing shortcode:\n{{\u003c tabpanex \u003e}} {{\u003c tabx header=\"Linux\" \u003e}} You can list all files in a folder using the command: ```shell ls -a -l ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Mac\" \u003e}} To get a list of all files in a folder, press **Cmd** + **Space** to open a spotlight search, type terminal, then press Enter. In the terminal window then run the command: ```shell ls -a -l ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Windows\" \u003e}} You can list all files in the current folder using the command: ```shell dir ``` or you can use PowerShell: ```powershell Get-ChildItem -Path .\\ ``` {{\u003c /tabx \u003e}} {{\u003c /tabpanex \u003e}} Will render as:\nLinux Mac Windows You can list all files in a folder using the command:\nls -a -l To get a list of all files in a folder, press Cmd + Space to open a spotlight search, type terminal, then press Enter. In the terminal window then run the command:\nls -a -l You can list all files in the current folder using the command:\ndir or you can use PowerShell:\nGet-ChildItem -Path .\\ Tip You can include any valid markdown content in each tab, including code or shell snippets. ","categories":"","description":"Helpers for writing EGI documentation","excerpt":"Helpers for writing EGI documentation","ref":"/about/contributing/shortcodes/","tags":"","title":"Shortcodes"},{"body":"What’s new in the latest release? Please see the release notes for further information.\nAccess to GGUS The EGI Helpdesk is reachable via a welcome page. Another way for accessing the ticket management interface is using the direct link provided in the notification emails sent after ticket updates. However either a valid X509 personal certificate or a Check-in account are required for accessing the system.\nFeatures of GGUS GGUS Home GGUS home provides a quick overview over tickets submitted by the current user, the latest tickets of other users and a collection of news and links to useful information. The navigation bar is located on the left side of GGUS pages. It provides links to:\nDid you know? Documentation Registration My dashboard Search Engine Submit form Support staff pages GGUS Home Legals Contact form Did you know After each release, a major change or new feature is explained in some sentences here.\nDocumentation In the documentation section there is a collection of links providing useful information around GGUS system.\nRegistration All information about registration can be found on the registration page. For registering as support staff, click the link “Apply” and fill in the registration form. After registering successfully you will receive a confirmation mail from GGUS team.\nIf you want to update your account data, you can do this using the link “Check your GGUS account” on the registration page. If you can’t access GGUS web interface due to changed DN string of your personal certificate, you can log into the system using your Check-in account. Then go to the registration page and click on Check/update your GGUS account. The system shows you all your user data. It detects the new DN string of your browser automatically. Just save the changes by clicking on button Update now. Additional information on GGUS accounts is available here.\nSupport staff page Access to the support staff page is restricted to users having support privileges. Depending on further privileges people may have there are links to e.g. news administration and other features. All support staffs can use the GGUS report generator and the GGUS ticket timeline tool as well as links to other information useful for support staffs.\nGGUS ticket timeline tool The link to the GGUS ticket timeline tool is located on support staff page. The ticket timeline tool provides a graphical overview of tickets concerning a selected site and time range. It shows all tickets that have been updated during the selected time range. When moving the mouse over one of the colored bars some additional information is displayed. Clicking on the ticket ID opens a new window showing the ticket details and the modify section of the ticket.\nGGUS Report Generator The link to the GGUS Report Generator is located on support staff page and on GGUS home page in section “GGUS tools/reports”. The GGUS Report Generator could be used for generating statistics and reports for all support units in GGUS. Further information on the report generator is available on the report generator.\nSubmit form Depending on the user’s privileges GGUS offers different ticket submit forms:\ncommon user ticket team ticket alarm ticket CMS ticket (for members of CMS VO) notify multiple sites which is a bulk submit for addressing multiple sites about the same issue. Ticket categories and ticket types GGUS offers two fields which help to classify various tickets into categories and types.\nTicket categories The ticket category is for differentiating between incidents and service requests. This distinction is helpful for supporters as well as for the GGUS reporting, e.g. for excluding test tickets. Other categories were added over the time. Currently the following values are available for selection:\nIncident (see the FitSM definition) Service request (see the FitSM definition). List of service requests for the internal and for the external services of the EGI portfolio Documentation (used to request creation and update on documentation.) Release (used when a new version of the operational tools is ready to be tested; see RDM2 Regular release process) CMS Internal (for cms VO specific tickets) EGI Coordination/Planning (to track activities not falling into the service request definition) WLCG Coordination/Planning Test When submitting a ticket the ticket category field is not visible. It defaults to “Incident” and is only editable for supporters, not for users. So it is up to the supporter to check and, if necessary, classify the ticket correctly. Please see details in section Changing ticket category.\nTicket types The ticket type field is for differentiating between standard user tickets and tickets for achieving the special requirements of various groups like the LHC VOs or EGI operations. It can’t be set manually, but is set automatically by the system based on several rules. The ticket type field could not be changed during ticket lifetime. Possible ticket types in GGUS are:\nUSER TEAM, ALARM OPS. USER tickets The ticket type USER is the default ticket type. User tickets are the usual tickets which can be submitted by everyone. They are visible to everybody. They can be updated either by the submitter himself or by any supporter.\nTEAM tickets The purpose of TEAM tickets is to allow a group of people to submit and modify ticketseditable by the whole group. TEAM tickets can only be submitted by people who have the appropriate permissions in the GGUS user database. These people belong to either one of the four LHC VOs (ALICE, ATLAS, CMS, LHCB) or to the BIOMED or BELLE VO and are nominated by the particular VO management. TEAM tickets are editable by all members of the VO team (which are the so called VO shifters) regardless own the ticket. TEAM tickets are visible to everyone. They can be submitted for all tier-1 sites in EGI and are routed to the appropriate NGI/ROC automatically, bypassing the TPM. Additionally the site is notified by mail about the ticket. By default TEAM tickets are routed to the appropriate NGI/ROC directly, bypassing TPM. But the submitter could also choose to route it to the TPM instead. Further information on TEAM tickets is available in the TEAM tickets page.\nALARM tickets The purpose of ALARM tickets is to notify tier-1 administrators about serious problems of the site at any time, independent from usual office hours. They can only be submitted by experts who have the appropriate permissions in the GGUS user database. These people belong to one of the four LHC VOs (ALICE, ATLAS, CMS, LHCB) and are nominated by the particular VO management. They are about 3 to 4 people per VO. ALARM tickets are editable by all members of the VO team (which are the so called VO shifters) regardless they own the ticket. They are visible to everyone. ALARM tickets can be submitted for all tier-1 sites in EGI and are routed to the appropriate NGI/ROC automatically, bypassing the TPM. Additionally the tier-1 site is notified by an ALARM email. It is up to the tier-1 site how to deal with this ALARM email. Further information on ALARM tickets is available in the ALARM tickets page.\nOPS tickets This ticket type is used for operations tickets submitted via the operations portal.\nMy dashboard This dashboard can be used for getting quick access to any ticket of interest. Each ticket has to be added manually.\nSearch Engine The search engine provides a large number of fields for creating dedicated query strings. Many of them have additional information hidden behind a question mark icon. The results of a query are shown in a results list. The default search displays all open tickets of last week. They were ordered by date of creation in descending manor. For further details on using the search engine see chapter Searching for tickets. The result list is showing a color schema reflecting the priority of tickets. The algorithm used for setting the priority colours is explained in chapter Reminder emails.\nSearching for tickets Various possibilities of searching tickets in GGUS are described in this FAQ. Please avoid searching for “all” tickets or “solved” tickets without any time-frame if not necessary for some reasons as these searches cause heavy load on the machine.\nSearching via Ticket ID Searching via ticket ID is the easiest and fastest way to look at a ticket. When searching via Ticket ID all other search parameters are ignored. Besides searching for all open tickets this is the recommended kind of search, because it avoids needless workload on the system. When searching via ticket ID the ticket details are shown in the same window. For getting back to the main page use the “Back” button of your browser. For Firefox users there is a nice add-on for adding a customized search for any web page to the browser’s search bar available here.\nSearching via parameters The search parameters can be combined in any way wanted. Description fields “Keyword”, “Involved supporter” and “Assigned to person” trigger a LIKE search to the database. Concatenating keywords with “AND” or “OR” is currently not possible. The result of a search by parameters is shown in the result list. For viewing ticket details just click on the ID. A new window opens showing ticket details. For getting back to the search result just close the window with the ticket details.\nCustomizing result list You can customize the result list in various ways. One way to customize the result list is by checking or un-checking the appropriate boxes in the blue bar. The related columns will then be added or removed. Another way for customizing the result list is by selecting another ticket order in field Order tickets by. After changing the result list layout you have to refresh the search result by clicking the Go! button.\nExporting search results Search results can be exported in csv or xml format for further processing. After clicking on the appropriate link a new window opens showing the results in the specified format. Out of this window you can save a local copy of this file.\nTicket data By clicking on the ticket ID of a ticket in the results list of the search engine you can access the ticket data. The ticket data is divided into 3 main sections:\nticket information ticket history ticket modify section Ticket information The system shows the information section after clicking on a ticket ID.It provides an overview of all relevant ticket parameters and could be divided into 5 areas:\nsubmitter data issue data ticket data description solution Additional features of the ticket information section are:\nexport of ticket information data, escalate button, duplicate ticket and convert team to alarm. Duplicate ticket Figure 5: Ticket duplication Supporters have the opportunity to duplicate an existing ticket up to 15 times. The duplicate feature is located right below ticket information. It is useful if a ticket concerns not only one support unit but has to be handled by several support units. The fields that are copied into the duplicated tickets are:\nInternal Diary Login Last Modifier Submitter Subject Attachments are not duplicated physically but linked to all duplicated tickets. The Responsible Unit is set to “TPM” by default.\nConvert TEAM tickets to ALARM tickets Figure 3: Convert team to alarm ticket Support staffs with ALARM privileges are able to convert TEAM tickets to ALARM tickets clicking on a button in the ticket information section. This feature is only available for the WLCG VOs.\nTicket history The ticket history is located below ticket information. It shows all relevant changes of the ticket in chronological manner. Changes of these fields lead to a new entry in ticket history:\nAssign ticket to support unit Assign ticket to one person Affected Site Public Diary Change ticket category Change status Change VO Change priority Involve others Type of issue Internal Diary Solution Related issue VO specific For making the history more readable solution entries and entries in the public diary are marked with green colour, entries of the internal diary with orange colour.\nTicket modify section The ticket modification area offers several fields for modifying a ticket. The fields are described in detail below.\nAssign ticket to support unit is showing all support units involved in GGUS. Assign ticket to specific person(s) allows assigning tickets to a dedicated person within the current support unit. If a mail address is typed in the system generates an email to inform the recipient about the ticket assignment. The length is limited to 254 characters. Change status is a drop-down-list with all possible status values. Change VO is a drop-down-list with all possible VO values. Type of issue provides a drop-down-list of possible issue types. Change priority provides a drop-down-list of possible priority values. Notify site is for specifying the site affected by the issue. For ticket types “ALARM” and “TEAM” this field is not editable. Change ticket category provides a drop-down list with values “Incident”, “Change Request”, “Documentation” and “Test”. Involve others allows contacting any people who may help to solve an issue. Several mail addresses can be typed in separated by “;”. The length is limited to 254 characters. VO specific is a flag indicating whether a issue is VO specific or not. Default is “no”. Change CC recipient is for editing the mail addresses specified during ticket submit for notifying any person about a ticket. MoU Area can only be set for tickets of type “TEAM” or “ALARM”. Possible values are documented here. Subject is for editing the subject of a ticket. Internal Diary can be used for internal remarks. It is only shown to people with support privileges and limited to 4000 characters. Public Diary updates always trigger an update mail to the submitter. It is limited to 4000 characters. Click here to insert… expands the solution field. Solution can be up to 4000 characters. It is used for explaining the solution. Hide solution hides the solution field. Reminders feature can be set to “Please send reminder on” if status is changed to “on hold” or “waiting for reply”. In this case a date can be selected on which a reminder mail was sent. This feature should help supporters not to forget tickets which were not worked on for a longer time. Related issue can be used to reference any URL. It is limited to 125 characters. Click here to expand … expands the ticket relation section. Hide ticket relation section hides ticket relation fields. Mark this ticket as Master is described in detail in chapter Master-Slave relations. Mark this ticket as Slave is described in detail in chapter Master-Slave relations. Mark this ticket as child this feature is described in detail in chapter Parent-Child relations. Cross reference is for referencing as much other GGUS tickets as necessary. For each ticket referenced here a symmetric link is added to the referenced ticket automatically. Want to upload attachment? is for adding attachments. Only one attachment can be added at a time. The total number of attachments is unlimited. Ticket Participation GGUS system offers various possibilities for participating in tickets. They are\nthe CC field, the Involve others field and the Subscribe field. An overview on these fields is given in the table below. Ticket participation can be done by adding a valid mail address to one of these fields. Please avoid adding closed mailing lists as such produce a lot of mail errors! Several mail addresses have to be separated by semicolon.\nUser submit User modify Supporter modify CC Yes No Yes Involve others No No Yes Subscribe No Yes Yes The “CC” field The CC field can be set by the user in the ticket submit form. Updates are only possible for supporters for correcting or removing invalid mail addresses. Every ticket update triggers a notification email to the mail address specified in the “CC” field.\nThe “Involve others” field The “Involve others” field is only for supporters use. Every ticket update triggers a notification email to the mail address specified in the “Involve others” field.\nSubscribe to this ticket field This field can be used by any user for participating in tickets at any time. The user has just to add his mail address. He will receive notifications for updates of the public diary or the solution for following the whole ticket life cycle. “Internal Diary” entries never go to the people who subscribed to a ticket.\nMaster-Slave relations Several tickets describing the same issue can be put into a master-slave relation. One of them can be marked as master, the other ones as slave. Only the master ticket has to be dealt with. The slave tickets are set to “on hold”. They can’t be updated directly as long as they are in a master-slave relation. The user gets an automated notification if a ticket is marked as slave. All updates of the master were pushed to the slaves. When solving the master the slaves are solved to. The master-slave relation is kept after the master is solved. Nevertheless each ticket can be reopened separately. Updates on reopened slave tickets are possible. A master-slave relation can be reset manually either by removing the master ID of a slave ticket or by un-checking the master checkbox of the master. If a master is unmarked as master all slaves were reset to “standard” tickets automatically.\nSelecting slave tickets Marking a ticket as slave is only possible if there is already a master ticket. If a ticket is marked as slave a popup window opens showing available master tickets. For selecting a master just click on the ID. The master ID is set automatically. Once you have chosen a wrong master ID click Reset Master-Slave relation and select another one.\nShowing a master’s slave tickets To show all related slave tickets click on link “show slaves for this ticket”. A popup window opens showing the IDs of all slave tickets.\nSearching for master/slave tickets If you want to search for master or slave tickets you can do this using field “Special attributes” of the search engine. The status value for searching is set to an appropriate value accordingly.\nParent-child relations Parent-child relationships work in reverse to master-slave relationships. The parent ticket cannot be resolved until all of its child tickets are resolved. The parent ticket is set to the status “on hold” while the child tickets are waiting for their solution. For each solved child ticket a note is added to the parent ticket history including the solution of the child ticket. After the last open child ticket has been solved the status of the parent ticket changes to “in progress” automatically. In addition, the system sends a notification mail to the responsible support unit that all child tickets have been solved now. So the parent ticket can be “solved” too.\nSelecting child tickets A ticket can be selected as child ticket by checking the box “Mark this ticket as child of ticket” and adding the ticket ID of the parent ticket. A comment is added to the ticket history automatically stating “This ticket is a child ticket of GGUS ticket # 18492”. Multiple child tickets can be related to one parent ticket by repeating this procedure. The parent ticket is flagged as “parent” automatically. A comment is added to the ticket history automatically stating “This ticket is a parent ticket. It has to wait the solving of all its child tickets. GGUS ticket #18493 is a child to this ticket.\".\nResetting child tickets For resetting child tickets just remove the tick from the checkbox “Mark this ticket as child of ticket”.\nSelecting parent tickets Selecting a parent ticket explicitly is not possible. The parent tickets are flagged automatically by the system while the parent ID is specified for a child ticket.\nSearching for parent/child tickets The search for parent or child tickets is similar to the search for master or slave tickets. It can be done using field “Special attributes” of the search engine.\nWorking on tickets This section is a description of how the GGUS ticketing system behaves. There are other documents which describe the system in more detail and include more of the implementation details. One of the most important fields of the system is the status field. Many workflows are triggered by status value changes. Please read the Short Guide for getting information on status values. Tickets are normally assigned to a support unit. This means that the ticket notification is sent to a mailing list composed of many supporters. One of these supporters assigns the ticket to himself and takes responsibility for it; the supporter changes the status to “in progress”. This person is then in charge of the ticket. He either solves it or reassigns the ticket to somebody else. The status of the ticket stays set to “in progress” if the ticket is under the responsibility of one supporter and until the ticket has been solved.\nUser ticketing work flow A graphical view of the ticket flow in GGUS is shown here: The GGUS Support is organized with two main lines of support:\nFirst line of support gets immediate notification of tickets Second line of support is only notified of tickets by the first line of support. The first line support is provided by an organisation called TPM – Ticket Processing Manager. The TPM team has members who have very good general grid knowledge. It is an organisation populated by people provided from the Czech NGI. This organisation is responsible for the routing and processing of all active tickets.\nThe second line support is formed by many support units. Each support unit is formed from members who are specialists in various areas of grid middleware, or NGI/ROC supporters for operations problems, or VO specific supporters. The membership of the support units is maintained on mailing lists. If the user responds to any email received from GGUS, then the reply is added to the ticket history. The subject of the email includes metadata to ensure the association of the response with the ticket.\nTickets waiting for user input The workflows for tickets waiting for user input is described in this FAQ.\nAdvice for TPMs The following advice is intended for people working on TPM.\nChange support unit to “TPM” if you are working on a ticket Just typing a comment into solution field does NOT cause an email to the user automatically. Only changing status to “solved” causes an automatic mail. If you want to contact the user you can do this using the “Public Diary”. Change status to “waiting for reply” while waiting for the user’s reply. Be careful when using the field “Assign ticket to one person”. Please avoid using mailing list names, or the mail list address of a remote help-desk system. With a mailing list, the mail may not reach the recipient because many mailing lists are closed lists and will not accept the message. Sending mail to a remote help-desk system can confuse the remote system and lead to trouble. Change the ticket category if you think the ticket is not dealing with an incident but describing a service request like a documentation update, adding someone to a mailing list and so on. See chapter Changing ticket category for details. Changing ticket category When submitting a ticket the ticket category field could not be set but default to “Incident”. It is up to the supporters to decide whether a ticket describes an “Incident” or a service ticket. Service tickets are tickets that request something be done like:\nadding someone to a mailing list updating documentation providing more space for storing data etc. They do not report issues. This differentiation is compliant to ITIL. Differentiating between incident tickets and service tickets can help supporters to order tickets they are responsible for by urgency. The GGUS reporting also relies on the correct setting of the ticket category field as it does ignore tickets of category “Test”.\nForwarding a ticket to another unit Tickets assigned to a support unit by error or tickets that need actions from other support units should be either assigned back to the TPM or assigned to the relevant support unit directly. In both cases an explanation in the public diary will avoid confusion.\nSolving a ticket In this section the usage of the different status values and input fields is described.\nThe system offers two groups of meta states, open states and terminal states.\nOpen states new: this is the default status for submitted tickets. It is set by the system and can’t be selected in the drop-down list menu. assigned: this status is set automatically and can’t be selected in the drop-down list menu. After a ticket is assigned to a support unit, this unit is informed via email about the ticket assignment. in progress: support staff who work on the ticket should change status to “in progress”. This is necessary to announce that somebody is taking care of this ticket and is working on it. waiting for reply: This status value should be set ONLY by the supporter and ONLY when asking the SUBMITTER for further information. The supporter can decide whether (s)he wants to be notified about this ticket by the system. (S)he can choose any date in the future (s)he wants to be notified and select the radio button “Please send reminder on”. on hold: some tickets are not solvable while needing a software patch or something similar for example. The reasons should be explained in field “Public Diary”. Additionally a hint in field “Related issue” may be useful. The supporter can decide whether he wants to be notified about this ticket by the system. He can choose any date in the future he wants to be notified and select the radio button “Please send reminder on”. reopened: normally this status value is set by the user, if he is not happy with the provided solution. It can also be used by supporters if a better solution is found for a ticket already solved. In this case status should be set to “reopened”, new solution put in and status changed to “solved” again. Leaving status as “solved” and just putting in the new solution does not cause an automatic solution mail to the user. Terminal states solved: If a solution is found and put into the Solution field, status has to be set to “solved”. Only on status change to “solved” the user receives a solution mail automatically. Please put a full explanation in Solution field of how the issue was solved. You can use qualification terms like: fixed fixed (work around) works as designed other unsolved: This status is for tickets that can not be solved due to any reason. Please add a comment in the solution field explaining why it can’t be solved. You can use qualification terms like: duplicate invalid wont fix verified: This status can only be set by the ticket submitter. TEAM tickets, by design, can be ‘verified’ by all TEAMers in the VO. ALARM tickets can also be ‘verified’ by all the authorized ALARMers in the VO, not only the submitter. This status indicates that a user is happy with the provided solution. “Verified” tickets cannot be further updated, nor re-opened. closed: Solved or unsolved tickets not verified by the submitter are set to “closed” automatically after 10 working days. Fields to fill in Single steps of the solution process can be documented in field “Public Diary”. Information and comments which should not be visible to the user can be put into the “Internal diary”. When a solution is found, the modifier types the solution into the solution field and changes status to “solved”. The “Solution” field provides 4000 characters. If 4000 characters are not sufficient, please add an attachment. After changing status to “solved” and saving all changes, the solution is sent to the submitter via mail automatically. Tasks for solving a ticket:\nchange status to “in progress” while working on it, fill in the solution fields and the internal diary if necessary, change the status to “solved” and you are done. Reminder emails Reminder mails are based on the priority colours. The algorithm of setting priority colours is described in the following chapters. Reminder mails are sent with the reply-to address ignored - atnospam - ggus.eu. All mails sent to this mail box are deleted regularly without reading them.\nWhat are the priority colours? Priority colours are:\nGreen: default for all new tickets Yellow Amber Red Light blue: for all tickets in status unsolved Blue: for all solved and verified tickets Priority colours depend on the expected response time and the expected solution time of a ticket.\nExpected response time The expected response times for support units that did not agree on a dedicated quality of service are listed in the relevant FAQ. This means, the status of a ticket should be set to another value than “assigned” for indicating that the support unit has acknowledged the ticket.\nExpected solution time The expected solution times are driven by the priority values of the tickets. All values are working days. The higher the priority the shorter is the duration within which the ticket is expected to be solved. For further details please see the Ticket Priority page.\nTicket follow-up Ticket follow-up is done by a team at KIT (Germany) for all tickets besides operations tickets. More information on the ticket follow-up processes are available at the pages about DMSU ticket follow-up and Ticket monitoring.\n","categories":"","description":"Guide for supporters","excerpt":"Guide for supporters","ref":"/internal/helpdesk/support-staff-guide/","tags":"","title":"Support Staff Guide"},{"body":"Introduction: purpose and conditions The purpose of TEAM tickets is to allow all team members to modify all tickets of their team although they may not have support access and they do not have submitted the ticket themselves. Only members of a few VOs are allowed to submit team tickets: Alice Atlas CMS LHCb Biomed Belle The VO menbers need to have the appropriate permissions in GGUS user database Other VOs can request this functionality by opening a GGUS ticket Team tickets are routed to the NGI/ROC the site belongs to automatically. They do not need a routing by the TPM. The NGI/ROC is notified about the ticket in the usual way. In parallel, the site receives a notification email. Becoming a Team-ticket member People who want to become a team member have to:\nregister in GGUS first be added to the appropriate group in a VOMS server. GGUS system synchronizes its user database once per night with the VOMS servers. The synchronization is based on the DN string. Please make sure the DN of your GGUS account is the same to the one registered in VOMS.\nTechnical description This section describes the workflows of team tickets from a technical point of view.\nTeam ticket submission Team tickets can either be submitted using the GGUS web portal. On top of the ticket submit form in GGUS web portal there is a link to the submit form for team tickets.\nAs team ticket submitters are expected to be experts who will hopefully provide all necessary information, the number of fields on the team ticket submit form is reduced to a minimum, compared to the number of fields on the user ticket submit form\nThree fields on this form are mandatory:\nSubject MoU Area Notified Site All of the other fields are optional.\nTeam ticket processing The processing of a team ticket consists of two main parts, the notification of the notified site and the routing of the ticket to the NGI/ROC the site belongs to.\nSite notification In parallel to the creation of a team ticket the GGUS system sends an email notification directly to the site specified in field “Notify Site”. This email is sent to a specific site contact mail address.\nTicket routing Team tickets are bypassing the TPMs and routed to the appropriate NGI/ROCs automatically as long as field “Routing type” is not changed. The decision to which NGI/ROC a ticket has to be routed is done automatically, based on the value of the “Notify Site” field.\nWorking on team tickets For working on team tickets and resolving please use the GGUS portal. A reference link to the team ticket is given in the team notification mail. Team tickets can be upgraded to alarm tickets clicking on the button “Click to convert to ALARM”.\nTeam tickets process schema See the schema of the Team Tickets process.\n","categories":"","description":"Definition and usage of the Team tickets\n","excerpt":"Definition and usage of the Team tickets\n","ref":"/internal/helpdesk/features/team-tickets/","tags":"","title":"Team tickets"},{"body":"Priority definition If you plan to adjust the ticket priority from the default value ’less urgent' to ‘urgent’, ‘very urgent’ or ’top priority’, please make sure you add a justification for this change in the problem description field.\nThe following table will help you to find an appropriate priority value:\nPriority Comment top priority service interrupted; needs to be addressed as soon as possible very urgent service degraded; no work-around available urgent service degraded; work-around available less urgent wishes and enhancements that are “nice to have” In particular, be very economical when choosing ’top priority’. This value, when reaching the supporters via another ticketing system interface, might become a beep or phone alert even in the middle of the night. This level of support is ONLY committed by WLCG Tier0 and Tier1s and ONLY for ALARM tickets.\nFinally, please be aware that the supporter who will try to solve your problem may change the value you have chosen to a more realistic one, putting their justification in the ticket’s public diary.\nPriority colours The purpose of the priority colours in GGUS The priority colours in GGUS should help supporters getting an overview of the most important tickets. The priority colours have no impact on the metrics generated with the GGUS report generator or manually by the GGUS team.\nThe colours used Currently the following colours are used for open states:\ngreen yellow amber red For terminal states GGUS uses:\nlight blue for status “unsolved” and blue for status “solved” and “verified”. Fields affecting the priority colour algorithm The priority colour algorithm is affected by the status value and the priority of a ticket. The priority colour calculation for tickets in status “assigned” is based on different values than for tickets in any other open status.\nOffice hours and weekends Office hours and weekends are taken into consideration when calculating the priority colour. The system assumes usual office hours from 07:00 to 15:00 o’clock UTC from Monday to Friday for all support units. Different time zones are considered as far as possible.\nPriority colour and status values The priority colour calculation starts after a ticket gets assigned to any support unit. The support units should change the ticket status to “in progress” or any other open status after receiving the ticket. Thus the support unit has responded to the ticket (response time). After such a status change the priority is reset to “green” and the calculation of the priority colour starts again from scratch.\nAlgorithm for middleware related tickets The priority colour for tickets dealing with middleware related problems is calculated according to the Quality of Support the PTs voted for. The priority colour changes after a dedicated amount of working hours as listed in the table below. The priority colour calculation is processed every 15 minutes.\nQoS Level Ticket priority Colour Working hours Colour Working hours Colour Working hours Base less urgent Yellow 20 Amber 30 Red 40 urgent 20 30 40 very urgent 20 30 40 top priority 20 30 40 Medium less urgent Yellow 20 Amber 30 Red 40 urgent 20 30 40 very urgent 4 6 8 top priority 4 6 8 Advanced less urgent Yellow 20 Amber 30 Red 40 urgent 4 6 8 very urgent 4 6 8 top priority 2 3 4 Algorithm for tickets other than middleware The priority colour changes after a dedicated amount of working hours as listed in the table below. The priority colour calculation is running every 15 minutes.\nTicket priority Colour Working hours less urgent Yellow 20 Amber 30 Red 40 urgent Yellow 20 Amber 30 Red 40 very urgent Yellow 16 Amber 24 Red 32 top priority Yellow 12 Amber 18 Red 24 ","categories":"","description":"Definition and computation of the ticket priority in relation to the QoS levels\n","excerpt":"Definition and computation of the ticket priority in relation to the …","ref":"/internal/helpdesk/features/ticket-priority/","tags":"","title":"Ticket Priority"},{"body":"The EGI Check-in Token Portal allows users to create Access and Refresh Tokens.\nObtain an Access Token In order to obtain an Access Token from EGI Check-in Token Portal, please follow the steps below:\nGo to https://aai.egi.eu/token and click on “Authorise” to authenticate yourself. After logging in you will obtain an Access Token as it is shown below: The value of the Access Token The command to get user’s information from userinfo endpoint Obtain a Refresh Token (more info in Obtain a Refresh Token section) View and manage the applications you have given permissions Obtain a Refresh Token In order to obtain an Refresh Token from EGI Check-in Token Portal, please follow the steps below:\nGo to https://aai.egi.eu/token and click on “Authorise” to authenticate yourself. After logging in click on “Create Refresh Token”: Then you will be redirected back to EGI Check-in Token Portal and you will have obtained a Refresh Token: The value of the Refresh Token The command to generate new Access Token using the Refresh Token ","categories":"","description":"Usage guide of EGI Check-in Token Portal\n","excerpt":"Usage guide of EGI Check-in Token Portal\n","ref":"/users/aai/check-in/obtaining-tokens/token-portal/","tags":"","title":"EGI Check-in Token Portal"},{"body":"VM Images are replicated using cloudkeeper, which has two components:\nfronted (cloudkeeper-core) dealing the with image lists and downloading the needed images, run periodically with cron backend (cloudkeeper-os) dealing with your glance catalogue, running permanently. Using the VM Appliance Every 4 hours, the appliance will perform the following actions:\ndownload the configured lists in /etc/cloudkeeper/image-lists.conf and verify its signature check any changes in the lists and download new images synchronise this information to the configured glance endpoint First you need to configure and start the backend. Edit /etc/cloudkeeper-os/cloudkeeper-os.conf and add the authentication parameters from line 117 to 136.\nThen add as many image lists (one per line) as you would like to subscribe to /etc/cloudkeeper/image-lists.conf. Use URLs with your AppDB token for authentication, check the following guides for getting such token and URLs:\nhow to access to VO-wide image lists, and how to subscribe to a private image list. Finally, you need to provide a /etc/cloudkeeper-os/mapping.json that configures the mapping of VOs supported in your OpenStack. The file should contain a json document that follows this format:\n{ \"\u003cVO_NAME\u003e\": { \"project\": \"\u003cid of project in OpenStack for VO_NAME\u003e\" }, \"\u003cVO_NAME_2\u003e\": { \"project\": \"\u003clocal name of project in OpenStack for VO_NAME_2\u003e\", \"domain\": \"\u003cdomain name for project in OpenStack\u003e\" } } Note that you can either specify a project ID or the project name with the domain name in the mapping. Add as many VOs as you are supporting.\nRunning the services cloudkeeper-os should run permanently, there is a cloudkeeper-os.service for systemd in the appliance. Manage as usual:\nsystemctl \u003cstart|stop|status\u003e cloudkeeper-os cloudkeeper core is run every 4 hours with a cron script.\n","categories":"","description":"cloudkeeper and AppDB integration\n","excerpt":"cloudkeeper and AppDB integration\n","ref":"/providers/cloud-compute/openstack/cloudkeeper/","tags":"","title":"VM Image synchronisation"},{"body":"Overview This tutorial describes the EGI Data Transfer using FTS transfers services and WebFTS. In the following paragraphs you will learn how to:\nuse the FTS command-line client use the WebFTS web interface to perform data transfers between two Grid storage.\nPrerequisites As first step please make sure that you have installed the FTS client as described in Data Transfer, and in particular Clients for the command-line FTS and to have your certificate installed in your browser to use WebFTS browser based client.\nTo access services and resources in the EGI Federated Cloud, you will need:\nAn EGI Check-in account, you can sign up here Enrollment into a Virtual Organisation (VO) that has access to the services and resources you need FTS client usage Step 1 Configuration check To verify that everything is configured properly you can check with the following command and pointing to the certificates directly:\n$ fts-rest-whoami --key ~/.globus/userkey.pem --cert ~/.globus/usercert.pem \\ -s https://fts3-public.cern.ch:8446/ User DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu VO: AndreaCristoforiac@egi.eu@tcs.terena.org VO id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Delegation id: XXXXXXXXXXXXXXXX Base id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX In general the commands can be used by specifying the user public and private key like shown in the example or by creating a proxy certificate as described in the following section.\nStep 2 Proxy creation As you have seen in the previous section it is possible to use the FTS commands by specifying the location of the user public and private key. With the use of voms-proxy-init it is possible to create a proxy certificate for the user. With this you don’t need to specify the location of the public and private key for each FTS command. When running voms-proxy-init it’s possible to specify the location of the public and private key. If this are not included as options, the tool expect to find them in:\n~/.globus/usercert.pem for the public key ~/.globus/userkey.pem for the private key with read access only for the owner Following is an example of running this command with the public and private key already setup as described:\n$ voms-proxy-init Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu Creating proxy ........................................... Done Your proxy is valid until Wed Aug 25 04:18:14 2021 The output of the command shows, a proxy certificate valid for 12 hours has been generated This is the default behaviour and can be usually increased, for example to 48 hours, with the following option:\n$ voms-proxy-init -valid 48:00 Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu Creating proxy ................................... Done Your proxy is valid until Thu Aug 26 16:23:01 2021 To verify for how long the proxy is still valid you can use the following command: command:\n$ voms-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu/CN=1451339003 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu type : RFC compliant proxy strength : 1024 bits path : /tmp/x509up_u1000 timeleft : 19:59:57 When the timeleft reaches zero the same command will produce the following message:\n$ fts-rest-whoami -s https://fts3-public.cern.ch:8446/ Error: Proxy expired! The last option that you need to use is specify the VO that you want to use for the proxy being created. In the following example the dteam VO has been used:\n$ voms-proxy-init --voms dteam Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu Creating temporary proxy ................................................................... Done Contacting voms2.hellasgrid.gr:15004 [/C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr] \"dteam\" Done Creating proxy .............................................................................. Done Your proxy is valid until Wed Sep 8 04:37:07 2021 With a proxy now available for the user it is now possible to execute the FTS commands without specifying the public and private keys as it will be shown in the following sections.\nStep 3 Find the storage In general, the source and destination storage for a specific project should be already known. However, to discover the available source or destination endpoints to be used for a transfer, you can use the VAPOR service.\nOnce the page is loaded on the left column it is possible to filter by VO or scroll the list and click the desired VO as show in the following picture:\nOnce selected, you can see all the resources associated with the specific VO. In particular in this case you are interested in the information on the status, capacity, type of storage, etc. Following is a screenshot of the visualisation of the list of storage available to dteam.\nStep 4 Starting a transfer Once you have identified the source and destination storage needed for the transfer you can proceed with the transfer between the two endpoints. To do that you can use a command of this type, returning the job ID corresponding to the transfer that you started:\n$ fts-transfer-submit -s https://fts3-public.cern.ch:8446/ \\ --source https://dc2-grid-64.brunel.ac.uk/dpm/brunel.ac.uk/home/dteam/1M \\ --destination https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/dteam/1M \\ -o cfc884f8-1181-11ec-b9c7-fa163e5dcbe0 To check the status of the transfer you can use the returned job ID and use the following command specifying the server controlling the transfer, the source and the transfer itself:\n$ fts-transfer-status -s https://fts3-public.cern.ch:8446/ \\ cfc884f8-1181-11ec-b9c7-fa163e5dcbe0 FINISHED The last option -o specify that the file should be overwritten if present on the destination. If this option is not present and a file with the same name exists on the destination, the transfer itself will fail. If you use this option you should make sure that it is safe to do so.\nUsing the WebFTS Data Transfer interface Step 1 Access the WebFTS interface The WebFTS is accessible at this CERN FTS URL. Similarly to what has been done from the command-line interface you need to provide our private key for delegation of the credential. To do that you use the following command:\n$ openssl pkcs12 -in yourCert.p12 -nocerts -nodes | openssl rsa Enter Import Password: writing RSA key (...) Which extract the private key in RSA format and you can paste it in the windows that opens:\nAnd select the desired VO. Once the delegation is set it’s possible to move to the following steps.\nWarning please be careful and avoid sharing this information with any third party or saving this information in plain text. WebFTS uses the key to acquire a proxy certificate on your behalf as described previously and does not store it. Step 2 Submitting a transfer The tab Submit a transfer is divided in two parts in which is possible to add two endpoints that can be used both as source or destination. After adding the URL for the two endpoints, it is possible to browse and select the files and directories to be transferred. In the destination select the destination directory. In the following example the file 1MB has been selected and by simply clicking the arrow in the middle directing to the right you are instructing the system to copy the file from the storage and path on the left to the one on the right:\nSimilarly to what can be done with the command-line interface, there is the option to overwrite the destination if it already exists. To enable this option tick the Overwrite Files below the arrow for the transfer. On the top of the page is also shown a confirmation that the transfer has been submitted successfully. This same web page shows the status of the current transfers in the My jobs tab shown in the following screenshot:\nEach line on the list shows a different job. When clicked it will expand to show additional details. The reason for the failure of a job which can be seen by moving the mouse pointer over the File ID on the detailed view.\n","categories":"","description":"Use EGI Data transfer to handle data in grid storage\n","excerpt":"Use EGI Data transfer to handle data in grid storage\n","ref":"/users/tutorials/data-transfer-grid-storage/","tags":"","title":"Data transfer with grid storage"},{"body":"Setting up GPU flavors Support for GPU can be added to flavors using the PCI passthrough feature in OpenStack. This allows to plug any kind of PCI device to the Virtual Machines.\nAs a summary of the OpenStack documentation, these are the steps needed to add a GPU enabled flavor (be aware this may need tuning to your specific hardware/configuration!):\nOn computing node, get vendor/product ID of your hardware: lspci | grep NVDIA to get pci slot of GPU, then virsh nodedev-dumpxml pci_xxxx_xx_xx_x On computing node, unbind device from host kernel driver. Unbinding is system dependent, and can be done in many ways, e.g.: if the kernel does not uses the devices (no GPU drivers included in kernel, or drivers disable in GRUB), nothing to unbind via pci-stub grubby --args=\"pci-stub.ids=10de:11fa\" --update-kernel DEFAULT (see RedHat manual, section 12.1, step 1-2; where the pci-stub.ids value is vendor_ID: product_id from lspci. via echo command: echo $dev \u003e /sys/bus/pci/devices/$dev/driver/unbind where $dev is the PCI device ID xx:xx.x or xxxx:xx:xx.x from lspci On computing node, add pci_passthrough_whitelist = {\"vendor_id\":\"xxxx\",\"product_id\":\"xxxx\"} to nova.conf (see nova-compute) On controller node, add pci_alias = {\"vendor_id\":\"xxxx\",\"product_id\":\"xxxx\", \"name\":\"GPU\"} to nova.conf (see nova-api) On controller node, enable PciPassthroughFilter in the scheduler (see nova-scheduler) Create new flavors with pci_passthrough:alias (or add key to existing flavor), e.g. openstack flavor set m1.large --property \"pci_passthrough:alias\"=\"GPU:2\" GPU description in flavor metadata Users should be able to easily discover the flavors that provide GPUs (or accelerators in general). The following table describes the agreed metadata for EGI providers to add to those flavors:\nMetadata Definition Comments Accelerator:Type Type of accelerator (e.g. GPU) Possible values: GPU, MIC, FPGA, TPU, NPU Accelerator:Number Number of accelerators available in the flavor (e.g. 1.0) Non integers allowed for the case of sharing GPU between VMs Accelerator:Vendor Name of accelerator Vendor (e.g. NVIDIA) Accelerator:Model Model of accelerator (e.g. Tesla V100) Need to make consensus and enforce. A100 is usually marketed without “Tesla” classname. Similarly, RTX A6000 usually marketed without “GeForce”. For clarity, full names should be used: “Tesla A100” and “GeForce RTX A6000” Accelerator:Version Version of the accelerator Some cards have different versions, e.g. A100 PCIe and NVLink. Openstack does not allow empty value, so we should give 0 if no version is specified Accelerator:Memory RAM in GBs of the accelerator Accelerator:VirtualizationType Type of virtualisation used (e.g. PCI passthrough) Not relevant for accounting, but may be still useful in some cases There are some extra fields that are defined in the GLUE2.1 schema but not so relevant for GPUs and therefore not considered at the moment. These are listed below for completeness:\nMetadata Definition Comments Accelerator:ComputeCapability Compute capabilities Defined by GLUE2.1, e.g. floating point type, NVLink, … may be used informally so far Accelerator:ClockSpeed Clockspeed of accelerator Defined by GLUE2.1, not so relevant, as ClockSpeed no longer related to performance. May be reserved for other types of accelerators Accelerator:Cores Number of cores of the accelerator Not so useful as there are several types of cores now (CUDA, tensor). May be reserved for other types of accelerators Adding metadata to flavors has no effects on site operations. End users can see the metadata easily via openstack flavor list --long or openstack flavor show \u003cflavor id\u003e commands without any additional tools, e.g.:\n$ fedcloud openstack flavor show gpu1cpu2 --site IISAS-GPUCloud --vo eosc-synergy.eu -f json Site: IISAS-GPUCloud, VO: eosc-synergy.eu { \"OS-FLV-DISABLED:disabled\": false, \"OS-FLV-EXT-DATA:ephemeral\": 0, \"access_project_ids\": null, \"disk\": 40, \"id\": \"a8082202-f647-4d1f-9b97-4f5ddb38ae8e\", \"name\": \"gpu1cpu2\", \"os-flavor-access:is_public\": false, \"properties\": \"Accelerator:Version='0', Accelerator:Memory='5', Accelerator:Model='Tesla K20m', Accelerator:Number='1.0', Accelerator:Type='GPU', Accelerator:Vendor='NVIDIA', Accelerator:VirtualizationType='PCI passthrough', pci_passthrough:alias='GPU:1'\", \"ram\": 8192, \"rxtx_factor\": 1.0, \"swap\": \"\", \"vcpus\": 2 } ","categories":"","description":"Configuring GPU flavors\n","excerpt":"Configuring GPU flavors\n","ref":"/providers/cloud-compute/openstack/gpu/","tags":"","title":"GPU Flavors"},{"body":"The EGI services are contributed by the EGI Council members, who can propose new services through the EGI Service Strategy, that defines the overall direction to EGI services for a few years time window. The services are designed, validated with the EGI Community, and with the help of the EGI Services and Solutions Board (SSB).\nThe evolution of the services is driven by 5 Strategic Objectives (SO) which are based on the knowledge of the user community needs that are being gathered through research and innovation projects, customer interviews and analysis of trends. Each defines an area for possible future work, and if approved, can be developed into more concrete goals and intended results.\nSO1: Federated compute continuum SO2: Federated Data Lakes and repositories SO3: Data analytics and scientific tools including AI SO4: Professional support and consultancy SO5: Investigation of a trusted compute platform for sensitive data processing If you want to propose a new service, you should approach your national EGI Council member, or fallback to the EGI SSB.\n","categories":"","description":"How new services are added to the EGI service catalogue","excerpt":"How new services are added to the EGI service catalogue","ref":"/providers/joining/new-service/","tags":"","title":"Introducing a new EGI service"},{"body":"Document control Property Value Title MAN05 top and site BDII High Availability Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Deploying top or site BDII service in High Availability Owner SDIS team Top BDII and Site BDII with High Availability This document objective is to provide guidelines to improve the availability of the information system, addressing three main areas:\nRequirements to deploy a top or site BDII service High Availability from a client perspective Configuration of a High Availability top or site BDII service Requirements to deploy a top or site BDII service Hardware dual core CPU 10GB of hard disk space 2-3 GB RAM. If you decide to set BDII_RAM_DISK=yes in your YAIM configuration, it’s advisable to have 4GB of RAM. Co-hosting Due to the critical nature of the information system with respect to the operation of the grid, the top or site BDII should be installed as a stand-alone service to ensure that problems with other services do not affect the BDII. In no circumstances should the BDII be co-hosted with a service which has the potential to generate a high load. Physical vs Virtual Machines There is no clear vision on this topic. Some managers complain that there are performance issues related to deploying a top or site BDII service under a virtual machine. Others argue that such performance issues are related to the configuration of the service itself. The only agreed feature is that the management and disaster recovery of any service deployed under a virtual machine is more flexible and easier. This could be an important point to take into account considering the critical importance of the top or site BDII service. Best practices from a client perspective for top BDII In gLite 3.2 and EMI you can set up redundancy of top BDIIs for the clients (WNs and UIs) setting up a list of top BDII instances to support the automatic failover in the GFAL clients. If the first Top level BDII fails to be contacted, the second will be used in its place, and so on. This mechanism is implemented defining the BDII_LIST YAIM variable according to the following syntax: BDII_LIST=topbdii.domain.one:2170[,topbdii.domain.two:2170[...]]. After running YAIM, the client enviroment should contain the following definition: LCG_GFAL_INFOSYS=topbdii.domain.one:2170,topbdii.domain.two:2170 The data management tools (lcg_utils) contact the information system for every operation (lcg-cr, lcg-cp, …). So, if you have your client properly configured with redundancy for the information system, the lcg_utils tools will use that mechanism in a transparent way. Be aware that lcg-infosites doesn’t work with multiple BDIIs. Only gfal, lcg_utils, lcg-info and glite-sd-query.\nSite administrators should configure their services with this failover mechanism where the first top BDII of the list should be the default top BDII provided by their NGI.\nBest practices for a top or site BDII High Availability service The best practice proposal to provide a high availability site or top BDII service is based on two mechanisms working as main building blocks: DNS round robin load balancing A fault tolerance DNS Updater We will provide a short introduction to some of these DNS mechanisms but for further information on specific implementations, please contact your DNS administrator.\nDNS round robin load balancing Load balancing is a technique to distribute workload evenly across two or more resources. A load balancing method, which does not necessarily require a dedicated software or hardware node, is called round robin DNS.\nWe can assume that all transactions (queries to top or site BDII generate the same resource load. For an effective load balancing, all top or site BDII instances should have the same hardware configurations. In other case, a load balancing arbiter is needed.\nSimple round robin DNS load balancing is easy to deploy. Assuming that there is a primary DNS server (dns.domain.tld) where the DNS load balancing will be implemented, one simply has to add multiple A records mapping the same hostname to multiple IP addresses under the core.top.domain DNS zone. It is equally applicable to site BDII.\n# In dns.domain.tld: Add multiple A records mapping the same hostname to multiple IP addresses Zone core.domain.tld topbdii.core.domain.tld IN A x.x.x.x topbdii.core.domain.tld IN A y.y.y.y topbdii.core.domain.tld IN A z.z.z.z The 3 records are always served as answer but the order of the records will rotate in each DNS query\nThis does NOT provide fault tolerance against problems in the top or site BDIIs themselves\nif one top or site BDII fails its DNS “A” record will still be served one in each three DNS queries will provide the failed top or site BDII first answer Fault tolerance DNS Updater The DNS Updater is a mechanism (to be implemented by you) which tests the different top or site BDIIs and decides to remove or add DNS entries through DNS dynamic updates. The fault tolerance is implemented by dynamically nsupdate introduced in bind V8 offers the possibility of changing DNS records dynamically: The nsupdate tool connects to a bind server on port 53 (TCP or UDP) and can update zone records Updates are authorized based on keys Updates can only be performed on the DNS primary server In the DNS bind implementation, the entire zone is rewritten by the DNS server upon “stop” to reflect the changes. Therefore, the zone should not be managed manually; and the changes are kept in a zone journal file until a “stop” happens. Implementation There are several alternatives to implement the DNS Updater: NAGIOS based tests a demonized service scripts running as crons What to test: BDII metrics Status information about the BDII is available by querying the o=infosys root for the UpdateStats object. This entry contains a number of metrics relating to the latest update such as the time to update the database and the total number of entries. And example of such entry is shown below. $ ldapsearch -x -h \u003cTopBDII/siteBDII\u003e -p 2170 -b \"o=infosys\" (...) dn: Hostname=localhost,o=infosys objectClass: UpdateStats Hostname: lxbra2510.cern.ch FailedDeletes: 0 ModifiedEntries: 4950 DeletedEntries: 1318 UpdateTime: 150 FailedAdds: 603 FailedModifies: 0 TotalEntries: 52702 QueryTime: 8 NewEntries: 603 DBUpdateTime: 11 ReadTime: 0 PluginsTime: 4 ProvidersTime: 113 More extensive information can be obtained (modifyTimestamp,createTimestamp) adding the +: $ ldapsearch -x -h \u003cTopBDII/siteBDII\u003e -p 2170 -b \"o=infosys\" + (...) # localhost, infosys dn: Hostname=localhost,o=infosys structuralObjectClass: UpdateStats entryUUID: 09bf40e0-7b23-4992-af55-fd74f036a454 creatorsName: o=infosys createTimestamp: 20110612223435Z entryCSN: 20110615120723.216201Z#000000#000#000000 modifiersName: o=infosys modifyTimestamp: 20110615120723Z entryDN: Hostname=localhost,o=infosys subschemaSubentry: cn=Subschema hasSubordinates: FALSE The following table shows the meaning of the most relevant metrics: Metric Desciption ModifiedEntries The number of objects to modify DeletedEntries The number of objects to delete UpdateTime To total update time in seconds FailedAdds The number of add statements which failed FailedModifies The number of modify statements which failed TotalEntries The total number of entries in the database QueryTime The time taken to query the database NewEntries The number of new objects DBUpdateTime The time taken to update the database in seconds ReadTime The time taken to read the LDIF sources in seconds PluginsTime The time taken to run the plugins in seconds ProvidersTime The time taken to run the information providers in seconds Previous BDII metrics can be checked to take a decision regarding the reliability and availability of a top or site BDII instance.\nMore information is available in gLite-BDII_top Monitoring.\nDNS caching DNS records obtained in queries are cached by the DNS servers (usually during 24 hours). Therefore to propagate DNS changes fast enough it is important to have very short TTL lifetimes. DNS has not been built to have very short TTL values and these may increase highly the number of queries and as result increase the load of the DNS server The TTL lifetime to be used will have to be tested. If the top BDII are only used by sites in the region and if queries are only from the DNS servers of these few sites then the number of queries may be low enough to allow for a very small TTL. This value should not be lower than 30s - 60s. Example 1: The IGI Nagios based mechanism In IGI, the DNS update of the number of instances participating in the DNS round robin mechanism depends on the results provided by a Nagios instance.\nWhen Nagios needs to check the status of a service it will execute a plugin and pass information about what needs to be checked. The plugin verifies the operational state of the service and reports the results back to the Nagios daemon.\nNagios will process the results of the service check and take appropriate action as necessary (e.g. send notifications, run event handlers, etc).\nEach instance is checked every 5 minutes. If a failure occurs, Nagios runs the event handler to restart the BDII service AND remove the instance from the DNS round robin set using dnsupdate:\nan email is sent as notification; If 4 (out of 5) instances are failing, a SMS message is sent as notification; If a failed instance appears to be restored, Nagios will re-add it to the DNS round robin mechanism. This approach has some single points of failures: The Nagios instance can fail The master DNS where the DNS entries are updated can fail Example 2: The IBERGRID scripting based mechanism In IBERGRID, an application (developed by LIP) verifies the health of each top BDII. The application can connect to the DNS servers and remove the “A” records of top BDIIs that become unavailable (non responsive to tests).\nThe monitoring application (nsupdater) is a simple program that performs tests, and based on their result acts upon DNS entries\nWritten in perl Can be run as daemon or at the command prompt The tests are programs that are forked Tests are added in a “module” fashion way Can be used to manage several DNS round robin scenarios Can manage multiple DNS servers To remove the DNS single point of failure as in previous example, one could configure all DNS servers serving the core.ibergrid.eu domain as primary Three primary servers would then exist for core.ibergrid.eu All three DNS servers could be dynamically updated independently The monitoring application should also have three instances, one running at each site The downside is that DNS information can become incoherent. It would be up to the monitoring application to manage the three DNS servers content and their coherence ","categories":"","description":"Deploying the BDII service in High Availability","excerpt":"Deploying the BDII service in High Availability","ref":"/providers/operations-manuals/man05_top_and_site_bdii_high_availability/","tags":"","title":"MAN05 top and site BDII High Availability"},{"body":"Downtimes To properly manage downtimes it is important to highlight the differences between the possible types of downtimes. Downtimes can’t be added retroactively, and the date is always defined in UTC. If the error messages received when adding a downtime is cryptic, usually it’s due to a parsing error on the time/date.\nDowntimes classification scheduled: e.g. for software/hardware upgrades, planned and agreed in advance. This needs to be announced at least 24h in advance. unscheduled: e.g. power outages, unplanned, usually triggered by an unexpected failure. Downtimes severities WARNING: Resource will probably be working as normal, but may experience problems. OUTAGE: Resource will be completely unavailable. WARNING, formerly known as AT_RISK, implies a type of severity that does not have operational consequences. It is only an information for users that some small temporary failures can appear. All failures during that time will be taken into account in the reliability calculations. Examples include:\nAdmins not present on site (conference, vacations). Reduced redundancy in network, power or cooling. Failed disk in RAID sets. OUTAGE implies that the site/node is completely unavailable and no tickets should be created. This does not affect site metrics.\nMore information about downtimes can be found in the Configuration Database user documentation.\nSites in downtime When a ticket has been raised against a site that subsequently enters in a downtime, the expiry date on the ticket can be extended.\nWhen a ticket is opened against a site that continues to add or increase the downtime the ticket must be closed, and the NGI requested to take action either by suspending or un-certifying the site until such time that the problem is resolved. This usually happens when a middleware upgrade is due or a bug in the middleware is causing a site to fail. Sites then may choose to wait for the next middleware release rather than spend effort trying to resolve the issue locally.\nSites that are in downtime will still have monitoring switched on and therefore may appear to be failing tests. ROD must take care that when opening tickets to ensure that they don’t open tickets against sites in downtime.\nIf a site is in downtime for more than a month, then it is advised that the site should go to the uncertified state.\nNodes in downtime When a node of a site is in downtime, alarms are generated but the Operations Portal distinguishes these alarms, and marks the downtime accordingly in the dashboard.\nROD should not open tickets against nodes that are in downtime.\nInstructions for accounting monitoring failures The Accounting monitoring tests are not run against the site but query the central accounting repository.\nIf there is more than one failure for a given site, create a ticket for one of the alarms and mask all others by this one. Edit the description of the ticket to state clearly that even though the failure is reported for a given CE, this is not a CE failure but a failure on the Accounting service for the whole site. Proceed with all sites in the same way. Please beware: Accounting tests are not helped by scheduling downtime, the site admins need to get Accounting publishing working again. Nodes not in production When a node of a production site is declared as non-production in the Configuration Database or the node appears in BDII but is not declared in the Configuration Database, then the ROD should do the following:\nRecommend to the sites to take these nodes out of their site BDII If this is not a possibility then the site should set those nodes in downtime in the EGI Configuration Database If the node is a test node and is in BDII but not in the Configuration Database, then the sites should register it and turn monitoring off. ","categories":"","description":"Description on how to deal with scheduled and unscheduled interventions and downtimes.","excerpt":"Description on how to deal with scheduled and unscheduled …","ref":"/providers/rod/downtimes/","tags":"","title":"Managing downtimes"},{"body":"What is it? The EGI Messaging Service is powered by ARGO Messaging Service (AMS), a real-time messaging service that allows the user to send and receive messages between independent applications.\nIt’s a Publish/Subscribe Service implementing the Google PubSub protocol and providing an HTTP API that enables Users/Systems to implement message oriented service using the Publish/Subscribe Model over plain HTTP.\nThis central service is used by other EGI central services in order to exchange messages, like for sending information about accounting or resources available at a cloud site. More specifically, the services that use the Messaging service are:\nAAI Federation Registry: uses the service to exchange information among the different components (examples: SimpleSamlPHP, MITREid, Keycloak). Operations Portal: reads the alarms from predefined topics, stores them in a database and displays them in the operations portal. Accounting: uses the service as a transport layer for collecting accounting data from the sites. The accounting information is gathered from different collectors into a central accounting repository where it is processed to generate statistical summaries that are available through the EGI Accounting Portal. FedCloud: used the service as a transport layer for the cloud information system. It makes use of the ams-authN. The entry point for users, topics and subscriptions is the Configuration Database. ARGO Availability and Reliability Monitoring Service: It uses the service to send the messages from the monitoring engine to other components. Features Ease of use: it supports an HTTP API and a python library so as to easily integrate with the service. Push Delivery: the service instantly pushes asynchronous event notifications when messages are published to the message topic. Subscribers are notified when a message is available. Replay messages: replay messages that have been acknowledged by seeking a timestamp. Schema Support: on demand mechanism that enables a) the definition of the expected payload schema, b) the definition of the expected set of attributes and values and c) the validation for each message if the requirements are met and immediately notify the client. Replicate messages on multiple topics: Republisher script that consumes and publishes messages for specific topics (e.g. sites). It supports both push and pull message delivery. In push delivery, the Messaging Service initiates requests to the subscriber application to deliver messages. In pull delivery, the subscription application initiates requests to the server to retrieve messages.\nApart from the main service a number of valuable components are also supported. These components are extensively used by the connected services.\nArgo-ams-library: a simple library written in python to interact with the ARGO Messaging Service. Argo-AuthN: Argo-authN is a new Authentication Service. This service provides the ability to different services to use alternative authentication mechanisms without having to store additional user info or implement new functionalities. The authentication service holds various information about a service’s users, hosts, API URLs, etc, and leverages them to provide its functionality. AMS Metrics: Metrics about the service Architecture Instead of focusing on a single Messaging service specification for handling the logic of publishing/subscribing to the broker network, the service focuses on creating nodes of Publishers and Subscribers as a Service. In the Publish/Subscribe paradigm, Publishers are users/systems that can send messages to named-channels called Topics. Subscribers are users/systems that create Subscriptions to specific topics and receive messages.\nAs shown in Figure below, the current deployment of messaging service comprises a haproxy server, which acts as a load balancer for the 3 AMS servers running in the backend.\nUsing the Messaging Service Note Documentation for the ARGO Messaging Service is available on the ARGO documentation site. ","categories":"","description":"Messaging service supporting other central services","excerpt":"Messaging service supporting other central services","ref":"/internal/messaging/","tags":"","title":"Messaging service"},{"body":"Information to provide when registering a new service (the fields marked with an asterisk are mandatory):\nHosting Site * (select the appropriate RC under which you are registering the service) Service Type * (select the appropriate service type) Service URL (Alphanumeric and $-_.+!*’(),:) Hostname * (valid FQDN format) Host IP a.b.c.d Host IPv6 (0000:0000:0000:0000:0000:0000:0000:0000[/int]) (optional [/int] range) Host DN (/C=…/OU=…/…) Description * (Alphanumeric and basic punctuation) Host Operating System (Alphanumeric and basic punctuation) Host Architecture (Alphanumeric and basic punctuation) Is it a beta service (formerly PPS service)? Y/N Is this service in production? Y/N Is this service monitored? Y/N Contact email * (valid email format) Scope Tags ✓ Optional Tags (At least 1 optional tags must be selected): EGI Local FedCloud ✓ Reserved Tags Inheritable from Parent Site: none ✓ Reserved Tags Directly Assigned (WARNING - If deselected you will not be able to reselect the tag - it will be moved to the ‘Protected Reserved Tags’ list): none ✗ Protected Reserved Tags (Can only be assigned on request): alice atlas cms elixir lhcb tier1 tier2 wlcg ","categories":"","description":"Information required for registering a service into the Configuration Database","excerpt":"Information required for registering a service into the Configuration …","ref":"/internal/configuration-database/service-registration-requirements/","tags":"","title":"Service registration requirements"},{"body":"Introduction GGUS is the Helpdesk service of the EGI Infrastructure. Incident and Service request tickets can be recorded, and their progress is tracked until the solution. The users of the service should not need to know any of the details of what happens to the ticket in order to get it from creation to solution. However, an understanding of the operation of the system may be helpful to explaining what happens when you request help.\nTickets can be created through the GGUS web interface, which is described in section “Accessing the web interface to GGUS” of this note.\nOnce the ticket has entered GGUS, it is processed by assigning it to the appropriate group to deal with the issue. The groups are generally addressed via mailing lists, and so GGUS assigns the ticket to a group, and an email message is sent to people on the appropriate list. Sometimes, a ticket is simple and it is assigned to the correct group immediately and dealt with immediately.\nSubmitting a ticket using the web interface Before using this route, it is essential to have either a digital certificate installed in the appropriate manner in the web browser or an EGI Check-in account.\nAccessing the web interface to GGUS If the user carries out all of the steps above, but is not registered to use GGUS, then the home page is like the one shown in the following figure:\nIf the user faces problems with their certificate, they may get help at wiki.egi.eu: Getting certificate\nFor accessing GGUS users have to register first. Registration process is described in chapter Registering at GGUS.\nRegistering at GGUS For getting supporter privileges users need to be registered. For registering at GGUS the user should go to GGUS home and click the registration link. This link opens the registration form that the user has to fill in. In addition, there are some links where the user can find information about X509 personal certificates. Users who do not have a valid digital certificate can access GGUS via EGI SSO. After filling in the registration form, the user receives an email from the GGUS team with his access data to GGUS.\nAccessing GGUS with certificate The primary address of the GGUS portal is: https://ggus.eu. If the user enters this in the browser, a warning will be displayed by the browser prompting the user to specify which certificate to use (if the user has not selected to automatically select the certificate in the browser settings). The reason for this warning is that GGUS has to validate the user in order to allow access. After that, the user will no longer be asked to identify himself.\nThe user identifies themselves with their digital certificate. At this point in time, GGUS has identified the user and displays the start page as shown in the following figure:\nNote that in this case, the user’s identity is displayed on the page and GGUS has recognized that this user is registered to submit tickets. If authentication by certificate fails, you will be taken to the login page.\nAccessing GGUS via EGI Check-in When choosing “Login via EGI AAI CheckIn” on the login page the user is guided to the EGI Check-in page were the users Identity Provider can be selected. After authenticating there, the user will be redirected to GGUS.\nThe ticket submit form on web interface The tickets submit form offers a set of fields which should help the user to describe his issue as detailed as possible. Most of the fields on the submit form are optional. Only field “Subject” is required. The submit form consists of three main sections: the user information, the issue information and the routing information.\nUser information Most fields in the user information section are pre-filled by GGUS system.\n“Name/Login” is taken from the GGUS user database. “E-Mail” is also taken from the GGUS user database. “Notification mode” defaults to “on solution”. The “Notification mode” manages the update notifications the user receives. “On solution” means that the user only gets notified when the ticket status is set to “solved” or a comment is added to the public diary. Additional information is available by clicking on the question mark on the right hand side of the field label. “CC to” could be filled with any mail address of people who should be informed about this ticket. Notifications are sent on every ticket update. The field content could only be changed by people who have support access. Additional information is available by clicking on the question mark. Issue information Although only one field is mandatory in this section, as much fields as possible should be filled with information.\n“Date/Time of issue” defaults to the submitting time. This field should be set if the ticket is submitted much later than the issue occurred. “Subject” is a mandatory field. It should give a short description of the issue. This field is limited to 250 characters. “Describe your issue…” is limited to 4000 characters. It should be used for a detailed description of the issue. If the user does not know which information to add here he could click on the question mark for getting additional information. “Concerned VO” provides a drop-down list of all VOs supported by GGUS. “VO specific” is a flag indicating whether an issue is VO specific or not. It defaults to “no”. This flag could only be set to “yes” in combination with a VO selected in field “Concerned VO”. “Affected Site” The site impacted by the issue. A drop-down list with all EGI sites registered in GOC DB and OSG sites registered in OIM DB is available. This is no routing information! “Affected ROC/NGI” The NGI/ROC impacted by the issue. A drop-down list of all NGIs/ROCs integrated in GGUS is provided. “Ticket category” provides a drop-down list with possible values. This field is for categorizing the issue. It defaults to “Incident”. “Type of issue” provides a drop-down list with possible values. This field is for categorizing the issue. It defaults to “Other”. “Priority” provides a drop-down list with possible priority values. They are “less urgent” which is the default, “urgent”, “very urgent” and “top priority”. “Attach file(s)” offers the possibility to upload 4 attachments (e.g. log files etc.) at a time. Attachments are limited to 2 MB. Please avoid uploading file formats “.exe”, “.php”, “.htm(l)”. Routing Information “Notify SITE” provides a drop-down list with all EGI sites registered in GOC DB and OSG sites registered in OIM DB. If selecting a site from the list this site will be notified about this ticket by mail. Additionally the ticket is assigned to the appropriate NGI/ROC directly, bypassing the TPM (Some words about TPM). If setting a site value the appropriate NGI/ROC is set automatically. Choosing an NGI/ROC simultaneously is not possible. “Assign to ROC/NGI” provides a drop-down list of all NGIs/ROCs integrated in GGUS. Choosing a value in this field assigns the ticket to the appropriate ROC directly, bypassing the TPM. If assigning a ticket to an NGI/ROC choosing a site value simultaneously is not possible. After clicking the “Submit” button the user gets a confirmation page showing the information submitted and the ticket ID.\nBypassing the TPM The TPM (Ticket Processing Manager) is the 1st Line Support in GGUS. Users can bypass the TPM if they have good knowledge about where the problem is. For this purpose at the bottom of ticket submit form there is a section “Routing information”. Selecting either a site from the “Notify SITE” drop-down menu or a support unit from the “Assign to support unit” drop-down menu routes the ticket directly to the selected support unit. If selecting a site name the NGI/ROC the site belongs to is set automatically. Hence the ticket is assigned to the relevant NGI/ROC. Additionally the site will receive a notification about the ticket. Selecting both, the “Notify SITE” and the “Assign to support unit” is not possible.\nTickets of type TEAM and ALARM are always routed to the relevant NGI/ROC by default.\nBrowsing your own tickets After authenticating themselves the user has access to the GGUS homepage. On this page they see a list of their own open tickets and a list of the latest open tickets of all users (Figure 2). Below the list of the user’s own open tickets there are two links for further browsing possibilities of the user’s own tickets:\nShow my complete ticket list, My Team Tickets. The system only shows the user tickets which have been created with the same authentication information the user is currently logged in. This means if a user submits tickets with different certificates he does not see all of their tickets. The reason for this is that the DN string of the certificate is stored in each ticket. Showing all tickets of a user can be done by using the GGUS search engine. In the GGUS search engine users can search by username amongst others. This search will show all tickets of a user independent from the authentication information.\nShow my complete ticket list This link opens a new window showing tables of all open and closed tickets of the user and all tickets of other users the user has subscribed to. Information on how to subscribe to a ticket is available in section “Subscribing to a ticket of another user.”\nModifying tickets Modifying your tickets using the web For modifying a ticket the user can just click on the ticket ID. He is guided to another page. On this page are three main sections:\nthe ticket information, the ticket history, the modify section. The ticket information gives an overview of the personal data the user provided, the issue description and the ticket status. The ticket history shows all actions that have been taken to solve the ticket, the date and time these actions have been taken and the name of the supporter who did them. In the modify section the user can add some additional information or comment on a question of a supporter to them. The user can add attachments, change the email notification behavior of the system and change some other attributes of his ticket.\nEscalating a ticket Between the information section and the ticket history there is a button which allows the user to escalate a ticket (Figure 9).\nThree escalation levels are available in GGUS:\nEscalating ticket to the support unit it is assigned to. Escalating the ticket to the support unit and the TPM on shift. Escalating the ticket to the support unit, the TPM and the GGUS ticket monitoring. The escalation levels are reached one by one. It is not possible to choose one of them. Additional information is available by clicking on the little question mark on the left hand side of the button.\nReopen a solved ticket If a ticket is already closed it could be reopened by adding a comment and changing the status to “reopened” in the “Modify section” (Figure 8). In this case the support unit which solved the ticket and the TPM receive an email about the ticket reopening.\nVerifying the solution When a ticket is solved, the user could confirm that the solution has solved their issue by verifying it. A solution could be verified by either:\nClicking the “Verify” button in the web portal or Replying to the solution mail without changing anything. Verifying a solution can help to increase the quality of solutions in GGUS.\nModify your tickets using email Updating a ticket using email is also possible if one fundamental requirement is achieved. The mail subject must contain the typical GGUS string “GGUS-Ticket-ID: #Ticket-ID” where “Ticket-ID” is the ID of the ticket which should be updated. The easiest way to do this is to reply to an update notification received from GGUS. When updating a ticket using email the whole mail body will be added to the ticket. Changing any other field (e.g. status, priority,…) is impossible\nTicket Participation GGUS system offers various possibilities for participating in tickets. They are\nthe CC field, the Involve others field and the Subscribe field. An overview on these fields is given in the table below. Ticket participation can be done by adding a valid mail address to one of these fields. Please avoid adding closed mailing lists as such produce a lot of mail errors! Several mail addresses have to be separated by semicolon.\nUser submit User modify Supporter modify CC Yes No Yes Involve others No No Yes Subscribe No Yes Yes The “CC” field The CC field can be set by the user in the ticket submit form. Updates are only possible for supporters for correcting or removing invalid mail addresses. Every ticket update triggers a notification email to the mail address specified in the “CC” field.\nThe “Involve others” field The “Involve others” field is only for supporters use. Every ticket update triggers a notification email to the mail address specified in the “Involve others” field.\nSubscribing to a ticket of another user Figure 11: Ticket subscribe Every user could subscribe to tickets of other users if he is interested in the solution of any. For subscribing a valid email address has to be provided. The user gets a notification once the ticket is solved. After subscribing to a ticket the user could change the notification mode or unsubscribe if he wants to (Figure 11). Accessing the system with the same credentials as used for subscription is necessary for this. Additional information on subscribing to tickets is available by clicking on the question mark at the right hand side of the label “Subscribe to this ticket”.\nWho gets what email notification from GGUS? The ticket “submitter” gets emails according to the “Notification mode” value (s)he selected when submitting the ticket. If the selected “Notification mode” value is “on every change” then all updates are sent to the “submitter”. “Public Diary” entries are sent to the submitter regardless the value of the “Notification mode”. “Internal Diary” entries never go to the “submitter”. They are reserved for exchanges amongst supporters.\nThe email addresses in the “Cc:” field can be entered by the “submitter” and receive the same notifications as the submitter. “Public Diary” entries are sent to the addresses in the “Cc:” field. “Internal Diary” entries never go to the people in the “Cc:” field. They are reserved for exchanges amongst supporters.\nThe email addresses in the “Involve others:” field can be entered by supporters only and receive the same notifications as the Support Unit (SU) to which the ticket is assigned. “Internal Diary” entries are sent to the relevant SU members AND the people in the “Involve others:” field, as they are supposed to be experts and contribute to the ticket solution.\nThe email address in the “Assign to one person:” field can be entered by supporters only and receive the same notifications as the Support Unit (SU) to which the ticket is assigned. “Internal Diary” entries are sent to the relevant SU members AND the people in the “Involve others:” field AND the email address in the “Assign to one person:” field as they are, all, supposed to be experts and contribute to the ticket solution.\nEvery ticket update triggers an email to the addresses in the “Cc:”, “Involve others:” and to ticket subscribers, i.e. GGUS users, unrelated to the specific ticket, who entered their own email in the “Subscribe to this ticket” field.\nGGUS email notifications highlight the fields changed with the specific update.\nPlease avoid including closed mailing lists, e-groups in these fields as mail delivery will fail.\nBrowsing all tickets At the bottom of the home page there are additional links for browsing:\nSearch ticket database, Show all open tickets, GGUS search engine For browsing all tickets the GGUS search engine is a useful tool.\nThe GGUS search engine can be entered by clicking on link “Search ticket database”. When accessing the search engine a default search is performed like shown in Figure 12: GGUS search engine. Searching via Ticket ID is the easiest and fastest way to look at a ticket. When searching via Ticket ID all other search parameters were ignored. Besides searching for all open tickets this is the recommended kind of search, because it avoids needless workload on the system. When searching via ticket ID the ticket details are shown in the same window. For getting back to the main page use the “Back” button of your browser. The various search parameters can be combined in any way wanted except “Untouched since”. Description fields “Keyword”, “Involved supporter” and “Assigned to person” trigger a LIKE search to the database. Concatenating keywords with “AND” or “OR” is currently not possible. The search can either be started by clicking on “go” or just hitting the return key. The result of a search by parameters is shown in the result list. For viewing ticket details just click on the ID. A new window opens showing ticket details. For getting back to the search result just close the window with the ticket details.\nShowing all open tickets Clicking on this link shows all open tickets that are currently in the system. Unlike “Showing all open tickets” the default search in GGUS search engine uses a timeframe of one week for showing open tickets.\nContacting the GGUS team Users can click on the “Envelope” icon in the menu bar for sending an email to the GGUS team with any comments. Note: This must not be used for submitting support requests as it does not create a ticket in the system!\nGGUS development plans On GGUS home page there are a couple of links where users can get more information on the GGUS development plans as well as submit own feature requests. Feature requests are collected in the GGUS shopping lists in JIRA (only for users having a CERN account) and EGI RT.\nOperation of GGUS The GGUS system is running on servers located at Karlsruhe Institute of Technology (KIT), Germany. Besides the GGUS production system a backup system is in place. Switching from production system to backup system currently needs manual interaction. An automatic switch in case of fail-over will be implemented. GGUS staffs at Karlsruhe are not providing support apart from requests concerning GGUS system itself. They can be contacted by email to support “at” ggus.eu. Usual office hours for GGUS staffs are from 07:00 to 15:00 o’clock (UTC).\nTPM TPM (Ticket Processing Manager) is the most important part of the support system for the grid. The purpose of TPM is:\nclosing simple trouble tickets, ensuring that other tickets are sent to the correct place for processing, reacting to alarms that tickets have not been processed, The TPM teams consist of people with a broad knowledge of the Grid.\nTicket Monitoring Besides developing and maintaining GGUS system the GGUS team is also doing the ticket monitoring. The ticket monitoring team is responsible for:\nreminding users in case their input is required for further ticket processing reminding supporters in case tickets are not processed assisting in any problem during ticket processing ","categories":"","description":"Guide for users","excerpt":"Guide for users","ref":"/internal/helpdesk/user-guide/","tags":"","title":"User guide"},{"body":"In this page you can find a summary of the needed steps for supporting a new VO in your OpenStack infrastructure.\nLocal project creation The usual method of supporting a VO is by creating a local project for it. You should assign quotas to this project as agreed in the OLA defining the support for the given VO.\nCreate a group where users belongig to the VO will be mapped to: :\ngroup_id=$(openstack group create -f value -c id \u003cnew_group\u003e) Add that group to the desired local project: :\n$ openstack role add member --group $group_id --project \u003cyour project\u003e Keystone Mapping Expand your mapping.json with the VO membership to the created group (substitute group_id and entitlement as appropriate). The expected mappings for the VOs are listed in vo-mappings.yaml of fedcloud-catchall-operations repository:\n[ \u003cexisting mappings\u003e, { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"\u003cgroup_id\u003e\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai.egi.eu/auth/realms/egi\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^\u003centitlement\u003e$\" ] } ] } ] And update the mapping in your Keystone IdP:\n$ openstack mapping set --rules mapping.json egi-mapping Accounting Add the project supporting the VO to cASO:\nIn the projects field of /etc/caso/caso.conf :\nprojects = vo_project1, vo_project2, \u003cyour_new_vo_project\u003e and as a new mapping in /etc/caso/voms.json :\n{ \"\u003cyour new vo\u003e\": { \"projects\": [\"\u003cyour new vo project\u003e\"] } } Be sure to include the user running cASO as member of the project if it does not have admin privileges:\nopenstack role add member --user \u003cyour caso user\u003e --project \u003cyour new vo project\u003e Information system Add the mapping to your site configuration with a new Pull Request to the fedcloud-catchall-operations repository\n--- vos: - name: \u003cvo name\u003e auth: project_id: \u003cyour new vo project\u003e VM Image Management cloudkeeper-core Add the new image list to the cloudkeeper configuration in /etc/cloudkeeper/cloudkeeper.yml (or /etc/cloudkeeper/image-lists.conf if using the appliance), new entry should look similar to:\nhttps://\u003cAPPDB_TOKEN\u003e:x-oauth-basic@vmcaster.appdb.egi.eu/store/vo/\u003cyour new vo\u003e/image.list\ncloudkeeper-os Add the user configured in cloudkeeper-os as member of the new project:\n$ openstack role add member \\ --user \u003cyour cloudkeeper-os user\u003e \\ --project \u003cyour new vo project\u003e Add the mapping of the project to the VO in /etc/cloudkeeper-os/mapping.json:\n{ \"\u003cyour new vo\u003e\": { \"tenant\": \"\u003cyour new vo project\u003e\" } } ","categories":"","description":"Summary of steps for configuring new VOs in OpenStack\n","excerpt":"Summary of steps for configuring new VOs in OpenStack\n","ref":"/providers/cloud-compute/openstack/vo_config/","tags":"","title":"VO Configuration guide"},{"body":"What is it? EGI Workload Manager is a service provided to the EGI community to efficiently manage and distribute computing workloads on the EGI infrastructure.\nWorkload Manager is configured to support a number of HTC and cloud resource pools from the EGI Federation. This pool of computing resources can be easily extended and customized to support the needs of new scientific communities.\nNote Workload Manager is based on DIRAC technology. The delivery of the service is coordinated by the EGI Foundation and IN2P3 provides the resources and operates the service. Main Features The EGI Workload Manager:\nMaximizes usage efficiency by choosing appropriately computing and storage resources on real-time.\nProvides a large-scale distributed environment to manage and handle data storage, data movement, accessing and processing.\nHandles job submission and workload distribution in a transparent way.\nImproves the general job throughput compared with native management of EGI Grid or Cloud computing resources.\nOffers pilot-based task scheduling method, that submits pilot jobs to resources to check the execution environment before to start the user’s jobs. From a technical standpoint, the user’s job description is delivered to the pilot, which prepares its execution environment and executes the user application. The pilot-based scheduling feature solves many problems of using heterogeneity and unstable distributed computing resources.\nIncludes easy extensions to customize the environment checks to address the needs of a particular community. Users can choose appropriately computing and storage resources maximising their usage efficiency for particular user requirements.\nHandles different storage supporting both cloud and grid capacity.\nProvides a user-friendly interface that allows users to choose among different DIRAC services.\nTarget User Groups The service suits for the established Virtual Organization communities, long tail of users, SMEs and Industry\nEGI and EGI Federation participants Research communities Architecture The EGI Workload Manager service is a cluster of DIRAC services running on EGI resources (HTC, Cloud, HPC) supporting multi-VO. All the DIRAC services are at or above TRL8. The main service components include:\nWorkload Management System (WMS) architecture is composed of multiple loosely coupled components working together in a collaborative manner with the help of a common Configuration Services ensuring reliable service discovery functionality. The modular architecture allows to easily incorporate new types of computing resources as well as new task scheduling algorithms in response to evolving user requirements. DIRAC services can run on multiple geographically distributed servers which increases the overall reliability and excellent scalability properties.\nREST server providing language neutral interface to DIRAC service.\nWeb portal provides simple and intuitive access to most of the DIRAC functionalities including management of computing tasks and distributed data. It also has a modular architecture designed specifically to allow easy extension for the needs of particular applications.\nThe DIRAC Web portal\nHow to access the EGI Workload Manager service There are several options to access the service:\nMembers of a scientific community whose resources pool is already configured in the EGI Workload Manager instance can use the EGI Workload Manager web portal to access the service, or use DIRAC Client. Individual researchers who want to do some number crunching for a limited period of time, with a reasonable (not too high) number of CPUs \u003e can use the catch-all VO resource pool (vo.access.egi.eu). Submit a request through the EGI Marketplace selecting:\nCompute \u003e Workload Manager from the top menu. Representatives of a community who want to try DIRAC and EGI \u003e Same as #2. Representative of a community who wants to request DIRAC for the community’s own resource pool \u003e Submit a request through the EGI Marketplace selecting\nCompute \u003e Workload Manager from the top menu. Getting Started Submit a service order via the Marketplace User can request access to the service submitting a service-order to use the EGI HTC service directly from the EOSC Marketplace:\nEOSC Marketplace: Workload Manager Service orders are usually handled within 5 working days by the EGI User Support Team on shift.\nBefore starting Apply for your user credentials DIRAC uses X.509 certificates to identify and authenticate users. These certificates are delivered to each individual by trusted certification authorities. If you have a personal certificate issued by a EUGridPMA-certified authority you can use it for this tutorial. Otherwise refer to the information available in this section, to obtain a certificate. Your certificate may take a few days to be delivered, so please ask for your certificate well in advance and in any case, before the tutorial starts.\nInstall your credentials Your personal certificate is usually delivered to you via a site and is automatically loaded in your browser. You need to export it from the browser and put it in the appropriate format for DIRAC to use. This is a one-time operation. Please follow the instructions in detailed in VOMS documentation page to export and in install your certificate.\nSend your certificate’s subject to the DIRAC team In order to configure the DIRAC server so that you gets registered as a user, the team needs to know your certificate’s subject.\nPlease use the command below on any Unix machine and send its output to\ndirac-support \u003cAT\u003e mailman.egi.eu\nopenssl x509 -in $HOME/.globus/usercert.pem -subject -noout The EGI Workload Manager Web Portal To access the EGI Workload Manager open a web browser to: https://dirac.egi.eu/DIRAC/\nThe EGI Workload Manager service Web portal\nIf you are a new user, you can see the welcome page where you can find links to user documentations.\nVO options: you can switch to different VOs that you have membership.\nLog In options: the service supports both X.509, Certificate and Check-in log-in.\nView options: allow to choose either desktop or tabs layout.\nMenu: a list of tools that enable the selected VO.\nUpload Proxy Before submitting your job, you need to upload your Proxy. Login to the portal. Go to:\nMenu \u003e Tools \u003e Proxy Upload, enter your certificates .p12 file and the passphrase, click Upload.\nThe wizard to upload the .p12 proxy certificate\nJob Submission Go to:\nMenu \u003e Tools \u003e Job Launchpad. First check the Proxy Status, click it until it shows Valid in green color.\nIn the Job Launchpad, you can select your jobs from the list; add parameters, indicating the output Sandbox location.\nNow, select Helloworld from the job list, and click Submit, you just launch your very first job to the EGI HTC cluster.\nSubmit a job with the Job Launchpad\nMonitor Job status Go to:\nMenu \u003e Applications \u003e Job Monitor.\nThe left panel gives all kinds of search options for your jobs. Set your search criteria, and click Submit, the jobs will list on the right panel.\nTry the various options to view different information about the jobs.\nMonitor the job execution with the Job Monitor panel\nGet Results from Sandbox Once the job has been successfully processed, the Status of the job will change to green. Right click the job, select:\nSandbox \u003e Get Output file(s), you can get the result file(s).\nFull User Guide for DIRAC Web Portal For further instructions, please refer to DIRAC Web Portal Guide\nThe DIRAC client tool The easiest way to install the client is via Docker Container. If you have a Docker client installed in your machine, install the DIRAC CLI as follows:\ndocker run -it -v $HOME:$HOME -e HOME=$HOME diracgrid/client:egi Once the client software is installed, it should be configured in order to access the EGI Workload Manager service:\nsource /opt/dirac/bashrc To proceed further a temporary proxy of the user certificate should be created. This is necessary to get information from the central Configuration Service:\n$ dirac-proxy-init -x Generating proxy... Enter Certificate password: ... Now the client can be configured to work in conjunction with the EGI Workload Manager service:\n$ dirac-configure defaults-egi.cfg Executing: /home/jdoe/DIRAC/DIRAC/Core/scripts/dirac-configure.py defaults-egi.cfg Checking DIRAC installation at \"/home/jdoe/DIRAC\" Created vomsdir file /home/jdoe/DIRAC/etc/grid-security/vomsdir/vo.formation.idgrilles.fr/cclcgvomsli01.in2p3.fr.lsc [..] Created vomsdir file /home/jdoe/DIRAC/etc/grid-security/vomsdir/fedcloud.egi.eu/voms2.grid.cesnet.cz.lsc Created vomses file `/home/jdoe/DIRAC/etc/grid-security/vomses/fedcloud.egi.eu` Generate the proxy containing the credentials of your VO. Specify the VO in the --group option:\nIn this example, we are going to use the resources allocated for the WeNMR project.\n$ dirac-proxy-init --debug --group wenmr_user -U --rfc $ dirac-proxy-init --debug --group wenmr_user -U --rfc Generating proxy... Enter Certificate password: Contacting CS... Checking DN /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu Username is jdoe Creating proxy for jdoe@wenmr_user (/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu) Requested adding a VOMS extension but no VOMS attribute defined for group wenmr_user Uploading proxy for wenmr_user... Uploading wenmr_user proxy to ProxyManager... Loading user proxy Uploading proxy on-the-fly Cert file /home/jdoe/.globus/usercert.pem Key file /home/jdoe/.globus/userkey.pem Loading cert and key User credentials loaded Uploading... Proxy uploaded Proxy generated: subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu/CN=0123456789 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu timeleft : 23:59:58 DIRAC group : wenmr_user rfc : True path : /tmp/x509up_u0 username : jdoe properties : LimitedDelegation, GenericPilot, Pilot, NormalUser Proxies uploaded: DN | Group | Until (GMT) /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | access.egi.eu_user | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | fedcloud_user | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | access.egi.eu_admin | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | wenmr_user | 2021/09/14 23:54 As a result of this command a user proxy with the same validity period of the certificate is uploaded to the DIRAC ProxyManager service.\nFor checking the details of you proxy, run the following command:\n$ dirac-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu/CN=0123456789 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu timeleft : 23:59:26 DIRAC group : wenmr_user rfc : True path : /tmp/x509up_u0 username : jdoe properties : LimitedDelegation, GenericPilot, Pilot, NormalUser Managing simple jobs DIRAC commands Note dirac-wms-job-status To check the status of a job dirac-wms-job-delete To delete a job dirac-wms-job-logging-info To retrieve history of transitions for a DIRAC job dirac-wms-job-get-output To retrieve the job output dirac-wms-job-submit To submit a job DIRAC commands\nHave a look at the official command reference documentation for the complete list of the Workload Management commands.\nIn general, you can submit jobs, check their status, and retrieve the output. For example:\nCreate a simple JDL file (test.jdl) to submit the job:\n[ JobName = \"Simple_Job\"; Executable = \"/bin/ls\"; Arguments = \"-ltr\"; StdOutput = \"StdOut\"; StdError = \"StdErr\"; OutputSandbox = {\"StdOut\",\"StdErr\"}; ] Submit the job:\ndirac-wms-job-submit test.jdl JobID = 53755998 Check the job status:\n$ dirac-wms-job-status 53755998 JobID=23844073 Status=Waiting; MinorStatus=Pilot Agent Submission; Site=ANY; $ dirac-wms-job-status 53755998 JobID=53755998 Status=Done; MinorStatus=Execution Completed; Site=EGI.NIKHEF.nl; Site=EGI.HG-08-Okeanos.gr; Retrieve the outputs of the job (when the status is Done):\n$ dirac-wms-job-get-output --Dir joboutput/ 53755998 Job output sandbox retrieved in joboutput/53755998/ Jobs with Input Sandbox and Output Sandbox In most cases the job input data or executable files are available locally and should be transferred to the grid to run the job. In this case the InputSandbox attribute can be used to move the files together with the job.\nCreate the InputAndOuputSandbox.jdl\nJobName = \"InputAndOuputSandbox\"; Executable = \"testJob.sh\"; StdOutput = \"StdOut\"; StdError = \"StdErr\"; InputSandbox = {\"testJob.sh\"}; OutputSandbox = {\"StdOut\",\"StdErr\"}; Create a simple shell script (testJob.sh)\n#!/bin/bash /bin/hostname /bin/date /bin/ls -la After creation of JDL file the next step is to submit the job, using the command:\ndirac-wms-job-submit InputAndOuputSandbox.jdl JobID = XXXXXXXX List of supported VOs acc-comp.egi.eu, beapps, compchem, eiscat.se, eli-laser.eu, eli-beams.eu, eng.vo.ibergrid.eu, enmr.eu, fedcloud.egi.eu, hungrid, km3net.org, lofar, opencoast.eosc-hub.eu, see, training.egi.eu, virgo, vlemed, vo.formation.idgrilles.fr, vo.plgrid.pl, vo.access.egi.eu, auger, biomed, bitp, eng_cloud and breakseq_cloud More details JDL language and simple jobs submission: JDLs and Job Management Basic Submitting Parametric jobs, using DIRAC API: Advanced Job Management Past tutorials Technical Support DIRAC User Guide: https://dirac.readthedocs.io/en/latest/UserGuide/\nFor technical issues and bug reports, please submit a ticket in GGUS, in Assign to support unit, indicate:\nEGI Services and Service Components \u003e Workload Manager (DIRAC).\n","categories":"","description":"Distribute and manage workloads in the EGI infrastructure\n","excerpt":"Distribute and manage workloads in the EGI infrastructure\n","ref":"/users/compute/orchestration/workload-manager/","tags":"","title":"EGI Workload Manager"},{"body":"GPU resources on EGI Cloud GPUs resources are available on selected providers of the EGI Cloud. These are available as specific flavours that when used to instantiate a Virtual Machine will make the hardware available to the user.\nThe table below summarises the available options:\nSite VM configuration options Flavors Supported VOs with GPUs Access conditions More information IISAS-FedCloud up to 8 NVIDIA Tesla K20m g1.c08r30-K20m, g1.c16r60-2xK20m vo.access.egi.eu, eosc-synergy.eu, enmr.eu, training.egi.eu Sponsored access for limited testing, conditions to be negotiated for long-term usage IFCA-LCG2 up to 80 NVIDIA T4, up to 20 NVIDIA V100 Pay-per-use IFCA-LCG2 Documentation CESNET-MCC up to 2 NVIDIA Tesla T4, up to 4 NVIDIA A40, up to 2 GeForce RTX 1080/2080 (experimental use only) hpc.8core-64ram-nvidia-1080-glados, hpc.19core-176ram-nvidia-1080-glados, hpc.19core-176ram-nvidia-2080-glados, hpc.32core-256ram-nvidia-t4-single-gpu, hpc.64core-238ram-nvidia-t4, hpc.64core-512ram-nvidia-t4, meta-hdm.16core-120ram-nvidia-a40, meta-hdm.64core-485ram-nvidia-a40-quad vo.clarin.eu, biomed, eosc-synergy.eu, peachnote.com, cryoem.instruct-eric.eu, fusion, icecube, vo.carouseldancing.org, vo.pangeo.eu, vo.neanias.eu, umsa.cerit-sc.cz, vip, vo.notebooks.egi.eu, vo.emphasisproject.eu, vo.inactive-sarscov2.eu Sponsored, conditions to be negotiated CESNET-MCC Documentation IN2P3-IRES up to 1 NVIDIA Tesla T4 per VM, up to 1 NVIDIA A40 per VM, up to 4 GeForce RTX 2080 per VM (experimental use only) g1.xlarge-4xmem, g2.xlarge-4xmem, g4.xlarge-4xmem biomed, vo.emphasisproject.eu, vo.france-grilles.fr Sponsored, conditions to be negotiated Access to GPU resources on EGI Cloud Check whether you belong to one of the supported Virtual Organisations (VOs). If you are not sure what VO to join, request access to the pilot VO vo.access.egi.eu by visiting the enrollment URL with your Check-In account. More information is available in the Check-in section.\nGPUs sites can be accessed in different ways: via site-specific dashboards and endpoints or via common federated-cloud services like the OpenStack Horizon dashboards, VMOps Dashboard, or a cloud orchestrator.\nIt is also possible to use the FedCloud Client for command-line access. Below is an example on how to use the fedcloud command to show the GPU properties of the available GPU flavors on all sites for the specific VO in the command:\nfedcloud openstack flavor list --long --site ALL_SITES --vo vo.access.egi.eu --json-output | \\ jq -r 'map(select(.\"Error code\" == 0)) | map(.Result = (.Result| map(select(.Properties.\"Accelerator:Type\" == \"GPU\")))) | map(select(.Result | length \u003e 0))' Site-specific dashboards and endpoints are described in the following table:\nSite Openstack Horizon dashboard Keystone endpoint IISAS-FedCloud https://cloud.ui.savba.sk https://cloud.ui.savba.sk:5000/v3/ IFCA-LCG2 https://portal.cloud.ifca.es https://api.cloud.ifca.es:5000/ CESNET-MCC https://dashboard.cloud.muni.cz https://identity.cloud.muni.cz/ A VM image with pre-installed NVIDIA driver and Docker is available at AppDB. Some VOs (vo.access.egi.eu, eosc-synergy.eu) have the image included in the VO image list.\n","categories":"","description":"GPU resources in the EGI Cloud\n","excerpt":"GPU resources in the EGI Cloud\n","ref":"/users/compute/cloud-compute/gpgpu/","tags":"","title":"GPUs"},{"body":"Overview One of the challenges for researchers in recent years is to manage an ever increasing amount of compute and storage services, which then form ever more complex end user applications or platforms.\nTo address this need, EGI provides several orchestrators that facilitate the manangement of your workload using resources on the federation. These tools have different levels of abstractions and features. See below to understand which to choose (or gets used automatically) in specific scenarios:\nService Name Workload Type Use-Case Infrastructure Manager VMs, containers, storage Used to run workloads on a single IaaS Cloud provider. Elastic Cloud Compute Cluster VMs, containers, storage Used when you need to run workloads on clusters that can be elastically scaled and potentially span more than one IaaS Cloud provider. Workload Manager Jobs Used to efficiently distribute, manage, and monitor computing workloads. Dynamic On-Demand Analysis Service Containers, storage (caches) Used when you need to process your data either interactively or via a batch system. The following sections offer more details about each of these orchestrators.\n","categories":"","description":"Compute resource orchestration in the EGI infrastructure\n","excerpt":"Compute resource orchestration in the EGI infrastructure\n","ref":"/users/compute/orchestration/","tags":"","title":"Compute Orchestration"},{"body":"Overview This tutorial describes the EGI Data Transfer using FTS transfers services and WebFTS. In the following paragraphs you will learn how to:\nuse the FTS command-line client use the WebFTS web interface to perform data transfers between a Grid storage and object storage or between two object storage.\nWarning This procedure has been tested with the FTS client 3.11. Older version do not support all the options necessary. To install the latest version please add the FTS3 Production repository to your configuration and update the client Prerequisites As first step please make sure that you have installed the FTS client as described in Data Transfer, and in particular Clients for the command-line FTS and to have your certificate installed in your browser to use WebFTS browser based client.\nTo access services and resources in the EGI Federated Cloud, you will need:\nAn EGI Check-in account, you can sign up here Enrollment into a Virtual Organisation (VO) that has access to the services and resources you need An Object Storage for which you need to have all the credentials available (any S3 compatible storage should work) Permission to add the Object Storage credential to the FTS server or alternatively for this operation you may contact support support at egi.eu. FTS client usage Step 1 Configuration check and Proxy creation For this two steps please refer to the “Data transfer with grid storage” tutorial.\nStep 2 Find the storage As for the “Data transfer with grid storage” tutorial you can look for the available storage on VAPOR service while the Object Store can be one created as described in the Object Storage section or trough a provider such as Amazon, Azure, etc\nStep 3 Add the Object Storage credential to the FTS server Following is an example of the command that can be used to add the Object Store credential to the FTS server. The fist step is to register the Object Storage. The name of the storage, is S3: + the domain part of the URL (for example https://s3.cl2.du.cesnet.cz -\u003e S3:s3.cl2.du.cesnet.cz)\n$ curl -E \"${X509_USER_PROXY}\" \\ --cacert \"${X509_USER_PROXY}\" \\ --capath \"/etc/grid-security/certificates\" \\ https://fts3devel01.cern.ch:8446/config/cloud_storage \\ -H \"Content-Type: application/json\" \\ -X POST \\ -d '{\"storage_name\":\"S3:s3.cl2.du.cesnet.cz\"}' And then, add the keys, so the requests can be signed.\n$ curl -E \"${X509_USER_PROXY}\" \\ --cacert \"${X509_USER_PROXY}\" \\ --capath \"/etc/grid-security/certificates\" \\ \"https://fts3devel01.cern.ch:8446/config/cloud_storage/S3:s3.cl2.du.cesnet.cz\" \\ -H \"Content-Type: application/json\" \\ -X POST \\ --data @config.json Where config.json is a JSON file with the following content:\n{ \"vo_name\": \"/dteam/Role=NULL/Capability=NULL\", \"access_key\": \"ACCESS_KEY\", \"secret_key\": \"SECRET_KEY\" } Where ACCESS_KEY and SECRET KEY are the corresponding key necessary to access the object storage. See also the S3 Support pages on the FTS docs pages.\nStep 4 Transfer between a grid storage and an object storage For the grid storage to use please follow the details described in the section “Find the storage” of the “Data transfer with grid storage” tutorial. In the following examples, an object storage available in s3://s3.cl2.du.cesnet.cz/ with an already available bucket is used. To manage the object storage is possible to use any compatible tool. Following will be only shown an example of transfer.\n$ fts-transfer-submit --s3alternate \\ -s https://fts3-public.cern.ch:8446 \\ https://dc2-grid-64.brunel.ac.uk/dpm/brunel.ac.uk/home/dteam/1M \\ s3://s3.cl2.du.cesnet.cz/bucket-name/1M.3 247b7ca2-4c4d-11ec-84d0-fa163e5dcbe0 The command returns a job ID that we can use to check the status of the transfer itself:\n$ fts-transfer-status -d \\ -s https://fts3-public.cern.ch:8446 \\ 247b7ca2-4c4d-11ec-84d0-fa163e5dcbe0 FINISHED Step 5 Transfer between two object storage We can also use the data transfer service to perform transfers between two object storage. In this case the transfer will be controlled by the FTS service you can use a command like:\n$ fts-transfer-submit --s3alternate \\ -s https://fts3-public.cern.ch:8446 \\ s3://s3.cl2.du.cesnet.cz/bucket-name/1M.3 \\ s3://s3.cl2.du.cesnet.cz/bucket-name/A c1d4a8e6-4c81-11ec-8926-fa163e5dcbe0 In this case too we can verify the status of the transfer with the same command as before using the new job ID.\n$ fts-transfer-status -d \\ -s https://fts3-public.cern.ch:8446 \\ c1d4a8e6-4c81-11ec-8926-fa163e5dcbe0 FINISHED Step 6 Web interface An alternative way to check the status of the job is to use the FTS web interface to see using one the FTS servers and appending the job ID returned from one of the previous examples. For instance the link below will show the status of the specified job:\nhttps://fts-public-003.cern.ch:8449/fts3/ftsmon/#/job/247b7ca2-4c4d-11ec-84d0-fa163e5dcbe0 ","categories":"","description":"Use EGI Data transfer to handle data in object storage\n","excerpt":"Use EGI Data transfer to handle data in object storage\n","ref":"/users/tutorials/data-transfer-object-storage/","tags":"","title":"Data transfer with object storage"},{"body":"The Dynamic DNS service provides a unified, federation-wide Dynamic DNS support for VMs in EGI infrastructure. Users can register their chosen meaningful and memorable DNS hostnames in given domains (e.g. my-server.vo.fedcloud.eu) and assign to public IPs of their servers.\nBy using Dynamic DNS, users can host services in EGI Cloud with their meaningful service names, can freely move VMs from sites to sites without modifying server/client configurations (federated approach), can request valid server certificates in advance (critical for security) and many other advantages.\nA short demonstration video is available at fedcloud.eu YouTube channel.\nDynamic DNS GUI portal The Dynamic DNS offers a web GUI portal where users can login using their Check-in credentials. For doing so, click on the Login link (top left) and then click on the egi button.\nOnce logged in, you will be presented with the following page:\nTo register a new DNS host name:\nclick on Overview and then on Add Host.\nType in the hostname you’d like to register and select the domain to use.\nThe portal will then show you a secret than can be used for updating the host ip whenever needed. Note it down so you can use it later.\nFrom the VM you’d like to assign the name to, run a command like follows:\ncurl \"https://\u003chostname\u003e:\u003csecret\u003e@nsupdate.fedcloud.eu/nic/update\" where \u003chostname\u003e is the full hostname generated before, e.g. myserver.fedcloud-tf.fedcloud.eu and \u003csecret\u003e is the secret generated in the previous step. You can add that as a boot command in your cloud-init configuration:\n#cloud-config runcmd: - [ curl, \"https://\u003chostname\u003e:\u003csecret\u003e@nsupdate.fedcloud.eu/nic/update\" ] You can also manually edit your registered hostnames in the Overview menu by clicking on the hostname you’d like to manage\nNote Hostnames/IP addresses are not expired so no need to refresh IP addresses if no changes, it is enough to run once. You can add the following command curl https://HOSTNAME:SECRET@nsupdate.fedcloud.eu/nic/update to cloud-init as described above to assign hostname automatically at VM start\nDNS server set Time-to-Live (max time for caching DNS records) to 1 min for dynamic DNS, but MS Windows seems to not respect that. You can clear DNS cache in Windows with ipconfig /flushdns command with Administrator account\nAPI Dynamic DNS update server uses dydns2 protocol, compatible with commercial providers like dyn.com, and noip.com. The API is specified as follows:\nGET /nic/update?hostname=yourhostname\u0026myip=ipaddress Host: nsupdate.fedcloud.eu Authorization: Basic base64-encoded-auth-string User-Agent: where:\nbase64-encoded-auth-string: base64 encoding of username:password username: your hostname password: your host secret hostname in the parameter string can be omitted or must be the same as username myip in the parameter string if omitted, the IP address of the client performing the GET request will be used Security For updating IP address, only hostname and its secret are needed. No user information is stored on VM in any form for updating IP.\nNS-update server uses HTTPS protocol, hostname/secret are encrypted as data and not visible during transfer so it is secure to use the update URL\nNS-update portal does not store host secret in recoverable form. If you forget the secret of your hostname, simply generate new one via “Show configuration” button in the host edit page. The old secret will be invalid.\nSupport Support for Dynamic DNS service is provided via EGI Helpdesk in “Dynamic DNS” support unit, where users can ask questions, report issues or make requests for additional domains for specific projects or user commnunities.\n","categories":"","description":"Dynamic domain names for VMs in the EGI Cloud\n","excerpt":"Dynamic domain names for VMs in the EGI Cloud\n","ref":"/users/compute/cloud-compute/dynamic-dns/","tags":"","title":"Dynamic DNS"},{"body":"Alarms Alarms are automatically generated notifications created by the Service Monitoring and are handled from within the Operations Portal dashboard.\nHandling alarms When an alarm is generated, the site administrators have 24 hours to start acting on the issue. If ROD spots an alarm, he can notify the site’s administrators about the problem.\nIf the problem is fixed within 24 hours and the solution is tested by the Service Monitoring (the alarm’s color turns green in the Dashboard), then ROD has to make sure that the results are not flapping and can close alarm without any other action. If the problem cannot be fixed within 24 hours, and the site administrators put the service into an unscheduled downtime, ROD should just wait until the problem is fixed or until the downtime is over. No other action is necessary. If the problem cannot be fixed within 24 hours and the administrators don’t put the service into a downtime, then a ticket must be issued. The procedure is described below in the Tickets section. If the service is in downtime and the problem is fixed (as verified by the Service Monitoring), then ROD can close the alarm. If the downtime is over and the problem is still present (e.g. if the administrators forgot to extend the downtime), then a ticket must be issued. If an alarm is raised for a service that has its monitoring status set to OFF in EGI Configuration Database (also visible in the Dashboard in the Nodes box or in the alarm row as Node status) then ROD should not open a ticket. The alarm can be cleared even if it is marked red by pressing the lightning icon and giving an explanation. For handling tickets during public holidays, see below. There is also a video tutorial on handling alarms available.\nTickets In contrast with alarms, which are mere notifications, tickets are created manually. They are used to report problems to the responsible support units. Additionally, they allow to track the actions taken in order to resolve the issue.\nCreating tickets Ticket creation occurs when the age of an alarm in an error state has passed 24 hours, whether or not a site has already made some action on the alarm. A ticket has to be created from the Operations Portal. In order to actually create a ticket, click on the double arrow in the upper left corner next to the NGI name. That opens the drop down box with information on the site. Then, open the New NAGIOS alarms drop down box. Click the “T+” icon to create the ticket.\nRefer to the Dashboard How-to if you need a more detailed guide.\nIf more than one alarm should be handled by the same ticket, proceed as follows:\nCreate a ticket for one of the alarms. Open the Assigned Alarms drop-down box. Click on the mask icon next to the alarm identifier. A window will open in which you can select the alarms to be masked. If an alarm, which is masked by another alarm, remains in “critical” condition because of another (unrelated) problem, you can unmask it by clicking on the mask icon again and close the ticket for the solved alarms.\nFill in the relevant information in the ticket section. If there was information in the site notepad, ensure that the ticket information reflects that information. Also ensure that the TO: select boxes, and FROM: and SUBJECT: fields are all correct. Generally, a ticket should go to all of site, NGI and ROD. Press the Submit button and a pop up window will appear confirming that the ticket was correctly submitted. Your ticket has now been assigned a Helpdesk ID, but also an internal (hidden) Dashboard ID, which means that if you create a ticket through Dashboard, you have to close it through Dashboard as well. If you close a ticket opened through Dashboard in the Helpdesk, it will remain open in Dashboard! Creating tickets without an alarm It is also possible to create a ticket for a site without an alarm. This can happen if there is an issue with one of the tools that does not create an alarm in Dashboard. In this case, click on the “T+” icon in the upper right corner of the site box - the one with “Create a ticket (without an alarm)” tooltip, and fill in the appropriate fields as when creating a ticket for an alarm.\nTicket content templates The email is addressed to the corresponding NGI, together with the site and ROD. To view the list of NGI email addresses, click the Regional List link in the Dashboard menu.\nGenerally, you should not remove any content from the template, but you are free to add any information you think the site might find helpful in any of the three editing fields (Header content, Main content, and Footer content).\nChanging the state of and closing a ticket When the state of an alarm for a site with an open ticket changes to OK, then the ticket associated with that alarm can be updated in the Dashboard. Do this by clicking Update for the ticket in the Tickets drop down. Now change Escalate to Problem solved and fill in any information about how the problem has been solved. Clicking Update will then close the ticket in both the Helpdesk and the Dashboard. If the Nagios alarm is in an unstable state, and the site has not responded to the problem in 3 days then a 2nd email can be sent to the site by updating the Escalate field to 2nd step. If a new failure is detected for the site, the existing ticket should not be modified (though the deadline can be extended) but a new ticket should be submitted for this new problem. If the site’s problem can not be fixed in 3 days from the 2nd step of the escalation procedure then escalate the ticket to Political procedure. This means that the NGI manager will contact both EGI Operations and the site to negotiate about suspending the site. Sites with multiple tickets open When opening a ticket against a site with existing tickets ROD should consider that these problems may be linked or dependant on pending solutions. If the problem is different but maybe linked the expiry dates for each ticket should be synchronized to the latest date.\nAlso consider masking new problems with an old ticket.\nHandling alarms and tickets during weekends and public holidays Due to the fact that weekends are not considered working days, it is noted that ROD teams do not have any responsibilities during weekends and that RODs should ensure that tickets do not expire during weekends. The alarm age does not increase during the weekend.\nCurrently there is no automatic mechanism for handling ticket expiration over public holiday periods, because they differ among countries. If some of the sites the ROD team is in charge of are located in another country, the ROD is encouraged to get them to announce their public holidays, so that ticket expiration can be set accordingly. (Correspondingly, ROD operators also have no duties when they are on public holidays.) The ROD can edit the ticket’s expiration day by clicking the “T+” (Edit Ticket) icon. The value is set to 3 days by default.\nPlease note that ROD is not requested to announce their national holidays to the EGI Operations team. However, the last day before a public holiday, ROD is requested to check\nif there are any tickets that are to be expired during the holiday and change their expiration date; if there are any alarms that will pass the 72 hour period during the holidays and handle them properly in advance. Workflow and escalation procedure The workflow and escalation procedures are documented in more detail at PROC01 Infrastructure Oversight escalation.\n","categories":"","description":"How the ROD should deal with alarms and tickets using the dashboard.","excerpt":"How the ROD should deal with alarms and tickets using the dashboard.","ref":"/providers/rod/alarms-tickets/","tags":"","title":"Handling alarms and tickets"},{"body":"Once the site services are registered in GOCDB (and flagged as \"monitored\") they will appear in the EGI service monitoring tools. EGI will check the status of the services. Check if your services are present in the EGI service monitoring tools and passing the tests; if you experience any issues (services not shown, services are not OK...) please contact back EGI Operations or your reference Resource Infrastructure.\nExtra checks for your installation:\nCheck in ARGO-Mon2 that your services are listed and are passing the tests. If all the tests are OK, your installation is already in good shape.\nCheck that you are publishing cloud information in your site BDII: :\nldapsearch -x -h \u003csite bdii host\u003e -p 2170 -b Glue2GroupID=cloud,Glue2DomainID=\u003cyour site name\u003e,o=glue Check that all the images listed in the AppDB for the VOs you support (e.g. AppDB page for fedlcoud.egi.eu VO) are listed in your BDII. This sample query will return all the template IDs registered in your BDII: :\nldapsearch -x -h \u003csite bdii host\u003e -p 2170 -b Glue2GroupID=cloud,Glue2DomainID=\u003cyour site name\u003e,o=glue objectClass=GLUE2ApplicationEnvironment GLUE2ApplicationEnvironmentRepository Try to start one of those images in your cloud. You can do it with [onetemplate instantiate]{.title-ref} or OCCI commands, the result should be the same.\nExecute the site certification manual tests against your endpoints.\nCheck in the accounting portal that your site is listed and the values reported look consistent with the usage of your site.\n","categories":"","description":"Validate your installation\n","excerpt":"Validate your installation\n","ref":"/providers/cloud-compute/validation/","tags":"","title":"Installation Validation"},{"body":"Document control Property Value Title MAN06 Failover for MySQL grid-based services Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Implementing failover of MySQL grid-based services. Owner SDIS team Introduction Several critical grid services such as the VO Management Service (VOMS) server represent single points of failure in a grid infrastructure. When unavailable, a user can no longer access to the infrastructure since it is prevented from issuing new proxies, or is no longer able to access to the physical location of his data.\nHowever, those services rely on MySQL backends which opens a window to replicate the relevant databases to different / backup services which could be used when the primary instances are unavailable. The MySQL DB replication process is one of the ways to get scalability and higher availability.\nArchitecture In this document we propose to follow a Master-Slave architecture for the MySQL replication, consisting in keeping DB copies of a main host (MASTER) in a secondary host (SLAVE). The slave host will only have read access to the Database entries.\nSecurity For better availability, it is preferable to deploy the Master and the Slave services in different geographical locations, which normally means exposing the generated traffic to the internet. In that case, you will have to find a mechanism to encrypt the communication between the two hosts.\nIn this document, we propose to use Stunnel:\nStunnel is a free multi-platform computer program, used to provide universal TLS/SSL tunneling service. Stunnel can be used to provide secure encrypted connections for clients or servers that do not speak TLS or SSL natively. It runs on a variety of operating systems, including most Unix-like operating systems and Windows. Stunnel relies on a separate library such as OpenSSL or SSLeay to implement the underlying TLS or SSL protocol. Stunnel uses Public-key cryptography with X.509 digital certificates to secure the SSL connection. Clients can optionally be authenticated via a certificate too For more references, please check www.stunnel.org There are other possibilities for encryption, like enabling SSL Support directly in MySQL, but these approach was not tested. Details can be obtained here. MySQL replication Assumptions We are assuming that both grid services instances were previously installed and configured (manually or via YAIM) so that they support the same VOs.\nGeneric information Description Value MASTER hostname server1.domain.one MASTER username root MASTER mysql superuser root MASTER mysql replication user db_rep SLAVE hostname server2.domain.two SLAVE username root SLAVE mysql superuser root DB to replicate DB_1 DB_2 … Setup the MySQL MASTER for replication Step 1: Install stunnel. It is available in the SL5 repositories. $ yum install stunnel (...) $ date; rpm -qa | grep stunnel Wed Jun 15 16:00:23 WEST 2011 stunnel-4.15-2.el5.1 Step 2: Configure stunnel (via /etc/stunnel/stunnel.conf) to: Accept incoming connections on port 3307, and allow to connect to port 3306 Use the server X509 certificates to encrypt data $ cat /etc/stunnel/stunnel.conf # Authentication stuff verify = 2 CApath = /etc/grid-security/certificates cert = /etc/grid-security/hostcert.pem key = /etc/grid-security/hostkey.pem # Auth fails in chroot because CApath is not accessible #chroot = /var/run/stunnel #debug = 7 output = /var/log/stunnel.log pid = /var/run/stunnel/master.pid setuid = nobody setgid = nobody # Service-level configuration [mysql] accept = 3307 connect = 127.0.0.1:3306 Step 3: Start the stunnel service and add it to rc.local $ mkdir /var/run/stunnel $ chown nobody:nobody /var/run/stunnel $ stunnel /etc/stunnel.conf $ echo 'stunnel /etc/stunnel.conf' \u003e\u003e /etc/rc.local Step 4: Setup the firewall to allow connections from the SLAVE instance (server2.domain.two with IP XXX.XXX.XXX.XXX) on port 3307 TCP # MySQL replication -A INPUT -s XXX.XXX.XXX.XXX -m state --state NEW -m tcp -p tcp \\ --dport 3307 -j ACCEPT Step 5: Configure MySQL (via /etc/my.cnf) setting up the Master server-id (usually 1), and declaring the path for the MySQL binary log and the DBs to be replicated. Please make sure that the path declared under log-bin has the mysql:mysql ownerships, and that the binary log exists. $ cat /etc/my.cnf (...) [mysqld] server-id=1 log-bin = /path_to_log_file/log_file.index binlog-do-db=DB_2 binlog-do-db=DB_1 Step 6: Restart MySQL $ /etc/init.d/mysqld restart Step 7: Add a specific user for the MySQL replication mysql \u003e GRANT REPLICATION SLAVE ON *.* TO 'db_rep'@'127.0.0.1' \\ IDENTIFIED BY '16_char_password'; Step 8: Backup the databases using the following command. Please note tha the option --master-data=2 writes a comment on dump.sql that shows the log file and the ID to be used on the Slave setup. This option also locks all the tables while they are being copied, avoiding problems. $ mysqldump -u root -p --default-character-set=latin1 \\ --master-data=2 --databases DB_1 DB_2 \u003e dump.sql Setup the MySQL SLAVE for replication Step 1: Configure stunnel (via /etc/stunnel/stunnel.conf) to:\nAccept incoming SSL connections on port 3307, and allow to connect to server1.domain.one on port 3307 Use the server X509 certificates to encrypt data $ cat /etc/stunnel/stunnel.conf # Authentication stuff verify = 2 CApath = /etc/grid-security/certificates cert = /etc/grid-security/hostcert.pem key = /etc/grid-security/hostkey.pem # Auth fails in chroot because CApath is not accessible #chroot = /var/run/stunnel #debug = 7 output = /var/log/stunnel.log pid = /var/run/stunnel/master.pid setuid = nobody setgid = nobody # Use it for client mode client = yes # Service-level configuration [mysql] accept = 127.0.0.1:3307 connect = server1.domain.one:3307 Step 2: Start stunnel and add it to rc.local $ mkdir /var/run/stunnel $ chown nobody:nobody /var/run/stunnel $ stunnel /etc/stunnel.conf $ echo 'stunnel /etc/stunnel.conf' \u003e\u003e /etc/rc.local Step 3: Configure MySQL to include the SLAVE server-id (typically 2) and the replicated databases $ cat /etc/my.cnf (...) [mysqld] server-id=2 replicate-do-db=DB_1 replicate-do-db=DB_2 Step 4: Restart MySQL and insert the dump.sql created on the Master $ /etc/init.d/mysql restart $ mysql -u root -p \u003c dump.sql Step 5: Start the Slave logging in the mysql of slave, and running the following queries, changing the values xxxxx and yyyyy by the values on top of dump.sql file: mysql \u003e CHANGE MASTER TO MASTER_HOST='127.0.0.1', MASTER_PORT=3307, \\ MASTER_USER='db_rep', MASTER_PASSWORD='16_char_password', \\ MASTER_LOG_FILE='xxxxx', MASTER_LOG_POS=yyyyy; mysql \u003e SLAVE START; Step 6: Your replication should be up and running. In case of troubles, check the Troubleshooting section. Troubleshooting On the Slave Check if you can connect to the Master on port 3307 from the Slave #root@server2.domain.two]$ telnet server1.domain.one 3307 Check the /var/log/stunnel.log on the Slave to see if stunnel is working. For established connections, you should find a message like: 2011.05.30 11:57:45 LOG5[9000:1076156736]: mysql connected from XXX.XXX.XXX.XXX:PORTY 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=1, /DC=COUNTRY/DC=CA/CN=NAME 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=0, /DC=COUNTRY/DC=CA/O=INSTITUTE/CN=host/HOSTNAME Check the MySQL process list. You should get an answer like the one bellow: mysql\u003e SHOW PROCESSLIST; +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ | Id | User | Host | db | Command | Time | State | Info | +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ | 1 | system user | | NULL | Connect | 1236539 | Waiting for master to send event | NULL | | 2 | system user | | NULL | Connect | 90804 | Slave has read all relay log; waiting for the slave I/O thread to update it | NULL | +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ Check the status of the slave mysql\u003e SHOW SLAVE STATUS; +----------------------------------+-------------+------------------+---------------------+ | Slave_IO_State | Master_Port | Master_Log_File | Read_Master_Log_Pos | +----------------------------------+-------------+------------------+---------------------+ | Waiting for master to send event | 3307 | mysql-bin.000001 | 126167682 | +----------------------------------+-------------+------------------+---------------------+ On the Master Check the /var/log/stunnel.log on the Slave to see if stunnel is working. For established connections, you should find a message like: 2011.05.30 11:57:45 LOG5[9000:1076156736]: mysql connected from XXX.XXX.XXX.XXX:PORTY 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=1, /DC=COUNTRY/DC=CA/CN=NAME 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=0, /DC=COUNTRY/DC=CA/O=INSTITUTE/CN=host/HOSTNAME Check if you have established connections from the Slave on port 3307 $ netstat -tapn | grep 3307 tcp 0 0 0.0.0.0:3307 0.0.0.0:* LISTEN 9000/stunnel tcp 0 0 XXX.XXX.XXX.XXX:3307 YYY.YYY.YYY.YYY:34378 ESTABLISHED 9000/stunnel Check the Master status on MySQL. You should get an answer like: mysql\u003e SHOW MASTER STATUS; +------------------+-----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+-----------+--------------+------------------+ | mysql-bin.000001 | 126167682 | DB_1 | | +------------------+-----------+--------------+------------------+ 1 row in set (0.00 sec) Check the MySQL process list. You should get an answer like the one bellow: mysql\u003e SHOW PROCESSLIST; +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ | Id | User | Host | db | Command | Time | State | Info | +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ | 2778 | ibrepifca | localhost:42281 | NULL | Binlog Dump | 1400477 | Has sent all binlog to slave; waiting for binlog to be updated | NULL | +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ The VOMS case A working example It is also possible to deploy a backup VOMS server with a MySQL replica of the main VOMS server. This will enable users to still start proxies even if the main VOMS server is down.\nThe VOMS Admin interface of the backup VOMS server should be switched off so that new users can only request registration via the main VOMS Admin Web interface.\nUser interfaces should be configured to use both VOMS servers\n$ voms-proxy-init --voms ict.vo.ibergrid.eu Enter GRID pass phrase: Your identity: /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=Goncalo Borges Creating temporary proxy .............................................................. Done Contacting voms01.ncg.ingrid.pt:40008 [/C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=voms01.ncg.ingrid.pt] \"ict.vo.ibergrid.eu\" Done Creating proxy .......................................................................................................................................................... Done Your proxy is valid until Thu Jun 16 07:27:31 2011 $ voms-proxy-init --voms ict.vo.ibergrid.eu Enter GRID pass phrase: Your identity: /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=Goncalo Borges Creating temporary proxy ............................................................. Done Contacting ibergrid-voms.ifca.es:40008 [/DC=es/DC=irisgrid/O=ifca/CN=host/ibergrid-voms.ifca.es] \"ict.vo.ibergrid.eu\" Done Creating proxy ............................................................................ Done Your proxy is valid until Thu Jun 16 07:27:37 2011 ","categories":"","description":"Implementing failover of MySQL grid-based services.","excerpt":"Implementing failover of MySQL grid-based services.","ref":"/providers/operations-manuals/man06_failover_for_mysql_grid_based_services/","tags":"","title":"MAN06 Failover for MySQL grid-based services"},{"body":"Document control Property Value Title MAN07 VOMS Replication Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement How to implement a MySQL VOMS server replication Owner SDIS team Introduction In this manual we will show you how to implement a MySQL VOMS server replication: you need one master server, on which you can perform writing operations, and you can have from 1 to “n” replica servers that will work in read-only mode. In such a scenario you can do a whatever intervention on one of the servers without breaking the service, i.e. proxies creation and grid-mapfile downloads: just the users registration and the usual VOs management operations might be forbidden during an intervention on the master server (because it is the only server in writing mode).\nThis failover procedure is simply based on MySQL replication therefore every MySQL setting is referred to the current MySQL version (5.0.77 in this moment)\nSettings on the MASTER SERVER In order to allow the replica server to read the master database, you have to create an user with which the slave will connect to the master. Suppose the replica hostname is vomsrep.cnaf.infn.it, the user is bonjovi and the password is always. What you have to launch on the master server is:\n$ mysql -p -e \"grant super, reload, replication slave, replication client \\ on *.* to bonjovi@'vomsrep.cnaf.infn.it' identified by 'always'\" ; Then for each DB (VO) you want to replicate, you have to assign the right permissions, by launching:\nmysql -p -e \"grant select, lock tables on voms_myvo.* to \\ bonjovi@'vomsrep.cnaf.infn.it'\" Eventually you have to modify the file /etc/my.cnf by adding the following lines into the section [mysqld]:\nlog-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 It is important that on the master server it is set server-id=1: it is the identification number that distinguish a master from its several slaves (each slave will have a unique number starting from 2)\nFor example, the content of my.cnf file may appear like this:\n# less /etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock user=mysql # Default to using old password format for compatibility with mysql 3.x # clients (those using the mysqlclient10 compatibility package). old_passwords=1 max_connections = 800 log-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 # Disabling symbolic-links is recommended to prevent assorted security risks; # to do so, uncomment this line: # symbolic-links=0 [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid At this point, you have to restart MySQL, by launching:\n$ service mysqld restart In order to check that on the master side the mechanism is working, you can launch for example:\nmysql\u003e show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 | 24844 | | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) Eventually, through the web interface, in the ACL section of each VO you want to replicate, add an entry granting all the permissions to the slave server:\nselect “a non VO member” from the menu fill in the replica server DN and a reference email address select “all” for the permissions and tick the “Propagate entry to children contexts” option In this way, when the slave server copies the DB, it will have the proper permissions on acting on the DB. Moreover, in order to avoid the sending of notification to the email address you filled in before, connect to the MySQL database and do the following:\nmysql\u003e use voms_myvo; mysql\u003e update admins set email_address=NULL where \\ email_address=\"what you filled before\"; Settings on the SLAVE SERVER Install a VOMS server as usual, configuring the VOs you want to replicate: keep in mind that every modification done on the slave DB breaks the replica mechanism, so that on this server disable the users registration, by setting the yaim variable:\nVOMS_ADMIN_WEB_REGISTRATION_DISABLE=true Then ask the VO managers to not perform any action on the slave server web interface.\nThen launch the following scripts:\nfirst_replica.sh for the first database you want to replicate or in the case it is the only one next_replicas.sh for the next databases (one database for each launch) For both the scripts, set the following variables:\nmaster_host, master_mysql_user, master_mysql_pwd that refers to the master server and to the user created on it mysql_username_admin and mysql_password_admin that refers to the slave Example:\nvoms_database=\"\" # VOMS database (leave unset) master_host=\"voms.cnaf.infn.it\" # Master hostname master_mysql_user=\"bonjovi\" # Master MySQL admin user for replication master_mysql_pwd=\"always\" # Master MySQL admin pass for replication user master_log_file=\"\" # Master LOG file (leave unset) master_log_pos=\"\" # Master LOG file (leave unset) mysql_username_admin=\"root\" # Slave MySQL admin username mysql_password_admin=\"secret\" # Slave MySQL admin pass With the launch of first-replica.sh, the file /etc/my.cnf will be properly written; if you need to replicate further databases, modify /etc/my.cnf adding the following lines related to the db you are replicating (similar to the first db you’ve replicated):\nreplicate-do-db=\u003cmaster_vo_db_name\u003e replicate-ignore-table=\u003cmaster_vo_db_name\u003e.seqnumber replicate-ignore-table=\u003cmaster_vo_db_name\u003e.realtime replicate-ignore-table=\u003cmaster_vo_db_name\u003e.transactions replicate-ignore-table=\u003cslave_vo_db_name\u003e.seqnumber replicate-ignore-table=\u003cslave_vo_db_name\u003e.realtime replicate-ignore-table=\u003cslave_vo_db_name\u003e.transactions Having set the variables in the way shown above, for replicating the first database the scripts launch syntax is the following:\n$ ./first_replica.sh --master-db=voms_myvo --db=voms_myvo In your /etc/my.cnf file you will find lines like the following:\n# Connection with master server-id=2 master-host=voms.cnaf.infn.it master-user=bonjovi master-password=always # Replicas settings replicate-do-db=voms_myvo replicate-ignore-table=voms_myvo.seqnumber replicate-ignore-table=voms_myvo.realtime replicate-ignore-table=voms_myvo.transactions replicate-ignore-table=voms_myvo.seqnumber replicate-ignore-table=voms_myvo.realtime replicate-ignore-table=voms_myvo.transactions Now you may want to replicate a second database, let’s say voms_hervo: therefore in my.cnf file add the following lines:\nreplicate-do-db=voms_hervo replicate-ignore-table=voms_hervo.seqnumber replicate-ignore-table=voms_hervo.realtime replicate-ignore-table=voms_hervo.transactions replicate-ignore-table=voms_hervo.seqnumber replicate-ignore-table=voms_hervo.realtime replicate-ignore-table=voms_hervo.transactions Modify the script next_replicas.sh in according to the VO parameters and launch it:\n$ ./next_replicas.sh --master-db=voms_hervo --db=voms_hervo When you finished to replicate all the desired VOs, in order to make active the database modifications, restart voms and voms-admin:\n$ /etc/init.d/voms-admin stop $ /etc/init.d/voms stop $ /etc/init.d/voms start $ /etc/init.d/voms-admin start Keep in mind that every modification done on the slave DB breaks the replica mechanism, so that on this server disable the users registration, by setting the yaim variable:\nVOMS_ADMIN_WEB_REGISTRATION_DISABLE=true And ask the VO managers to not perform any action on the slave server web interface.\n","categories":"","description":"How to implement a MySQL VOMS server replication","excerpt":"How to implement a MySQL VOMS server replication","ref":"/providers/operations-manuals/man07_voms_replication/","tags":"","title":"MAN07 VOMS Replication"},{"body":"What is it? The EGI Operations Portal is a central portal for supporting the operations and coordination of the EGI Infrastructure. It offers a bundle of different capabilities, such as:\nThe Broadcast tool: used to contact and inform the different actors of the project for specific problems or global announcements. The operations and security dashboards: to detect, track and follow-up problems and incident on the resource centers. The VO Management tools: to register, update, consult information about virtual communities. The Metrics module: to provide metrics and indicators about the different activities of the project linked to VOs, users. The SLA module: to provide information about performances for the cloud sites and SLA Violations for different group of services. Note Documentation for the Operations Portal is available in the EGI Wiki. ","categories":"","description":"Central portal supporting EGI infrastructure operations","excerpt":"Central portal supporting EGI infrastructure operations","ref":"/internal/operations-portal/","tags":"","title":"Operations Portal"},{"body":"What is it? Security Coordination improves the capabilities of local security activities for a safer federated infrastructure environment.\nThe EGI Computer Security Incident Response Team (EGI CSIRT) has the tools and the knowledge to run Security Coordination on behalf of the federation. The EGI CSIRT is a certified Trusted Introducer since 2015.\nSecurity Coordination is especially important in a federated environment, where incidents are often not isolated and can affect several service providers. A coordinated response is essential to minimize the impact of incidents and vulnerabilities.\nThis service provides:\nSecurity Operations Coordination - Central coordination of the security activities ensures that policies, operational security, and maintenance are compatible amongst all partners, improving availability and lowering access barriers for use of the infrastructure. Security Policy Coordination - The Security Policy Group (SPG) develops policies covering diverse aspects, including operational policies (agreements on vulnerability management, intrusion detection and prevention, regulation of access, and enforcement), incident response policies (governing the exchange of information and expected actions), participant responsibilities (including acceptable use policies, identifying users and managing user communities), traceability, legal aspects, and the protection of personal data. Software Vulnerability Group Coordination - The Software Vulnerability Group SVG aims to eliminate existing software vulnerabilities from the deployed infrastructure and prevent the introduction of new ones, and runs a process for handling software vulnerabilities reported. International Grid Trust Federation (IGTF) and EUGridPMA - Representation of EGI in IGTF and EUGridPMA. A common authentication trust domain is required to persistently identify all EGI participants. EGI Computer Security Incident Response Team (EGI CSIRT) expertise Security Incident Response Coordination - Coordination of incident response activities in collaboration with the Incident Response Task Force (EGI-CSIRT IRTF). Security monitoring - Monitoring services to check for security vulnerabilities and other security-related problems in the EGI production infrastructure. Tools for Security Service Challenge support - Security challenges are a mechanism to check the compliance of sites/NGIs/EGI with security requirements. Runs of Security Service Challenges need a set of tools that are used during various stages of the runs. ","categories":"","description":"Enhance local security for a safer global infrastructure\n","excerpt":"Enhance local security for a safer global infrastructure\n","ref":"/internal/security-coordination/","tags":"","title":"Security Coordination"},{"body":"Description of some of the Helpdesk workflows.\n","categories":"","description":"Workflows","excerpt":"Workflows","ref":"/internal/helpdesk/workflows/","tags":"","title":"Workflows"},{"body":"The OpenStack sites in the EGI Cloud that provide compute resources to run virtual machines (VMs) allow nearly everything to be done via an Application Programming Interface (API) or a command-line interface (CLI). This means that repetitive tasks or complex architectures can be turned into shell scripts.\nBut creating VMs happens so often in the EGI Cloud that tools were developed to capture the provisioning of these VMs, and allow users to recreate them in a flash, in a deterministic and repeatable way, using an Infrastructure-as-Code (IaC) approach.\nAutomating this activity will help researchers to:\nNot forget important configuration (e.g. the size and type of the hardware resources needed). Ensure the same steps are performed, in the same order (e.g. making sure the correct datasets are attached to each VM). Easily share scientific pipelines with collaborators. Make scientific applications cloud agnostic. To automate VM deployment, you can use any of the cloud orchestrators available in the EGI Cloud.\n","categories":"","description":"Use Infrastrucure-as-Code in the EGI Cloud\n","excerpt":"Use Infrastrucure-as-Code in the EGI Cloud\n","ref":"/users/compute/cloud-compute/automate/","tags":"","title":"Automating Deployments"},{"body":"This page is about the CVMFS service operated for EGI by RAL.\nOverview The CernVM-File System (CVMFS) provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics collaborations to deploy software on the worldwide distributed computing infrastructure used to run data processing applications. CVMFS is implemented as a POSIX read-only file system in user space. Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs. CernVM-FS uses outgoing HTTP connections only, thereby it avoids most of the firewall issues of other network file systems. It transfers data and metadata on demand and verifies data integrity by cryptographic hashes. CVMFS is actively used by small and large collaborations. In many cases, it replaces package managers and shared software areas on cluster file systems as means to distribute the software used to process experiment data.\nThis documentation is for the VO content managers.\nOfficial CVMFS pages CVMFS Documentation Q\u0026As and Discussion Forum Requesting the creation of a new repository In the case of a new repository for EGI, steps are described in PROC22.\nOnboarding new Content Managers Steps for a new VO Content Manager to be granted access to the Stratum-0.\nRequesting access Request access to the service sending an email to cvmfs-support@gridpp.rl.ac.uk In the email, include the following information:\nName of the VO or CVMFS repository. Distinguish Name (DN) from your X509 grid certificate. Mailing list All VO content managers should join the CVMFS-UPLOADER-USERS mailing list in JISCMAIL.\nDistributing new content To login to the service, make sure you have a valid X509 proxy (with the same DN provided in this step), and execute the following command:\n$ gsissh -p 1975 cvmfs-upload01.gridpp.rl.ac.uk If you are the Content Manager for more than one repository, you would need to specify explicit which account you want to login to:\n$ gsissh -p 1975 \u003cmyreposgm\u003e@cvmfs-upload01.gridpp.rl.ac.uk To copy data:\n$ gsiscp -P 1975 \u003csource\u003e cvmfs-upload01.gridpp.rl.ac.uk:\u003cdestination\u003e After login, you will find a single directory in the home directory:\n[myreposgm@cvmfs-uploader02 ~]$ ls cvmfs_repo Add to that directory the new content you want to distribute.\nFiles and directories cannot be distributed with CVMFS if they are not world-wide readable. You may want to ensure they have the right permissions with the following commands:\n$ find . -type d -exec chmod go+rx {} \\; $ find . -type f -exec chmod go+r {} \\; Building your software CVMFS is an infrastructure to distribute software world-wide. However, the uploader host should not be used for the purposes of building and compiling it prior to distribution.\nThe right approach is for you to have your own local building environment, and use the uploader host only to upload the new content for distribution.\nIf you have non-relocatable software, then you will need a /cvmfs/\u003cmyrepo\u003e/ directory on your building host. One option is to use an actual CVMFS client, so you have ready all the existing content being already distributed by CVMFS. By default, the /cvmfs/ directory on a CVMFS client host is read-only, but that can be solved using an ephemeral writable container.\n","categories":"","description":"Content (software) distribution in the EGI infrastructure\n","excerpt":"Content (software) distribution in the EGI infrastructure\n","ref":"/users/compute/content-distribution/","tags":"","title":"Content Distribution"},{"body":"Why joining the EGI Cloud? To support international communities supported by EGI (e.g. these research communities and applications or these research infrastructures in EOSC-hub or these business pilots in the EOSC Digital Innovation Hub. To participate in e-Infrastructure projects (H2020, EOSC) as an EGI compliant IaaS cloud provider. To participate in resource allocation and in pay-for-use campaigns run by EGI. To align access policies and operational model of your cloud with international good practices. To adopt best practices of multi-cloud federation for the benefit of your local users. Do I lose control on who can access my resources if I join federated cloud? No. EGI uses the concept of Virtual Organisation (VO) to group users. The resource provider has complete control on which VOs he wants to allow on its resources and which quotas or restrictions to assign to each VO. In the case of OpenStack, each VO is mapped to a regular OpenStack project that can be managed as any other and are isolated to other projects you may have configured in your deployment. Although not recommended, you can even restrict the automatic access of users within a VO and manually enable individual members.\nHow many components do I have to install? Depending on your cloud management framework and the kind of integration this will vary.\nIn general, the federation requires your cloud management framework to be configured to support Federated AAI with EGI Check-in. This may require changes in your current setup.\nOther components are designed to access your cloud management framework public APIs and do not require modification of your deployment. For OpenStack, these components can be run on a single VM that encapsulates them for convenience.\nWhich components of my cloud will interact with the federated cloud components? For OpenStack they are:\nKeystone Nova Glance Swift (optional) Users will also interact with:\nNeutron Cinder to perform their regular activities.\nHow will my daily operational activities change? For the most part daily operations will not change.\nA resource centre part of the EGI Federation, and supporting international communities, needs to provide support through the EGI channels. This means following up GGUS tickets. This includes requests from user communities and tickets triggered by failures detected by the monitoring infrastructure.\nA resource centre needs to maintain the services federated in EGI properly configured with the EGI AAI.\nThe resource centre will have to comply with the operational and security requirements. All the EGI policies aim at implementing service provisioning best practices and common requirements. EGI operations may conduct campaigns targeted to mitigate security vulnerabilities and to update unsupported operating system and software. These activities are part of the regular activities of a resource centre anyways (also for the non-federated ones). EGI and the Operations Centres coordinate these actions in order to have them implemented in a timely manner.\nIn summary, most of the site activities that are coordinated by EGI and the NGIs are already part of the work plan of a well-maintained resource centre, the additional task for a site manager is to acknowledge to EGI that the task has been performed.\n","categories":"","description":"Frequently Asked Questions\n","excerpt":"Frequently Asked Questions\n","ref":"/providers/cloud-compute/faq/","tags":"","title":"FAQ"},{"body":"How to handle issues during weekends and public holidays? Due to the fact that weekends and public holidays are not considered working days it is noted that ROD teams do not have any responsibilities during these days. RODs should ensure that in these days tickets do not expire and alarms will not age above 72h.\nWhat to do with alarms when node is not in production and is part of production site? It often happens that testing nodes on production sites are set as non-production. In such case Nagios monitoring system will send information about all nodes. As a result ROD will see on their dashboard alarms for non-production node. If it necessary to monitor such testing node it is recommended to put such non-production node in downtime.\nWhat to do when a sites have multiple alarms/ticket? When opening a ticket against a site with existing tickets ROD should consider that these problems may be linked or dependant on pending solutions. In such case ROD should use grouping mechanism to gather and assign alarms to one ticket rather than open a ticket for each alarm.\nIf the problem is different but maybe linked the expiry dates for each ticket should be synchronized to the latest date.\nHow to handle issues for site/node in downtime? Handling tickets for site/node in downtime When a ticket has been raised against a site that subsequently enters in downtime, the expiry date on the ticket can be extended.\nSites that are in downtime will still have monitoring switched on and therefore may appear to be failing tests but no alarms on Operations Portal will be raised against them. ROD must take care that when opening tickets to ensure that they don’t open tickets against sites in downtime.\nHandling alarms for site/node in downtime It often happens that a failure occurred generating a lot of alarms and then site manager decided to put site in Downtime. Getting these alarms OK may take more than 72h when the issue is escalated to Operations.\nROD should not create a ticket for sites/nodes in Downtime and is not obligated to deal with such alarms but it is recommended to close these alarms to avoid being escalated to Operations. In such case as a reason of closing NON-OK alarm ROD should put link to the downtime in the EGI Configuration Database.\nSite in downtime for more than a month If a site is in DOWNTIME for more than a month then it is advised that the site should go to the suspended status.\nWhat to do in case of accounting issue? In case of problems with accounting it is not recommended to suggest downtime at the second step of the escalation process for this test. Accounting service is not a functionality which is critical for users but it still need to be follow up.\nWatch out for flapping states You may want to wait for a second test to be run before closing an alarm which is in an OK status. This ensures that the OK result for that tests is stable. The waiting period is, of course, dependent on how long the test takes and how frequently it is checked.\nHow to handle the eu.egi.lowAvailability alarm? Go to procedure PROC04 Quality verification of monthly availability and reliability statistics.\n","categories":"","description":"FAQ concerning the ROD activity.","excerpt":"FAQ concerning the ROD activity.","ref":"/providers/rod/faq/","tags":"","title":"FAQ"},{"body":"Document control Property Value Title Accounting data publishing Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement How to publish accounting information for different middleware Owner SDIS team Introduction In this manual we will show you how to publish accounting information from different middleware.\nGeneral information Publishing with the APEL Client/SSM To start sending accounting records:\nregister each endpoint sending the accounting records to the central repository as ‘gLite-APEL’ endpoint in GOCDB with host DN information either HTCondorCE or ARC-CE endpoints it is needed to authorise the endpoint to use the ARGO Message Service (AMS) Changes in GOCDB can take up to 4 hours to make it to AMS. install the APEL client and APEL SSM on your publisher host and edit client.cfg and sender.cfg install the APEL parser relevant to your batch system on each CE and edit parser.cfg documentation for APEL here GitHub apel/apel GitHub apel/ssm The old ActiveMQ network was dismissed: have a look at the new settings to properly publish the accounting records via AMS configure the client to publish data from the start of the current month. Run the parser(s) and client. If there are no errors messages and the client says it has sent messages then wait a day and you should see summaries here If it doesn’t open a GGUS ticket for the APEL Team. Monitoring the accounting data publication In order to monitor the regular publication of accounting data, it is enough registering only one CE with the APEL service type.\nSee here the details about the related Nagios probes.\nPublishing summarized records or single ones Sites can send either but the preferred option is summaries. Larger sites are recommended to send summaries.\nThe frequency for sending aggregated/summary records to APEL database? We recommend sending data daily for all sites, whether sending summaries or individual records. I think this is in the interests of people who are using the portal so that what they see is accurate. The summaries are for a complete month or the current month so far.\nAuthorization and authentication Authorization and authentication is made by the host certificate (which is signed by a trusted CA from the ca_policy_core package). Certificate should be registered in GOCDB. The certificate must not have the X509 extension: Netscape Cert Type: SSL Server because the message brokers will reject it.\nARC ARC uses its own system to publish the accounting data through AMS, so please refer to the NorduGrid ARC 6 documentation:\nInformation relevant only for 6.4 ARC releases and beyond: Accounting-NG The old ActiveMQ network was dismissed. ARC 6.12 introduces new settings for publishing the accounting records via AMS. HTCondor-CE To collect and publish the accounting data you need to install the APEL software as explained in the general information section.\n","categories":"","description":"Publishing accounting information for different middleware.","excerpt":"Publishing accounting information for different middleware.","ref":"/providers/operations-manuals/man09_accounting_data_publishing/","tags":"","title":"MAN09 Accounting data publishing"},{"body":"The EGI Application Database (AppDB) includes a web GUI for management of Virtual Machines (VMs) on the federated infrastructure.\nThis GUI is available for a set of selected VOs. If your VO is not listed and you are interested in getting support, please open a ticket or contact us at support _at_ egi.eu.\nMain user features User identification with Check-in, with customised view of the VAs and resource providers based on the VO membership of the user. Management of VMs in topologies, containing one or more instances of a given VA. Attachment of additional block storage to the VM instances. Start/Stop VMs without destroying the VM (for all VMs of a topology or for individual instances within a topology) Single control of topologies across the whole federation. Quick start Log into the VMOps dashboard using EGI Check-in.\nClick on \"Create a new VM Topology\" to start the topology builder, this will guide you through a set of steps:\nSelect the Virtual Appliance you want to start, these are the same shown in the AppDB Cloud Marketplace, you can use the search field to find your VA;\nselect the VO to use when instantiating the VA;\nselect the provider where to instantiate the VA; and finally\nselect the template (VM instance type) of the instance that will determine the number of cores, memory and disk space used in your VM.\nNow you will be presented with a summary page where you can further customise your VM by:\nAdding more VMs to the topology\nAdding block storage devices to the VMs\nDefine contextualisation parameters (e.g. add new users, execute some script)\nClick on \"Launch\" and your deployment will be submitted to the infrastructure.\nThe topology you just created will appear on your \"Topologies\" with all the details about it, clicking on a VM of a topology will give you details about its status and IP. VMOps will create a default cloudadm user for you and create ssh-key pair for login (you can create as many users as needed with the contextualisation options of the wizard described above).\nVMOps was presented in one of the EGI Webinars in 2020. The indico page contains more details and there is also a video recording available on YouTube.\n","categories":"","description":"Use VMOPs Dashboard to monitor and manage VMs in the EGI Cloud\n","excerpt":"Use VMOPs Dashboard to monitor and manage VMs in the EGI Cloud\n","ref":"/users/compute/cloud-compute/monitor/","tags":"","title":"Monitoring and Management"},{"body":"What is it? The EGI Service Monitoring keeps an eye on the performance of the EGI services to quickly detect and resolve issues.\nThe service monitors the infrastructure by collecting data generated by functional probes. The raw data is merged into statistics and available through the user interface in a user-friendly way. It provides automated reporting tools with minimal development or operational effort for setting up monitoring.\nNote Documentation for Service Monitoring is available on the ARGO site. Accessing the monitoring information In order to access the information it’s required to have an X509 client certificate provided by an IGTF-accredited Certificate Authority.\nFor people not having access to an IGTF client certificate it’s possible to access Availability and Reliability information on the EGI ARGO page.\nService Monitoring endpoints Different service instances are available for different purposes:\nCertified sites Uncertified sites The endpoint for uncertified sites is using a certificate from a Certificate Authority (CA) that is part of the IGTF distribution but that is not in the default Operating System and browser stores.\nYour browser may be presenting you a security warning about an unknown CA, it’s a known issue with certificate having IGTF trust but not public trust (ie. not by default in the Operating Systems and browsers’ trust stores).\nIf you want to address this you can try to manually download the certificate for the ROOT CA and add it to your trust store and mark it as trusted. The exact process is dependant on the Operating System and browser that you use.\nAccess rules For an individual site, people having specific roles for the site can access using the X509 certificate linked to their account in Configuration Database:\nSite Administrator Site Operations Manager For all sites, the members of the dteam VO can access.\n","categories":"","description":"Monitor performance of EGI services\n","excerpt":"Monitor performance of EGI services\n","ref":"/internal/monitoring/","tags":"","title":"Service Monitoring"},{"body":"Overview Achieving the core objective of the EGI Federation to advance research and innovation, embracing Open Science, is possible by empowering scientists and researchers to easily perform domain-specific research tasks, without having to deal with the complexities of the underlying infrastructure.\nAn essential tool that contributes to this objective is a flexible development environment, where users can create and share documents containing live code, equations, narrative text, and rich media output.\nThe following development environments are available:\nNotebooks is a browser-based, scalable tool for interactive data analysis Binder allows re-creation of custom computing environments for reproducible execution of notebooks The following sections offer a more detailed description of each development environment.\n","categories":"","description":"Interactive environments for scientific development in the EGI Cloud\n","excerpt":"Interactive environments for scientific development in the EGI Cloud\n","ref":"/users/dev-env/","tags":"","title":"Development Environments"},{"body":"Introduction GOCDB, the software powering the EGI Configuration Database, is multi-tenanted; it can host multiple projects in the same instance. There are a number of different deployment scenarios that can be used to support new projects detailed below. Please contact the EGI Configuration Database admins and EGI Operations to discuss the options available.\n1) Add resources (sites/services) to an existing project Resources (NGIs, Sites, Services) would be hosted under an existing project, e.g. the ‘EGI’ project. The new resources would be subject to the rules of the existing project, such as site certification status changes and project controlled user memberships. The resources could not be filtered using a custom scope tag. 2) Add resources (sites/services) to an existing project and add a new Scope tag to represent a sub-grouping Resources would be hosted under an existing project, and a new scope tag would be added for the purposes of resource filtering. Since the resources are still hosted under an existing project, the new resources would still be subject to the rules of the existing project, such as site certification status changes and project controlled user memberships. The resources could be filtered using the new scope tag, but this scope tag would not strictly represent a project, rather a sub-grouping under the existing project, e.g. get_services\u0026scope=SubGroupX Note, resources can be tagged multiple times to declare support for multiple projects and sub-groups:\nget_services\u0026scope=SubGroupX,EGI\u0026scope_match=all 3) Add resources (sites/services) to a new Project and add a new Scope tag to filter by project Resources would be hosted under a new project, and a new scope tag would be added named after the project for the purposes of resource filtering. The resources would not be subject to the rules of other projects, for example, allowing the project to control its site certification status changes and project controlled user memberships. The resources could be filtered using the scope tag named after the new project, e.g. get_services\u0026scope=ProjectX Note, resources can be tagged multiple times to declare support for multiple projects:\nget_services\u0026scope=ProjectX,EGI\u0026scope_match=all ","categories":"","description":"How to create a new project in EGI Configuration Database.","excerpt":"How to create a new project in EGI Configuration Database.","ref":"/internal/configuration-database/adding-new-projects/","tags":"","title":"Adding a new project"},{"body":"Overview This tutorial describes how to start a Virtual Machine in the EGI Federation that runs a browser-accessible Jupyter server with DataHub spaces mounted. This setup can be used in the EGI Federation or in any other provider which synchronise images from AppDB but is not part of the federation.\nRequirements This tutorial assumes you have:\nA valid EGI account: learn to can create one in Check-in. Access to a cloud provider where the Jupyter DataHub VM is available. Alternatively, this VM can be run on your computer using a virtualisation tool like VirtualBox. Create a VM with Jupyter and DataHub Step 1: Start your VM Start your VM on your cloud provider or virtualisation tool. You can check the tutorial on how to start a VM to learn how to start a VM at EGI’s Federated Cloud infrastructure.\nThis VM does not contain any default credentials, in order to access it you need a ssh key. Check this FAQ entry for more information. If you are starting this VM on VirtualBox, you will need to pass some valid context for cloud-init, see here how to prepare it.\nThe VM image is ready to listen on port 22 for ssh access and port 80 for accessing the notebooks interface. Make sure your have those ports open on your security groups, otherwise you will not be able to reach the Jupyter notebooks.\nOnce your instance is ready, assign it a public IP so you can reach it from your computer.\nStep 2: Get a hostname and certificate for your VM Your VM is ready to be accessible, but runs a plain HTTP server, which is not secure enough. If you try to connect with your browser to your VM, you will get a message as shown in the screenshot below:\nYou must enable HTTPS to encrypt requests and responses, thus making your VM safer and more secure.\nFirstly, you need a valid name for your VM. You can use the FedCloud Dynamic DNS to create a name. See Dynamic DNS docs for more information on the service. Once you have your name ready, assign it your VM’s IP.\nSecondly, you need to get a certificate to enable HTTPS. The VM has certbot already installed, you just need to run it with the hostname you have allocated and your email address as shown here:\n# log into your VM $ ssh ubuntu@\u003cyour VM's IP\u003e # now request the certificate $ sudo certbot --nginx -d \u003cyour registered name\u003e -m \u003cyour email\u003e Finally, open your browser and go to https://\u003cyour registered name\u003e/ to see Jupyter started. Follow next steps for getting the credentials to access the service.\nEnabling insecure access If you really need to use HTTP (e.g. your VM is not accessible publicly and you cannot create a certificate for it), you can disable the error shown by default in the nginx configuration.\nOpen /etc/nginx/sites-enabled/default and comment out lines 14-16:\n# if ( $https != 'on' ) { # return 406; # } And restart nginx:\n$ sudo systemctl restart nginx Step 3: Get your token for the Jupyter server Your VM will spawn a Jupyter notebooks server upon starting. This server runs as an unprivileged user named jovyan with the software installed using micromamba. The server uses a randomly generated token for authentication that you can obtain by logging into the VM and becoming jovyan:\n$ ssh ubuntu@\u003cyour VM's IP\u003e # become jovyan user and activate the default environment $ sudo su - jovyan $ micromamba activate $ jupyter server list --jsonlist | jq -r .[].token \u003cyour token\u003e Step 4: Start your notebooks Now point your browser to http://\u003cyour VM's IP\u003e and you will be able to enter the token to get started with Jupyter.\nYou can install additional packages with mamba from a terminal started from Jupyter or via ssh. For example for installing tensorflow:\n$ micromamba activate $ micromamba install -c conda-forge tensorflow Step 5: Mount DataHub spaces Log into EGI’s DataHub and create a token for mounting your data in the VM.\nYou will also need the IP or address of your closest Oneprovider for the spaces you are interested in accessing. This information is easily obtainable via DataHub’s web interface.\nGo to your Jupyter session in your browser and edit the mount.sh file in your home directory. Set the ONECLIENT_ACCESS_TOKEN and ONECLIENT_PROVIDER_HOST values to get access to DataHub:\nOpen a terminal from the launcher screen and execute the mount.sh script:\nYou should now see a datahub folder with all your spaces available directly from your Jupyter interface\n","categories":"","description":"Step by step guide to get a Virtual Machine for Jupyter and DataHub in your cloud provider\n","excerpt":"Step by step guide to get a Virtual Machine for Jupyter and DataHub in …","ref":"/users/tutorials/jupyter-datahub-virtual-machine/","tags":"","title":"Create a Virtual Machine with Jupyter and DataHub"},{"body":"I am responsible for a site that has recently entered the EGI infrastructure. How do I register it? Only registered users with an approved role on an NGI can add a new site. If you are the site administrator, the first thing to do is to contact your NGI staff and ask them to add the site for you. Then, register to EGI Configuration Database (see the user account section) and ask for a site admin role for your site (see the requesting a role section). Once your role approved, you will be able to edit and change your site information.\nHow do I extend a declared schedule downtime? Because of EGI policies it is not possible to extend a downtime. Recommended good practice for any downtime extension is to declare a new unscheduled downtime, starting just when the first one finishes. Please refer to the downtimes section of this documentation for more information, especially the “downtime extension” paragraph.\nI have declared a downtime “at risk”, and it turns out to be an outage. How can I declare this properly? If you have declared the downtime as being at risk and an outage actually happens half way through, you need to update the Configuration Database to reflect the fact that your site is now down. There is currently no way of doing this by updating the downtime on the fly without having the system considering the whole downtime as being an outage. The best way to proceed is:\nModify end date of your “at risk” downtime, so that it ends in a few minutes Enter a new “outage” downtime, starting when the other ends How do I switch monitoring on/off for my nodes? Monitoring status in Configuration Database cannot always be switched off. If a node is declared as delivering a production service, rules apply and the node has to be monitored. If you are running a test node and want to switch monitoring off, set both “monitoring” and “production” to “N”.\nWhy nobody has approved my role request yet? Someone has to approve any request you make, in order to ensure nobody is trying to get inappropriate roles. If yours is not getting approved, this can either be because your request was not legitimate, or most likely because the people that are supposed to do it forgot about it. Please refer to the Roles permissions definitions section of this documentation to determine who should validate your role, and try to get in touch with them. If you are requesting a site admin role, they are likely to be your fellow site admins or your NGI operators.\nI am not an EGI user but need access to the backend to retrieve information for my project. What can I do? Accessing the backend through another way than the Configuration Database web interface is out of the scope of this documentation. Please refer to the technical documentation instead, which is available from GOCDB Documentation.\n","categories":"","description":"Frequently Asked Questions","excerpt":"Frequently Asked Questions","ref":"/internal/configuration-database/faq/","tags":"","title":"FAQ"},{"body":"To ensure interoperability within and outside of EGI, the Policy on Acceptable Authentication Assurance defined a common set of trust anchors (in a PKIX implementation “Certification Authorities”) that all sites in EGI should install. In short, all CAs accredited to the Interoperable Global Trust Federation under the classic, MICS or SLCS Authentication Profiles are approved for use in EGI. When installing the ‘combined-assurance’ bundle, also IOTA issuers complying with assurance level DOGWOOD are included. Of course, sites may add additional CAs as long as the integrity of the infrastructure as a whole is not compromised. Also, if there are site or national policies/regulations that prevent you from installing a CA, these regulations take precedence – but you then must inform the EGI Security Officer (see EGI CSIRT) about this exception.\nRelease notes Review the release notes containing important notices about the current release, as well as a list of changes to the trust fabric.\nInstallation To install the EGI trust anchors on a system that uses the RedHat Package Manager (RPM) based package management system, we provide a convenience package to manage the installation. To install the currently valid distribution, all RPM packages are provided at\nhttp://repository.egi.eu/sw/production/cas/1/current/ The current version is based on the IGTF release with the same version number. Install the meta-package ca-policy-egi-core (or ca-policy-egi-cam) and its dependencies to implement the core EGI policy on trusted CAs.\nUsing YUM package management Add the following repo-file to the /etc/yum.repos.d/ directory:\n[EGI-trustanchors] name=EGI-trustanchors baseurl=http://repository.egi.eu/sw/production/cas/1/current/ gpgkey=http://repository.egi.eu/sw/production/cas/1/GPG-KEY-EUGridPMA-RPM-3 gpgcheck=1 enabled=1 and then update your installation. How to update depends on your previous activity:\nif you have previously ever installed the lcg-CA package, remove any references to http://linuxsoft.cern.ch/LCG-CAs/current from your YUM setup, and run $ yum clean cache metadata $ yum update lcg-CA and you are done. This will update the packages installed to the latest version, and also install the new ca-policy-egi-core package as well as a ca-policy-lcg package. All packages encode the same set of dependencies\nif you are upgrading from a previous EGI version only, just run\n$ yum update ca-policy-egi-core although at times you may need to clean the yum cache using $ yum clean cache metadata if you are installing the EGI trust anchors for the first time, run $ yum install ca-policy-egi-core Using the distribution on a Debian or Debian-derived platform The 1.39+ releases experimentally add the option to install the trust anchors from Debian packages using the APT dependency management system. Although care has been taken to ensure that this distribution is installable and complete, no guarantees are given, but you are invited to report your issues through GGUS. You may have to wait for a subsequent release of the Trust Anchor release to solve your issue, or may be asked to use a temporary repository. To use it:\nInstall the EUGridPMA PGP key for apt: $ wget -q -O - \\ https://dist.eugridpma.info/distribution/igtf/current/GPG-KEY-EUGridPMA-RPM-3 \\ | apt-key add - Add the following line to your sources.list file for APT: #### EGI Trust Anchor Distribution #### deb http://repository.egi.eu/sw/production/cas/1/current egi-igtf core Populate the cache and install the meta-package $ apt-get update $ apt-get install ca-policy-egi-core Using the distribution on other (non-RPM) platforms The trust anchors are provided also as simple ’tar-balls’ for installation on other platforms. Since there is no dependency management in this case, please review the release notes carefully for any security issues or withdrawn CAs. The tar files can be found in the EGI repository at\nhttp://repository.egi.eu/sw/production/cas/1/current/tgz/ Once you have downloaded the directory, you can unpack all the CA tar,gz as follows to your certificate directory:\n$ for tgz in $(ls \u003cca download dir\u003e); do tar xzf \u003cca download dir\u003e/$tgz --strip-components=1 \\ -C /etc/grid-security/certificates done Installing the distribution using Quattor Quattor templates are povided as drop-in replacements for both QWG and CDB installations. Update your software repository (re-generating the repository templates as needed) and obtain the new CA templates from:\nhttp://repository.egi.eu/sw/production/cas/1/current/meta/ca-policy-egi-core.tpl for QWG http://repository.egi.eu/sw/production/cas/1/current/meta/pro_software_meta_ca_policy_egi_core.tpl for CDB Make sure to mirror (or refer to) the new repository at http://repository.egi.eu/sw/production/cas/1/current/ and create the appropriate repository definition file.\nFor WLCG sites that are migrating from the lcg-CA package: the WLCG policy companion of the EGI templates can be found at QWG and CDB and can be included in the profile in parallel with the EGI core template. All packages needed are also included in the EGI repository, so only a single repository reference is necessary.\nCombined Assurance/Adequacy Model The release contains a “cam” (combined assurance/adequacy) package based on the approved policy on differentiated assurance. Technically, this means that you must ONLY install the new ca-policy-egi-cam packages if you ALSO at the same time implement VO-specific authorization controls in your software stack. This may require reconfiguration or a software update. Otherwise, just only install or update the regular ca-policy-egi-core package. There are no changes in this case. The ca-policy-egi-core package is approved for all VOs membership and assurance models. No configuration change is needed.\nAcceptable Authentication Assurance If a VO registration service or e-infrastructure registration service is accredited by EGI to meet the approved authentication assurance, an IGTF “DOGWOOD” accredited Authority - used solely in combination with said registration service - is also adequate for user authentication. See the policy for details.\nThis additional restriction policy must be implemented by each service in the authorization software. The “combined assurance” model package MUST NOT be installed unless the additional authorization is in place. You will need to reconfigure and may need to install upgrades. Not installing the new “cam” package does not have any detrimental effect on current users - only a new class of users (that can only obtain an opaque identifiers and do not do full vetting at their electronic identity provider) could be affected, and then only those users that are member of one of the communities that has part of the combined-assurance programme: LCG-Atlas, LCG-Alice, LCG-LHCb, and LCG-CMS.\nPatches and workarounds Reminder notice for VOMS AA operators Several updates to this trust anchor distribution incorporate changes to the name of the issuing authority, but the name of the end-entities and the users remains exactly the same. To make the change transparent, all operators of VOMS and VOMS-Admin services are requested to enable the subject-only name resolution mechanisms in VOMS and VOMS Admin, see additional documentation in VOMS services configuration reference:\non the VOMS core Attribute Authority service, configure the -skipcacheck flag on start-up. Set voms.skip_ca_check=True in the service properties. Concerns, issues and verification If you experience problems with the installation or upgrade of your trust anchors, or with the repository, please report such an issue through the GGUS system. For issues with the contents of the distribution, concerns about the trust fabric, or with questions about the integrity of the distribution, please contact the EGI IGTF liaison at egi-igtf-liaison@nikhef.nl.\nYou can verify the contents of the EGI Trust Anchor (CA) release with those of the International Grid Trust Federation, or its mirror. See the IGTF and EUGridPMA web pages for additional information.\nMake sure to verify your trust anchors with TACAR, the TERENA Academic CA Repository, where applicable.\n","categories":"","description":"Using the IGTF CA distribution","excerpt":"Using the IGTF CA distribution","ref":"/providers/operations-manuals/howto01_using_igtf_ca_distribution/","tags":"","title":"HOWTO01 Using IGTF CA distribution"},{"body":"Generic documentation The Operations start guide will help you start with EGI Operations duties.\nThe Resource Centre integration check list provides an overview of what are the required steps to get a new Resource Centre integrated into the EGI Federation.\nManuals The EGI Operations Manuals are approved technical documents that provide prescriptive guidelines on how to complete a given task. These documents are periodically reviewed, and need to be followed by all partners (as opposed to a best practice documents that provide optional guidelines).\nMAN01 How to publish Site Information MAN02 Service intervention management MAN04 Tool intervention management MAN05 Top and site BDII High Availability MAN06 failover for MySQL grid-based service MAN07 VOMS replication MAN09 Accounting data publishing How-Tos Miscellaneous collection of short How-Tos which are relevant to various other documentation and procedures.\nHOWTO01 Using IGTF CA distribution HOWTO02 Site Certification Required Documentation HOWTO03 Site Certification GIIS Check HOWTO04 Site Certification Manual tests ","categories":"","description":"Operations manuals for Service Providers","excerpt":"Operations manuals for Service Providers","ref":"/providers/operations-manuals/","tags":"","title":"Operations manuals and How-Tos"},{"body":"Authentication Some EGI services authentication is based on X.509 certificates. The certificates are issued by Certification Authorities (CAs) part of the EUGridPMA federation which is also part of IGTF (International Global Trust Federation).\nThe role of a Certification Authority (CA) is to guarantee that users are who they claim to be and are entitled to own their certificate. It is up to the users to discover which CA they should contact. In general, CAs are organised geographically and by research institutes. Each CA has its own procedure to release certificates.\nEGI sites, endpoints and tools accept certificates part of the EUGridPMA distribution. If your community VO is enabled on that site, your certificate will be accepted by that site since all certificates are recognized at site level.\nUsually, a certificate can be installed by command-line tools, but they can also be stored in the web browser to access EGI web tools and services.\nGet a Certificate The list of EGI recognised CAs provides a clickable map to find your nearby CA. Several of these offer the option to get an ’eScience Personal’ certificate online from the Terena Certificate Service CA. Check the countries where this is available.\nIf eScience Personal certificate is not available in your country, then request a certificate from a regular IGTF CA. The request is normally generated using either a web-based interface or console commands. Details of which type of request a particular CA accepts can be found on each CA’s site.\nFor a web-based certificate request, a form must usually be filled in with information such as the name of the user, home institute, etc. After submission, a pair of private and public keys are generated, together with a request for the certificate containing the public key and the user data. The request is then sent to the CA, while the private key stays in the browser, hence the same browser must be used to retrieve the certificate once it is issued.\nUsers must usually install the CA root certificate in their browser first. This is because the CA has to sign the user certificate using its private key, and the user’s browser must be able to validate the signature.\nFor some CAs, the certificate requests are generated using a command line interface. The details of the exact command and the requirements of each CA will vary and can be found on the CA’s site.\nOnce received the request, the CA will have to confirm your authenticity through your certificate. This usually involves a physical meeting or a phone call with a Registration Authority (RA). A RA is delegated by the CA to verify the legitimacy of a request, and approve it if it is valid. The RA is usually someone at your home institute, and will generally need some kind of ID to prove your identity.\nInstall a Certificate After approval, the certificate is generated and delivered to you. This can be done via email, or by giving instructions to you to download it from a web page.\nBrowser installation Install the certificate in your browser. If you don’t know how to upload your certificate in your browser have a look at the examples.\nHost installation To use EGI services with your certificate, you must first save your certificate to disk.\nThe received certificate will usually be in one of two formats:\nPrivacy Enhanced Mail Security Certificate (PEM) with extension .pem or Personal Information Exchange File (PKCS12) with extensions .p12 or .pfx. The latter is the most common for certificates exported from a browser (e.g. Internet Explorer, Mozilla and Firefox), but the PEM format is currently needed on EGI user interface. The certificates can be converted from one format to the other using the openssl command.\nIf the certificate is in PKCS12 format, then it can be converted to PEM using pkcs12:\nFirst you will need to create the private key, use -nocerts. Open your terminal, enter the following command:\nopenssl pkcs12 -nocerts -in my_cert.p12 -out userkey.pem where:\nFilename Description my_cert.p12 is the input PKCS12 format file; userkey.pem is the output private key file; usercert.pem is the output PEM certificate file. When prompted to “Enter Import Password”, simply press enter since no password should have been given when exporting from keychain. When prompted to “Enter PEM pass phrase”, enter the pass phrase of your choice, e.g. 1234.\nNow you can create the certificate, use -clcerts, (use -nokeys hereu will not output private key), and the command is:\nopenssl pkcs12 -clcerts -nokeys -in my_cert.p12 -out usercert.pem When prompted to “Enter Import Password”, simply press enter since no password should have been given when exporting from keychain.\nFor further information on the options of the pkcs12 command, consult man pkcs12\nIt is strongly recommended that the names of all these files are kept as shown. Once in PEM format, the two files, userkey.pem and usercert.pem, should be copied to a User Interface (UI). For example, the ‘standard’ location for Mac would be .globus directory in your $HOME. I.e. $HOME/.globus/\nRenewing the Certificate CAs issue certificates with a limited duration (usually one year); this implies the need to renew them periodically. The renewal procedure usually requires that the certificate holder sends a request for renewal signed with the old certificate and/or that the request is confirmed by a phone call; the details depend on the policy of the CA. The certificate usually needs to be renewed before the old certificate expires; CAs may send an email to remind users that renewal is necessary, but users should try to be aware of the renewal date, and take appropriate action if they are away for extended periods of time.\nTaking Care of Private Keys A private key is the essence of your identity. Anyone who steals it can impersonate the owner and if it is lost, it is no longer possible to do anything. Certificates are issued personally to individuals, and must never be shared with other users. To user EGI services, users must agree to an Acceptable Use Policy, which among other things requires them to keep their private key secure.\nOn a UNIX UI, the certificate and private key are stored in two files. Typically they are in a directory called $HOME/.globus and are named usercert.pem and userkey.pem, and it is strongly recommended that they are not changed. The certificate is public and world-readable, but the key must only be readable by the owner. The key should be stored on a disk local to the user’s UI rather than, for example, an NFS-mounted disk. If a certificate has been exported from a browser, a PKCS12-format file (.p12 or .pfx), which contains the private key, will have been locally stored and this file must be either encrypted, hidden or have its access rights restricted to only the owner.\nIf a private key is stored under the Andrew File System (AFS), access is controlled by the AFS Access Control Lists (ACL) rather than the normal file permissions, so users must ensure that the key is not in a publicly-readable area.\nWeb browsers also store private keys internally, and these also need to be protected. The details vary depending on the browser, but password protection should be used if available; this may not be the default (it is not with Internet Explorer). The most secure mode is one in which every use of the private key needs the password to be entered, but this can cause problems as some sites ask for the certificate many times. Reaching a compromise between security and convenience is vital here, so that neither come too short.\nIt is important not to lose the private key, as this implies loss of all access to the services, and registration will have to be started again from scratch. Having several securely protected copies in different places is strongly advised, so the certificate can be used from a web browser and several UI machines.\nA private key stored on a UI must be encrypted, meaning that a passphrase must be typed whenever it is used. A key must never be stored without a passphrase. The passphrase should follow similar rules to any computer password. Users should be aware of the usual risks, like people watching them type or transmitting the passphrase over an insecure link.\nAuthorisation The sites authorise the access to their resources to a VO according to their own access policies, resource location, how many resources is the VO allowed to use. There are finer authorization policies, including groups, roles, in this way, the users can be structured in a VO. So, it is not a 0/1 authorization policy.\nThe community has full control of the access to the VO according to community authorization policies. The VO membership, groups and roles are managed by VO managers (Privileged VO members) independently by using the Virtual Organization Membership Service (VOMS).\nVOMS The Virtual Organization Membership Service (VOMS) is an attribute authority which serves as central repository for VO user authorization information, providing support for sorting users into group hierarchies, keeping track ofu their roles and other attributes in order to issue trusted attribute certificates and SAML assertions used in the Grid environment for authorization purposes. VOMS is composed of two main components:\nthe VOMS core service, which issues attribute certificates to authenticated clients the VOMS Admin service, which is used by VO manager to administer VOs and manage user membership details. How does it work? Usually, users submit tasks/jobs to the infrastructure that are attached with their own credential, and the credential is attached with a proxy certificate that is a short-term credential signed with the user certificate and is extended with the VO attributes. In general speaking, a user credential is just an ID, and a proxy contains the VO details, so a resource site by receiving the proxy can recognize that the user is part of such a VO with such a role from such a group. A user can be part of multiple VO, thus can generate multiple proxies.\nRegister to a VO Visit Operation Portal to search for existing VOs\nIf there are any community VOs matching your requirements (with Registry System is VOMS), then click Action-\u003e Details to look at the VO information. In the VO ID Card page, click the link for Enrollment URL, it will take you to the VO VOMS page. You should have already discussed with the EGI support team, they would help you to contact the VO managers and get approval for your access. If there are no relevant VOs, you can send a request to register a new VO. (Note, for EGI services, you should request for VOMS configuration, once VO is configured, you will be notified about your VO VOMS link). More information can be found at Guideline for VO registration. Again, this is usually guided by the EGI support team. You should already have a meeting with them to discuss your requirements. They will help you to get resources from EGI providers, and sign SLA with you.\nRequest your VO membership at VO VOMS page. You will have to enter required information and then wait for approval.\nCreating a proxy VOMS configuration Every VO needs two different pieces of information:\nthe vomses configuration files, where the details of the VO are stored (e.g. name, server, ports). These are stored by default at /etc/vomses and are normally named following this convention: \u003cvo name\u003e.\u003cserver name\u003e (e.g. for fedcloud.egi.eu VO, you would have fedcloud.egi.eu.voms1.grid.cesnet.cz and fedcloud.egi.eu.voms2.grid.cesnet.cz. the .lsc files that describe the trust chain of the VOMS server. These are stored at /etc/grid-security/vomsdir/\u003cvo name\u003e and there should be one file for each of the VOMS server of the VO. You can check specific configuration for your VO at the Operations portal. Normally each VOMS server has a Configuration Info link where the exact information to include in the vomses and .lsc files is shown.\nProxy creation Once you have the VO information configured (vomses and .lsc) and your certificate available in your $HOME/.globus directory you can create a VOMS proxy to be used with clients with:\nvoms-proxy-init --voms \u003cname of the vo\u003e --rfc See for example, using fedcloud.egi.eu VO:\nvoms-proxy-init --voms fedcloud.egi.eu --rfc Enter GRID pass phrase: Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=EGI/OU=UCST/CN=Enol Fernandez Creating temporary proxy ......................................................... Done Contacting voms1.grid.cesnet.cz:15002 [/DC=cz/DC=cesnet-ca/O=CESNET/CN=voms1.grid.cesnet.cz] \"fedcloud.egi.eu\" Done Creating proxy ................................................................... Done Your proxy is valid until Mon Feb 4 23:37:21 2019 ","categories":"","description":"X.509 / VOMS based authentication and authorisation\n","excerpt":"X.509 / VOMS based authentication and authorisation\n","ref":"/users/aai/check-in/vos/voms/","tags":"","title":"VOMS"},{"body":"Documents required for Resource Centre registration and certification.\nThe Resource Centre administrators should read and understand the first five documents. It is a further requirement that the OLA between the NGI and RC, and the Service Operations Policy are accepted by the Site Operations Manager. Further, documents referred within the Service Operations Policy should also be understood.\nDocumentation PROC09 Resource Centre Registration and Certification: Procedure document describing the steps required to register new Resource Centres and certify new and suspended ones. Service Operations Policy: This security policy presents the conditions that apply to anyone running a Service on the Infrastructure, or to anyone providing a Service that is part of the Infrastructure. Resource Centre Operational Level Agreement (OLA) PROC11 Resource Centre Decommissioning Procedure: Procedure document describing the steps required to decommission a Resource Centre. PROC12 Production Service Decommissioning Procedure: Procedure document describing the steps required to decommission a production service. Helpdesk: A collection of documentation relevant to the EGI Helpdesk (GGUS). Tools Configuration Database (GOCDB) Operations Portal ARGO Monitoring: RCs status, A/R reports: The ARGO Monitoring service periodically checks the functionality of the production services. EGI Operations Architecture: Document describing the roles and responsibilities of the various layers in EGI and also the architectural nomenclature. ","categories":"","description":"Documents required for Resource Center registration and certification.","excerpt":"Documents required for Resource Center registration and certification.","ref":"/providers/operations-manuals/howto02_site_certification_required_documentation/","tags":"","title":"HOWTO02 Site Certification Required Documentation"},{"body":"Introduction ROD (Regional Operator on Duty) is a role which oversees the smooth operation of EGI infrastructure in the respective NGI. ROD team is responsible for solving problems on the infrastructure within own Operations Centre according to agreed procedures. They ensure that problems are properly recorded and progress according to specified timelines. They ensure that necessary information is available to all parties. The team is provided by each Operations Centre and requires procedural knowledge on the process. The role is usually covered by a team of people and is provided by each NGI. Depending on how an NGI is organised there might be a number of members in the ROD team who work on duty roster (shifts on a daily or weekly basis), or there may be one person working as ROD on a daily basis and a few deputies who take over the responsibilities when necessary. This latter model is generally more suitable for small NGIs.\nIn this text, the acronym ROD will be used both for the whole team, or for the person who is actually working on shift.\nIn order to become a ROD member, one first needs to go through the steps described in the Overview for ROD.\nThe following text describes the duties that ROD (teams) are responsible for.\nContact: all-operator-on-duty AT mailman.egi.eu\nDuties A list describing duties.\nAlarms and tickets Information on how to deal with alarms raised in the Operations Portal Dashboard and how to generate and deal with tickets.\nDowntimes How downtimes are managed.\nCommunication Communication channels for ROD to Sites and to management.\nSecurity How ROD should deal with security issues.\nManuals and procedures In this section are linked manuals and procedures which RODs should be familiar with:\nPROC01: Infrastructure Oversight escalation Dashboard How-Tos and Training Guides Webinar shortcuts. Introduction. ROD duties ROD procedures Becoming ROD team Member Obtaining X.509 certificate Registration in GOCDB Registration in GGUS Registration in dteam VO ROD shift Dashboard overview Issues aka alarms Tickets Notepads Handover Webinar Presentation Slides ROD FAQ Resources Operational tools Procedures ","categories":"","description":"Description of the NGI oversight activity.","excerpt":"Description of the NGI oversight activity.","ref":"/providers/rod/","tags":"","title":"Regional Operator on Duty (ROD)"},{"body":"Basics How can I get access to the cloud compute service? There is a VO available for 6 months piloting activities that any researcher in Europe can join. Just request access to the pilot Virtual Organisation.\nHow can I get an OAuth2.0 token? Authentication via CLI or API requires a valid Access Token from Check-in. The EGI Check-in Token Portal allows you to get one as needed. Check the Authentication and Authorisation guide for more information.\nIs OCCI still supported? OCCI is now deprecated as API for the EGI Cloud providers using OpenStack. Some providers still support OCCI (a list of active endpoints can be queried at GOCDB) but it should note be used for any new developments.\nMigration from rOCCI CLI to OpenStack CLI is quite straightforward, we summarize the main commands in rOCCI and OpenStack equivalent in the table below:\nAction rOCCI OpenStack List images occi -a list -r os_tpl openstack image list Describe images occi -a describe -r \u003cimage_id\u003e openstack image show \u003cimage_id\u003e List flavors occi -a list -r resorce_tpl openstack flavor list Describe flavors occi -a describe -r \u003ctemplate_id\u003e openstack flavor show \u003cimage_id\u003e Create VM occi -a create -r compute -t occi.core.title=\"MyFirstVM\" -M \u003cflavor id\u003e -M \u003cimage id\u003e -T user_data=\"file://\u003cfile\u003e\" openstack server create --flavor \u003cflavor\u003e --image \u003cimage\u003e --user-data \u003cfile\u003e MyFirstVM Describe VM occi -a describe -r \u003cvm id\u003e openstack server show \u003cvm id\u003e Delete VM occi -a delete -r \u003cvm id\u003e openstack server delete \u003cvm id\u003e Create volume occi -a create -r storage -t occi.storage.size='num(\u003csite in GB\u003e)' -t occi.core.title=\u003cstorage_resource_name\u003e openstack volume create --size \u003csize in GB\u003e \u003cstorage resource name\u003e List volume occi -a list -r storage openstack volume list Attach volume occi -a link -r \u003cvm_id\u003e -j \u003cstorage_resource_id\u003e openstack server add volume \u003cvm id\u003e \u003cvolume id\u003e Dettach volume occi -a unlink -r \u003cstorage_link_id\u003e openstack server remove volume \u003cvm id\u003e \u003cvolume id\u003e Delete volume occi -a delete -r \u003cvolume id\u003e openstack volume delete \u003cvolume id\u003e Attach public IP occi -a link -r \u003cvm id\u003e --link /network/public openstack server add floating ip \u003cvm id\u003e \u003cip\u003e If you still rely on OCCI for your access, please contact us at support _at_ egi.eu for support on the migration.\nDiscovery How can I get the list of the EGI Cloud providers? The list of certified providers is available in GOCDB. The following command with the fedcloud client can help you to get that list:\n$ fedcloud site list 100IT BIFI CESGA CESNET-MCC CETA-GRID CLOUDIFIN CYFRONET-CLOUD DESY-HH GSI-LCG2 IFCA-LCG2 IISAS-FedCloud IISAS-GPUCloud IN2P3-IRES INFN-CATANIA-STACK INFN-CLOUD-BARI INFN-PADOVA-STACK Kharkov-KIPT-LCG2 NCG-INGRID-PT SCAI TR-FC1-ULAKBIM UA-BITP UNIV-LILLE fedcloud.srce.hr The providers also generate dynamic information about their characteristics via the Argo Messaging System which is easily browsable from AppDB.\nHow can I choose which site to use? Sites offer their resources to users through Virtual Organisations (VO). First, you need to join a Virtual Organisation that matches your research interests, see authorisation section on how VOs work. AppDB shows the supported VOs and for each VO you can browse the resource providers that support it.\nHow can I get information about the available VM images? The Application Database contains information about the VM images available in the EGI Cloud. Within the AppDB Cloud Marketplace, you can look for a VM and get all the information about which VO the VM is associated, the sites where the VM is available and the endpoints and identifiers to use it in practice.\nManaging VMs The disk on my VM is full, how can I get more space? There are several ways to increase the disk space available at the VM. The fastest and easiest one is to use block storage, creating a new storage disk device and attaching it to the VM. Check the storage guide for more information.\nHow can I keep my data after the VM is stopped? After a VM has been stopped and unless backed up in a block storage volume, all data in the VM is destroyed and cannot be recovered. To ensure your data will be available after the VM is deleted, you need to use some form of persistent storage.\nHow can I assign a public IP to my VM? Some providers do not automatically assign a public IP address to a VM during the creation phase. In this case, you can attach a public IP by first allocating a new public IP and then assigning it to the VM.\nHow can I assign a DNS name to my VM? If you need a domain name for your VMs, we offer a Dynamic DNS service that allows any EGI user to create names for VMs under the fedcloud.eu domain.\nJust go to EGI Cloud nsupdate and login with your Check-in account. Once in, you can click on \"Add host\" to register a new hostname in an available domain.\nWhat is contextualisation? Contextualisation is the process of installing, configuring and preparing software upon boot time on a predefined virtual machine image. This way, the predefined images can be stored as generic and small as possible, since customisations will take place on boot time.\nContextualisation is particularly useful for:\nConfiguration not known until instantiation (e.g. data location). Private Information (e.g. host certs) Software that changes frequently or under development. Contextualisation requires passing some data to the VMs on instantiation (the context) and handling that context in the VM.\nHow can I inject my public SSH key into the machine? The best way to login into the virtual server is to use SSH keys. If you don't have one, you need to generate it with the ssh-keygen command:\nssh-keygen -f fedcloud This will generate two files:\nfedcloud, the private key. This file should never be shared fedcloud.pub, the public key. That will be sent to your VM. To inject the public SSH key into the VM you can use the key-name option when creating the VM in OpenStack. Check keypair management option in OpenStack documentation. This key will be available for the default configured user of the VM (e.g. ubuntu for Ubuntu, centos for CentOS).\nYou can also create users with keys with a contextualisation file:\n#cloud-config users: - name: cloudadm sudo: ALL=(ALL) NOPASSWD:ALL lock-passwd: true ssh-import-id: cloudadm ssh-authorized-keys: - \u003cpaste here the contents of your SSH key pub file\u003e Warning YAML format requires that the spaces at the beginning of each line is respected in order to be correctly parsed by cloud-init. How can I use a contextualisation file? If you have a contextualisation file, you can use it with the --user-data option to server create in OpenStack.\nopenstack server create --flavor \u003cyour-flavor\u003e --image \u003cyour image\u003e \\ --user-data \u003cyour contextualisation file\u003e \\ \u003cserver name\u003e Note We recommend using cloud-init for contextualisation. EGI images in AppDB do support cloud-init. Check the documentation for more information. How can I pass secrets to my VMs? EGI Cloud endpoints use HTTPS so information passed to contextualise the VMs can be assumed to be safe and only readable within your VM. However, take into account that anyone with access to the VM may be able to access also the contextualisation information.\nWarning Take into account that anyone with access to the VM may be able to access also the contextualisation information, so ensure that no sensitive data like clear text passwords is used during contextualisation. How can I use ansible? Ansible relies on ssh for accessing the servers it will configure. VMs at EGI Cloud can be also accessed via ssh, just make sure you inject the correct public keys in the VMs to be able to access.\nIf you don't have public IPs for all the VMs to be managed, you can also use one as a gateway as described in the Ansible FAQ.\nHow can I release resources without destroying my data? Whenever you delete a VM, the ephemeral disks associated with it will be also deleted. If you don’t plan to use your VM for some time, there are several ways to release resources consumed by the VM (e.g. CPU, RAM) and recover the the data or boot your VM in a previous state when you need it back. We list below the main strategies you can use:\nUse a volume to store the data to be kept: Check the Storage section of the documentation to learn how to use volumes. If you start your VM from a volume, the VM can be destroyed and recreated easily. OpenStack documentation cover how to start a VM from a volume with CLI or using the Horizon dashboard\nSuspend or shelve instance: Suspending a VM will pause a VM, releasing CPU and memory, and allowing to resume later in time at the exact same state. Shelving shuts down the VM, thus RAM contents will be lost but disk will be kept. This releases more resources from the provider while still allows to easily boot the VM back without losing disk contents.\nCreate snapshot of instance: a snapshot will create a new VM image at your provider that can be used to boot a new instance of the VM with the same disk content. You can use this technique for creating a base template image that can be later re-used to start similar VMs easily.\nHow can I find all the VMs that I own in the EGI Federated Cloud? We suggest using the fedcloudclient:\n# list the Virtual Organisations that you belong to fedcloud token list-vos # then, for each VO, run: list-all-my-own-vms.sh --vo \u003cvirtual-organisation\u003e See the fedcloudclient documentation for more information.\n","categories":"","description":"Most frequent questions about EGI Cloud Compute\n","excerpt":"Most frequent questions about EGI Cloud Compute\n","ref":"/users/compute/cloud-compute/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"Be sure that Resource Centre GIIS URL is contained in the BDII you use for certification.\nCheck the consistency of the published information These are the main branches of the LDAP tree:\nGlueSiteUniqueID GlueSubClusterUniqueID GlueCEUniqueID GlueCESEBind GlueSEUniqueID GlueServiceUniqueID It is recommended to use the Apache Studio LDAP browser, although in this page ldapsearch queries are shown.\nContact information Under the branch GlueSiteUniqueID check the values of the following fields:\nGlueSiteName GlueSiteUserSupportContact GlueSiteSysAdminContact GlueSiteSecurityContact GlueSiteOtherInfo Example:\n$ ldapsearch -x -LLL -H ldap://sibilla.cnaf.infn.it:2170 \\ -b mds-vo-name=INFN-CNAF,o=grid 'objectClass=GlueSite' \\ GlueSiteName GlueSiteUserSupportContact GlueSiteSysAdminContact \\ GlueSiteSecurityContact GlueSiteOtherInfo dn: GlueSiteUniqueID=INFN-CNAF,Mds-Vo-name=INFN-CNAF,o=grid GlueSiteSecurityContact: mailto:grid-sec@cnaf.infn.it GlueSiteSysAdminContact: mailto:grid-operations@lists.cnaf.infn.it GlueSiteName: INFN-CNAF GlueSiteOtherInfo: CONFIG=yaim GlueSiteOtherInfo: EGEE_SERVICE=prod GlueSiteOtherInfo: EGI_NGI=NGI_IT GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: WLCG_TIER=3 GlueSiteUserSupportContact: mailto:grid-operations@lists.cnaf.infn.it Information related to software environment Under the branch GlueSubClusterUniqueID check the values of the following fields:\nCheck that the GlueHostApplicationSoftwareRunTimeEnvironment contains a list of software tags supported by the site. The list can include VO-specific software tags. In order to ensure backwards compatibility it should include the entry ‘LCG-2’, the current middleware version and the list of previous middleware tags (i.e. LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 LCG-2_3_0 LCG-2_3_1 LCG-2_4_0 LCG-2_5_0 LCG-2_6_0 LCG-2_7_0 GLITE-3_0_0 GLITE-3_1_0 GLITE-3_2_0 R-GMA). GlueHostProcessorOtherDescription (see FAQ HEP SPEC06) GlueHostOperatingSystemName, GlueHostOperatingSystemVersion and GlueHostOperatingSystemRelease (see publishing the OS name). Example:\n$ ldapsearch -x -LLL -H `\u003cldap://virgo-ce.roma1.infn.it:2170\u003e` \\ -b mds-vo-name=resource,o=grid 'objectClass=GlueSubCluster' \\ GlueHostProcessorOtherDescription dn: GlueSubClusterUniqueID=virgo-ce.roma1.infn.it,GlueClusterUniqueID=virgo-ce.roma1.infn.it,Mds-Vo-name=resource,o=grid GlueHostProcessorOtherDescription: Cores=4, Benchmark=7.83-HEP-SPEC06 Publishing the OS name It has been decided that the 3 fields\nGlueHostOperatingSystemName GlueHostOperatingSystemRelease GlueHostOperatingSystemVersion should be parsed from the output of /usr/bin/lsb_release, like this:\nGlueHostOperatingSystemName: lsb_release -i | cut -f2 GlueHostOperatingSystemRelease: lsb_release -r | cut -f2 GlueHostOperatingSystemVersion: lsb_release -c | cut -f2 yielding values like\nGlueHostOperatingSystemName: CentOS GlueHostOperatingSystemRelease: 7.9.2009 GlueHostOperatingSystemVersion: Core This has been tested on various Linux flavours and should work on every serious GNU/Linux distribution.\nInformation about the batch system Under the branch GlueCEUniqueID check the values of the following fields:\nGlueCEInfoTotalCPUs: Check that the value is higher than 0. GlueCEStateWaitingJobs: If there is a “44444”, the information providers are not working properly. GlueCEInfoLRMSType: any supported batch system (sge, pbs, lsf…) GlueCEStateStatus: Production, Draining, Queuing or Closed are accepted values. GlueCEAccessControlBaseRule: VOs enabled on the queue GlueCECapability Example:\n$ ldapsearch -x -LLL -H ldap://virgo-ce.roma1.infn.it:2170 \\ -b mds-vo-name=INFN-ROMA1-VIRGO,o=grid 'objectClass=GlueCE' \\ GlueCEInfoTotalCPUs GlueCEInfoJobManager GlueCEImplementationName dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-theophys,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-cert,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-virgoglong,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-argo,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-virgogshort,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 $ ldapsearch -x -LLL -H ldap://cmsrm-bdii.roma1.infn.it:2170 \\ -b mds-vo-name=INFN-ROMA1-CMS,o=grid 'objectclass=GlueCE' GlueCECapability dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsgcert,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsglong,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsgshort,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsglong,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsgcert,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsgshort,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 Information on Computing Element about Storage Elements For each SE, on the CEs the following values must be present:\nGueCESEBindSEUniqueID. GlueCESEBindCEAccesspoint and GlueCESEBindMountInfo. Example:\n$ ldapsearch -x -LLL -H ldap://cremino.cnaf.infn.it:2170 \\ -b mds-vo-name=resource,o=grid 'objectClass=GlueCESEBind' \\ GlueCESEBindSEUniqueID GlueCESEBindCEUniqueID GlueCESEBindMountInfo dn: GlueCESEBindSEUniqueID=sunstorm.cnaf.infn.it,GlueCESEBindGroupCEUniqueID=cremino.cnaf.infn.it:8443/cream-pbs-cert,Mds-Vo-name=resource,o=grid GlueCESEBindSEUniqueID: sunstorm.cnaf.infn.it GlueCESEBindMountInfo: n.a GlueCESEBindCEUniqueID: cremino.cnaf.infn.it:8443/cream-pbs-cert dn: GlueCESEBindSEUniqueID=sunstorm.cnaf.infn.it,GlueCESEBindGroupCEUniqueID=cremino.cnaf.infn.it:8443/cream-pbs-prod,Mds-Vo-name=resource,o=grid GlueCESEBindSEUniqueID: sunstorm.cnaf.infn.it GlueCESEBindMountInfo: n.a GlueCESEBindCEUniqueID: cremino.cnaf.infn.it:8443/cream-pbs-prod Information on Storage Elements Under the branch GlueSEUniqueID check the values of the following fields:\nGlueSALocalID: VO information GlueSEAccessProtocolLocalID : rfio, srm_v2, gsiftp, gsidcap GlueSEImplementationName (deprecated) GlueSEArchitecture GlueSAStateUsedSpace GlueSAStateAvailableSpace GlueSACapability Example:\n$ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\ -b mds-vo-name=resource,o=grid 'objectclass=GlueSE' dn: GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSEImplementationVersion: 1.7.4 GlueSETotalOnlineSize: 8795 GlueSEStatus: Production objectClass: GlueTop objectClass: GlueSE objectClass: GlueKey objectClass: GlueSchemaVersion GlueSETotalNearlineSize: 0 GlueSEArchitecture: multidisk GlueSESizeTotal: 8795 GlueSESizeFree: 5458 GlueSEName: INFN-PAVIA DPM server GlueSchemaVersionMinor: 3 GlueSEUsedNearlineSize: 0 GlueForeignKey: GlueSiteUniqueID=INFN-PAVIA GlueSEUsedOnlineSize: 3336 GlueSchemaVersionMajor: 1 GlueSEImplementationName: DPM GlueSEUniqueID: grid-se.pv.infn.it $ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\ -b mds-vo-name=resource,o=grid 'objectclass=GlueSA' \\ GlueSAAccessControlBaseRule GlueSACapability dn: GlueSALocalID=storage:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSAAccessControlBaseRule: VO:atlas GlueSAAccessControlBaseRule: VO:dteam GlueSAAccessControlBaseRule: VO:infngrid GlueSAAccessControlBaseRule: VO:ops GlueSACapability: InstalledOnlineCapacity=8258 GlueSACapability: InstalledNearlineCapacity=0 dn: GlueSALocalID=ATLASHOTDISK:SR:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSAAccessControlBaseRule: VOMS:/atlas/Role=production GlueSACapability: InstalledOnlineCapacity=536 GlueSACapability: InstalledNearlineCapacity=0 $ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\ -b mds-vo-name=resource,o=grid '(\u0026(objectclass=GlueSA)(GlueSALocalID=storage:replica:online))' \\ GlueSAReservedNearlineSize GlueSAFreeNearlineSize \\ GlueSATotalNearlineSize GlueSAUsedNearlineSize GlueSACapability \\ GlueSATotalOnlineSize GlueSAFreeOnlineSize \\ GlueSAReservedOnlineSize GlueSAStateAvailableSpace \\ GlueSAUsedOnlineSize GlueSAStateUsedSpace dn: GlueSALocalID=storage:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSATotalNearlineSize: 0 GlueSAFreeOnlineSize: 4921 GlueSAUsedNearlineSize: 0 GlueSAFreeNearlineSize: 0 GlueSAReservedNearlineSize: 0 GlueSAStateAvailableSpace: 4921376492 GlueSAReservedOnlineSize: 0 GlueSAUsedOnlineSize: 3336 GlueSAStateUsedSpace: 3336991982 GlueSATotalOnlineSize: 8258 GlueSACapability: InstalledOnlineCapacity=8258 GlueSACapability: InstalledNearlineCapacity=0 Information about other services There is a branch GlueServiceUniqueID for each service published by the site (WMS, LFC, DPM, GRIDICE, LB, MYPROXY, BDII, etc): what discriminates the services are the values of GlueServiceType, example:\nlcg-file-catalog org.glite.wms.WMProxy org.glite.lb.Server srm_v1, SRM Example:\n$ ldapsearch -x -LLL -H ldap://sibilla.cnaf.infn.it:2170 \\ -b mds-vo-name=INFN-CNAF,o=grid 'objectClass=GlueService' \\ GlueServiceType GlueServiceEndpoint GlueServiceName dn: GlueServiceUniqueID=lfcserver.cnaf.infn.it,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: lfcserver.cnaf.infn.it GlueServiceName: INFN-CNAF-lfc GlueServiceType: lcg-file-catalog dn: GlueServiceUniqueID=local-lfcserver.cnaf.infn.it,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: lfcserver.cnaf.infn.it GlueServiceName: INFN-CNAF-lfc GlueServiceType: lcg-local-file-catalog dn: GlueServiceUniqueID=\u003chttp://lfcserver.cnaf.infn.it:8085/,Mds-Vo-name=INFN-CNAF,o=grid\u003e GlueServiceEndpoint: \u003chttp://lfcserver.cnaf.infn.it:8085/\u003e GlueServiceName: INFN-CNAF-lfc-dli GlueServiceType: data-location-interface dn: GlueServiceUniqueID=myproxy.cnaf.infn.it_MyProxy_4027652676,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: myproxy://myproxy.cnaf.infn.it:7512/ GlueServiceName: INFN-CNAF-MyProxy GlueServiceType: MyProxy dn: GlueServiceUniqueID=sibilla.cnaf.infn.it_bdii_site_3877936872,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003cldap://sibilla.cnaf.infn.it:2170/mds-vo-name=INFN-CNAF,o=grid\u003e GlueServiceName: INFN-CNAF-bdii_site GlueServiceType: bdii_site dn: GlueServiceUniqueID=local-http://lfcserver.cnaf.infn.it:8085/,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttp://lfcserver.cnaf.infn.it:8085/\u003e GlueServiceName: INFN-CNAF-lfc-dli GlueServiceType: local-data-location-interface dn: GlueServiceUniqueID=mon-it.cnaf.infn.it_Regional-NAGIOS_2937827985,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://mon-it.cnaf.infn.it:443/nagios\u003e GlueServiceName: INFN-CNAF-Regional-NAGIOS GlueServiceType: Regional-NAGIOS dn: GlueServiceUniqueID=httpg://sunstorm.cnaf.infn.it:8444/srm/managerv2,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: httpg://sunstorm.cnaf.infn.it:8444/srm/managerv2 GlueServiceName: INFN-CNAF-SRM GlueServiceType: SRM dn: GlueServiceUniqueID=albalonga.cnaf.infn.it_org.glite.lb.server_889826742,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://albalonga.cnaf.infn.it:9003/\u003e GlueServiceName: INFN-CNAF-server GlueServiceType: org.glite.lb.server dn: GlueServiceUniqueID=gridit-ce-001.cnaf.infn.it_org.edg.gatekeeper_715226072,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: gram://gridit-ce-001.cnaf.infn.it:2119/ GlueServiceName: INFN-CNAF-gatekeeper GlueServiceType: org.edg.gatekeeper dn: GlueServiceUniqueID=egee-wms-01.cnaf.infn.it_org.glite.wms.WMProxy_2200630265,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://egee-wms-01.cnaf.infn.it:7443/glite_wms_wmproxy_server\u003e GlueServiceName: INFN-CNAF-WMProxy GlueServiceType: org.glite.wms.WMProxy dn: GlueServiceUniqueID=cremino.cnaf.infn.it_org.glite.ce.CREAM_860197007,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://cremino.cnaf.infn.it:8443/ce-cream/services\u003e GlueServiceName: INFN-CNAF-CREAM GlueServiceType: org.glite.ce.CREAM dn: GlueServiceUniqueID=cremino.cnaf.infn.it_org.glite.ce.Monitor_2670664997,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://cremino.cnaf.infn.it:8443/ce-monitor/services/CEMonitor\u003e GlueServiceName: INFN-CNAF-Monitor GlueServiceType: org.glite.ce.Monitor dn: GlueServiceUniqueID=top-bdii01.cnaf.infn.it_bdii_top_1813027130,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003cldap://egee-bdii.cnaf.infn.it:2170/mds-vo-name=local,o=grid\u003e GlueServiceName: INFN-CNAF-bdii_top GlueServiceType: bdii_top [...] See the Site Certification Manual tests HOWTO04\nSee to Resource Centre registration and certification procedure PROC09.\n","categories":"","description":"How to test a Resource Centre during certification","excerpt":"How to test a Resource Centre during certification","ref":"/providers/operations-manuals/howto03_site_certificatoin_giis_check/","tags":"","title":"HOWTO03 Site Certification GIIS Check"},{"body":"This page provides instructions on how to test manually the functionality of the grid and cloud services offered by a site. These checks are executed by the EGI Operations Team for sites that want to be formally included into EGI Production infrastructure. The site must successfully pass either the grid or the cloud certification tests to become part of the EGI Production infrastructure.\nCheck the functionality of the grid elements Be sure that the site’s GIIS URL is contained in the Top level BDII/Information System your NGI will use for your certification.\nNote that the examples here use the Italian NGI and sites. Please substitute with YOUR OWN NGI and site credentials when running the test.\nARC CE checks A first test can be done using ARC’s ngstat command:\n$ /usr/bin/ngstat -q -l -c \u003cCE hostname\u003e -t 20 ... ... plenty of output ... If a monitoring host of your NGI is available, then the probes can easily be executed from there:\nCheck the status of the CE with:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-status \\ -H \u003cCE hostname\u003e -x /etc/nagios/globus/userproxy.pem-ops Status is active Test gsiftp:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-auth -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops gsiftp OK Test the versions of the CA’s:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-caver -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops version = 1.38 - All CAs present Check the versions of ARC and Globus:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-softver \\ -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops nordugrid-arc-0.8.3.1, globus-5.0.3 Copy a file:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-gridftp -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Submit a test job:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-jobsubmit \\ -H \u003cCE hostname\u003e \\ --vo ops -x /etc/nagios/globus/userproxy.pem-ops Job submission successful Check the LFC:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-lfc -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Check the SRM:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-srm -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Before continuing, you may want to make sure that the probes for all services which the CE intends to offer, do actually succeed.\nStorage Element (SE) checks Check if gridftp server on SE works:\n$ uberftp inaf-se-01.ct.pi2s2.it For STORM SE: check if SRM client works (on the published information you can find the right port to use)\n$ /opt/storm/srm-clients/bin/clientSRM \\ ping -e httpg://sunstorm.cnaf.infn.it:8444 ============================================================ Sending Ping request to: httpg://sunstorm.cnaf.infn.it:8444 ============================================================ Request status: statusCode=\"SRM_SUCCESS\"(0) explanation=\"SRM server successfully contacted\" ============================================================ SRM Response: versionInfo=\"v2.2\" otherInfo (size=2) [0] key=\"backend_type\" [0] value=\"StoRM\" [1] key=\"backend_version\" [1] value=\"\u003cFE:1.5.0-1.sl4\u003e\u003cBE:1.5.3-4.sl4\u003e\" ============================================================ Try to write on SE. Be sure your UI is pointing to an IS the SE is contained in (you may use your certification BDII)\n1) Setting a top-bdii that is publishing the SE you have to test\n$ export LCG_GFAL_INFOSYS=\u003cTopBDII hostname\u003e:2170 2) Copy a file from the local filesystem to the SE, registering it in the LFC. This command output will return a SURL that you can use latter for other tests.\nA SURL is a path of the type: srm://srm01.ncg.ingrid.pt/ibergrid/iber/generated/2011-02-01/file4034a935-8d7a-48f4-914f-16f2634d4802\n$ lcg-cr -v --vo \u003cVO\u003e-d\u003cYour SE\u003e \\ -l lfn:/grid/\u003cVO\u003e/test.txt file:\u003c/path/to/your/local/file\u003e 3) Create a new replica in other SE (to check the third-party transfer between 2 SEs)\n$ lcg-rep -v --vo \u003cVO\u003e-d\u003cOther SE\u003e \u003cSURL\u003e 4) List Replicas\n$ lcg-lr -v --vo \u003cVO\u003e lfn:/grid/\u003cVO\u003e/test.txt 5) Delete all replicas\n$ lcg-del -v --vo \u003cVO\u003e-a\u003cguid\u003e Job submission Submit a test job to Cream-CE through the WMS, i.e. using the glite-wms-job-submit command. In case, submit a mpi test job. The NGI_IT certification WMS is gridit-cert-wms.cnaf.infn.it.\nRegistration into 1st level HLR NOTE: this step is needed if your infrastructure uses DGAS as accounting system\nAfter the site entered in production, it needs to register the site resources in the HLR. Ask the site admins to open a ticket towards the HLR administrators, passing them the following information:\ngrid queues names, in the form: gridit-ce-001.cnaf.infn.it:2119/jobmanager-lcgpbs-cert not-grid queues names, in the form: hostname:queue Name, surname ad certificate subject of each site admin Certificate subject of Computing Element Eventually, the site admins have to open a ticket to DGAS support unit asking to enable the forwarding of accounting data from the 2° level HLR to APEL.\nCertification Job The test job checks several things, like the environment on WN and installed RPMs. Moreover it performs some replica management tests. With a grep TEST you may get a summary of the results: in case of errors, you have to see in detail what is gone wrong!\nGlobus checks These checks should be executed depending on the services registered in GOCDB under a Resource Centre. Not all services are compulsory for a RC, but upon registration of new ones, the corresponding tests should be executed.\nGSISSH Initialize grid proxy and check if GSISSH works:\n$ grid-proxy-init $ gsissh USER@HOST -p 2222 /bin/date (Debug with: USER@HOST -vvv -p 2222 /bin/date) GridFTP Check if upload works:\n$ globus-url-copy file:/tmp/test.txt gsiftp://HOST:2811/tmp/test.txt (Debug with: globus-url-copy -dbg -v -vb file:/tmp/test.txt gsiftp://HOST:2811/tmp/test.txt) Check if download works:\n$ globus-url-copy gsiftp://HOST:2811/tmp/test.txt file:/tmp/test.txt (Debug with: globus-url-copy -dbg -v -vb gsiftp://HOST:2811/tmp/test.txt file:/tmp/test.txt) Delete the remote file:\n$ uberftp HOST 'rm /tmp/test.txt' (Debug with: uberftp HOST 'rm /tmp/test.txt' -debug 3) GRAM Check authentication:\n$ globusrun -a -r HOST:2119 Check job submission:\n$ globusrun -s -r HOST:2119 \"\u0026(executable=\"/bin/date\")\" QCG checks QCG Computhng checks The presented tests of QCG-Computing service use the qcg-comp, the client program for QCG-Computing, that may be installed from provided RPMS. In order to connect to QCG-Computing the grid proxy must be created.\nGenerate user’s proxy:\n$ grid-proxy-init Your identity: /C=PL/O=GRID/O=PSNC/CN=Mariusz Mamonski Enter GRID pass phrase for this identity: Creating proxy ............................ Done Your proxy is valid until: Fri Jun 10 06:23:32 2011 Query the QCG-Computing service:\n# the xmllint is used only to present the result in more pleasant way $ qcg-comp -G | xmllint --format - \u003cbes-factory:FactoryResourceAttributesDocument xmlns:bes-factory=\"http://schemas.ggf.org/bes/2006/08/bes-factory\"\u003e … a lot of information … \u003c/bes-factory:FactoryResourceAttributesDocument\u003e Submit a sample job:\n$ qcg-comp -c -J /opt/plgrid/qcg/share/qcg-comp/doc/examples/date.xml Activity Id: ccb6b04a-887b-4027-633f-412375559d73 Query its status:\n$ qcg-comp -s -a ccb6b04a-887b-4027-633f-412375559d73 status = Executing $ qcg-comp -s -a ccb6b04a-887b-4027-633f-412375559d73 status = Finished exit status = 0 QCG Notification checks The tests of QCG-Notification require qcg-ntf-client program to be installed in a system. The program is provided in RPM package.\nCreate a sample subscription:\n$ qcg-ntf-client -d \\ -S \"cons=http://127.0.0.1:2212 top=http://schemas.qoscosgrid.org/comp/2011/04/notification/topic;//*;Full\" ... INF May 17 14:15:51 1128 0xa0262720 [qcg-client-gsoa] Subscribed, subRef: '810917963' ... Remove the created subscription:\n$ qcg-ntf-client -d -U \"id=810917963\" ... INF May 17 14:41:48 3318 0xa0262720 [qcg-client-gsoa] Unsubscribed: '810917963' … Checking the connection with QCG-Computing: In one shell run ‘tail -f’ on the QCG-Computing log file and in the other try to submit a sample job using the qcg-comp program (as described above). Check the tail output if there are no error messages on sending notifications. E.g. the following lines means that the connection problems occurred:\n$ tail -f /opt/qcg/var/log/qcg-comp/qcg-compd.log INF Oct 04 10:55:33 18929 0x2adadc2abe30 [notification_ws] Sending notify: 320f014c-3181-4daf-bbd9-1824b7d8216a -\u003e Queued NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultCode: 'SOAP-ENV:Client' NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultString: 'smcm:ActivityState' NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultDetail: '\u003cSOAP-ENV:Detail xmlns:SOAP-ENV=\"http://schemas.xmlsoap.org/soap/envelope/\"\u003econnect failed in tcp_connect()\u003c/SOAP-ENV:Detail\u003e' ERR Oct 04 10:55:33 18929 0x2adadc2abe30 [notification_ws] Failed to send notification to http://grass1.man.poznan.pl:19011/ QCG Broker checks The basic tests of QCG-Broker service may be proceeded with help of qcg-simple-client, the software that provides a set of commands for interaction with QCG-Broker. qcg-simple-client may be installed from RPMs.\nCreate a sample job description:\n$ cat \u003e sleep.qcg \u003c\u003c EOF #!/bin/bash #QCG queue=plgrid #QCG host=nova.wcss.wroc.pl #QCG persistent sleep 30 EOF Submit a job:\n$ qcg-sub sleep.qcg https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl Your identity: C=PL,O=GRID,O=PSNC,CN=Bartosz Bosak Enter GRID pass phrase for this identity: Creating proxy, please wait... Proxy verify OK Your proxy is valid until Tue Mar 12 14:50:27 CET 2013 UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 59 Minutes 58 Seconds jobId = J1360936230540__0152 Check the job statuses:\n$ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 59 Minutes 49 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: ProxyLifetime: PT0S Status: PREPROCESSING StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: PREPROCESSING StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: LocalFinishTime: $ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 59 Minutes 23 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: ProxyLifetime: PT0S Status: RUNNING StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: RUNNING StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: Fri Feb 15 14:50:47 CET 2013 LocalFinishTime: $ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 56 Minutes 10 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: Fri Feb 15 14:52:17 CET 2013 ProxyLifetime: PT0S Status: FINISHED StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: FINISHED StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: Fri Feb 15 14:52:12 CET 2013 LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: Fri Feb 15 14:50:47 CET 2013 LocalFinishTime: Fri Feb 15 14:52:09 CET 2013 Check the functionality of the cloud elements Sites can provide any (not necessarily all) of the interfaces listed below:\nOpenStack Compute for VM Management CDMI for Object Storage Cloud Compute checks prerequisites AppDB integration Go to AppDB and look for a OS image member of the fedcloud.egi.eu VO (all sites should support), e.g. EGI CentOS 7 image\nCheck that the site is visible in to the AppDB “Availability and Usage” panel for the image. If not, probably the site has not registered the FedCloud VO into their middleware (vmcatcher) or it did not properly configured the BDII provider script.\nFrom that “Availability and Usage” panel, click on the Site name, then on the latest VM Image version, select a resource template (preferably with the smallest quantity of resources (RAM \u0026 CPU)) and click on the “get IDs” button on the right of the resource template. You will get the “Site Endpoint”, “Template ID” and “OCCI ID”. Save these values since they will be needed in the next steps.\nCredentials Generate a set of keys for your user (it is not required to set a phassphrase for the keys, since these are just temporary keys for the test), make sure to set key permissions to 400:\n$ ssh-keygen -t rsa -b 2048 -f tempkey Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in tempkey. Your public key has been saved in tempkey.pub. The key fingerprint is: (...) $ chmod 400 tempkey Create a simple contextualization script, to setup access keys on the machine and test contextualization\n$ cat \u003c\u003c EOF \u003e ctx.txt Content-Type: multipart/mixed; boundary=\"===============4393449873403893838==\" MIME-Version: 1.0 --===============4393449873403893838== Content-Type: text/x-shellscript; charset=\"us-ascii\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\"deploy.sh\" #!/bin/bash echo \"OK\" \u003e /tmp/deployment.log --===============4393449873403893838== Content-Type: text/cloud-config; charset=\"us-ascii\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\"userdata.txt\" #cloud-config users: - name: testadm sudo: ALL=(ALL) NOPASSWD:ALL lock-passwd: true ssh-import-id: testadm ssh-authorized-keys: - `cat tempkey.pub` --===============4393449873403893838==-- EOF Create a proxy in RFC format for your tests:\n$ voms-proxy-init -rfc -voms fedcloud.egi.eu Your identity: /DC=es/DC=irisgrid/O=ifca/CN=Enol-Fernandez-delCastillo Creating temporary proxy .......................................................................... Done Contacting voms2.grid.cesnet.cz:15002 [/DC=org/DC=terena/DC=tcs/OU=Domain Control Validated/CN=voms2.grid.cesnet.cz] \"fedcloud.egi.eu\" Done Creating proxy .............................................................................. Done Your proxy is valid until Fri Nov 14 04:59:26 2014 OpenStack Compute checks (org.openstack.nova service type) Export the following variables on your shell (keystone URL can be obtained from GOCDB URL of the endpoint)\n$ export OS_AUTH_URL= \u003ckeystone URL\u003e $ export OS_AUTH_TYPE=v2voms $ export OS_X509_USER_PROXY=$X509_USER_PROXY Get the list of tenants supporting your proxy\n$ keystone_tenants Tenant id: 999f045cb1ff4684a15ebb338af69460 Tenant name: VO:fedcloud.egi.eu Enabled: True Description: VO fedcloud.egi.eu Export the tenant name as shown from the command in the following variable:\n$ export OS_PROJECT_NAME=\"VO:fedcloud.egi.eu\" Describe the available flavors, check also that the template ID provided by AppDB is listed:\n$ openstack flavor list +----+-----------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +----+-----------+-------+------+-----------+-------+-----------+ | 1 | m1.tiny | 512 | 0 | 0 | 1 | True | | 2 | m1.small | 2000 | 10 | 20 | 1 | True | | 3 | m1.medium | 4000 | 10 | 40 | 2 | True | | 4 | m1.large | 7000 | 20 | 80 | 4 | True | | 5 | m1.xlarge | 14000 | 30 | 160 | 8 | True | +----+-----------+-------+------+-----------+-------+-----------+ Describe available images, again check that the ID provided by AppDb is listed\n$ openstack image list +--------------------------------------+----------------------------------------------+ | ID | Name | +--------------------------------------+----------------------------------------------+ | 1414f242-d6d1-4a8c-8b26-8c0ada32f343 | IFCA Fedora Cloud 23 | | 8a700834-04b4-4e91-a3d5-9246ef95167e | LW Jupyter-R Ubuntu 14.04 | | 7f361fba-21d6-40ca-892d-17aa60b63a66 | IFCA CentOS 7 | | f3544cc8-421f-4d93-ac35-eba7fdc75329 | IFCA CentOS 6 | ... +--------------------------------------+----------------------------------------------+ Start a VM (using the flavor and images checked above and the context file created previously). The returned ID will be used in the following commands:\n$ openstack server create \\ --flavor \u003cflavor\u003e --image \u003cimage id\u003e \\ --user-data ctx.txt test +--------------------------------------+------------------------------------------------------+ | Field | Value | +--------------------------------------+------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | config_drive | | | created | 2016-03-01T13:07:10Z | | flavor | m1.tiny (1) | | hostId | | | id | 5d3ed7d6-d5ac-4f09-a353-0c2bd0fbd0ea | | image | IFCA CentOS 7 (7f361fba-21d6-40ca-892d-17aa60b63a66) | | key_name | None | | name | test | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | project_id | 999f045cb1ff4684a15ebb338af69460 | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2016-03-01T13:07:11Z | | user_id | a31b8c452b594369a49a8329103e241a | +--------------------------------------+------------------------------------------------------+ Check that the VM is active by describing it (it may take a few minutes):\n$ openstack server show \u003cvm id\u003e +---------------------+--------+ | Field | Value | +---------------------+--------+ (...) | OS-EXT-STS:vm_state | active | (...) +---------------------+--------+ If the VM does not have a public IP, you will need to get an IP for it\n$ openstack ip floating pool list +-------------+ | Name | +-------------+ | nova | +-------------+ $ openstack ip floating pool create \u003cpool name\u003e +-------------+----------------+ | Field | Value | +-------------+----------------+ | fixed_ip | None | | id | 1265 | | instance_id | None | | ip | 193.146.75.245 | | pool | nova | +-------------+----------------+ $ openstack ip floating add \u003cip\u003e \u003cvm id\u003e The VM should have now a public IP available when shown:\n$ openstack server show \u003cvm id\u003e +-----------+-------------------------------------+ | Field | Value | +-----------+-------------------------------------+ (...) | addresses | private=172.16.8.14, 193.146.75.245 | (...) +-----------+-------------------------------------+ ssh to the machine to the provided IP (the options avoid problems when different VMs have the same IP, don’t use them in production) and check that contextualization script was executed:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\ testadm@ \u003cip\u003e \"cat /tmp/deployment.log\" Warning: Permanently added '193.146.75.245' (ECDSA) to the list of known hosts. OK Create a storage volume:\n$ openstack volume create --size 1 test +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | created_at | 2016-03-01T13:29:44.392423 | | display_description | None | | display_name | test | | id | 8f46046d-cd9b-4219-9ce7-f0abe30ad992 | | properties | | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | None | +---------------------+--------------------------------------+ And show its status:\n$ openstack volume show \u003cvol id\u003e Attach it to the running VM:\n$ openstack server add volume \u003cvm id\u003e \u003cvol id\u003e And check it’s attached:\n$ openstack volume show 8f46046d-cd9b-4219-9ce7-f0abe30ad992 +---------------------+---------------------------------------------------------+ | Field | Value | +---------------------+---------------------------------------------------------+ | attachments | [{u'device': u'/dev/vdb', | | | u'server_id': u'21f6123f-926c-4816-b4fb-53df91907a63', | | | u'id': u'8f46046d-cd9b-4219-9ce7-f0abe30ad992', | | | u'volume_id': u'8f46046d-cd9b-4219-9ce7-f0abe30ad992'}] | | availability_zone | nova | | bootable | false | | created_at | 2016-03-01T13:29:44.000000 | | display_description | None | | display_name | test | | id | 8f46046d-cd9b-4219-9ce7-f0abe30ad992 | | properties | | | size | 1 | | snapshot_id | None | | source_volid | None | | status | in-use | | type | None | +---------------------+---------------------------------------------------------+ And login into the machine to create a filesystem, mount it, create a file, and umount:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\ testadm@ \u003cip_addr\u003e testadm $ sudo mke2fs \u003cdevice_id\u003e (...) testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ touch /mnt/test testadm $ ls -l /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Delete the VM:\n$ openstack server delete \u003cvm id\u003e And create a new one with the volume attached directly (substitute vdb for the same device name shown above):\n$ openstack server create \\ --flavor \u003cflavor\u003e --image \u003cimage id\u003e \\ --block-device-mapping vdb= \u003cvolume id\u003e \\ --user-data ctx.txt test +-----------------------------+---------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | 9imSRC9EZhhX | | config_drive | | | created | 2016-03-01T13:45:21Z | | flavor | m1.tiny (1) | | hostId | | | id | 10000d50-239b-4e86-bbd8-224143d6d346 | | image | Image for EGI Centos 6 [CentOS/6/KVM]_egi | | key_name | None | | name | test | | progress | 0 | | project_id | fffd98393bae4bf0acf66237c8f292ad | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2016-03-01T13:45:23Z | | user_id | 6c254b295af64644904a813db0d3d88a | +-----------------------------+---------------------------------------------+ Assign the public IP (if it does not have one already):\n$ openstack ip floating add \u003cip\u003e \u003cvm id\u003e And check that the volume is attached and usable:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\ testadm@ \u003cnew_vm_ip_addr\u003e testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ ls -ltr /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Finally delete VM, volume and IP address (NOTE: use the id of the IP as returned by openstack ip floating list\\!): $ openstack server delete \u003cvm id\u003e $ openstack volume delete \u003cvolume id\u003e $ openstack ip floating delete \u003cip id\u003e Cloud Storage (CDMI) checks (eu.egi.cloud.storage-management.cdmi service type) List the content of the repository:\n$ bcdmi -e \u003ccdmi_endpoint\u003e list / Create a test directory:\n$ bcdmi -e \u003ccdmi_endpoint\u003e mkdir test { \"completionStatus\": \"Complete\", \"objectName\": \"test/\", \"capabilitiesURI\": \"/cdmi/AUTH_df37f5b1ebc94604964c2854b9c0551f/cdmi_capabilities/container/\", \"parentURI\": \"/cdmi/AUTH_df37f5b1ebc94604964c2854b9c0551f/\", \"objectType\": \"application/cdmi-container\", \"metadata\": {} } Create a test file and upload it to the created directory:\n$ echo \"TEST OK\" \u003e testfile $ bcdmi -e \u003ccdmi_endpoint\u003e put -T testfile test/test.txt** Try to download back the file and compare to previous:\n$ bcdmi -e \u003ccdmi_endpoint\u003e get test/test.txt -o testfile.downloaded $ diff testfile testfile.downloaded \u0026\u0026 echo \"Files are equal\" \\ || echo \"Files differ\" Files are equal Delete the file:\n$ bcdmi -e \u003ccdmi_endpoint\u003e delete test/test.txt Check that the file is not present anymore\n$ bcdmi -e \u003ccdmi_endpoint\u003e list test/ Upload the file again and recursively delete the directory:\n$ bcdmi -e \u003ccdmi_endpoint\u003e put -T testfile test/test.txt $ bcdmi -e \u003ccdmi_endpoint\u003e delete -r test/ Check that the folder does not exist anymore\n$ bcdmi -e \u003ccdmi_endpoint\u003e list / See Site Certification GIIS Check HOWTO03.\nSee to Resource Centre registration and certification procedure PROC09.\n","categories":"","description":"Manual testing of services offered by a site","excerpt":"Manual testing of services offered by a site","ref":"/providers/operations-manuals/howto04_site_certification_manual_tests/","tags":"","title":"HOWTO04 Site Certification Manual tests"},{"body":"Transition to HEP SPEC, a new CPU benchmark Q1: Why adopting HEP SPEC 06? The traditional si2k CPU benchmark is now obsolete and it is time to move to HEP SPEC, a new CPU benchmark that will replace si2k and will become the reference benchmark for accounting purposes.\nDetailed description of the reasons are provided on the transition to a new CPU benchmarking unit for the WLCG.\nQ2: What is HEP SPEC 06? The HEP-SPEC06 benchmark is designed to scale with the performances of the high-energy physics codes on similar machines. The goal was to have an accuracy of ± 5% but for the moment the agreement is significantly higher.\nThe measurement of HEP-SPEC repeated on identical machines varies less than 1%. If the computing machines are similar, i.e. same processors and at least 2 GB per core, the results obtained are very close, within some percent, so that it is unnecessary to perform measures on all computing hosts. It is enough to do the measurement for one type of processor and consider it valid for all the machines with the same processor.\nIf you are using different OS and specially different compilers, the data will change.\nQ3: Where can I find information about HEP SPEC 06 measurements? Some example results are available on the HEPIX group-page, where one can see the differences between gcc3.4.x and gcc4.1.x.\nAdditional results tables are available from various EGI partners:\nGRIDPP If you don’t find your computing machine in that table, then it is better to try to do the measurement because extrapolating the results increases further the error.\nQ4: How can I run the HEP SPEC 06 benchmark? If you want to make HEP-SPEC06 on your own own, detailed instructions are available at CERN wiki.\nIn Short you need the following:\nA machine with any version of Linux compatible with Scientific Linux (RHEL, SL, SLC, CentOS) The gcc compiler should be installed Configuration files and run script (available as a gzipped tar archive from the CERN Wiki). The archive’s md5sum is 9fed92b8d515b88904705f76809c4028 A tar ball of the SPECcpu2006 DVD called SPEC2006_v11.tar.bz2 that should be in the same directory as the run script Q5: My site already adopted HEP SPEC 06. Do I still need to publish SpecInt2000? The transition to HEP-SPEC does not eliminate the need to publish the computing power in SpecInt2000 (due to backward compatibility with sites not publishing yet HEP-SPEC). In this case you may calculate the value SpecInt2000 starting from HEP-SPEC through the following relation:\nvalue_kSI2K = value_HEP-SPEC / 4 (or value_HEP-SPEC = 4 * value_kSI2K) For the GlueHostBenchmarkSI00 attribute in the GLUE v1.3 schema the following relation is easier to use:\nvalue_SI00 = value_HEP-SPEC * 250 rounded to the nearest integer Q6: How are HEP SPEC 06 results set in YAIM? The YAIM variable CE_OTHERDESCR is used to set the GlueHostProcessorOtherDescription attribute. The value of this variable MUST be defined in your site-info.def file as:\nCores=\u003cCE_LOGCPU/CE_PHYSICALCPU\u003e [, Benchmark=\u003cvalue\u003e-HEP-SPEC06] where the ratio CE_LOGCPU / CE_PHYSICALCPU means the average number of cores per physical CPU in a sub-cluster; in the case of (slightly) heterogeneous sub-clusters it could be non-integer. The second value of this attribute MUST be published only in the case the CPU power of the sub-cluster has been computed using the HEP-SPEC06 benchmark. The Benchmark value must be the average HEP_SPEC06 result per core, in the sub-cluster.\nThese variables are set in your site-info.def file. After this, the variables need to be published by the CE’s resource BDII, configured e.g. by standard YAIM commands.\nThe total CPU capacity of the cluster is computed as Benchmark * CE_LOGCPU.\n","categories":"","description":"Questions about Transition to HEP SPEC, a new CPU benchmark.","excerpt":"Questions about Transition to HEP SPEC, a new CPU benchmark.","ref":"/providers/operations-manuals/faq-hepspec06/","tags":"","title":"FAQ HEP SPEC 06"},{"body":"Overview This tutorial describes how to create a Virtual Machine in the EGI Federation, leveraging oidc-agent to retrieve ODIC tokens from EGI Check-in, fedcloudclient to simplify interacting with the EGI Cloud Compute service, terraform and Ansible to simplify deploying an infrastructure. EGI Dynamic DNS is also used to assign a domain name to the virtual machine, which can then be used to get a valid TLS certificate from Let’s Encrypt.\nStep 1: Signing up for an EGI Check-in account Create an EGI account with Check-in.\nStep 2: Enrolling to a Virtual Organisation Once your EGI account is ready you need to join a Virtual Organisation (VO). Here are the steps to join a VO. Explore the list of available VOs in the Operations Portal. We have a dedicated VO called vo.access.egi.eu for piloting purposes. If you are not sure about which VO to enrol to, please request access to the vo.access.egi.eu VO with your EGI account by visiting the enrolment URL. Check AppDB to see the list of Virtual Appliances and Resource Providers participating in the vo.access.egi.eu VO. AppDB is one of the service in the EGI Architecture.\nThis tutorial will assume you are using vo.access.egi.eu, adapt as required for your specific environment.\nStep 3: Creating a VM Once your membership to a VO has been approved you are ready to create your first Virtual Machine.\nThe OpenID Connect (OIDC) protocol is used to authenticate users and authorise access to Cloud Compute resources that are integrated with EGI Check-in.\nWhile it’s not mandatory, a convenient way to manage the OIDC token is to use oidc-agent.\nSetting up oidc-agent oidc-agent is a set of tools to manage OpenID Connect tokens and make them easily usable from the command line.\nInstall oidc-agent according to official documentation, once oidc-agent is installed it can be used to retrieve an OIDC access token from EGI Check-in.\n# Generating configuration for EGI Check-in $ oidc-gen --pub --issuer https://aai.egi.eu/auth/realms/egi \\ --scope \"email \\ eduperson_entitlement \\ eduperson_scoped_affiliation \\ eduperson_unique_id\" egi # Listing existing configuration $ oidc-add -l # Requesting an OIDC access token $ oidc-token egi # Exporting a variable with a Check-in OIDC access token to be used with OpenStack # XXX access tokens are short lived, relaunch command to obtain a new token # This is *not* required for following this tutorial, it's an example $ export OS_ACCESS_TOKEN=$(oidc-token egi) It’s possible to automatically start oidc-agent in your shell initialisation, example that can be added to ~/.bash_profile or ~/.zshrc:\nif command -v oidc-agent-service \u0026\u003e /dev/null eval $(oidc-agent-service use) # for fedcloudclient, selecting egi configuration generated with oidc-gen export OIDC_AGENT_ACCOUNT=egi fi When using oidc-agent-service, fedcloudclient will be able to automatically request a new access token from oidc-agent.\nSee full documentation.\nInstalling fedcloudclient and ansible fedcloudclient is an high-level Python package for a command-line client designed for interaction with the OpenStack services in the EGI infrastructure. The client can access various EGI services and can perform many tasks for users including managing access tokens, listing services, and mainly execute commands on OpenStack sites in EGI infrastructure.\nfedcloudclient can leverage oidc-agent if it’s installed and properly configured.\nfedcloudclient and openstackclient, the official OpenStack python client, will be used to interact with the EGI Cloud Compute service.\nRequired python dependencies are documented in a requirements.txt file (Ansible will be used at a later stage, but is installed at the same time):\nopenstackclient fedcloudclient ansible For keeping the main system tidy and isolating the environment, the python packages will be installed in a dedicated python virtualenv:\n# Creating an arbitrary directory where to store python virtual environments $ mkdir -p ~/.virtualenvs # Creating a python 3 virtual environment $ python3 -m venv ~/.virtualenvs/fedcloud # Activating the virtual environment $ source ~/.virtualenvs/fedcloud # Installing required python packages in the virtual environment $ pip install -r requirements.txt Identifying a suitable cloud site It’s possible to deploy an OpenStack Virtual Machine (VM) on any of the sites supporting the Virtual Organisations (VO) you are a member of.\nOnce fedcloudclient is installed it’s possible to get information about the OIDC token accessed via oidc-agent.\n# Listing the VO membership related to the OIDC access token $ fedcloud token list-vos In order to look for sites supporting a particular VO, you can use the EGI Application Database.\nYou can retrieve information from the AppDB about the sites supporting the vo.access.egi.eu VO.\nIn the following example, the IN2P3-IRES site supporting the vo.access.egi.eu VO will be used, see Step 2: Enrolling to a Virtual Organisation to request access.\nDeploying the Virtual Machine with terraform Instead of creating the server manually, it is possible to use terraform with EGI Cloud Compute.\nThe Terraform OpenStack provider provides official documentation.\nTerraform provides installation instructions for all usual platforms.\nOnce terraform is installed locally, we will create a deployment as documented in the following sections.\nSetting up the environment The OS_* variables that will be used by terraform can be generated using fedcloudclient.\n# Activating the virtual environment $ source ~/.virtualenvs/fedcloudclient/bin/activate # Exporting variable for VO and SITE to avoid having to repeat them $ export EGI_VO='vo.access.egi.eu' $ export EGI_SITE='IN2P3-IRES' eval $(fedcloud site env) # Obtaining an OS_TOKEN for terraform # XXX this breaks using openstackclient: use fedcloudclient # or unset OS_TOKEN before using openstackclient $ export OS_TOKEN=$(fedcloud openstack token issue --site \"$EGI_SITE\" \\ --vo \"$EGI_VO\" -j | jq -r '.[0].Result.id') Describing the terraform variables The main terraform configuration file, main.tf is using variables that have to be described in a vars.tf file:\n# Terraform variables definition # Values to be provided in a *.tfvars file passed on the command line variable \"internal_net_id\" { type = string description = \"The id of the internal network\" } variable \"public_ip_pool\" { type = string description = \"The name of the public IP address pool\" } variable \"image_id\" { type = string description = \"VM image id\" } variable \"flavor_id\" { type = string description = \"VM flavor id\" } variable \"security_groups\" { type = list(string) description = \"List of security groups\" } The SITE and VO specific values for those variables will be identified and documented in a $EGI_SITE.tfvars file.\nIdentifying the cloud resources Once the environment is properly configure, fedcloudclient is used to gather information and identify flavor, image, network and security groups for the site you want to use.\nfedcloud openstack currently requires an explicit --site parameter, this will be addressed in a future fedcloud release. In the meantime the $EGI_SITE environment variable can be reused using --site \"$EGI_SITE\".\n# Selecting an image $ fedcloud select image --image-specs \"Name =~ 'EGI.*22'\" # Selecting a flavor $ fedcloud select flavor --flavor-specs \"RAM\u003e=2096\" \\ --flavor-specs \"Disk \u003e 10\" --vcpus 2 # Identifying available networks $ fedcloud openstack --site \"$EGI_SITE\" network list $ fedcloud select network --network-specs default # Identifying security groups $ fedcloud openstack --site \"$EGI_SITE\" security group list # Listing rules of a specific security group $ fedcloud openstack --site \"$EGI_SITE\" security group rule list default Documenting the cloud resources for the selected site The chosen flavor, image, network and security group should be documented in a $EGI_SITE.tfvars file that will be passed as an argument to terraform commands.\nThe network configuration can be tricky, and is usually dependant on the site. For IN2P3-IRES, one has to request a floating IP from the public network IP pool ext-net, and assign this floating IP to the created instance. For another site it may not be needed, in that case the main.tf will have to be adjusted accordingly.\nSee the example IN2P3-IRES.tfvars below, to be adjusted according to the requirements and to the selected site and VO:\n# Internal network internal_net_id = \"7ae7b0ca-f122-4445-836a-5fb7af524dcb\" # Public IP pool for floating IPs public_ip_pool = \"ext-net\" # Flavor: m1.medium flavor_id = \"ab1fbd4c-324d-4155-bd0f-72f077f0ebce\" # Image for EGI CentOS 7 # https://appdb.egi.eu/store/vappliance/egi.centos.7 image_id = \"09093c70-f2bb-46b8-a87f-00e2cc0c8542\" # Image: EGI CentOS 8 # https://appdb.egi.eu/store/vappliance/egi.centos.8 # image_id = \"38ced5bf-bbfd-434b-ae41-3ab35d929aba\" # Image: EGI Ubuntu 22.04 # https://appdb.egi.eu/store/vappliance/egi.ubuntu.22.04 # image_id = \"fc6c83a3-845f-4f29-b44d-2584f0ca4177\" # Security groups security_groups = [\"default\"] Creating the main terraform deployment file To be more reusable, the main.tf configuration file is referencing variables described in the vars.tf file created previously, and will take the values from the $EGI_SITE.tfvars file passed as an argument to the terraform command.\n# Terraform versions and providers terraform { required_version = \"\u003e= 0.14.0\" required_providers { openstack = { source = \"terraform-provider-openstack/openstack\" version = \"~\u003e 1.35.0\" } } } # Allocate a floating IP from the public IP pool resource \"openstack_networking_floatingip_v2\" \"egi_vm_floatip_1\" { pool = var.public_ip_pool } # Creating the VM resource \"openstack_compute_instance_v2\" \"egi_vm\" { name = \"egi_test_vm\" image_id = var.image_id flavor_id = var.flavor_id security_groups = var.security_groups user_data = file(\"cloud-init.yaml\") network { uuid = var.internal_net_id } } # Attach the floating public IP to the created instance resource \"openstack_compute_floatingip_associate_v2\" \"egi_vm_fip_1\" { instance_id = \"${openstack_compute_instance_v2.egi_vm.id}\" floating_ip = \"${openstack_networking_floatingip_v2.egi_vm_floatip_1.address}\" } # Create inventory file for Ansible resource \"local_file\" \"hosts_cfg\" { content = templatefile(\"${path.module}/hosts.cfg.tpl\", { ui = \"${openstack_networking_floatingip_v2.egi_vm_floatip_1.address}\" } ) filename = \"./inventory/hosts.cfg\" } The last resource is relying on templatefile to populate the inventory file that will later be used by ansible.\nInitial configuration of the VM using cloud-init cloud-init is the industry standard multi-distribution method for cross-platform cloud instance initialization.\nThe initial configuration of the VM is done using a cloud-init.yaml file.\nThe curl call from the runcmd block in the cloud-init.yaml configuration below, will register the IP of the virtual machine in the DNS zone managed using the EGI Dynamic DNS service, allowing to access the virtual machine using a fully qualified hostname and allowing to retrieve a Let’s Encrypt certificate.\nPlease look at the EGI Dynamic DNS documentation for instructions on creating the configuration for a new host.\nThe users block in the cloud-init.yaml configuration below, will create a new user with password-less sudo access.\nWhile this egi user can only be accessed via the specified SSH key(s), setting a user password and requesting password verification for using sudo should be considered, as a compromise of this user account would mean a compromise of the complete virtual machine.\nReplace \u003cNSUPATE_HOSTNAME\u003e, \u003cNSUPDATE_SECRET\u003e, \u003cSSH_AUTHORIZED_KEY\u003e (the content of your SSH public key) by the proper values.\n--- # cloud-config runcmd: - [ curl, \"https://\u003cNSUPATE_HOSTNAME\u003e:\u003cNSUPDATE_SECRET\u003e@nsupdate.fedcloud.eu/nic/update\", ] users: - name: egi gecos: EGI primary_group: egi groups: users shell: /bin/bash sudo: ALL=(ALL) NOPASSWD:ALL ssh_authorized_keys: - \u003cSSH_AUTHORIZED_KEY\u003e packages: - vim package_update: true package_upgrade: true package_reboot_if_required: true Launching the terraform deployment Now that all the files have been created, it’s possible to deploy the infrastructure, currently only a single VM, but it can easily be extended to a more complex setup, using terraform:\n# Initialising working directory, install dependencies $ terraform init # Reviewing plan of actions for creating the infrastructure # Use relevant site-specific config file $ terraform plan --var-file=\"${EGI_SITE}.tfvars\" # Creating the infrastructure # Manual approval can be skipped using -auto-approve # The SERVER_ID will be printed (openstack_compute_instance_v2.scoreboard) $ terraform apply --var-file=\"${EGI_SITE}.tfvars\" # Wait a few minutes for the setup to be finalised # Connecting to the server using ssh $ ssh egi@$NSUPATE_HOSTNAME From here you can extend the cloud-init.yaml and/or use Ansible to configure the remote machine, as well as doing manual work via SSH.\nDebugging terraform The token used by Terraform for accessing OpenStack is short lived, it will have to be renewed from time to time.\n# Creating a new token to access the OpenStack endpoint $ export OS_TOKEN=$(fedcloud openstack token issue --site \"$EGI_SITE\" \\ --vo \"$EGI_VO\" -j | jq -r '.[0].Result.id') It is possible to print a verbose/debug output to get details on interactions with the OpenStack endpoint.\n# Debugging $ OS_DEBUG=1 TF_LOG=DEBUG terraform apply --var-file=\"${EGI_SITE}.tfvars\" Destroying the resources created by terraform # Destroying the created infrastructure $ terraform destroy --var-file=\"${EGI_SITE}.tfvars\" Step 4: Using Ansible Ansible can be used to manage the configuration of the crated virtual machine.\nThe terraform deployment generated an Ansible inventory, inventory/hosts.cfg, that can directly be used by Ansible.\nConfigure a basic Ansible environment in the ansible.cfg file:\n[defaults] # Use user created using cloud-init.yml remote_user = egi # Use inventory file generated by terraform inventory = ./inventory/hosts.cfg [privilege_escalation] # Escalate privileges using password-less sudo become = yes Then you can verify that the Virtual Machine is accessible by Ansible:\n# Confirming ansible can reach the VM $ ansible all -m ping Once this works, you can create advanced playbooks to configure your deployed host(s).\nVarious Ansible roles are available in the egi-qc/ansible-playbooks repository and in the EGI Federation GitHub organisation.\nA style guide for writing Ansible roles is providing a skeleton that you can use fore creating new roles.\nAdditional resources Additional resources are available, and can help with addressing different use cases, or be used as a source of inspiration:\negi-qc/deployment-howtos: Deployment recipes extracted from Jenkins builds for the UMD and CMD products EGI-ILM/fedcloud-terraform: providing an advanced helper script allowing to interact with EGI Cloud Compute. EGI-ILM/automated-containers: providing documentation for automated on-demand execution of Docker containers Asking for help If you find issues please do not hesitate to contact us.\n","categories":"","description":"Step by step guide to automating the deployment using Ansible with Terraform, oidc-agent and fedcloudclient\n","excerpt":"Step by step guide to automating the deployment using Ansible with …","ref":"/users/tutorials/oidc-agent-fedcloudclient-terraform/","tags":"","title":"Automating with oidc-agent, fedcloudclient, terraform and Ansible"},{"body":"Overview The training infrastructure is a resource pool within the EGI Federated Cloud infrascture providing IaaS as well as access services (login, application catalogue and application management portal) for face-to-face events, online training courses or self-paced learning modules.\nThe training infrastructure is integrated with Check-in allowing trainers to generate short-lived user accounts for training participants. Such accounts can identify students individually, and for a limited lifetime - typically few hours or days, depending on the length of the training event - allow them to interact with the services.\nThe infrastructure currently includes enough capacity to scale up to class-room size audiences, approximately up to 100 participants.\nAn introductory slideset and poster of the infrastructure from EGI Community Forum 2015, Bari. (Outdated in some parts).\nUsage models The training infrastructure is suitable for two types of courses:\nCloud computing courses: Such courses teach students about IaaS clouds and on how Virtual Appliances, Virtual Machines, block storage and other types of ’low level’ resources are managed. For such courses, the trainer does not need to deploy applications or online services in advance of the course. The applications/services will be deployed by the students themselves as training exercises. Such courses typically target developers or other rather technical members of scientific communities or projects. Scientific courses: Such courses teach scientists or developers about a specific software suite relevant for their work. For example a specific gene sequence analysis application, an earthquake visualisation tool, or a data processing pipeline. In this operational mode, the trainer deploys the domain specific application/tool on the training infrastructure before the training and the students interact directly with those applications/tools without even knowing where those are deployed and running. Depending on how computationally or data intensive the exercises are, multiple students may share a single software deployment instance, or each student can have their own. The configuration can be controlled by the trainer when the setup is deployed. In both cases the deployment of applications/tools/services can happen in the form of ‘Virtual Appliances’ (VAs), and block storage - the latter basically behaving like a virtual USB drive that can be attached/detached to VMs to provide data and storage space for applications.\nThe AppDB has a growing catalogue of Virtual Appliances that includes both basic applications (e.g. latest version of clean Linux distribution) and more specialised applications (e.g. Jupyter Notebook). The list of VAs available on the training infrastructure is configurable and listed in the training.egi.eu VO entry of AppDB.\nThe VMOps can be used as web interface for both trainers and students to deploy and manage VMs.\nAvailable resources The available resources are offered by a set of providers included in the training.egi.eu VO Operation Level Agreement (OLA). Check the document for the exact amount of resources and conditions of access for each provider.\nJoin the training infrastructure! Do you want to join as a resource provider? Please email at support _at_ egi.eu. The list of providers and VAs is also discoverable in the training.egi.eu VO entry of AppDB. The VO is also described at the EGI Operations Portal training.egi.eu VO id card.\nBooking the infrastructure The infrastructure currently includes enough capacity to scale up to class-room size audiences, approximately up to 100 participants.\nDo you want to book the infrastructure for a course? Please send a request through our site.\n","categories":"","description":"The training infrastructure on EGI Cloud\n","excerpt":"The training infrastructure on EGI Cloud\n","ref":"/users/training/","tags":"","title":"Training Infrastructure"},{"body":"Here we provide specific documentation for communities that are using EGI services.\n","categories":"","description":"Documentation for EGI Communities\n","excerpt":"Documentation for EGI Communities\n","ref":"/users/communities/","tags":"","title":"Community-specific Documentation"},{"body":"This section contains guidelines for software development to be considered when developing a product for the EGI Federation.\nNote Those guidelines are providing a set of aspects to consider together with some reference documentation, and are not meant to be exhaustive, further research is welcome. Licensing Adopt an OSI-approved license; we recommend a business compatible license such as MIT or Apache 2.0 The license should provide unlimited access rights to the EGI Community Source code access Maintain the source code in a publicly-accessible software repository like GitHub You can request to use the EGI Federation GitHub organisation If using another repository, a copy can be kept synchronized under the EGI Federation GitHub organisation All releases must be tagged appropriately Code style Style guidelines must be defined and documented. A general style guide may be made available by EGI as a default. If you are extended an existing software component, then you must use the code style defined by the related product team If you are developing a new component, then you must use the code style practices for the programming language of your choice Code style compliance should be checked by automated means for every change Best practices The industry best practices should be adopted As far as possible adopt 12 factor application pattern Configuration Management modules to deploy and configure the products should be provided and distributed through the corresponding distribution channels Ansible is recommended, roles can be hosted under the EGI Federation organization in Ansible Galaxy Security best practices Security best practices must be taken into account Security-related aspects must be considered from the beginning Security issues must be addressed in priority and following the EGI SVG recommendations and must take into account the points mentioned in the SVG Secure Coding and Software Security Checklist The Open Web Application Security Project (OWASP) provides extensive documentation, standards (such as ASVS) and tools to ensure that your software has capabilities to defend against common attacks. Suggested material and references Microsoft® Open Source Software (OSS) Secure Supply Chain (SSC) Framework Simplified Requirements Secure Software Development Framework | CSRC NIST: Cybersecurity Cybersecurity \u0026 Infrastructure Security Agency Microsoft Security Development Lifecycle (SDL) Introduction to Software Security (video course by Trusted CI, WISC, UAB) Tooling and telemetry If the project is an application or an infrastructure component, it should follow as close as possible the monitoring guidelines set by the Site Reliability Engineer book Testing Unit tests should be provided Unit testing should be automated Code coverage should be computed as part of the continuous integration When possible functional and integration tests should be automated If it’s not possible for some components the product team should provide report about those tests for the new releases Code Review A team of code reviewers shall be specified for each project Changes must be reviewed by the code review team prior to be merged using a Pull Request-like workflow Documentation Documentation must be treated like code Written in a plain text-based format Management in a repository and versioning Markdown and reStructuredText formats are recommended The documentation for an EGI Service should be submitted for publishing on the EGI Documentation site using the related GitHub repository A “Community First” approach should be followed Contributing, onboarding and community guidelines should be available from the start of the project Documentation should be available for Developers Administrators (Deployment and administration) End users Artefacts Release and delivery Artefacts should be tagged according to Semantic Versioning or to Calendar Versioning that can be more appropriate for OS images Artefacts should have a DOI and associated short writeup (see Documentation above) Artefacts should be published in publicly available repositories EGI Application Database can be used for this UMD can be used to distribute middleware components It should be possible to automatically build production-grade distribution artefacts from the repository using provided build scripts and files Where appropriate, native packages for EGI Federation-supported Operating System should be provided: CentOS 7 (rpm) Ubuntu 20.04 LTS (deb) Containers are accepted Containers must be compatible with EGI Cloud services Request for information You can ask for more information about them by contacting us via our site.\n","categories":"","description":"Guidelines for software development","excerpt":"Guidelines for software development","ref":"/internal/guidelines-software-development/","tags":"","title":"Guidelines for software development"},{"body":"","categories":"","description":"","excerpt":"","ref":"/_footer/","tags":"","title":""},{"body":"The EGI documentation is written in Markdown, uses the Docsy theme, and is built using Hugo.\nNote The documentation covering the EGI Services is maintained by the EGI Community and coordinated by the EGI Foundation. Everyone is invited to participate by following the Contributing Guide. ","categories":"","description":"About EGI Documentation","excerpt":"About EGI Documentation","ref":"/about/","tags":"","title":"About"},{"body":" Welcome to the EGI Documentation! EGI is an international e-Infrastructure providing advanced computing and data analytics services for research and innovation. The EGI Federation offers a wide range of services for compute, storage, data and support. This is the start page for the user and provider documentation of the EGI Services. For Users How to access and use the EGI services to benefit from advanced computing. For Providers How to join the EGI Federation and operate services for advanced computing. Learn about EGI The EGI Federation is supporting many different user communities and open science projects. Contribute! We use a GitHub contributions workflow, new authors are always welcome! Follow on Twitter Find out about new EGI features and see how our users are using EGI services. ","categories":"","description":"Documentation related to EGI activities","excerpt":"Documentation related to EGI activities","ref":"/","tags":"","title":"EGI documentation"},{"body":"While most other data management services are available in the EGI infrastructure, there are specialized services in the EGI portfolio that are offered for in-house installations by research communities, with support for customization and configuration from EGI.\nWhat is it? The openRDM service, an Research Data Management (RDM) tool, offers advanced organisation of data during ongoing research projects, as an integrated environment with data management and digital lab notebook.\nopenRDM combines a data management platform with a digital lab notebook and a sample and protocol management system. It enables scientists to meet the ever-increasing requirements from funding agencies, journals, and academic institutions to publish data according to the FAIR data principles – according to which data should be Findable, Accessible, Interoperable and Reusable.\nNote The openRDM service is based around the active research data management (ARDM) platform openBIS, see the documentation for more details. ","categories":"","description":"Organise data in research projects with openRDM\n","excerpt":"Organise data in research projects with openRDM\n","ref":"/users/data/management/open-rdm/","tags":"","title":"openRDM"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"Support for EGI services is available through the EGI Helpdesk, an internal EGI service.\nThe EGI Helpdesk is a distributed tool with central coordination, which provides the information and support needed to troubleshoot product and service problems. Users can report incidents, bugs or request changes.\nThe support activities are grouped into first and second level support.\nNote Support is also available by contacting us at support \u003cat\u003e egi.eu. ","categories":"","description":"Support for EGI services","excerpt":"Support for EGI services","ref":"/support/","tags":"","title":"Support"}]