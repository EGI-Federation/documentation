




























































































































































































































[{"body":"The Notebooks Service is based on JupyterHub. It enables remote use of Jupyter Notebooks in a managed environment integrated with other EGI services. Notebooks main interface uses JupyterLab, a highly extensible environment for running and authoring computational notebooks.\nYou can find links below to the upstream documentation of the EGI Notebooks components:\nJupyterLab Documentation for use of the integrated user environment (menus, working tabs, etc.) JupyterHub Documentation to manage your remote resources (starting and stopping servers, collaborative sharing, etc.) Documentation for your core programming languages: Python Julia R Octave Getting Started Start by creating an EGI account\nThen enroll vo.notebooks.egi.eu VO\nGo to https://notebooks.egi.eu/\nStart the authentication process by clicking on Continue with EGI Check-in! button\nOnce logged in, you will prompted to select the environment, pick the “Default EGI environment”\nClick on “Start” to get your JupyterLab instance You will see the Jupyter interface once your personal server is started\nLaunching a notebook Click on the “Python 3 (ipykernel)” option to launch your notebook with Python 3 kernel. When you create this notebook, a new tab will be presented with a notebook named Untitled.ipynb. You can easily rename it by right-clicking on the current name.\nStructure of a notebook The notebook consists of a sequence of cells. A cell is a multi-line text input field, and its contents can be executed by using Shift-Enter, or by clicking either the “Play” button in the toolbar, or Cell -\u003e Run in the menu bar.\nThe execution behaviour of a cell is determined by the cell’s type.\nThere are three types of cells: cells, Markdown, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be “Code”, initially).\nCode cells A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel.\nWhen a code cell is executed, its content is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell’s output. The output is not limited to text, with many other possible forms of output are also possible, including figures and HTML tables.\nMarkdown cells You can document the computational process in a literate way, alternating descriptive text with code, using rich text. This is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc.\nIf you want to provide structure for your document, you can also use markdown headings. Markdown headings consist of 1 to 6 hash # signs followed by a space and the title of your section. The Markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.\nWhen a Markdown cell is executed, the Markdown code is converted into the corresponding formatted rich text. Markdown allows arbitrary HTML code for formatting.\nRaw cells Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook.\nKeyboard shortcuts All actions in the notebook can be performed with the mouse, but keyboard shortcuts are also available for the most common ones. These are some of the most common:\nShift-Enter: run cell. Execute the current cell, show any output, and jump to the next cell below. If Shift-Enter is invoked on the last cell, it creates a new cell below. This is equivalent to clicking the Cell -\u003e Run menu item, or the Play button in the toolbar. Esc: Command mode. In command mode, you can navigate around the notebook using keyboard shortcuts. Enter : Edit mode. In edit mode, you can edit text in cells. Tutorials You can find links to sample notebooks that we have used in past trainings that may be useful to explore the system:\nA very basic notebook to get started Getting data and doing a simple plot. Connect to NOAA's GrADS Data Server to plot wind speed. Installing new libraries. Interact with Check-in ","categories":"","description":"Getting started with EGI Notebooks\n","excerpt":"Getting started with EGI Notebooks\n","ref":"/users/dev-env/notebooks/quickstart/","tags":"","title":"Notebooks Quick Start"},{"body":"Introduction The Operations Start Guide will help you start with EGI Operations duties. It covers the responsibilities of the various parties involved in the running of the EGI infrastructure and guide how to join operations. As a newcomer, you need to understand the structure of the infrastructure and roles of operators at different levels. Reading the whole document will give you a complete overall picture of daily operations within EGI.\nResource Centres and Resource Infrastructures Resources are geographically distributed and are contributed by Resource Centres. A Resource Centre is the smallest resource administration domain within EGI. It can be either localized or geographically distributed. A Resource Centre is also known as a site. It provides a minimum set of local or remote IT Services, such as High Throughput Compute (HTC), Cloud Compute, Storage and Data Transfer, compliant to well-defined IT Capabilities necessary to make resources accessible to authorised users. Users can access the services using common interfaces.\nThe EGI Federation is a Resource Infrastructure federating Resource Centres to constitute a homogeneous operational domain, and the Resource Infrastructure Provider is the legal organisation that is responsible of establishing, managing and of operating directly or indirectly the operational services to an agreed level of quality needed by the Resource Centres and the user community. It holds the responsibility of integrating them in EGI to enable uniform resource access and sharing for the benefit of their consuming end users. Examples of a Resource infrastructure Provider are the European Intergovernmental Research Organisations (EIRO) and the NGIs.\nIn Europe, Resource Centres are required to be affiliated to the respective NGIs, which (a) have a mandate to represent their national users community in all matters falling within the scope of the EGI Infrastructure, and (b) are the only organization having the mandate described in (a) for its country and thus provide a single contact point at the national level.\nOperations Centres A Resource Infrastructure Provider is responsible for delivering, to different groups of customers, a number of services, either technical or humans : services supporting the activities of the given Resource Infrastructure Provider and facilitating and making secure the access to the provided resources and services.\nIn order to contribute resources to EGI, a Resource Infrastructure Provider must be associated to an Operations Centre which will deliver the services on behalf of them.\nA single Operations Centre can be federated and operate services for multiple Resource Infrastructure Providers. Federation of operations can be a cost-effective approach for Resource Infrastructure Providers wishing to share their effort and services, or in order to operate the infrastructures in an early stage of maturity.\nThe Operations Centre provides services in collaboration with the respective Resource Centres via the Resource Centre Operations Manager, and globally with EGI Foundation and other Operations Centres.\nLocally, Operations Centres are responsible for supporting the Resource Centres, for monitoring their Quality of Service (QoS), for collecting requirements and representing them in the EGI operations boards. Globally they are in charge of contributing to the evolution of the EGI Infrastructure. Roles The following sections covers the roles that are commonly involved in the operations of the EGI Infrastructure. The correspondent roles defined in Configuration Database (GOCDB) give specific rights in the Configuration Database itself and in other EGI services. There are roles whose scope is limited to the operation of a Resource Centre. A number of other roles act on a higher level, involving the operations activities related either to the NGI (Regional level) or to the EGI infrastructure as a whole (global level). Other terms and definitions can be found in the EGI Glossary.\nResource Centre level Resource Centre Administrator An individual responsible for installing, operating, maintaining and supporting one or more Resources or IT Services in a Resource Centre. In the scope of Operations, Resource Centres (RC) administrators primarily receive and react on incidents at their Resource Centre and on service requests notified through tickets created on the EGI Helpdesk service. They should respond to the tickets in a suitable time frame as defined in the Resource Centre Operational Level Agreement (OLA) and be aware of the alarms at their site, eg. through the Operations Dashboard. Sites MUST only operate supported middleware versions. This implies upgrading it regularly. Emergency releases are treated in a special way. Resource Centre Administrators MUST react to security issues that are at a global level, but affect their site. See SEC03 EGI-CSIRT Critical Vulnerability Handling.\nResource Centre Operations Manager An individual who leads the Resource Centre operations, who is the official technical contact person in the connected organisation, and who is locally supported by a team of Resource Centre Administrators. The Resource Centre Operations Manager is responsible for the site at the political and legal levels and for signing the Operational Level Agreement) between the Resource Centre and its hosting NGI. The Resource Centre Operations Manager is also responsible for enforcing the EGI policies and procedures by the Resource Centre and for assigning and approving the other site roles in the Configuration Database. Further, they should ensure that administrators are subscribed to the relevant mailing lists.\nResource Centre Security Officer The person responsible for keeping the site compliant with the EGI security policies. They are the primary contact for the NGI Security officer and EGI Computer Security Incident Response Team (CSIRT). The Site Security Officer deals with security incidents and shall respond to enquiries in a timely fashion as defined in the collection of security procedures and policies.\nRegional level Regional Operator on Duty (ROD) A team responsible for solving problems and incidents in the infrastructure according to agreed procedures. ROD teams monitor the Resource Centres in their region, react to incidents identified by the monitoring tools, and oversee incidents and related problems through to their resolution. They ensure that incidents are properly recorded and that the solutions progress according to specified time lines. They also provide support to Resource Centres and Virtual Organisations (VOs) and provide support to oversight bodies in cases of unresponsive Resource Centres. They ensure that all the necessary information is available to all parties. The team is provided by each NGI and requires procedural knowledge on the process (rather than technical skills) for their work. New ROD team members are required to read the ROD Welcome page and be familiar with ROD wiki page.\nNGI Security officer The NGI Security Officer is member of the EGI-CSIRT.\nThe NGI Security Officer coordinates the security activities within its NGI and serves as the primary contact for all security related requests, in particular from EGI-CSIRT’s IRTF concerning issues with the sites within the NGI.\nFurther, the NGI Security Officer is responsible for overseeing the security related aspects of the operations of the NGI and coordinates the security activities within its NGI.\nNGIs and Sites MUST respond in a timely manner to the security requests and alerts coming from the EGI-CSIRT’s IRTF.\nThe NGI Security Officer name and contact address needs to be registered in the Configuration Database (GOCDB) and the information maintained by the NGI.\nNGI Operations manager The NGI Operations manager is the contact point for all operational matters and represents the NGI within the Operations Management Board (OMB).\nThey are mainly responsible for:\nkeeping up to date the NGI entry in the Configuration Database and managing the status of all sites under their NGI, and ensuring their information is also kept current addressing problems with Site availability or reliability. The reports are issued on a monthly basis and the NGI operations managers have 10 days to respond to identified problems attending regular Operations Management Board meetings All NGI operations management responsibilities are listed in the Resource Infrastructure Provider OLA document.\nGlobal level Chief Operations Officer The Chief Operations Officer leads EGI Operations, and is responsible for coordinating the operations of the infrastructure across the project.\nEGI-CSIRT EGI-CSIRT is the official security coordination team and contact point at project level.\nEGI-CSIRT site EGI-CSIRT profile according to RFC-2350 EGI-CSIRT Terms of References EGI-CSIRT’s IRTF The Incident Response Team for the Federation (IRTF) is a subgroup of EGI-CSIRT which acts as the primary contact for all security related requests concerning the Federation and the projects the EGI Foundation is involved in.\nEGI-CSIRTs IRTF provides the Security Officer on Duty role on a weekly rota basis. Details are in the EGI-CSIRT Terms of References.\nEGI Security Officer The EGI Security Officer, having the EGI CSIRT Officer role in the Configuration Database, is leading and coordinating EGI-CSIRT.\nThe role of the EGI Security Officer is provided by a member of EGI-CSIRT’s IRTF on a weekly rota basis.\nEGI Foundation SDIS team The EGI Foundation Service Delivery and Information Security (SDIS) team, formerly known as the EGI Operations team, is responsible of coordinating and supporting the operational activities of all the EGI Infrastructure.\nVO A Virtual Organisation (VO) is a group of users and, optionally, resources, often not bound to a single institution or national borders, who, by reason of their common membership and in sharing a common goal, are given authority to use a set of resources. Each VO member signs the VO Acceptable Usage Policy (AUP) (during registration) which is the policy describing the goals of the VO thereby defining the expected and acceptable use of the resources by the users of the VO. User documentation can be found in the users section.\nVO manager An individual responsible for the management of the membership registry of the VO ensuring its accuracy and integrity.\nJoining operations In order to join any of the organisational groups in your NGI, you will need to go through the following steps in order:\nAuthentication In general the authentication in EGI infrastructure works either with X509 personal certificates or through federated identities using Check-in. For some services (HTC and Storage) the access is granted only by using a X509 personal certificate due to legacy reasons: the process of moving the authentication and authorisation mechanism to federated identities has started but it will takes time before having everything compliant with federated identities.\nObtaining a X509 personal certificate If you do not already have a X509 personal certificate the EUGridPMA worlmap provides a map of all certification authorities according to the country (or NGI). Select your country on the map to find out who is your local CA. Follow the procedure of your local CA to request a certificate. When you have received your certificate, install it into your web browser.\nIf case of setting up a new Resource Centre please request Host certificates.\nEUGridPMA provides a web page allowing to test your certificate. Please use this resource and contact your CA if your certificate does not work.\nCreate a federated identity: registration in EGI Check-in As soon as you try to access an EGI service with your federated identity, you will be requested to register an account in EGI Check-in if not existing yet. The Check-in sign up guide explains how to sign up for an EGI account. If you already own an account, you will be simply asked to login through EGI Check-in.\nJoining dteam VO It is recommended to join the dteam VO at the dteam Registration page. You should request group membership for /dteam and /dteam/YOUR_NGI. The dteam group manager will then be notified and review your application. The membership to dteam VO is possible only by using a X09 personal certificate and it is useful to test the RCs and to debug related issues.\nRequesting GOCDB access Read Input System User Documentation first. Go to the Configuration Database and follow the instruction. All members should notify their NGI operations manager about their role requests, to be sure they are considered on time.\nRegistering into GGUS GGUS can be accessed with your federated identity. To register as a supporter in GGUS please submit a GGUS ticket to TPM to request the “Supporter” role in GGUS.\nIf your NGI also have a local helpdesk interfaced with GGUS, please ensure that you are properly registered also there: your NGI managers will take care of that.\nSubscribing to mailing lists NGIs and Sites have local mailing lists for ROD team members and Site Administrators respectively. Please ensure that you subscribe to them. Depending on your role ask your NGI operations manager or Site operations manager to have you included on the necessary mailing lists if there is no automatic subscription process.\nNGI operations manager should contact operations@egi.eu and state that wish to be subscribed to noc-managers mailing list noc-managers@mailman.egi.eu.\nDocumentation Procedures and policies are accessible on the EGI Policies and Procedures space. Additional documentation relevant to EGI operations is available at EGI Documentation.\nTools A list of services relevant to EGI operations can be found at the section about internal services.\n","categories":"","description":"Starting with EGI Operations duties","excerpt":"Starting with EGI Operations duties","ref":"/providers/operations-manuals/operations-start-guide/","tags":"","title":"Operations Start Guide"},{"body":"JupyterHub is an extensible platform that supports different combination of software and hardware for running your applications. For the EGI service, you have the choice to run:\nthe default environment that provides a data-science ready stack with support for Python, R, Julia, and Octave. the MATLAB environment, for running MATLAB. After logging into the service, you will be shown a form for selecting the environment, pick the desired one and click start\nThe list of options is customised for your community, depending on the VOs you are member of, you will be able to select new options tuned to your needs, e.g. eiscat.se VO members are presented with a runtime environment built matching their needs:\nIf you have special needs that cannot be covered with the default environment environment, let us know by opening a ticket to the Notebooks Support Unit in GGUS.\nAdditionally, you can build your own environment via Replay. This will allow you to build reproducible and shareable environments for your notebooks.\n","categories":"","description":"Runtime environments available in EGI Notebooks\n","excerpt":"Runtime environments available in EGI Notebooks\n","ref":"/users/dev-env/notebooks/kernels/","tags":"","title":"Notebooks Environments"},{"body":"This page provides an overview of what are the required steps to get a new Resource Centre integrated into the EGI Federation.\nBe aware of the security policies.\nAcceptance of the RC OLA (agreed with the NGI the RC belongs to).\nRegistration on the Configuration Database:\nan entry for the resource centre was created Required mailing lists are registered people operating the services are registered service endpoints are registered associated to the proper service types (e.g. for cloud providers), and the flags monitored and production are set to yes Support through the EGI Helpdesk service:\nThe RC name is listed in the GGUS fields “Affected site” and “Notify Site” The site administrators can modify and reply to the tickets assigned to their RC The Supporter role can be requested either directly on GGUS by using the own X509 personal certificate or by enrolling to the ggus-supporters group in Check-in. Security:\nHTC: pakiti is installed and the outcome of the EGI CSIRT assessment is positive; Cloud: the EGI security Survey was sent to the EGI CSIRT and the outcome was positive. AAI:\nHTC/Storage Compliant with X509 certificates and VOMS attributes OpenStack clouds integration with Check-in General integration with Check-in for SPs Monitoring: the registered endpoints are automatically detected by ARGO and monitored according to their type registered in the Configuration Database.\nAccounting: The accounting records are properly sent to the accounting repository and displayed by the Accounting Portal.\nAccounting probes and APEL SSM are properly installed and configured: HTC: APEL client Cloud: cASO. Information discovery:\nHTC/Storage: the information about compute and storage endpoints are published by the site-bdii into the Top-BDIIs. Cloud: site is added to the fedcloud-catchall-operations repository Middleware: latest version of technology products are installed using the UMD and CMD releases.\nSoftware distribution:\nHTC (Optional): CVMFS Cloud: VM image synchronisation is configured with a HEPIX VM image list compliant software (e.g. cloudkeeper) ","categories":"","description":"Steps required to integrate a Resource Centre","excerpt":"Steps required to integrate a Resource Centre","ref":"/providers/operations-manuals/integration-checklist/","tags":"","title":"Resource Centre Integration Check List"},{"body":"The EGI Notebooks service relies on the following technologies to provide its functionality:\nJupyterHub with custom EGI Check-in oauthentication configured to spawn pods on Kubernetes. Kubernetes as container orchestration platform running on top of EGI Cloud resources. Within the service it is in charge of managing the allocated resources and providing the right abstraction to deploy the containers that build the service. Resources are provided by EGI Federated Cloud providers, including persistent storage for users notebooks. CA authority to allocate recognised certificates for the HTTPS server Prometheus for monitoring resource consumption. Specific EGI hooks for monitoring, accounting and backup. VO-Specific storage/Big data facilities or any pluggable tools into the notebooks environment can be added to community specific instances. Kubernetes A Kubernetes (k8s) cluster deployed into a resource provider is in charge of managing the containers that will provide the service. On this cluster there are:\n1 master node that manages the whole cluster Support for load balancer or alternatively 1 or more edge nodes with a public IP and corresponding public DNS name (e.g. notebooks.egi.eu) where a k8s ingress HTTP reverse proxy redirects requests from user to other components of the service. The HTTP server has a valid certificate from one CA recognised at most browsers (e.g. Let's Encrypt). 1 or more nodes that host the JupyterHub server, the notebooks servers where the users will run their notebooks. Hub is deployed using the JupyterHub helm charts. These nodes should have enough capacity to run as many concurrent user notebooks as needed. Main constraint is usually memory. Support for Kubernetes PersistentVolumeClaims for storing the persistent folders. Default EGI-Notebooks installation uses NFS, but any other volume type with ReadWriteOnce capabilities can be used. Prometheus installation to monitor the usage of resources so accounting records are generated. All communication with the user goes via HTTPS and the service only needs a publicly accessible entry point (public IP with resolvable name)\nMonitoring and accounting are provided by hooking into the respective monitoring and accounting EGI services.\nThere are no specific hardware requirements and the whole environment can run on commodity virtual machines.\nEGI Customisations EGI Notebooks is deployed as a set of customisations of the JupyterHub helm charts.\nAuthentication EGI Check-in can be easily configured as a OAuth2.0 provider for JupyterHub's oauthenticator. See below a sample configuration for the helm chart using Check-in production environment:\nhub: extraEnv: OAUTH2_AUTHORIZE_URL: https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/auth OAUTH2_TOKEN_URL: https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token OAUTH_CALLBACK_URL: https://\u003cyour host\u003e/hub/oauth_callback auth: type: custom custom: className: oauthenticator.generic.GenericOAuthenticator config: login_service: \"EGI Check-in\" client_id: \"\u003cyour client id\u003e\" client_secret: \"\u003cyour client secret\u003e\" oauth_callback_url: \"https://\u003cyour host\u003e/hub/oauth_callback\" username_key: \"sub\" token_url: \"https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token\" userdata_url: \"https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo\" scope: [\"openid\", \"profile\", \"email\", \"eduperson_scoped_affiliation\", \"eduperson_entitlement\"] To simplify the configuration and to add refresh capabilities to the credentials, we have created a new EGI Check-in authenticator that can be configured as follows:\nauth: state: enabled: true cryptoKey: \u003csome unique crypto key\u003e type: custom custom: className: oauthenticator.egicheckin.EGICheckinAuthenticator config: client_id: \"\u003cyour client id\u003e\" client_secret: \"\u003cyour client secret\u003e\" oauth_callback_url: \"https://\u003cyour host\u003e/hub/oauth_callback\" scope: - openid - profile - email - offline_access - eduperson_scoped_affiliation - eduperson_entitlement The auth.state configuration allows to store refresh tokens for the users that will allow to get up-to-date valid credentials as needed.\nAccounting Warning This is Work in progress, expect changes! Accounting module generates VM-like accounting records for each of the notebooks started at the service. It's available as a helm chart that can be deployed in the same namespace as the JupyterHub chart. The only needed configuration for the chart is an IGTF-recognised certificate for the host registered in GOCDB as accounting.\nssm: hostcert: |- \u003chostcert\u003e hostkey: |- \u003chostkey\u003e Monitoring Monitoring is performed by trying to execute a user notebook every hour. This is accomplished by registering a new service in the hub that has admin permissions. Monitoring is then deployed as a helm chart that must be deployed in the same namespace as the JupyterHub chart. Configuration of JupyterHub must include this section:\nhub: services: status: url: \"http://status-web/\" admin: true apiToken: \"\u003ca unique API token\u003e\" Likewise the monitoring chart is configured as follows:\nservice: api_token: \"\u003csame API token as above\u003e\" Docker images Our service relies on custom images for the hub and the single-user notebooks. Dockerfiles are available at EGI Notebooks images git repository and automatically build for every commit pushed to the repository to eginotebooks @ dockerhub.\nHub image Builds from the JupyterHub k8s-hub image and adds:\nEGI and D4Science authenticators EGISpawner EGI look and feel for the login page Single-user image Builds from Jupyter datasicence-notebook and adds a wide range of libraries as requested by users of the services. We are currently looking into alternatives for better managing this image with CVMFS as a possible solution.\nSample helm configuration If you want to build your own EGI Notebooks instance, you can start from the following sample configuration and adapt to your needs by setting:\nsecret tokens (for proxy.secretToken, hub.services.status.api_token, auth.state.cryptoKey). They can be generated with openssl rand -hex 32. A valid hostname (\u003cyour notebooks host\u003e below) that resolves to your Kubernetes Ingress Valid EGI Check-in client credentials, these can be obtained by creating a new Service for the demo instance of Check-in through the EGI Federation Registry. When moving to EGI Check-in production environment, make sure to remove the hub.extraEnv.EGICHECKIN_HOST variable. --- proxy: secretToken: \"\u003csome secret\u003e\" service: type: NodePort ingress: enabled: true annotations: kubernetes.io/tls-acme: \"true\" hosts: [\u003cyour notebooks host\u003e] tls: - hosts: - \u003cyour notebooks host\u003e secretName: acme-tls-notebooks enabled: true hosts: [\u003cyour notebooks host\u003e] singleuser: storage: capacity: 1Gi dynamic: pvcNameTemplate: claim-{userid}{servername} volumeNameTemplate: vol-{userid}{servername} storageAccessModes: [\"ReadWriteMany\"] memory: limit: 1G guarantee: 512M cpu: limit: 2 guarantee: .02 defaultUrl: \"/lab\" image: name: eginotebooks/single-user tag: c1b2a2a hub: image: name: eginotebooks/hub tag: c1b2a2a extraConfig: enable-lab: |- c.KubeSpawner.cmd = ['jupyter-labhub'] volume-handling: |- from egispawner.spawner import EGISpawner c.JupyterHub.spawner_class = EGISpawner extraEnv: JUPYTER_ENABLE_LAB: 1 EGICHECKIN_HOST: aai-demo.egi.eu services: status: url: \"http://status-web/\" admin: true api_token: \"\u003cmonitor token\u003e\" auth: type: custom state: enabled: true cryptoKey: \"\u003ca unique crypto key\u003e\" admin: access: true users: [\u003clist of EGI Check-in users with admin powers\u003e] custom: className: oauthenticator.egicheckin.EGICheckinAuthenticator config: client_id: \"\u003cyour egi checkin_client_id\u003e\" client_secret: \"\u003cyour egi checkin_client_secret\u003e\" oauth_callback_url: \"https://\u003cyour notebooks host\u003e/hub/oauth_callback\" enable_auth_state: true scope: - openid - profile - email - offline_access - eduperson_scoped_affiliation - eduperson_entitlement ","categories":"","description":"Internal Service Architecture","excerpt":"Internal Service Architecture","ref":"/providers/notebooks/architecture/","tags":"","title":"Architecture"},{"body":"The EGI Federated Cloud (FedCloud) is a multi-national cloud system that integrates community, private and/or public clouds into a scalable computing platform for research. The Federation pools resources from a heterogeneous set of cloud providers using a single authentication and authorisation framework that allows the portability of workloads across multiple providers, and enables bringing computing to data. The current implementation is focused on Infrastructure-as-a-Service (IaaS) services, but can be easily applied to Platform-as-a-Service (PaaS) and Software-as-a-Servcice (SaaS) layers.\nEach resource centre of the federated infrastructure operates a Cloud Management Framework (CMF) according to its own preferences and constraints and joins the federation by integrating this CMF with components of the EGI service portfolio. CMFs must at least be integrated with EGI Authentication and Authorization Infrastructure (AAI) so users can access services with a single identity, integration with other components and APIs to be provided are agreed by the community the resource centre provides services to.\nEGI follows a Service Integration and Management (SIAM) approach to manage the federation with processes that cover the different aspects of the IT Service Management. Providers in the federation keep complete control of their services and resources. EGI creates Virtual Organizations (VOs) for each research community, and EGI VO Operation Level Agreements (OLAs) establish a reliable, trust-based communication channel between the community and the providers, by agreeing on the services, their levels and the types of support.\nNote EGI VO OLAs are not legal contracts but, as agreements, they outline the clear intentions to collaborate and support research. Federated IaaS The EGI FedCloud IaaS resource centres deploy a Cloud Management Framework (CMF) that provide users with an API-based service for management of Virtual Machines and associated Block Storage to enable persistence and Networks to enable connectivity of the Virtual Machines (VMs) among themselves and third party resources.\nThe IaaS federation is a thin layer that brings the providers together with:\nFederated authentication Resource discovery Central VM image catalogue Usage accounting Monitoring The IaaS capabilities (VM, block storage, network management, etc.) must be provided via community agreed APIs (OpenStack is supported at the moment) that allow integration with EGI Check-in for authentication and authorisation of users.\nNote Those providers that limit the interaction to web dashboards and do not expose APIs to direct consumption for users cannot be considered part of the EGI IaaS Cloud services. Users and Community platforms built on top of the EGI IaaS can interact with the cloud providers at three different layers:\nDirectly using the IaaS APIs or CLIs to manage individual resources. This option is recommended for preexisting use cases with requirements on specific APIs. Using Provisioning systems allow users to define infrastructure as code, then manage and combine resources from different providers, thus enabling the portability of application deployments between them (e.g. Infrastructure Manager or Terraform). EGI provides ready-to-use software components to enable the federation for OpenStack. These components rely on public APIs of the IaaS system and use Check-in accounts for authenticating into the provider.\nImplementation Authentication and authorization Federated identity ensures that users of the federation can use a single account for accessing the resources.\nOpenID Connect Providers of the EGI Cloud support authentication with OAuth2 tokens provided by Check-in OpenID Connect Identity provider. Support builds on the AAI guide for SPs with detailed configuration provided at the EGI IaaS Service providers documentation.\nThe integration relies on the OpenStack Keystone OS-FEDERATION API.\nInformation discovery The Configuration Database contains the list of resource centres and their endpoints, while the AppDB Information System collects this information in a central service for discovery, providing a real-time view of the actual capabilities of federation participants (can be used by both human users and machine services).\nConfiguration Database The EGI Configuration Database is used to catalogue the static information of the production infrastructure topology (e.g. the list of resource centres and their endpoints).\nTo allow resource providers to expose IaaS federation endpoints, the following service types are available:\norg.openstack.horizon org.openstack.nova org.openstack.swift eu.egi.cloud.accounting eu.egi.cloud.vm-metadata.marketplace All providers must enter cloud service endpoints into the Configuration Database to enable integration with EGI.\nThe Cloud Info Provider extracts information from the resource centres using their native APIs and formats it following Glue, an OGC recommended standard. This information is pushed to the Argo Messaging System and consumed by AppDB to provide a central information discovery service that aggregates several other sources of information about the infrastructure.\nVirtual Machine Image management In a distributed, federated IaaS service, users need solutions for efficiently managing and distributing their VM images across multiple resource providers. EGI provides a catalogue of VM images (VMIs) that allows any user to share their VMI, and communities to select those VMIs relevant for distribution across providers. These images are automatically replicated at the providers supporting the community and converted as needed to ensure the correct instantiation when used.\nAppDB includes a Virtual Appliance Marketplace supporting Virtual Appliances (VAs), which are clean and lean virtual machine images designed to run on a virtualisation platform, that provide a software solution out-of-the-box, ready to be used with minimal or no set-up.\nAppDB allows representatives of research communities (VOs) to generate a VM image list that resource centres subscribe to. The subscription enables the periodic download, conversion and storage of those images to the image repository of the indicated resource centres, using HEPiX image list format. cloudkeeper provides this automated synchronisation between AppDB and the cloud provider.\nAccounting Federated Accounting provides an integrated view about resource/service usage: it pulls together usage information from the federated sites and services, integrates the data and presents them in such a way that both individual users as well as whole communities can monitor their own resource/service usage across the whole federation.\nUsage of resources is gathered centrally using EGI Accounting repository and available for visualisation at EGI Accounting portal.\nCloud Usage Record The federated cloud task force has agreed on a Cloud Usage Record, which inherits from the OGF Usage Record (GFD.204). This record defines the data that resource providers must send to EGI’s central Accounting repository.\nVersion 0.4 of the Cloud Accounting Usage Record was agreed at the FedCloud Face to Face in Amsterdam in January 2015. A summary table of the format is shown below:\nCloud Usage Record Property Type Null Definition VMUUID varchar(255) No Virtual Machine's Universally Unique Identifier concatenation of CurrentTime, SiteName and MachineName SiteName varchar(255) No GOCDB SiteName - GOCDB now has cloud service types and a cloud-only site is allowed. CloudComputeService (NEW) varchar(255) Name identifying cloud resource within the site. Allows multiple cloud resources within a site, i.e. a level of granularity. MachineName varchar(255) No VM ID - the site name for the VM LocalUserId varchar(255) Local username LocalGroupId varchar(255) Local group name GlobalUserName varchar(255) Global identity of user (certificate DN) FQAN varchar(255) Use if VOs part of authorization mechanism Status varchar(255) Completion status - completed, started or suspended StartTime datetime Must be set when Status = started EndTime datetime Set to NULL until Status = completed SuspendDuration datetime Set when Status = suspended (Timestamp) WallDuration int WallClock time - actual time used CpuDuration int CPU time consumed (Duration) CpuCount int Number of CPUs allocated NetworkType varchar(255) Needs clarifying NetworkInbound int GB received NetworkOutbound int GB sent PublicIPCount (NEW) int Number of public IP addresses assigned to VM Not used. Memory int Memory allocated to the VM Disk int Size in GB allocated to the VM BenchmarkType (NEW) varchar(255) Name of benchmark used for normalization of times (eg HEPSPEC06) Benchmark (NEW) Decimal Value of benchmark of VM using ServiceLevelType benchmark’ StorageRecordId varchar(255) Link to other associated storage record Need to check feasibility ImageId varchar(255) Every image has a unique ID associated with it. For images from the EGI FedCloud AppDB this should be VMCATCHER_EVENT_AD_MPURI; for images from other repositories it should be a vmcatcher equivalent; for local images - local identifier of the image. CloudType varchar(255) Type of cloud infrastructure: OpenNebula; OpenStack; Synnefo; etc. Public IP Usage Record The fedcloud task force has agreed on an IP Usage Record. The format uses many of the same fields as the Cloud Usage Record. The Usage Record should be a \"snapshot\" of the number of IPs currently assigned to a user. A table defining v0.2 of the format is shown below:\nCloud Usage Record Property Type Null Definition Notes MeasurementTime datetime No The time the usage was recorded. In the message format, must be a UNIX timestamp, i.e. the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970) SiteName varchar(255) No The GOCDB site assigning the IP CloudComputeService varchar(255) Yes See Cloud Usage Record CloudType varchar(255) No See Cloud Usage Record LocalUser varchar(255) No See Cloud Usage Record LocalGroup varchar(255) No See Cloud Usage Record GlobalUserName varchar(255) No See Cloud Usage Record FQAN varchar(255) No See Cloud Usage Record IPVersion byte No 4 or 6 IPCount int(11) No The number of IP addresses of IPVersion this user currently assigned to them A JSON schema defining a valid Public IP Usage message can be found at: https://github.com/apel/apel/blob/9476bd86424f6162c3b87b6daf6b4270ceb8fea6/apel/db/__init__.py\nGPU Usage Record The fedcloud task force has agreed on an GPU Usage Record. The format uses many of the same fields as the Cloud Usage Record. A table defining Draft 4 – 24/02/2021 is shown below:\nGPU Usage Record Property Type Null Definition MeasurementMonth int No The month/year the reported usage should be assigned to. If the month/year is the current month/year, the usage should be up to the point of reporting. MeasurementYear int No AssociatedRecordType varchar(255) No The context in which the reported usage was used. I.e. “cloud” for an accelerator attached to a VM. AssociatedRecord varchar(255) No VMUUID if AssociatedRecordType is “cloud” GlobalUserName varchar(255) Yes See the definition of your AssociatedRecordType FQAN varchar(255) No See the definition of your AssociatedRecordType SiteName varchar(255) No See the definition of your AssociatedRecordType Count decimal No A count of the Accelerators attached to the VM. At the moment Accelerators are not shared among VMs but it will change when Accelerator virtualization is applied, so we should have the field at decimal type instead of integer (e.g. Count = 0.5 when it is shared between two VMs). Cores int(11) Yes Total number of cores. i.e. So if an Accelerator has 64 cores and a VM has 2 like that attached then we would report: Count=2 and Processors=128 ActiveDuration int(11) Yes Actual usage duration of the Accelerator in seconds for the given month/year (in case some systems could report actual usage). At the moment, ActiveDuration will be the same as the AvailableDuration due to the limitation of currently used technologies (impossible to get ACCELERATOR utilization from outside of the VM, no ACCELERATOR hot-plug into running VM) but it may change in near future so it is good to have the fields separately. Set to AvailableDuration if ActiveDuration is omitted from the record AvailableDuration int(11) No Time accelerator was available in seconds for the given month/year (Wall)Time that a GPU was attached to a VM. BenchmarkType varchar(255) Yes Name of benchmark used for normalization of times Benchmark decimal Yes Value of benchmark of Accelerator Type varchar(255) No High level description of accelerator, i.e. GPU, FPGA, Other Model varchar(255) Yes model number, spec, some other concept that 2 ACCELERATORs with the same number of cores might be different etc APEL and accounting portal Once generated, records are delivered to the central accounting repository using APEL SSM (Secure STOMP Messenger). SSM client packages can be obtained at https://apel.github.io. A Cloud Accounting Summary Usage Record has also been defined and summaries created on a daily basis from all the accounting records received from the Resource Providers are sent to the EGI Accounting Portal. The Accounting portal also runs SSM to receive these summaries and provides a web view of the accounting data received from the Resource Providers.\ncASO delivers an implementation of the extractor probes for OpenStack.\nMonitoring The endpoints published in the Configuration Database are monitored via ARGO. Specific probes to check functionality and availability of services must be provided by service developers.\nThe current set of probes used for monitoring IaaS resources consists of:\nAccounting probe (eu.egi.cloud.APEL-Pub): Checks if the cloud resource is publishing data to the Accounting repository TCP checks (org.nagios.Broker-TCP, org.nagios.CDMI-TCP, and org.nagios.CloudBDII-Check): Basic TCP checks for services. VM Marketplace probe (eu.egi.cloud.AppDB-Update): gets a predetermined image list from AppDB and checks its update interval. PERUN probe (eu.egi.cloud.Perun-Check): connects to the server and checks the status by using internal PERUN interface. Roadmap The TCB-Cloud board defines the roadmap for the technical evolution of the EGI Cloud. All the components are continuously maintained to:\nImprove their programmability, providing complete APIs specification in adequate format for facilitating the generation clients (e.g. following the OpenAPI initiative and Swagger). Lower the barriers to integrate and operate resource centres in the federation by a) minimizing the number of components used; b) contributing code to upstream distributions; and c) use only public APIs of the Cloud Management Frameworks. Currently, the EGI FedCloud TaskForce is focused on moving to a central operations model, where providers only need to integrate their system with EGI Check-in but do not need to deploy and configure the different tools (accounting, discovery, VMI management, etc.) locally but delegate this to a central EGI team.\n","categories":"","description":"The architecture of the EGI Federation\n","excerpt":"The architecture of the EGI Federation\n","ref":"/users/getting-started/architecture/","tags":"","title":"EGI Architecture"},{"body":" What is it? Block storage provides block-level storage volumes for use within virtual machines (VMs). Block storage volumes are raw, unformatted block devices, which can be mounted as devices in VMs.\nBlock storage volumes that are attached to a VM are exposed as storage volumes that persist independently from the life of the VM, and need to be explicitly destroyed when data is not needed anymore. Users can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive).\nThe content of block storage volumes can be accessed only from within the VM they are mounted to, and they can be mounted to a single VM at any given time.\nThe main features of block storage:\nBlock storage is recommended for data that must be quickly accessible and requires long-term persistence. Block storage volumes are well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes. Users can create point-in-time snapshots of block storage volumes, which protect data for long-term durability, and they can be used as the starting point for new block storage volumes. Note Block storage volumes can only be mounted to VMs running at the same provider where the block storage is located. Note Block storage usage is accounted for the entire block storage device, regardless how much of it is actually used. Important There is a limit on the number of block storage devices you can attach on a VM and there is a limit to the maximum size of such virtual disks. These values will depend on the particular provider and your SLA. Manage volumes The block storage in the EGI Cloud is offered via OpenStack deployments that implement the Cinder service.\nUsers can manage block storage using the OpenStack Horizon dashboard of a provider, from a command-line interface (CLI), or via the OpenStack Block Storage API.\nManage from the command-line Multiple command-line interfaces (CLIs) are available to manage block storage:\nThe OpenStack CLI The FedCloud Client is a high-level CLI for interaction with the EGI Federated Cloud (recommended) The Cinder CLI has some advanced features and administrative commands that are not available through the OpenStack CLI The main FedCloud commands for managing volumes are detailed below.\nNote For more information see the documentation about volume management. List volumes For example, to list the volumes in the site IN2P3-IRES via the Pilot VO (vo.access.egi.eu), use the following FedCloud command:\nLinux / Mac Windows PowerShell To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n$ export EGI_SITE=IN2P3-IRES $ export EGI_VO=vo.access.egi.eu $ fedcloud openstack volume list --site $EGI_SITE Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e set EGI_SITE=IN2P3-IRES \u003e set EGI_VO=vo.access.egi.eu \u003e fedcloud openstack volume list --site %EGI_SITE% Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e $Env:EGI_SITE=\"IN2P3-IRES\" \u003e $Env:EGI_VO=\"vo.access.egi.eu\" \u003e fedcloud openstack volume list --site $Env:EGI_SITE Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+ Create volume To create a new volume use the FedCloud command below:\n$ fedcloud openstack volume create --size 10 my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume create --size 10 my-volume +---------------------+------------------------------------------------------------------+ | Field | Value | +---------------------+------------------------------------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-08-05T13:10:42.000000 | | description | None | | encrypted | False | | id | a711296-5cff-46ac-bbe3-58e00712ee3e | | multiattach | False | | name | my-volume | | properties | | | replication_status | None | | size | 10 | | snapshot_id | None | | source_volid | None | | status | creating | | type | ceph | | updated_at | None | | user_id | 1a3ef4b64714f86ac71f1c9512345678c157a94ae1b37f167b6a663baa3b915b | +---------------------+------------------------------------------------------------------+ The status of the new volume will probably be returned as creating. To check if the volume finished creating, look at the details of the volume, or list only the newly created volume (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 10 | | +--------------------------------------+-----------+-----------+------+-------------+ When the status of the volume is available the volume is ready to be attached to a VM.\nSee volume details To view details of a volume use the FedCloud command below:\nTip The volume can be specified either by its ID or by its name (if it has one). $ fedcloud openstack volume show Matlab Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume show Matlab +------------------------------+------------------------------------------------------------+ | Field | Value | +------------------------------+------------------------------------------------------------+ | attachments | [{'server_id': 'a4ab00a1-d458-41a9-a091-ee707bc9357e', | | | 'attachment_id': '291111d3-f43c-494c-b64c-5c0abcda3d37', | | | 'attached_at': '2021-08-05T08:50:19.000000', | | | 'host_name': 'sbgcsrv17.in2p3.fr', | | | 'volume_id': '9a4000fb-0bcc-47e8-96fb-85a222295402', | | | 'device': '/dev/vdb', | | | 'id': '9a4000fb-0bcc-47e8-96fb-85a222295402'}] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-08-05T08:48:41.000000 | | description | | | encrypted | False | | id | 9a4000fb-0bcc-47e8-96fb-85a222295402 | | multiattach | False | | name | Matlab | | os-vol-tenant-attr:tenant_id | 7a910xxxxae74ed9yyyy7497zzzz9499 | | properties | | | replication_status | None | | size | 50 | | snapshot_id | None | | source_volid | None | | status | in-use | | type | ceph | | updated_at | 2021-08-05T08:50:20.000000 | | user_id | babzz8c3b4cxxxx6286dacyyyy01e5a3 | +------------------------------+------------------------------------------------------------+ Attach volume to VM Mapping block devices to VMs is described in detail in the OpenStack documentation.\nTo attach a volume to a VM use the FedCloud command below:\nNote To be able to attach a volume to a VM, the volume must not be attached to any VM (volume status must be available). Caution The optional --device argument to specify the device name in the VM should not be used. It does not work properly, and will be removed in the near future. $ fedcloud openstack server add volume my-server my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: server add volume my-server my-volume You can check that the volume got attached to the VM (and with what device name) by either looking at the details of the volume, the details of the VM, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-----------------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-----------------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | in-use | 10 | my-server on /dev/vdc | +--------------------------------------+-----------+-----------+------+-----------------------+ When the volume status is in-use the volume is attached to a VM, and it can be used from the VM.\nDetach volume from VM To detach a volume from a VM use the FedCloud command below:\n$ fedcloud openstack server remove volume my-server my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: server remove volume my-server my-volume You can check that the volume got detached by either looking at the details of the volume, the details of the VM, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 10 | | +--------------------------------------+-----------+-----------+------+-------------+ When the volume status is available the volume is not attached to any VM.\nResize volume To resize a volume set its size property to the desired size. For example, if there is a volume named my-volume with a size of 10GB, it can be resized using the FedCloud command below:\nTip Other volume properties can be altered in the same way. Note To be able to resize a volume, the volume must not be attached to any VM (volume status must be available), unless the volume driver supports in-use extend. $ fedcloud openstack volume set --size 20 my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume set --size 20 my-volume You can check that the volume got resized by either looking at the details of the volume, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 20 | | +--------------------------------------+-----------+-----------+------+-------------+ Snapshot a volume Users can create snapshots of a volume that can later be used to create other volumes or to rollback to a precedent point in time. Volume snapshots are pointers in the read-write history of a volume.\nTo take a snapshot of a volume, use the FedCloud command below:\nTip See also the documentation about the other snapshot management commands. Note To be able to create a snapshot of a volume, the volume must not be attached to any VM (volume status must be available). To create a snapshot while the volume is attached to a VM, use the --force command flag, but be aware that there may be inconsistencies if the VM’s OS is not aware of the snapshot being taken. $ fedcloud openstack volume snapshot create --volume my-volume my-snapshot --force Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume snapshot create --volume my-volume my-snapshot +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2021-08-06T16:08:37.631226 | | description | None | | id | 15149b42-032f-4cec-b6f3-41aa5c958081 | | name | my-snapshot | | properties | | | size | 10 | | status | creating | | updated_at | None | | volume_id | aa711296-5cff-46ac-bbe3-58e0712ee3e9 | +-------------+--------------------------------------+ The status of the snapshot will probably be returned as creating. To check if the snapshot is ready, look at the details of the snapshot, or list only the newly created snapshot (filter by snapshot name or ID):\n$ fedcloud openstack volume snapshot list --name my-snapshot Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume snapshot list --name my-snapshot +--------------------------------------+-------------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+-------------+-------------+-----------+------+ | 15149b42-032f-4cec-b6f3-41aa5c958081 | my-snapshot | None | available | 10 | +--------------------------------------+-------------+-------------+-----------+------+ When the status of the snapshot is available the snapshot is ready, and can be used to create new volumes or to restore the source volume to the point-in-time when the snapshot was taken.\nBackup a volume Users can also create backups of a volume, but backups can only be used later to create or replace other volumes. A volume backup is a copy of a volume saved to cold storage, which is cheaper than the performant storage used for volumes and snapshots.\nTo make a backup of a volume, use the FedCloud command below:\nTip See also the documentation of the other backup management commands. Note Not all OpenStack deployments support volume backups. Note Backups use as much storage as the source volume, albeit in a cheaper storage layer. This means backups take a long time to create (~4h for 500GB), and the space is accounted for the same way as for regular volumes (the entire size of the backed up volume, regardless of how much of the volume space is actually used). Thus backups should be deleted when no longer needed. Note To be able to make a backup of a volume, the volume must not be attached to any VM (volume status must be available). To make a backup while the volume is attached to a VM, use the --force command flag, but be aware that there may be inconsistencies if the VM’s OS is not aware of the backup being taken. $ fedcloud openstack volume backup create --name my-backup my-volume --force Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume backup create --name my-backup my-volume +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2021-08-06T16:08:37.631226 | | description | None | | id | 158bcb42-032f-4cec-b6f3-890a5c958081 | | name | my-backup | | properties | | | size | 10 | | status | creating | | updated_at | None | | volume_id | aa711296-5cff-46ac-bbe3-58e0712ee3e9 | +-------------+--------------------------------------+ The status of the backup will probably be returned as creating. To check if the backup has finished, look at the details of the backup, or list only the newly created backup (filter by backup name or ID):\n$ fedcloud openstack volume backup list --name my-backup Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume backup list --name my-backup +--------------------------------------+-----------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+-----------+-------------+-----------+------+ | 158bcb42-032f-4cec-b6f3-890a5c958081 | my-backup | None | available | 10 | +--------------------------------------+-----------+-------------+-----------+------+ When the status of the backup is available the backup operation is complete.\nDelete volume To delete a volume use the following FedCloud command:\nNote To be able to delete a volume, the volume must not be attached to any VM (volume status must be available). $ fedcloud openstack volume delete my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume delete my-volume This starts deletion of the specified volume (the volume status changes to deleting):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | deleting | 10 | | +--------------------------------------+-----------+----------+------+-------------+ Once deletion of the specified volume is complete, it will no longer show up in the list of volumes.\nAccess from your VMs Block storage volumes attached to a VM will appear as a block device in the VM.\nTo find out the device name that got assigned to a volume when it was attached to a VM, look at the details of the volume, or list only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+--------+------+-----------------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+--------+------+-----------------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | in-use | 10 | my-server on /dev/vdb | +--------------------------------------+-----------+--------+------+-----------------------+ In this example the device name is /dev/vdb. To validate this, run the following command in the VM:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 20G 0 disk └─vda1 252:1 0 20G 0 part / vdb 252:16 0 10G 0 disk Usually these devices are empty upon creation. The first time you attach them to a VM, you will need to partition the device and create filesystem(s) on it, by running the following command in the VM:\nTip Using a file system volume label is useful to avoid the need to find the out the device name, especially when multiple block storage volumes are attached to a VM. It is recommended to use a file system volume label in the VM that is the same as the name of the block storage volume. Tip The filesystem type can be one supported by the Linux distribution in the VM, but xfs and ext4 are the most widely used. Caution Only run this command the first time you use the device, as it deletes all data! Make sure you use the correct device name, otherwise you will destroy data on other devices! $ sudo mkfs.ext4 -L my-volume /dev/vdb Once you created a filesystem on the device, you can mount it at any desired path by running the following command in the VM:\n$ sudo mount /dev/vdb1 /\u003cpath\u003e Continuing with the example above, if we check again the block devices by running the following command in the VM:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 20G 0 disk └─vda1 252:1 0 20G 0 part / vdb 252:16 0 10G 0 disk └─vdb1 252:1 0 10G 0 part /\u003cpath\u003e If non-root users should be able to access the mounted volume similar to the way e.g. /tmp is accessible, set the sticky bit on the mount point, with this command in the VM:\n$ sudo chmod +t /\u003cpath\u003e If the desired behaviour is to mount the file system automatically on VM restart, add it to /etc/fstab. Using the LABEL parameter will ensure the correct volume is chosen if multiple volumes are attached:\nLABEL=my-volume /\u003cpath\u003e ext4 noatime,nodiratime,user_xattr,nofail 0 0\nNote The use of option nofail is recommended in order to skip (and not block on) mounting the file system volume if it is unavailable, e.g. in case of network issues. Remove this option from the fstab line if you want the VM to block the boot process if the volume is unavailable. Note The use of option nobarrier is not recommended as volumes are accessed via a cache, and ignoring the correct ordering of journal commits may result in a corrupted file system in case of a hardware problem. With that you can access /\u003cpath\u003e inside the VM, where all data stored on the volume will be available. Applications will not see any difference between a block storage device and a regular disk, thus no major changes should be required in the application logic.\nStorage Encryption This section describes the usage of the tool cryptsetup to enable the permanent encryption of the data stored in the disk. The tool is available in standard linux distributions, and for this guide we assume the installation in an Ubuntu distribution.\n$ sudo su - $ apt -y install cryptsetup To encrypt the disk, it must be first initialized correctly. In the example below, the disk named /dev/vdb is first filled with random data and then initialized using the cryptsetup luksFormat command below. This first step can be quite long.\n$ dd if=/dev/urandom of=/dev/vdb bs=4k $ cryptsetup -v --cipher aes-xts-plain64 --key-size 512 --hash sha512 \\ --iter-time 5000 --use-random luksFormat /dev/vdb If this last command slows down or even blocks with the following message:\nSystem is out of entropy while generating volume key. Please move mouse or type some text in another window to gather some random events. Generating key (0% done). you can make the cryptsetup luksFormat command running faster by first installing the haveged program in your virtual machine.\nThe following command verifies that the disk is now of type LUKS:\n$ cryptsetup luksDump /dev/vdb LUKS header information for /dev/vdb Version: 1 Cipher name: aes Cipher mode: xts-plain64 Hash spec: sha512 Payload offset: 4096 MK bits: 512 MK digest: c4 f7 4b 02 2a 3f 12 c1 2c ba e5 c9 d2 45 9a cd 89 20 6c 73 MK salt: 98 58 3e f3 f6 88 99 ea 2a f3 cf 71 a0 0d e5 8b d5 76 64 cb d2 5c 9b d1 8a d3 1d 18 0e 04 7a eb MK iterations: 81250 UUID: c216d954-199e-4eab-a167-a3587bd41cb3 Key Slot 0: ENABLED Iterations: 323227 Salt: a0 45 3e 98 fa cf 60 74 c6 09 3d 54 97 89 be 65 5b 96 7c 1c 39 26 47 b4 8b 0e c1 3a c9 94 83 c2 Key material offset: 8 AF stripes: 4000 Key Slot 1: DISABLED Key Slot 2: DISABLED Key Slot 3: DISABLED Key Slot 4: DISABLED Key Slot 5: DISABLED Key Slot 6: DISABLED Key Slot 7: DISABLED The disk is now ready for use. The first time you use it, you must perform the following steps:\nStep 1: Open the encrypted disk with the cryptsetup luksOpen command. The name storage1 is only indicative, you can choose what you want:\n$ cryptsetup luksOpen /dev/vdb storage1 Step 2: Create a filesystem on disk:\n$ mkfs.ext4 /dev/mapper/storage1 Step 3: Create the disk mount point:\n$ mkdir /storage1 Step 4: Mount the disk:\n$ mount -t ext4 /dev/mapper/storage1 /storage1 Step 5: Check available space (this may be slightly different from what was entered during the openstack volume create command):\n$ df -h /storage1 Filesystem Size Used Avail Use% Mounted on /dev/mapper/storage1 2.0G 6.0M 1.9G 1% /storage1 Once the disk is operational, steps 2 and 3 are no longer necessary.\nYou can now send files (for example DATA.dat) from your personal computer to your virtual machine in a secure way, for example with scp:\n$ scp -i ${HOME}/.ssh/cloudkey DATA.dat ubuntu@134.158.151.224:/storage1 DATA.dat 100% 82 0.1KB/s 00:00 When you are done with your work on the disk, you can remove it cleanly with the following commands:\n$ umount /storage1 $ cryptsetup close storage1 For subsequent uses of the persistent disk, there will be no need to perform all these operations, only the following are necessary:\n$ cryptsetup luksOpen /dev/vdb storage1 $ mkdir -p /storage1 $ mount -t ext4 /dev/mapper/storage1 /storage1 Note that directory /storage1 will be created only if it does not already exist.\nAccess via EGI Data Transfer EGI Data Transfer allows you to move any type of data files asynchronously from one storage to another. If you want to copy data from/to one VM running on the EGI cloud, you will need to run a compatible server (Webdav/HTTPS, GridFTP, xrootd, SRM, S3, GCloud) that can interact with the FTS3 software.\nAn easy way to provide a GridFTP server on your VM is to use the gridftp-le ready2go docker stack for deploying a GridFTP Docker Container with certificates from Let’s Encrypt. Take into account:\nSecurity groups for the VM must allow ports 80, 2811 and the 50000-50200 range.\nThe VM must have a valid DNS entry (you can use Dynamic DNS in FedCloud to get one)\nThe default setup uses /srv as path to expose and maps users to the nobody user. Make sure that nobody is able to read (and write if needed) on that location or set the mapping to the appropriate users.\nThe Let’s Encrypt certificates may not be accepted by some of the EGI infrastructure endpoints, you may want to consider using IGTF certificates instead. Check your CA for instructions on how to get those.\nYou can add direct mappings for specific DNs by adding a /etc/localgridmap.conf file in your running container. See the example below to map the /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Enol Fernandez del Castillo DN to nobody. You can add as many lines as needed:\n\"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Enol Fernandez del Castillo\" nobody You need to specify the environment variables in the docker-compose.yml file for obtaining the Let’s Encrypt certificate. An extra variable GLOBUS_HOSTNAME must be also set:\nenvironment: - TESTCERT - EMAIL=youremail@domain.com - DOMAIN=mygridftp.example.com - GLOBUS_HOSTNAME=mygridftp.example.com If you are running on a site with MTU smaller than 1500 (e.g. CESNET-MCC), make sure that you set the MTU to a value smaller than the interface MTU(you can check this with ip addr). In your docker-compose.yml, add:\nnetworks: default: driver: bridge driver_opts: com.docker.network.driver.mtu: 1434 ","categories":"","description":"Block Storage offered by EGI Cloud providers\n","excerpt":"Block Storage offered by EGI Cloud providers\n","ref":"/users/compute/cloud-compute/block-storage/","tags":"","title":"Block Storage"},{"body":"The integration of OpenStack service providers into the EGI Check-in is a two-step process:\nTest integration with the demo instance of EGI Check-in. This will allow you to check the complete functionality of the system without affecting the production Check-in service. Once the integration is working correctly, register your provider with the production instance of EGI Check-in to allow members of the EGI User Community to access your service. Registration into Check-in demo instance Before your service can use the EGI Check-in OIDC Provider for user login, you need to register a client through the EGI Federation Registry in order to obtain OAuth2.0 credentials and register one or more redirect URIs.\nMake sure that you fill in the following options:\nGeneral tab:\nSet Integration Environment to Demo and fill the form with the information about your Service. Protocol Specific tab:\nSet Select Protocol to OIDC Service Set redirect URL to https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect. Recent versions of OpenStack may deploy Keystone at /identity/, be sure to include that in the \u003cyour keystone endpoint\u003e part of the URL if needed. Enable openid, profile, email, eduperson_entitlement in the Scope field Enable authorization code in the Grant Types field Set Proof Key for Code Exchange (PKCE) Code Challenge Method to SHA-256 hash algorithm (recommended) Make sure Allow calls to the Introspection Endpoint? is enabled in Introspection field Submit the request for review by the Check-in operations team. Once the request has been approved, you will get a client ID and client secret. Save them for the following steps\nKeystone setup Pre-requisites Keystone must run as a WSGI application behind an HTTP server (Apache is used in this documentation, but any server should be possible if it has OpenID connect/OAuth2.0 support). Keystone project has deprecated eventlet, so you should be already running Keystone in such way. Keystone must be run with SSL You need to install mod_auth_openidc for adding support for OpenID Connect to Apache. IGTF CAs EGI monitoring checks that your Keystone accepts clients with certificates from the IGTF CAs. Please ensure that your server is configured with the correct Certificate and Revocation path:\nFor Apache HTTPd HTTPd is able to use CAs and CRLs contained in a directory:\nSSLCACertificatePath /etc/grid-security/certificates SSLCARevocationPath /etc/grid-security/certificates For haproxy CA and CRLS have to be bundled into one file.\nClient verification should be set as optional otherwise accepted CAs won't be presented to the EGI monitoring:\n# crt: concatenated cert, key and CA # ca-file: all IGTF CAs, concatenated as one file # crl-file: all IGTF CRLs, concatenated as one file # verify: enable optional X.509 client authentication bind XXX.XXX.XXX.XXX:443 ssl crt /etc/haproxy/certs/host-cert-with-key-and-ca.pem ca-file /etc/haproxy/certs/igtf-cas-bundle.pem crl-file /etc/haproxy/certs/igtf-crls-bundle.pem verify optional For nginx CA and CRLS have to be bundled into one file.\nClient verification should be set as optional otherwise accepted CAs won't be presented to the EGI monitoring:\nssl_client_certificate /etc/ssl/certs/igtf-cas-bundle.pem; ssl_crl /etc/ssl/certs/igtf-crls-bundle.pem; ssl_verify_client optional; Managing IGTF CAs and CRLs IGTF CAs can be obtained from UMD, you can find repository files for your distribution at EGI CA repository\nIGTF CAs and CRLs can be bundled using the examples command hereafter.\nPlease update CAs bundle after IGTF updates, and CRLs bundle after each CRLs update made by fetch-crl:\ncat /etc/grid-security/certificates/*.pem \u003e /etc/haproxy/certs/igtf-cas-bundle.pem cat /etc/grid-security/certificates/*.r0 \u003e /etc/haproxy/certs/igtf-crls-bundle.pem # Some CRLs files are not ending with a new line # Ensuring that CRLs markers are separated by a line feed perl -pe 's/----------/-----\\n-----/' -i /etc/haproxy/certs/igtf-crls-bundle.pem Apache Configuration Include this configuration on the Apache config for the virtual host of your Keystone service, using the client ID and secret obtained above:\nOIDCResponseType \"code\" OIDCClaimPrefix \"OIDC-\" OIDCClaimDelimiter ; OIDCScope \"openid profile email eduperson_entitlement\" OIDCProviderMetadataURL https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration # PKCE method OIDCPKCEMethod S256 OIDCClientID \u003cclient id\u003e OIDCClientSecret \u003cclient secret\u003e OIDCCryptoPassphrase \u003csome crypto pass phrase\u003e OIDCRedirectURI https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect # OAuth for CLI access OIDCOAuthIntrospectionEndpoint https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect OIDCOAuthClientID \u003cclient id\u003e OIDCOAuthClientSecret \u003cclient secret\u003e # Increase Shm cache size for supporting long entitlements OIDCCacheShmEntrySizeMax 65536 \u003cLocation ~ \"/v3/auth/OS-FEDERATION/websso/openid\"\u003e AuthType openid-connect Require valid-user \u003c/Location\u003e \u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e If you have multiple keystone hosts, configure an alternative caching mechanism as per https://github.com/zmartzone/mod_auth_openidc/wiki/Caching\nFor example, using memcache\nOIDCCacheType memcache OIDCMemCacheServers \"memcache1 memcache2 memcache3\" Be sure to enable the mod_auth_oidc module in Apache, in Ubuntu:\nsudo a2enmod auth_openidc Note If running Keystone behind a proxy, make sure to correctly set the X-Forwarded-Proto and X-Forwarded-Port request headers, e.g. for haproxy:\nhttp-request set-header X-Forwarded-Proto https if { ssl_fc } http-request set-header X-Forwarded-Proto http if !{ ssl_fc } http-request set-header X-Forwarded-Port %[dst_port] Multiple OIDC providers If your OpenStack deployment needs to support multiple identity providers (besides EGI Check-in) you will need to configure mod_auth_openidc to support multiple providers and use an OAuth2.0 token introspection proxy like ESACO.\nmod_auth_openidc configuration First, create a directory to host each of the providers configuration, in our case we will use /var/lib/apache2/oidc/metadata, but adapt this to your specific needs. Ensure this directory is writable by the user running apache:\nmkdir -p /var/lib/apache2/oidc/metadata chown -R www-data:www-data /var/lib/apache2/oidc/metadata Set in your Apache configuration the OIDCMetadataDir pointing to that directory\nOIDCMetadataDir /var/lib/apache2/oidc/metadata You may remove the OIDCProviderMetadataURL, OIDCClientID and OIDCClientSecret options from the Apache configuration as these will be now set in new files created in the metadata directory. For every provider you will support, you need to create 3 files:\n\u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.provider with the OpenID Connect Discovery OP JSON metadata. The easiest way to create this file is getting its content from the OIDC server itself. For EGI Check-in:\ncurl https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration \u003e \\ /var/lib/apache2/oidc/metadata/aai-demo.egi.eu%2Foidc.provider \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.client with the client credentials. For EGI Check-in (aai-demo.egi.eu%2Foidc.client):\n{ \"client_id\": \"\u003cyour client id\u003e\", \"client_secret\": \"\u003cyour secret id\u003e\" } \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.conf with any extra configuration for the provider. This may not be needed if all your providers are similar. For example to specify the scopes to use for Check-in, use a aai-demo.egi.eu%2Foidc.conf as follows:\n{ \"scope\": \"openid email profile eduperson_entitlement\", \"pkce_method\": \"S256\" } Now add for the providers you support new configuration in Apache to facilitate the use of the dashboard. This is for a configuration of an egi.eu identity provider with openid as protocol:\n\u003cLocation ~ \"/v3/auth/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/websso\"\u003e AuthType openid-connect # This is your Redirect URI with a new iss=\u003cyour idp iss\u003e option added OIDCDiscoverURL https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect?iss=https%3A%2F%2Faai-demo.egi.eu%2Foidc%2F # Ensure that the user is authenticated with the expected iss Require claim iss:https://aai-demo.egi.eu/auth/realms/egi Require valid-user \u003c/Location\u003e ESACO configuration ESACO will handle OAuth tokens when users hit your Keystone from API/CLI. It needs to run as a daemon that listens (by default) on port 8156. We will use docker for facilitating the deployment:\nCreate a yaml file with the configuration of the different providers (application.yaml):\noidc: clients: - issuer-url: https://aai-demo.egi.eu/auth/realms/egi client-id: \"\u003cyour check-in client id\u003e\" client-secret: \"\u003cyour check-in client secret\u003e\" - issuer-url: \u003canother idp\u003e client-id: \"\u003cyour client id for second idp\u003e\" client-secret: \"\u003cyour client secret for second idp\u003e\" Create a environment file with the ESACO credentials you want to use (esaco.env):\n# User name credential requested from clients introspecting tokens ESACO_USER_NAME=\u003cesaco user name\u003e # Password credential requested from clients introspecting tokens ESACO_USER_PASSWORD=\u003cesaco password\u003e Run the ESACO server (adapt this as it better fits to run on your servers and make it run permanently):\ndocker run -p 8156:8156 -d -env-file=esaco.env \\ -v application.yml:/esaco/config/application.yml:ro \\ indigoiam/esaco:latest Configure Keystone’s Apache to use ESACO as OAuth introspection endpoint:\n# point this to the host where ESACO is running OIDCOAuthIntrospectionEndpoint http://localhost:8156/introspect OIDCOAuthClientID \u003cesaco user name\u003e OIDCOAuthClientSecret \u003cesaco password\u003e OIDCIDTokenIatSlack 3600 Configure also the locations in Apache that should use OAuth:\n\u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e \u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/other_idp/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e Horizon configuration In your Horizon configuration, set the list of providers and their mappings:\n# this is the list that will show up in the dropdown menu WEBSSO_CHOICES = ( (\"credentials\", _(\"Keystone Credentials\")), (\"egi.eu\", _(\"EGI Check-in\")), (\"other-idp\", _(\"Other IdP\")), ) # this maps the options above to keystone's idps and protocols WEBSSO_IDP_MAPPING = { \"egi.eu\": (\"egi.eu\", \"openid\"), \"other-idp\": (\"other-idp.com\", \"openid\") } Keystone Configuration Configure your keystone.conf to include in the [auth] section openid in the list of authentication methods:\n[auth] # This may change in your installation # add openid to the list of the methods you support methods = password, token, openid Add a [openid] section as follows:\n[openid] # this is the attribute in the Keystone environment that will define the # identity provider remote_id_attribute = HTTP_OIDC_ISS Add your horizon host as trusted dashboard to the [federation] section:\n[federation] trusted_dashboard = https://\u003cyour horizon\u003e/dashboard/auth/websso/ Finally copy the default template for managing the tokens in horizon to /etc/keystone/sso_callback_template.html. This template can be found in keystone git repository at https://github.com/openstack/keystone/blob/master/etc/sso_callback_template.html\ncurl -L https://raw.githubusercontent.com/openstack/keystone/master/etc/sso_callback_template.html \\ \u003e /etc/keystone/sso_callback_template.html Now restart your Apache (and Keystone if running in uwsgi) so you can configure the Keystone Federation support.\nKeystone Federation Support First, create a new egi.eu identity provider with remote id https://aai-demo.egi.eu/auth/realms/egi:\n$ openstack identity provider create --remote-id https://aai-demo.egi.eu/auth/realms/egi egi.eu +-------------+-----------------------------------------+ | Field | Value | +-------------+-----------------------------------------+ | description | None | | domain_id | 1cac7817dafb4740a249cc9ca6b14ea5 | | enabled | True | | id | egi.eu | | remote_ids | https://aai-demo.egi.eu/auth/realms/egi | +-------------+-----------------------------------------+ Check the name of the egi.eu domain name:\n$ openstack domain show -f value -c name $(openstack identity provider show -f value -c domain_id egi.eu) Set the name to egi.eu (if it was set to random auto-generated number):\n$ openstack domain set --name egi.eu $(openstack identity provider show -f value -c domain_id egi.eu) Create a group and add a domain-wide role for auditing purposes (see below):\n# Support for https://operations-portal.egi.eu/vo/view/voname/cloud.egi.eu $ openstack group create --domain egi.eu egi-staff $ openstack role add --domain egi.eu --group egi-staff reader Every VO you want to support should be mapped to a local project. The ops VO is used by EGI monitoring to ensure the correct functioning of your site. Create a group for this vo and add the group as a member of it:\n# Support for https://operations-portal.egi.eu/vo/view/voname/ops $ openstack group create ops $ openstack role add --domain egi.eu --group ops --project \u003cyour local ops project\u003e member Now you can define the mapping of EGI Check-in users into the groups you just created and restrict with the OIDC-eduperson_entitlement attribute which users will be members of those groups. Substitute the group IDs to the adequate values for your deployment:\n$ cat mapping.egi.json [ { \"local\": [ { \"user\": { \"name\": \"{0}\", \"email\": \"{1}\" }, \"group\": { \"id\": \"_ops_group_ID_\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_EMAIL\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai-demo.egi.eu/auth/realms/egi\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:ops:role=vm_operator#aai.egi.eu$\" ] } ] }, { \"local\": [ { \"user\": { \"name\": \"{0}\", \"email\": \"{1}\" }, \"group\": { \"id\": \"_egi-staff_group_ID_\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_EMAIL\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai.egi.eu/auth/realms/egi\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:cloud.egi.eu:role=auditor#aai.egi.eu$\" ] } ] } ] Note Note the use of the HTTP_OIDC_EMAIL in the mapping will allow to store the user’s email in your local database. Create the mapping in Keystone:\n$ openstack mapping create --rules mapping.egi.json egi-mapping Finally, create the federated protocol with the identity provider and mapping created before:\n$ openstack federation protocol create \\ --identity-provider egi.eu \\ --mapping egi-mapping openid +-------------------+-------------+ | Field | Value | +-------------------+-------------+ | id | openid | | identity_provider | egi.eu | | mapping | egi-mapping | +-------------------+-------------+ Keystone is now ready to accept EGI Check-in credentials.\nVO auditing Sometimes it is easy to leave behind Virtual Machines that are no longer used, consuming unnecessary resources. Owners of unused VMs should be notified to check whether occupied resources can be freed.\nEGI Check-in users get an ePUID (i.e. a long hash ending in @egi.eu) which are translated into local OpenStack user IDs. When VMs are created the owner of the VM is set to the OpenStack user ID instead of the ePUID. However, only the ePUID is linked to the user email in order for the user to be notified. The mapping between OpenStack user IDs and ePUIDs is shown with:\n$ openstack user list Problem is that regular users will not have the permissions to execute the command above. The steps above to configure a mapping for the cloud.egi.eu VO grant access to selected staff at EGI.eu to execute the command, using the default keystone policy:\n\"identity:list_users\": \"(role:reader and system_scope:all) or (role:reader and domain_id:%(target.domain_id)s)\" This has been tested in production on OpenStack Ussuri thanks to the collaboration between EGI.eu and IISAS-Fedcloud. It should also work with newer versions of OpenStack.\nEGI.eu staff belonging to the cloud.egi.eu VO having the auditor role should use the below setup to get the OpenStack user list:\nexport OS_INTERFACE=public # get OS_AUTH_URL with \"fedcloud site env --vo \u003cvo\u003e --site \u003csite\u003e\" export OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3 export OS_USERNAME=\u003cePUID\u003e # get it from https://aai.egi.eu/ export OS_IDENTITY_PROVIDER=egi.eu export OS_AUTH_TYPE=v3oidcaccesstoken export OS_PROTOCOL=openid export OS_IDENTITY_API_VERSION=3 # get OS_ACCESS_TOKEN following https://docs.egi.eu/users/aai/check-in/obtaining-tokens/ export OS_ACCESS_TOKEN=\u003ctoken\u003e export OS_DOMAIN_NAME=egi.eu $ openstack user list With this configuration EGI.eu staff is able to proactively notify creators of long-running VMs that may not be making an effective use of the cloud resources.\nAdditional VOs To configure additional VOs please follow steps in the VO Configuration guide.\nHorizon Configuration Edit your local_settings.py to include the following values:\n# Enables keystone web single-sign-on if set to True. WEBSSO_ENABLED = True # Allow users to choose between local Keystone credentials or login # with EGI Check-in WEBSSO_CHOICES = ( (\"credentials\", _(\"Keystone Credentials\")), (\"openid\", _(\"EGI Check-in\")), ) Once horizon is restarted you will be able to choose \"EGI Check-in\" for login.\nCLI Access The OpenStack Client has built-in support for using OpenID Connect Access Tokens to authenticate. You first need to get a valid Access Token from EGI Check-in (e.g. from https://aai-demo.egi.eu/token/) and then use it in a command like:\n$ openstack --os-auth-url https://\u003cyour keystone endpoint\u003e/v3 \\ --os-auth-type v3oidcaccesstoken --os-protocol openid \\ --os-identity-provider egi.eu \\ --os-access-token \u003cyour access token\u003e \\ token issue +---------+---------------------------------------------------------------------------------------+ | Field | Value | +---------+---------------------------------------------------------------------------------------+ | expires | 2017-05-23T11:24:31+0000 | | id | gAAAAABZJA3fbKX....nEMAPi-IsFOCkU9QWGTISYElzYJsI3z0SJGs7QsTJv4aJQq0JDJUBz6uE85SqXDj3 | | user_id | 020864ea9415413f9d706f6b473dbeba | +---------+---------------------------------------------------------------------------------------+ Moving to EGI Check-in production instance Once tests in the development instance of Check-in are successful, you can move to the production instance. Go to EGI Federation Registry and submit a Service Request for the production instance of EGI Check-in. After the approval of the request, you will need to update your configuration as follows:\nUpdate the remote-id of the identity provider:\nopenstack identity provider set --remote-id https://aai.egi.eu/auth/realms/egi egi.eu Update the HTTP_OIDC_ISS filter in your mappings, e.g.:\nsed -i 's/aai-demo.egi.eu/aai.egi.eu/' mapping.egi.json openstack mapping set --rules mapping.egi.json egi-mapping Update Apache configuration to use aai.egi.eu instead of aai-demo.egi.eu, if you have multiple OIDC providers, you should as well update the providers metadata and ESACO configuration. For the basic Apache configuration you should set these values:\nOIDCProviderMetadataURL https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration OIDCOAuthIntrospectionEndpoint https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Changes in the client settings If you want to make any changes to the client configuration, you need to submit a reconfiguration request through the Federation Registry. Additional VOs Once ops VO is working, you can include any further VOs you want to support as documented in the VO Configuration guide.\n","categories":"","description":"Authentication and Authorization integration\n","excerpt":"Authentication and Authorization integration\n","ref":"/providers/cloud-compute/openstack/aai/","tags":"","title":"Check-in"},{"body":"The EGI Federated Cloud Compute (FedCloud) service offers a multi-cloud IaaS federation that brings together research clouds as a scalable computing platform for data and/or compute driven applications and services for research and science.\nThis documentation focuses on using the service. Those resource providers willing to integrate into the service, please see the EGI Federated Cloud Integration documentation.\nCloud Compute gives you the ability to deploy and scale virtual machines on-demand. It offers computational resources in a secure and isolated environment controlled via APIs without the overhead of managing physical servers.\nCloud Compute service is provided through a federation of IaaS cloud sites that offer:\nSingle Sign-On via EGI Check-in, users can log into every provider with their institutional credentials and use modern industry standards like OpenID Connect. Global VM image catalogue at AppDB with pre-configured Virtual Machine images that are automatically replicated to every provider based on your community needs. Resource discovery features to easily understand which providers are supporting your community and what are their capabilities. Global accounting that aggregates and allows visualisation of usage information across the whole federation. Monitoring of Availability and Reliability of the providers to ensure SLAs are met. The flexibility of the Infrastructure as a Service can benefit various use cases and usage models. Besides serving compute/data intensive analysis workflows, Web services and interactive applications can be also integrated with and hosted on this infrastructure. Contextualisation and other deployment features can help application operators fine tune services in the cloud, meeting software (OS and software packages), hardware (number of cores, amount of RAM, etc.) and other types of needs (e.g. orchestration, scalability).\nSince the opening of the EGI Federated Cloud, the following usage models have emerged:\nService hosting: the EGI Federated Cloud can be used to host any IT service as web servers, databases, etc. Cloud features, as elasticity, can help users to provide better performance and reliable services. Example: NBIS Web Services, Peachnote analysis platform. Compute and data intensive applications: for those applications needing considerable amount of resources in terms of computation and/or memory and/or intensive I/O. Ad-hoc computing environments can be created in the EGI cloud providers to satisfy extremely intensive HW resource requirements. Example: VERCE platform, The Genetics of Salmonella Infections, The Chipster Platform. Datasets repository: the EGI Cloud can be used to store and manage large datasets exploiting the large amount of disk storage available in the Federation. Disposable and testing environments: environments for training or testing new developments. Example: Training infrastructure Eager to test this service? Have a look at how to create your first Virtual Machine in EGI.\n","categories":"","description":"Run virtual machines in the EGI Cloud\n","excerpt":"Run virtual machines in the EGI Cloud\n","ref":"/users/compute/cloud-compute/","tags":"","title":"Federated Cloud Compute"},{"body":"Overview This tutorial describes how to create your first Virtual Machine in the EGI Federation.\nStep 1: Signing up Create an EGI account with Check-in.\nStep 2: Enrolling to a Virtual Organisation Once your EGI account is ready you need to join a Virtual Organisation (VO). Here are the steps to join a VO. Explore the list of available VOs in the Operations Portal. We have a dedicated VO called vo.access.egi.eu for piloting purposes. If you are not sure about which VO to enrol to, please request access to the vo.access.egi.eu VO with your EGI account by visiting the enrollment URL. Check AppDB to see the list of Virtual Appliances and Resource Providers participating in the vo.access.egi.eu VO. AppDB is one of the service in the EGI Architecture.\nStep 3: Creating a VM Once your membership to a VO has been approved you are ready to create your first Virtual Machine. There are several ways to achieve this. The simplest way is to use the Infrastructure Manager Dashboard. On the other hand, advanced users may prefer to use the command-line interface.\nTo know more about the Cloud Compute Service in EGI please visit its dedicated section.\nAsking for help If you find issues please do not hesitate to contact us.\n","categories":"","description":"Step by step guide to get your first Virtual Machine up and running\n","excerpt":"Step by step guide to get your first Virtual Machine up and running\n","ref":"/users/tutorials/adhoc/create-your-first-virtual-machine/","tags":"","title":"Create your first Virtual Machine (VM)"},{"body":"Every user of the EGI Notebooks catch-all instance has a persistent home to store any notebooks and associated data. The content of this home directory will be kept even if your notebook server is stopped (which can happen if there is no activity for more than 1 hour). Modifications to the notebooks environment outside the home directory are not kept (e.g. installation of libraries). If you need those changes to persist, let us know via a GGUS ticket to the Notebooks Support Unit. You can also ask for increasing your default home allocation via ticket.\nNote Your home directory is backed up daily. Import notebooks into your workspace The Notebooks service default environment includes nbgitpuller, an extension to sync a git repository one-way to a local path. You can generate a shareable link by filling in the nbgitpuller link generator with your git repository.\nAlternatively, you can also use Replay for providing a link to notebooks and their computing environment.\nGetting data in/out Your notebooks have outgoing internet connectivity so you can connect to any external service to bring data in for analysis. As with input data, you can connect to any external service to deposit the notebooks output.\nThis is convenient for smaller datasets but not practical for larger ones, for those cases we can offer integration with several data services.\nEGI DataHub EGI DataHub provides a scalable distributed data infrastructure. It offers a tight integration with Jupyter and notebooks with specific drivers that make the DataHub Spaces accessible from any notebook.\nWhenever you log into the service, supported DataHub spaces will be available under the datahub folder. If you need support for any additional space, please open a ticket in GGUS to add it.\nBy default the notebooks-shared space is open for writing to any EGI Notebooks user part of the vo.notebooks.egi.eu VO. Please check the File Management section in the EGI DataHub documentation for more information on how to upload files.\nEUDAT B2DROP EUDAT B2DROP is a low-barrier, user-friendly and trustworthy storage environment which allows users to synchronise their active data across different desktops and to easily share this data with peers. EUDAT offers a free public instance of B2DROP for any researcher with a limited quota.\nThe data on B2DROP can be synchronised with EGI Notebooks so you can share content between the two services. This offers an easy-to-use storage and compute platform for the long-tail of science.\nHere is how you can get them synchronised. First, make sure you have access to B2DROP. Then, configure app username and app password on B2DROP’s security settings. Now, back to EGI Notebooks, click on the B2DROP connection drop-down menu when you start your session:\nEnter the app username and app password created previously, with the option to save them for future logins:\nYou will see a b2drop folder in the list of folders (left panel) of the EGI Notebooks that is synchronised with the content on B2DROP:\nD4Science Workspace D4Science VREs provide a shared workspace via a dedicated API. EGI Notebooks embedded in D4Science VREs will automatically show the user’s workspace at the workspace directory. You can browse and use as any regular file.\nShared folders The Notebooks service can enable shared folders for users, either in read-only or read-write mode. These are specially meant for community instances for easing the sharing of data between all the users of the service. In the catch-all instance the datasets directory serves as an example of such feature.\nOther services We are open for integration with other services for facilitating the access to input and output data. Please contact support _at_ egi.eu with your request so we can investigate the best way to support your needs.\n","categories":"","description":"How to access and manage data in EGI Notebooks\n","excerpt":"How to access and manage data in EGI Notebooks\n","ref":"/users/dev-env/notebooks/data/","tags":"","title":"Data Management in Notebooks"},{"body":"The default environment includes a set of kernels that are automatically built from the EGI-Federation/egi-notebooks-images GitHub repository.\nThese are the available kernels:\nPython: Default Python 3 kernel, it includes commonly used data analysis and machine learning libraries. Created from the jupyter/scipy-notebook stack.\nJulia: The Julia programming language with the libraries described in jupyter/datascience-notebook.\nR: The R programming language with several packages from the R ecosystem as provided by jupyter/r-notebook and some extra libraries.\nRStudio: RStudio Server offers a RStudio IDE from the Notebooks interface.\nOctave: The Octave programming language installed on its own conda environment (named octave).\nIf you want to add a new kernel, just let us know, and we will discuss the best way to support your request.\nCVMFS Notebooks mounts several CVMFS repositories where you can find software relevant to your community. These are accessible from the default CVMFS location /cvmfs and also linked in your home directory /home/jovyan/cvmfs. These repositories are available:\natlas-condb.cern.ch atlas.cern.ch auger.egi.eu biomed.egi.eu cms.cern.ch dirac.egi.eu eiscat.egi.eu grid.cern.ch notebooks.egi.eu If you need access to any other repositories, please open a request in GGUS.\nInstalling your own kernels/environments permanently If you want to have a completely customised environment for your Notebooks that persists across sessions, you can create your own conda environment in your home directory. Thanks to the nb_conda_kernels plugin these will show up automatically as an option to start notebooks with by following these steps:\nCreate a $HOME/.condarc file specifying where your environments will be created, e.g. in /home/jovyan/conda-envs/:\nenv_dirs: - /home/jovyan/conda-envs/ Create your environments as needed, make sure to install a kernel (ipykernel) for it to show automatically:\n$ conda create -p /home/jovyan/conda-envs/myenv ipykernel scipy The environment will show up in the launcher as a new option\n","categories":"","description":"The default environment in EGI Notebooks\n","excerpt":"The default environment in EGI Notebooks\n","ref":"/users/dev-env/notebooks/kernels/default/","tags":"","title":"Default Environment"},{"body":"The following guide is intended for researchers who want to use ECAS, a complete environment enabling data analysis experiments, in the EGI cloud.\nECAS (ENES Climate Analytics Service) is part of the EOSC-hub service catalog and aims to:\nprovide server-based computation, avoid data transfer, and improve reusability of data and workflows. It relies on Ophidia, a data analytics framework for eScience, which provides declarative, server-side, and parallel data analysis, jointly with an internal storage model able to efficiently deal with multidimensional data and a hierarchical data organization to manage large data volumes (“datacubes”), and on JupyterHub, to give users access to ready-to-use computational environments and resources.\nThanks to the Elastic Cloud Compute Cluster (EC3) platform, operated by the Polytechnic University of Valencia (UPV), researchers will be able to rely on the EGI Cloud Compute service to scale up to larger simulations without being worried about the complexity of the underlying infrastructure.\nThis guide will show how to:\ndeploy an ECAS elastic cluster of VMs in order to automatically install and configure the whole ECAS environment services, i.e. JupyterHub, PyOphidia, several Python libraries such as numpy, matplotlib and Basemap; perform data intensive analysis using the Ophidia HPDA framework; access the ECAS JupyterHub interface to create and share documents containing live code, equations, visualizations and explanatory text. Deploy an ECAS cluster with EC3 In the latest release of the EC3 platform a new Ansible receipt is available for researchers interested to deploy ECAS cluster on the EGI Infrastructure. The next sections provide details on how to configure and deploy an ECAS cluster on EGI resources.\nConfigure and deploy the cluster To configure and deploy a Virtual Elastic Cluster using EC3, access the EC3 platform front page and click on the \"Deploy your cluster\" link as shown in the figure below:\nA wizard will guide you through the cluster configuration process. Specifically, the general wizard steps include:\nLRMS selection: choose ECAS from the list of LRMSs (Local Resource Management System) that can be automatically installed and configured by EC3. Endpoint: the endpoints of the providers where to deploy the ECAS elastic cluster. The endpoints serving the vo.access.egi.eu VO are dynamically retrieved from the EGI Application DataBase using REST APIs. Operating System: choose EGI CentOS7 as cluster OS. Instance details, in terms of CPU and RAM to allocate for the front-end and the working nodes. Cluster’s size and name: the name of the cluster and the maximum number of nodes of the cluster, without including the frontend. This value indicates the maximum number of working nodes that the cluster can scale to. Initially, the cluster is created with the frontend and only one working node: the other working nodes will be powered on on-demand. Resume and Launch: a summary of the chosen cluster configuration. To start the deployment process, click the Submit button. When the frontend node of the cluster has been successfully deployed, you will be notified with the credentials to access via SSH.\nThe cluster details are available by clicking on the \"Manage your deployed clusters\" link on the front page:\nNote The configuration of the cluster may take some time. Please wait for its completion before starting to use the cluster. Accessing the cluster To access the frontend of the cluster:\ndownload the SSH private key provided by the EC3 portal; change its permissions to 600; access via SSH providing the key as identity file for public key authentication. [user@localhost EC3]$ ssh -i key.pem cloudadm@\u003cYOUR_CLUSTER_IP\u003e Last login: Mon Nov 18 11:37:29 2019 from torito.i3m.upv.es [cloudadm@oph-server ~]$ sudo su - [root@oph-server ~]# Both the frontend and the working nodes are configured by Ansible. This process usually takes some time. You can monitor the status of the cluster configuration using the is_cluster_ready command-line tool:\n[root@oph-server ~]# is_cluster_ready Cluster is still configuring. The cluster is successfully configured when the command returns the following message:\n[root@oph-server ~]# is_cluster_ready Cluster configured! As SLURM is used as workload manager, it is possible to check the status of the working nodes by using the sinfo command, which provides information about Slurm nodes and partitions.\n[root@oph-server ~]# sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 1 down* oph-io2 debug* up infinite 1 idle oph-io1 Accessing the scientific eco-system ECAS provides two different ways to get access to its scientific eco-system: Ophidia client (oph_term) and JupyterHub.\nPerform some basic operations with Ophidia Run the Ophidia terminal as ophuser user.\nThe default parameters are already defined as environmental variables inside the .bashrc file:\nexport OPH_SERVER_HOST=\"127.0.0.1\" export OPH_SERVER_PORT=\"11732\" export OPH_PASSWD=\"abcd\" export OPH_USER=\"oph-test\" Create an empty container and a new datacube with random data and dimensions.\nNow, you can submit your first operation of data transformation: let’s reduce the whole datacube in a single value for grid point using the average along the time:\nLet’s have a look at the environment by listing the datacubes and containers in the session:\nBy default, the Ophidia terminal will use the last output datacube PID. So, you can use the oph_explorecube operator to visualize the first 100 values.\nFor further details about the Ophidia operators, please refer to the official documentation.\nAccessing the Jupyter interface To access the Jupyter interface, open the browser at https://\u003cYOUR_CLUSTER_IP\u003e:443/jupyter and log in to the system using the username and password specified in the jupyterhub_config.pyp configuration file (see the c.Authenticator.whitelist and c.DummyAuthenticator.password lines) located under the /root folder.\nFrom JupyterHub in ECAS you can do several things such as:\ncreate and run a Jupyter Notebook exploiting PyOphidia and other Python libraries for data manipulation, analysis and visualization (e.g. NumPy, matplotlib, Cartopy); browse the directories, download and update files in the home folder; execute operators and workflows directly from the Ophidia Terminal; access to a read-only data repository hosted in a Onedata space and perform any analysis on this shared data. The ECAS space shared in the ECAS environment through the Onedata services is available at the onedata/ecas_provider/ECAS_space folder located under the /data directory.\nTo get started with the ECAS environment capabilities, open the ECAS_Basics.ipynb notebook available under the notebooks/ folder in the home directory.\nAccessing the Grafana UI This section will show how to monitor the ECAS environment and the resource usage and get aggregated information over time.\nTo access the Grafana monitoring interface, open the browser at https://\u003cYOUR_CLUSTER_IP\u003e:3000 and log in to the system using the admin username and the password specified in the .grafana_pwd file located under the /root folder.\nThe Grafana-based monitoring system provides two dashboards in order to monitor the ECAS cluster both at system and application level.\nThe infrastructure dashboard provides information about the percentage of CPU, RAM, SWAP and disk used on each Node.js (the frontend and the working nodes). frontend node working node The application dashboard shows information about which operator/workflow is being executed and its current execution status and provides aggregated information over time (e.g. number of total, completed and failed workflows/tasks, hourly weighted average of running cores). Destroy the cluster To destroy the running cluster use the delete action from the cluster management page.\nReferences ECASLab CMCC ECASLab DKRZ Ophidia GitHub: ECAS-Lab GitHub: Ansible role Ophidia cluster EC3 GitHub EC3 ","categories":"","description":"Using Elastic Cloud Computing Cluster (EC3) platform to create an ECAS environment.\n","excerpt":"Using Elastic Cloud Computing Cluster (EC3) platform to create an ECAS …","ref":"/users/compute/orchestration/im/ec3/apps/ecas/","tags":"","title":"ECAS"},{"body":"This section offers an introduction to the EGI services, together with tutorials about how to set up, use, and combine these services.\nNote See the Service Providers section for details on how to integrate providers into the EGI Federation. Request for information You can ask for more information about the public EGI services on our site.\n","categories":"","description":"Documentation for public EGI services","excerpt":"Documentation for public EGI services","ref":"/users/","tags":"","title":"User Guides"},{"body":" Target Audience: Newcomers of the EGI Infrastructure Are you new to EGI? And maybe a bit confused about what EGI is, what we do, how we are structured and how you can collaborate with us? Then this is the webinar for you! \"An introduction to EGI\" (October, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5464/contributions/15771/ About: The EGI Federation is an international e-Infrastructure to provide advanced computing and data analytics services for research and innovation. EGI federates compute resources, data management services, support teams and various online, thematic services from over 30 countries and international institutes, to make those available for researchers, innovators and educators. This webinar will provide a basic introduction to EGI, covering all the fundamental topics that you need to know before deep-diving on the advanced services to conduct world-class research and innovation. Target Audience: Scientific communities, projects, research infrastructures who wish to learn about the services of EGI. E-infrastructure and other service providers who want to be part of a pan-European federation to support international science. \"EGI Federation - Advanced Computing for Research\" (April 2020) Agenda, slides and recording: https://indico.egi.eu/event/5083/ About: The EGI federation is the largest distributed computing infrastructure in the world, and brings together hundreds of data centres worldwide and also includes the largest community cloud federation in Europe with tens of cloud providers across most of the European countries offering IaaS cloud and storage services. The current federated resources represent altogether more than 350 Petabytes of online storage and 380 Petabytes of archival storage supported by approximately 1 M Cores. EGI expanded the federation of its facilities with other non-European digital infrastructures in North America, South America, Africa-Arabia and the Asia-Pacific region, as such EGI fully realised the “Open to the World” vision. In order to interoperate at international level, EGI and its partners operate in the context of a lightweight collaboration framework defining rules of participations via a corpus of policies and technical guidelines. EGI offering includes a federated IaaS cloud to run compute- or data-intensive tasks and host online services in virtual machines or docker containers on IT resources accessible via a uniform interface; high-throughput data analysis to run compute-intensive tasks for producing and analysing large datasets and store/retrieve research data efficiently across multiple service providers; federated operations to manage service access and operations from heterogeneous distributed infrastructures and integrate resources from multiple independent providers with technologies, processes and expertise offered by EGI; consultancy for user-driven innovation to assess research computing needs and provide tailored solutions for advanced computing. The notion of a distributed infrastructure offering advanced resources and services for data-intensive processing in research and innovation has been part of the EGI mission and vision since the EGI design and implementation that started with the DataGrid project back in 2000 under the leadership of CERN. Distributed processing of data supported by a pan-European broadband network infrastructure, solutions for trust and identity management and the Grid middleware, have been the enablers of two Nobel prizes in Physics (2013 and 2017), and many more data-driven scientific discoveries in high energy physics, astronomy and astrophysics, health and medicine, and earth sciences resulting in more than 3,000 open access scientific publications enabled each year. In this webinar Gergely will provide an overview of EGI, the pan-european federation of national e-infrastructures and he will explain how EGI supports big data based Open Science and contributes to the implementation of the EOSC vision. The talk goes through the services that EGI provides for scientific communities, and for the members of the federation, and will show how these services benefit research, science and innovation in Europe and worldwide. ","categories":"","description":"These tutorials provides an overview of the EGI federation and of the EGI services, with highlights of their typical use cases.\n","excerpt":"These tutorials provides an overview of the EGI federation and of the …","ref":"/users/tutorials/foundation/","tags":"","title":"Foundation level"},{"body":"Use this section to get started quickly with internal EGI services:\nThe complete list of internal EGI services supporting the coordination of the EGI Federation offers insight into how EGI is able to offer advanced public cloud services The Configuration Database records the topology of the sites in the EGI federation Service Monitoring tracks and controls the performance of the services Accounting tracks service and resource usage, providing insights and reports on consumption The Helpdesk lets users and providers report incidents and bugs, or request changes ","categories":"","description":"Introduction to internal EGI services","excerpt":"Introduction to internal EGI services","ref":"/internal/getting-started/","tags":"","title":"Getting Started"},{"body":"Overview EGI is a federation of compute and storage resource providers united by a mission to support research and innovation.\nThe resources in the EGI infrastructure are offered by service providers that either run their own data centres or rely on community, private and/or public cloud services. These service providers offer:\nSingle Sign-On via EGI Check-in allows users to log in with their institutional (community) credentials Global image catalogue at AppDB with pre-configured virtual machine images Resource discovery features to easily understand which providers are supporting your community, and what are their capabilities Global accounting that aggregates and allows visualisation of usage information Monitoring of availability and reliability to ensure SLAs are met The EGI infrastructure supports a multitude of science and research communities, each with their own virtualised resources built around open standards. The development of these communities is driven by by their own scientific requirements.\nAccessing resources Access to resources (services) in the EGI infrastructure is based on OpenID Connect (OIDC), which replaces the legacy authentication and authorization based on X.509 certificates.\nNote Some services still rely on X.509 certificates, e.g. High Throughput Compute. EGI uses Virtual Organisations (VOs) to control access to resources. VOs are fully managed by research communities, allowing communities to manage their users and grant access to their services and resources. This means communities can either own their resources and use EGI services to share (federate) them, or can use the resources available in the EGI infrastructure for their scientific needs.\nBefore users can access an EGI service, they have to:\nObtain a supported ID, by signing up with either EGI Check-in directly, or with one of the community identity providers from the EGI infrastructure. Enroll into one VO. Users need to be part of a VO before using EGI services. Explore the list of available VOs in the Operations Portal. Authenticate to EGI Check-in to obtain an OAuth2 access token (and optionally a refresh token). Manage or use the service by leveraging the access token, either implicitly (web interfaces and dashboards usually hide this from users) or explicitly (e.g. when using command-line tools). Note See the EGI Check-in documentation for a detailed description of the Authentication and Authorization Infrastructure (AAI) of the EGI Federation, and to gain a better understanding of the concepts that act as building blocks for the AAI implementation. See the Communities section for guidance on how to access resources allocated to specific research communities (projects).\nRequesting resources Depending on the access conditions, a service (or an instance of the service) may be open for any user, or it may require requesting access (ordering).\nEGI services use the following types of access conditions:\nWide access - Users can freely access the service. Login may be required but it is possible with various institutional accounts (through EduGAIN), or with social accounts (e.g. Google). For example you can create a test Virtual Machine or launch a Jupyter Notebook. Policy based - Users are granted access based on specific policies defined by the service providers. Access needs to be requested, and will be checked for such services. Example: Compute resources and tools allocated to researchers in medical imaging (Biomed VO). Pay-for-use - Services are provided for a fee. Example: FitSM Training The EGI user community support team handles access requests (orders) for the Policy based and Pay-for-use access modes. They will respond to the request within maximum 5 work days. We normally contact you to have a short teleconference meeting to better understand your requirements, and to be able to identify resources and services that best match your needs. The meeting typically covers two topics:\nWhat is the background of your request? Scientific domain, partner countries, user bases, pay-for-use or not, etc. What are the technical details of your use case? How many CPU cores, how much RAM per CPU, which software services, and for how long do you need them, etc. Contact us if you want to discuss further.\nCapacity allocation When EGI is able to support a request for resources, it can do so in two ways:\nWe grant you access to an existing service, for example to compute resource pools (Virtual Organisations) that already exist in EGI for specific scientific disciplines or for researchers in specific regions. You can browse these in the EGI Operations Portal. If there is a suitable VO, we help you join it and use its services. We create a new VO for your community when none of the existing resource pools are suitable for your use case. The procedure is as follows: We will contact our provider and negotiate resources for you If there are providers willing to support you, we will sign a Service Level Agreement (SLA) with you A new VO will be created for your community Pilot your application EGI offers a playground allocation for users to get access to the services and understand how to port applications and develop new data analytics tools that can be turn into online services that can be accessed by scientist worldwide.\nRequirements and user registration Access requires acceptance of Acceptable Use Policy (AUP) and Conditions of the 'EGI Applications on Demand Service'.\nAcknowledgment Users of the service are asked to provide appropriate acknowledgement of the use in scientific publications. The following acknowledgement text can be used for this purpose (you should adapt to match the exact providers in your case):\nThis work used advanced computing resources from the 100%IT, CESGA, CLOUDIFIN, CYFRONET-CLOUD, GSI-LCG2, IFCA-LCG2, IN2P3-IRES, INFN-CATANIA-STACK, INFN-PADOVA-STACK, SCAI, TR-FC1-ULAKBIM, UA-BITP and UNIV-LILLE resource centres of the EGI federation. The services are co-funded by the EGI-ACE project (grant number 101017567).\nWhen requesting access users are guided through a registration process. Members of the EGI support team will perform a lightweight vetting process to validate the users’ requests before granting the access to the resources.\nGet access to pilot allocation Create an EGI Check-In account. Enroll the vo.access.egi.eu Virtual Organisation by following the enrollment URL. Make sure you use your EGI Check-In account for the enrollment. Once your petition is approved, you will be granted access. The grant to run applications is initially valid for 6 months and can be extended/renewed upon request. To review the list of providers for the vo.access.egi.eu VO, don’t forget to login to access the full list.\nFor pre-configured services (e.g.: EGI Notebooks), the capacity allocated includes up to 4 vCPU cores, 6 GB of RAM and 10 GB of block storage. For the EGI Cloud Compute or the EGI Container Cloud Compute, the EGI grant includes up to 4 vCPU cores, 8GB of RAM and 100GB of block storage.\nYou can manage those resources via command-line or web interface like the Infrastructure Manager dashboard.\nUnused resources Users of the EGI services may gain opportunistic usage to unused resources. These are resources that are not dedicated to the user’s organization, but are accessible when the research center(s) have some spare resources. This enables the most efficient use of resources.\nNote Users should not rely on (unused) resources not dedicated to their organisation, as access can be revoked without warning, and data may be lost if not properly backed up. ","categories":"","description":"Introduction to EGI services\n","excerpt":"Introduction to EGI services\n","ref":"/users/getting-started/","tags":"","title":"Getting Started"},{"body":"For collaboration purposes, it is best if you create a GitHub account and fork the repository to your own account. Once you do this, you will be able to push your changes to your GitHub repository for others to see and use, and you will be able to create pull requests (PRs) in the official EGI documentation repository based on branches in your fork.\nIf you are new to git and GitHub you are advised to start by the two following articles providing simple tutorials:\nStep by step guide to git Creating pull request with GitHub GitHub official documentation is available at docs.github.com.\nTip The first-contributions is a repository allowing anyone to freely learn and test creating a real Pull Request to an existing GitHub repository. Additional documentation about the main steps for working with GitHub is also available in this section.\nThe GitHub contribution flow In order to be able to send code update to the repository you need to:\nfork the repository to your GitHub account clone the repository on your local computer create a feature branch where you will commit your changes push the feature branch to the repository fork in your GitHub account open a Pull Request against the upstream repository In this process three git repositories are used:\nThe upstream repository: EGI-Federation/documentation Your fork, also named origin: \u003cyour_username\u003e/documentation A local clone of your fork, containing references to your fork, its origin and to the upstream repository Add an SSH key to your GitHub account The most convenient way to authenticate with GitHub is to use SSH keys over the SSH protocol.\nYou can add an SSH public key to your GitHub account in the Settings on GitHub, at https://github.com/settings/keys.\nRefer to Connecting to GitHub with SSH for an extensive documentation on using SSH keys with GitHub.\nIt’s worth to mention that your ssh public keys can easily be retrieved using a URL like https://github.com/\u003cyour_username\u003e.keys.\nIn order to manage repositories over ssh, you will have to clone them via SSH, not HTTPS.\nIf you already have a local clone of a repository created via HTTPS, you can switch it to SSH by following Switching remote URLs from HTTPS to SSH.\nStarting with the GitHub CLI The GitHub command-line interface greatly helps with working with GitHub repositories from a terminal.\nIt can be installed using the packages available on their homepage. There is also a manual.\nOnce installed you will have to start by setting up authentication.\n# Authenticate with GitHub, favor SSH protocol $ gh auth login $ gh config set git_protocol ssh Working with repositories The easiest way is to do it via the GitHub CLI that will also clone it locally. But it can also be done via the web interface, using the fork button and then cloning it locally manually.\nFork and clone This command will fork the repository to your GitHub account and clone a local copy for you to work with.\n$ gh repo fork EGI-Federation/documentation Clone existing fork If you want to clone an existing fork you should use:\n$ gh repo clone \u003cyour_username\u003e/documentation Validate the local clone If your local clone of you fork is correctly setup you should see references to the origin and upstream repositories.\n$ git remote -v origin git@github.com:\u003cyour_username\u003e/documentation (fetch) origin git@github.com:\u003cyour_username\u003e/documentation (push) upstream git@github.com:EGI-Federation/documentation.git (fetch) upstream git@github.com:EGI-Federation/documentation.git (push) Run the site locally The documentation site is built from the source files using Hugo. The repository README can be used as a reference for building instructions.\nRequirements Hugo Node.js and other docsy theme dependencies: postcss-cli autoprofixer These dependencies can be either installed manually or reusing a flox environment. Please see the steps below.\nInstalling dependencies manually To install npm+Node.js please check the official instructions.\nEverything has been tested with Node.js 12.\nThe dependencies of the docsy theme can be installed as follows:\n# From the root of the repository clone $ npm ci Hugo can be installed following the official documentation.\nHugo (extended) releases can be downloaded at the Hugo releases page. The recommended version for Hugo is specified under the GitHub Action in .github/workflows/hugo_version.txt.\nReuse the flox environment Flox is a virtual environment and package manager all in one. We provide a flox environment in our GitHub repository in a way that it is easier for everybody to work with the same software dependencies and contribute to this repository.\nAfter installing flox you can reuse the provided environment with:\n# From the root of the repository clone $ flox activate # Then install docsy dependencies with $ npm ci Installing pre-commit hooks Git hooks are small bits of code which run during parts of the git commit process. In particular the pre-commit hook runs before committing your changes, in order to enforce contribution guidelines, code quality and style conventions, etc.\nThe pre-commit framework is used to manage the pre-commit hooks in this repository, and should be installed before making your contribution:\n$ pip install pre-commit $ pre-commit install pre-commit installed at .git/hooks/pre-commit See .pre-commit-config.yaml in the repository for more details of which hooks are enabled.\nBuilding the site To build and run the site, from the repository root:\n$ git submodule update --init --recursive --depth 1 $ hugo --minify Testing the site locally To launch the site locally, from the repository root:\n$ hugo serve -D The site is available locally at: http://localhost:1313/.\nBranches and commits You should submit your patch as a git branch ideally named with a meaningful name related to the changes you want to propose. This is called a feature branch (sometimes also named topic branch). You will commit your modifications to this feature branch and submit a Pull Request (PR) based on the differences between the upstream main branch and your feature branch.\nCreate a feature branch Try to avoid committing changes to the main branch of your clone to simplify management, creating a dedicated feature branch helps a lot. Try to pick a meaningful name for the branch (my_nice_update in the example).\n# This should be done from the up-to-date main branch # Read furthermore to see documentation on updating a local clone $ git checkout -b my_nice_update Write changes The documentation being made of plain text files you are free to use whatever text editor or Integrated Development Environment (IDE) suits you, from neovim to Visual Studio Code.\nSome environments may provide you plugins helping with syntax or offering a preview, they are worth checking.\nBe sure to commit files with having been formated using Prettier as documented in our style guide.\nCommit changes It is the best practice to have your commit message have a summary line that includes the issue number, followed by an empty line and then a brief description of the commit. This also helps other contributors understand the purpose of changes to the code.\n#3 - platform_family and style * use platform_family for platform checking * update notifies syntax to \"resource_type[resource_name]\" instead of resources() lookup * GH-692 - delete config files dropped off by packages in conf.d * dropped debian 4 support because all other platforms have the same values, and it is older than \"old stable\" debian release # Select the modified files to be committed $ git add files1 path2/ # Commit the changes $ git commit -m \u003ccommit_message\u003e Push feature branch to the fork in preparation of a PR From inside a feature branch you can push it to your remote fork.\n# Ask git to keep trace of the link between local and remote branches $ git push --set-upstream Once done, the output will show a URL that you can click to generate a Pull Request (PR). Accessing GitHub upstream of forked repositories may also propose you to submit a PR.\nIf needed GitHub CLI can also be used to prepare the PR:\n$ gh pr create \u003cyour_username\u003e:\u003cfeature_branch\u003e --web Previewing a pull request If a repository maintainer adds the label safe for preview to a pull request it will be possible to preview it using a pull request-specific URL: https://docs.egi.eu/documentation/[PR_NUMBER]\nThe preview can be used as an alternative to testing a pull request locally, and the preview can easily be shared with other contributors.\nOnly collaborators having write permission to the repository are able to mark a pull request as safe for review.\nThis should be carefully considered, especially for external and first time contributors.\nUpdate local feature branch with changes made on the PR Once you PR have been opened it will be reviewed, and reviewers can propose and commit changes to your PR. If you need to make further changes be sure to update the local clone with the remote changes.\n# Retrieve changes made on your PR in the upstream repository $ git pull Then you can commit new changes and push them to your remote fork.\nUpdate repository clone with the upstream changes # If you are still in a branch created for a previous PR, move to main $ git checkout main # Get the latest data from the upstream repository $ git fetch upstream # Update your local copy with this data $ git rebase upstream/main main # Update your remote GitHub fork with those changes $ git push Update local feature branch with changes made on the main branch In case the main branch evolved since the feature branch was created, it may be required to merge the new changes in the feature branch.\nIt can easily be done via the PR page on the GitHub web interface, but it can also be done in your repository clone using git rebase.\n# Retrieve changes made in the upstream repository $ git fetch upstream # Check out the feature branch $ git checkout feature_branch # Apply the new changes on main to your feature branch $ git rebase upstream/main In case some files have been changed on both sides you will have to merge the conflicts manually.\nRunning checks from GitHub actions locally The repository leverages GitHub Actions to do automatic checks.\nThe main checks are:\nFile lint using Super-Linter that includes many linters. Markdown files are processed using markdownlint-cli. Markdown link check, relying on markdown-link-check. Spelling using Check Spelling. It’s possible to run those linters locally, it can be useful to test and debug why they are reporting errors, and test without waiting on the automatic checks.\nTo save time, we recommend running the specific you are interested in, but as documented later for Check-Spelling, you can use act to run the real GitHub actions workflows.\nRunning markdownlint locally markdownlint-cli can be installed locally on some platforms, in that case it can be used like this:\n$ markdownlint -c .github/linters/.markdownlint.json \\ content/en/about/_index.md If you cannot or don’t want to install it locally, you can rely on using Docker and GitHub Packages:\n$ docker run -v $PWD:/workdir:ro --rm -i ghcr.io/igorshubovych/markdownlint-cli:latest \\ --config .github/linters/.markdownlint.json \\ content/en/about/_index.md Running markdown-link-check locally You can use Docker and GitHub Packages:\n$ docker run -v $PWD:/tmp:ro --rm -i ghcr.io/tcort/markdown-link-check:stable \\ --config /tmp/.github/linters/mlc_config.json \\ /tmp/content/en/about/_index.md Running Check-Spelling locally For spelling, it may be easier to rely on a spell checker integrated in your content editor.\nNevertheless, if you are adventurous, you can use act, that will use docker to run the checks locally.\nWhile the spell check should work, other part of the job interacting with GitHub will likely fail, like when errors are identified, so be sure to properly scan through the command output.\n# Listing available jobs $ act -l # Running the spelling job $ act -j spelling Running Super-Linter locally Should you want to do this, Super-Linter can be run locally.\nYou can try using act:\n# Listing available jobs $ act -l # Running the lint job $ act -j super-lint If it doesn’t work as expected, or if you prefer another solution, you should look at the Super-Linter documentation for running locally.\nClone PR to edit/test/review locally It’s possible to clone a Pull Request to a local branch to test it locally. It’s done using the PR number.\n# List available PR and their identifiers. $ gh pr list # Clone specific PR, updating sudmodules $ gh pr checkout XX --recurse-submodules Once done it’s possible to build and run the site locally:\n# From the root of the repository clone # Here on MacOS X, adapt depending on your platform $ hugo serve -D The documentation will then be accessible on http://localhost:1313.\nPeople having write access to the repository hosting the branch related to the PR (ie. usually the PR author) will be able to add and edit files.\n# From the local clone of the repository $ gh pr checkout XXX --recurse-submodules $ vim yyy.zz $ git add yyy.zz $ git commit yyy.zz -m \u003ccommit_message\u003e $ git push Update a local clone of a PR # It will ask you to merge changes $ git pull Then you can refer to the README.md to see how to test it locally.\nIn case the PR got commits that were forced pushed you may have troubles, in that case it may be easier to delete the local branch and do another checkout of the PR.\nClean a local clone of a PR In case you have troubles updating the local clone, as it can happens if changes were forced pushed to it, it maybe easier to delete the local copy of the PR and recreate it.\n# Switch to main branch $ git checkout main # Check local branches $ git branch -vv # Delete a specific branch $ git branch -d \u003cbranch_name\u003e # If you need to force the deletion use -D $ git branch -D \u003cbranch_name\u003e Using stashes Sometimes we realise just before committing a change that we are not in the correct branch (ie. that we forgot to create a dedicated feature branch), when this happens git stash can be helpful.\n# Saving a change $ git stash save \u003coptional message\u003e # Creating the forgotten branch $ git checkout -b \u003cmy_feature_branch\u003e # Reviewing the saved changes, use TAB completion $ git stash show \u003cTAB\u003e # Applying the saved changes, use TAB completion $ git stash pop \u003cTAB\u003e # Review the changes to be committed $ git diff If you already committed your change(s) you may have to look at git reset.\n# Viewing the diff of the two last commits $ git log -n 2 -p # Reverting the last change, keeping the change in the local directory $ git reset HEAD^ ","categories":"","description":"First steps with Git and GitHub","excerpt":"First steps with Git and GitHub","ref":"/about/contributing/git/","tags":"","title":"Git and GitHub"},{"body":"HTCondor-CE can be configured to automatically publish accounting data to EGI Accounting service, if compute resources are provided by an HTCondor pool. Optionally, the HTCondor pool can also be configured to provide per-machine performance data for increased accounting accuracy.\nEnabling accounting records on each HTCondor-CE Each HTCondor-CE must be configured separately to create accounting data for its jobs and feed them to APEL.\nSee MAN09 for general configuration instructions of APEL.\nInstall htcondor-ce-apel, available from the EGI UMD or HTCondor RPM repositories.\nConfigure the APEL parser via parser.cfg to read the HTCondor-CE accounting records:\n[blah] enabled = true dir = /var/lib/condor-ce/apel/ filename_prefix = blah [batch] enabled = true dir = /var/lib/condor-ce/apel/ filename_prefix = batch type = HTCondor Configure the APEL client via client.cfg section [spec_updater] to know the cluster specs:\nsite_name must equal your EGI Configuration Database (GOCDB) site name.\nmanual_spec1 must equal the CE identifier, spec type and average spec value of the cluster. The format is:\nmanual_spec1 = \u003cfqdn\u003e:9619/\u003cfqdn\u003e-condor,\u003cspec_type\u003e,\u003cspec_value\u003e Commonly, the spec type is one of HEPscore23 or HEPSPEC. The spec value is per core.\nFor example, if the CE has the Fully Qualified Domain Name (FQDN) my-htcondor-ce.example.com and resources average 12.5 HEPSPEC per core:\nmanual_spec1 = my-htcondor-ce.example.com:9619/my-htcondor-ce.example.com-condor,HEPSPEC,12.5 Start and enable the condor-ce-apel.timer unit.\nEnabling per-machine performance information By default, HTCondor-CE APEL accounting assumes that all machines in the cluster have comparable performance. If this is not the case, accounting accuracy can be improved by adding performance information per machine.\nPerformance information may be added to each HTCondor StartD. There are two separate ways to do so:\nAn absolute spec value, similar to the average spec value on to the CE. HTCondor-CE APEL accounting then weights resource usage by comparing the StartD spec value to the average spec value.\nIn the StartD configuration, define ApelSpecs as a new-style classad mapping of spec types and their values; multiple spec types are supported. Also add ApelSpecs to the classad attributes of the StartD:\n# The absolute performance per core on this StartD ApelSpecs = [HEPSPEC=14.37; HEPscore23=14.409; SI2K=2793] STARTD_ATTRS = $(STARTD_ATTRS) ApelSpecs A relative spec value, as a factor to the average spec value on to the CE. HTCondor-CE APEL accounting then weights resource usage by the relative spec factor.\nIn the StartD configuration, define ApelScaling as a number; values above 1 mean the performance is above average. Also add ApelScaling to the classad attributes of the StartD.\n# The relative performance per core on this StartD ApelScaling = 1.15 STARTD_ATTRS = $(STARTD_ATTRS) ApelScaling If both ApelSpecs and ApelScaling are defined, ApelSpecs takes precedence.\n","categories":"","description":"HTCondor-CE Accounting","excerpt":"HTCondor-CE Accounting","ref":"/providers/high-throughput-compute/htcondor-ce-accounting/","tags":"","title":"HTCondor-CE Accounting"},{"body":"This page contains information about integrating your identity provider (IdP) with Check-in in order to allow users in your community to access EGI tools and services.\nOrganisations who want to register their IdP in Check-in needs to fill this form in case the IdP is not publishing REFEDS R\u0026S and Sirtfi compliance in eduGAIN. A PDF scan of a printed and signed copy should be sent to operations_at_egi.eu\nIdentity Provider integration workflow To integrate your Identity Provider with the EGI Check-in service, you need to submit a GGUS ticket indicating your request. The responsible support unit is AAI Support. The integration follows a two-step process:\nRegister your Identity Provider and test integration with the development instance of EGI Check-in. The development instance allows for testing authentication and authorisation to EGI services and resources without affecting the production environment of EGI. Note that the development instance is not connected to the production service and no information is shared between the two systems. Register your Identity Provider with the production instance of EGI Check-in to allow members of your Community to access production EGI services and resources protected by Check-in. This requires that your Identity Provider meets all the policy requirements and that integration has been thoroughly tested during Step 1. The most important URLs for each environment are listed in the table below but more information can be found in the protocol-specific sections that follow.\nProduction Demo Development Protocol Production environment SAML https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso OpenID Connect See client registration Protocol Demo environment SAML https://aai-demo.egi.eu/proxy/module.php/saml/sp/metadata.php/sso OpenID Connect See client registration Protocol Development environment SAML https://aai-dev.egi.eu/proxy/module.php/saml/sp/metadata.php/sso OpenID Connect See client registration General requirements for integrating identity providers An institution or a community may connect their IdP with Check-in to allow their users to access EGI services, or any other services that have enabled Check-in as an authentication provider. This section presents the general requirements for integrating an IdP with EGI Check-in, while protocol-specific instructions are provided in the sections that follow.\nAttribute release requirements As a bare minimum, the IdP of a user’s Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community. The unique identifier must be accompanied with a minimum set of attributes which the Check-in Service Provider Proxy will attempt to retrieve from the user’s IdP. If this is not possible, the missing user attributes will be acquired and verified through the user registration process with the EGI Account Registry. The following table describes the data requested from the user’s Home Organisation, which are communicated to the Check-in SP as either SAML attributes or OIDC claims, depending on the protocol supported by the authenticating IdP.\nDescription Notes At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier Preferred name for display purposes For example to be used in a greeting or a descriptive listing First name Surname Email address Affiliation within Home Organisation or Community To be released only if relevant for accessing EGI services Note that the above set of requested attributes, particularly the identifier, name, email and affiliation information, complies with the REFEDS R\u0026S attribute bundle.\nInformation about group membership and role information released by your IdP should follow the URN scheme below (see also AARC-G002):\n\u003cNAMESPACE\u003e:group:\u003cGROUP\u003e[:\u003cSUBGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e where:\n\u003cNAMESPACE\u003e is in the form of urn:\u003cNID\u003e:\u003cDELEGATED-NAMESPACE\u003e[:\u003cSUBNAMESPACE\u003e*], where \u003cNID\u003e is the namespace identifier associated with a URN namespace registered with IANA, as per RFC8141, ensuring global uniqueness. Implementers can and should use one of the existing registered URN namespaces, such as urn:geant and urn:mace; \u003cDELEGATED-NAMESPACE\u003e is a URN sub-namespace delegated from one of the IANA registered NIDs to an organisation representing the e-infrastructure, research infrastructure or research collaboration.\n\u003cGROUP\u003e is the name of a VO, research collaboration or a top level arbitrary group. \u003cGROUP\u003e names are unique within the urn:mace:egi.eu:group namespace; zero or more \u003cSUBGROUP\u003e components represent the hierarchy of subgroups in the \u003cGROUP\u003e; specifying sub-groups is optional the optional \u003cROLE\u003e component is scoped to the rightmost (sub)group; if no group information is specified, the role applies to the VO \u003cGROUP-AUTHORITY\u003e is a non-empty string that indicates the authoritative source for the entitlement value. For example, it can be the FQDN of the group management system that is responsible for the identified group membership information Example entitlement values expressing VO/group membership and role information:\nurn:geant:dariah.eu:group:egi-interop:role=member#aaiproxy.de.dariah.eu urn:geant:dariah.eu:group:egi-interop:role=vm_operator#aaiproxy.de.dariah.eu Operational and security requirements The IdP needs to comply with additional requirements to achieve a higher level of assurance and allow its users to gain access to a wider set of EGI services. A first group of additional requirements are defined by the Sirtfi framework v1.0. Adherence to these requirements can be asserted either by publishing Sirtfi compliance in the eduGAIN metadata or by declaring it in this form. These requirements are in the areas of operational security, incident response, traceability and IdPs and users responsibility.\nBranding requirements Check-in provides a central Discovery Service (or “Where Are You From” - WAYF) page where users in your Home Organisation or Community will be automatically redirected when necessary to select to authenticate at your IdP. You can provide us with a logo of your Organisation or Community (in high-res PNG or preferably in svg format) to include a dedicated login button that will allow users to easily identify your IdP.\nSAML Identity Provider To allow users in your community to sign into federated EGI applications, you need to connect to the EGI AAI SP Proxy as a SAML Identity Provider (IdP). Users of the application will be redirected to the central Discovery Service page of the EGI AAI Proxy where they will able to select to authenticate at your IdP. Once the user is authenticated, the EGI AAI Proxy will return a SAML assertion to the application containing the information returned by your IdP about the authenticated user.\nMetadata registration SAML authentication relies on the use of metadata. Both parties (you as an IdP and the EGI AAI SP) need to exchange metadata in order to know and trust each other. The metadata include information such as the location of the service endpoints that need to be invoked, as well as the certificates that will be used to sign SAML messages. The format of the exchanged metadata should be based on the XML-based SAML 2.0 specification. Usually, you will not need to manually create such an XML document, as this is automatically generated by all major SAML 2.0 IdP software solutions (e.g., Shibboleth, SimpleSAMLphp). It is important that you serve your metadata over HTTPS using a browser-friendly SSL certificate, i.e. issued by a trusted certificate authority.\nTo exchange metadata, please send an email including the following information:\nentityID Metadata URL Depending on the software you are using, the authoritative XML metadata URL for your IdP might be in the following form:\nhttps://your.idp.example.eu/idp/shibboleth (Shibboleth) https://your.idp.example.eu/simplesaml/module.php/saml2/idp/metadata.php (SimpleSAMLphp) Note that if your IdP is part of a federation, then it would be preferred to send us the URL to a signed federation metadata aggregate. We can then cherry pick the appropriate entityID from that.\nYou can get the metadata of the EGI Check-in SP Proxy on a dedicated URL that depends on the integration environment being used:\nProduction Demo Development Production environment https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso Demo environment https://aai-demo.egi.eu/proxy/module.php/saml/sp/metadata.php/sso Development environment https://aai-dev.egi.eu/proxy/module.php/saml/sp/metadata.php/sso For the production environment, it is recommended that you get the metadata for the EGI Check-in SP (entityID: https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso) from a signed eduGAIN metadata aggregate. For example, the following aggregates are provided by GRNET:\nGRNET federation's metadata eduGAIN SP metadata Attribute release The SAML based Identity Provider of your Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community, along with a set of additional information as described in the following table (see also general attribute release requirements):\nDescription SAML attribute At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier SubjectID (public) or eduPersonUniqueIdeduPersonPrincipalNameSubjectID (pairwise) or eduPersonTargetedID or SAML persistent identifier Preferred name for display purposes displayName First name givenName Surname sn Email address mail Affiliation within Home Organisation or Community eduPersonScopedAffiliation Group(s)/role(s) within Home Organisation or Community eduPersonEntitlement OpenID Connect Identity Provider Users in your community can sign into federated EGI applications through the Check-in service using your OpenID Connect or OAuth 2.0 based Identity Provider.\nClient registration To enable your OIDC Identity Provider for user login, Check-in needs to be registered as a client in order to obtain OAuth 2.0 credentials, such as a client ID and client secret, and to register one or more redirect URIs. Once Check-in is registered as a client, your users will be redirected to the central Discovery Service page of Check-in when logging into EGI federated applications, where they will able to select to authenticate at your IdP. Once the user is authenticated, Check-in will be responsible for communicating the information returned by your IdP about the authenticated user to the connected application. Depending on the protocol, this information will be expressed through a SAML assertion, a set of OIDC claims or a (proxy) X.509 certificate.\nProvider configuration Check-in needs to obtain your OpenID Provider's configuration information, including the location of the Authorisation, Token and UserInfo endpoints. Your OpenID Provider is expected to make a JSON document available at the path formed by concatenating the string /.well-known/openid-configuration to the Issuer, following the OpenID Connect Discovery 1.0 specification.\nAttribute release The OpenID Connect or OAuth 2.0 based Identity Provider of your Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community, along with a set of additional information as described in the following table (see also general attribute release requirements):\nDescription OIDC claim At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier sub (public)N/Asub (pairwise) Preferred name for display purposes name First name given_name Surname family_name Email address email Affiliation within Home Organisation or Community eduperson_scoped_affiliation Group(s)/role(s) within Home Organisation or Community eduPerson_entitlement ","categories":"","description":"Check-in guide for Identity Providers","excerpt":"Check-in guide for Identity Providers","ref":"/providers/check-in/idp/","tags":"","title":"Identity Providers"},{"body":"What is it? Infrastructure Manager (IM) is a tool that orchestrates the deployment of custom virtual infrastructures on multiple backends. It streamlines the access and usability of Infrastructure-as-a-Service (IaaS) clouds by automating the configuration, deployment, software installation and update, and monitoring of virtual infrastructures.\nIM is integrated with the EGI Check-in Service and supports a wide variety of backends, either federated (such as EGI Cloud Compute), public (such as Amazon Web Services, Google Cloud or Microsoft Azure) or on-premises (such as OpenStack), thus making user applications cloud agnostic. IM features a web-based GUI, an XML-RPC API, a REST API and a command-line interface (CLI). It supports OASIS TOSCA Simple Profile in YAML.\nTip An easy way to deploy your first VM in the EGI Federation is from the Infrastructure Manager dashboard. A tutorial and demo videos are also available. Note For detailed information about Infrastructure Manager please see its documentation. It was also presented in one of the EGI Webinars, more details are available on the indico page and the video recording is available on YouTube. The following sections document how to use IM from its GUI or CLI.\n","categories":"","description":"Automating deployment of virtual infrastructures on EGI Cloud\n","excerpt":"Automating deployment of virtual infrastructures on EGI Cloud\n","ref":"/users/compute/orchestration/im/","tags":"","title":"Infrastructure Manager"},{"body":"Introduction A Resource Centre (RC) is the smallest resource administration domain in the EGI Federation. It can be either localised or geographically distributed and provides a minimum set of local or remote IT Services compliant with well-defined IT Capabilities (HTC, Cloud, Storage, etc.) necessary to make resources accessible to Users. EGI is a Resource Infrastructure federating RCs to constitute a homogeneous operational domain.\nRegistration and certification Note Related procedure: PROC09 Resource Centre Registration and Certification In order to join the EGI Infrastructure, a RC needs to present the request to the Research Infrastructure Provider (RP) existing in its country. A RP is a legal organisation, part of the EGI Resource Infrastructure, responsible for managing and operating a number of operational services at national level, supporting EGI RCs and user communities. Please have a look at the Operations Start Guide to get familiar with the terms mentioned above, and to have a complete picture of the several actors participating in our landscape.\nThe RP operators are going to guide and support the RC during the registration and certification procedures.\nFirstly, the RC will be asked to read, understand, and accept:\nthe RCs Operational Level Agreement (OLA), an agreement made between the RC and its RP that defines the minimum set of operational services and the respective quality parameters that a Resource Centre is required to provide in EGI; the Security Policies defined in EGI to guarantee that all the security aspects with the service delivery are fulfilled and enforced. Secondly, the RC should be registered in the EGI Configuration Database: the provided information, from the generic contacts and roles of people to the service endpoints details, is needed to trigger the daily operations of other services and activities provided by the EGI Infrastructure such as the Monitoring of the resources, the Accounting, the Support, and the Security activities.\nOnce the entry in the Configuration Database is complete, the RP changes the RC status from “Candidate” to “Uncertified”, and the certification procedure can start: it comprises a series of technical controls to verify that the provided services work according to the expectations defined in the RC OLA. Any identified issue is notified by the RP operators to the RC and investigated until its solution.\nWhen all the certification controls are successfully passed, the RC status is changed to “Certified” meaning that the RC is included in the EGI production infrastructure and its resources can be consumed by the users of the infrastructure.\nResource Centres responsibilities Incidents and service requests Providing support is a fundamental part of the daily activity of a provider participating in a large research infrastructure such as EGI. The support is not only for the users accessing the resources but also for those who are involved in the management and oversight of the infrastructure.\nAs defined in the RC OLA, the RC will handle incidents and service requests registered as tickets in the EGI Helpdesk service, with the expectation to acknowledge and process any notified issue, within the agreed response time associated with the priority of the ticket.\nThe response time is defined by the Quality of Support levels, and for the RCs the level will be Medium, meaning that there will 4 priorities for the incidents (requiring for example up to 5 working days for the “less urgent” tickets and up to 1 working day for the “top priority” ones), while any service request will be processed as “less urgent” ticket.\nSecurity topics The security posture of the infrastructure is framed by the set of policies constituting the Security Policies. Those policies cover different complementary activities including the operation of services, the processing of personal data and the management of security incidents and vulnerabilities.\nDealing with security incidents The Security Incident Response Policy aims at coordinating the incident response across the infrastructure, ensuring that that incidents are promptly reported, and that all incidents are investigated as fully as possible.\nSecurity incidents are to be treated as serious matters and their investigation must be resourced appropriately.\nResources Centres must report suspected security incidents to their RP Security Officer and to EGI Computer Security Incident Response Team (CSIRT) within 4 hours of discovery. This initial step will start the coordination of the incident response as documented in the procedure SEC01 EGI CSIRT Security Incident Handling Procedure.\nThis procedure has been implemented according to the Security Incident Response Policy, to minimise the impact of security incidents affecting the Resource Centres part of the infrastructure It covers guidance on how the incident response should be coordinated, describing the responsibilities of the various parties, and encourages post-mortem analysis and promotes cooperation between Resource Centres. Handling of vulnerabilities The handling of vulnerabilities is a very formal process involving many different entities.\nAnyone can report a software vulnerability via a form or by email contacting the Software Vulnerability Group (SVG).\nThe report will trigger an assessment, following the SEC02 Software Vulnerability Issue Handling procedure, by the Software Vulnerability Group, of the risk level associated with this vulnerability in the context of the activities of the EGI Infrastructure.\nOnce a vulnerability has been identified as presenting a risk to the infrastructure, it will be decided if an advisory should be prepared and circulated to the security contacts of the sites. After an agreed period of time, and depending on their confidentiality, advisories are made public.\nVulnerabilities identified as critical are handled according to the procedure SEC03 EGI-CSIRT Critical Vulnerability Handling. When applicable, this usually involves developing a custom security monitoring probe created to identify on High Throughput Compute RCs if their resources are vulnerable to the vulnerability. The status is closely monitored by the security team and accessible to the affected RCs.\nUsing this information correlated with the one from Pakiti, the patch management service collecting information about the patches deployed at the various High Throughput Compute RCs, the Incident Response Task Force (IRTF) on duty Security Officer will open tickets against the impacted sites according to the WI07 Security Vulnerability Handling procedure.\nThe EGI Service Delivery and Information Security (SDIS) team of the EGI Foundation, formerly known as the EGI Operations team, will follow up with the resource provider to work on resolving the ticket. The first duty of the resource provider is to acknowledge the vulnerability and then work on a prompt resolution as suggested in the ticket. In case a satisfactory resolution is not reached in due time, or a sign of active progress on addressing the vulnerability is not visible, the specific Resource Centre may be suspended.\nServing user communities Once part of the production infrastructure, the RC is ready to deliver its resources to any of the users’ communities consuming the infrastructure.\nThe RC can continue serving local user communities, and at the same time deliver capacity for international user communities that approach EGI and therefore reach the federated RCs.\nInternational communities reach EGI through the following channels:\nThe EGI site where they can request access to services. The EGI-ACE Call for Use Cases. Service and Operation Level Agreements (SLAs, OLAs) The User Community Support team of the EGI Foundation receives these requests and negotiates the details of access, with the involvement of relevant and ‘fit-for-purpose’ RCs. This Service Level Management (SLM) process intervenes as a matchmaker between service expectations and needs of the user communities, acting as ‘customers’, and the capabilities of the RCs. Customers are enabled access to the resources in the form of Virtual Organisations (VOs).\nIn order to select providers for provisioning services to a given customer, technical requirements are collected from the customer then transferred to relevant providers. The Expression of Interests for support (EoIs) are collected from the interested providers during the negotiation phase, resulting in the best match with customer’s requirements and expectations (both technical and financial). Several aspects are considered during the negotiation phase, including the geographical location of the customer, national roadmap and priority of the providers, and costs of the service provisioning in case of a pay-for-use model.\nThe result of the negotiation is a ‘Service Level Agreement’ (SLA), and several ‘Operation Level Agreements’ (OLA), one with each contributing provider. (See SLAs-OLAs examples.)\nSLAs and OLAs are typically signed for at least 1 year, and are automatically renewed, as long as the provider(s) or the customer do not express a decision to terminate the Agreement at least a month before the expiration date.\nPerformance reports: enforcing OLAs As defined in the RCs OLA, the performance of the delivered services should meet the Service Level Targets: the monthly performance of the RCs is monitored, and when the targets are not achieved for three consecutive months, the affected RCs are notified through a ticket about the OLA violation and requested to provide within 10 working days an explanation for the low performance and a plan for improvement.\nThe RCs not providing a satisfactory explanation or not replying at all are eligible for suspension.\nIn order to re-join the EGI Infrastructure, any suspended RC should undergo a new certification procedure. The “Suspended” status cannot last for more than 4 months, after which a RCs is either in production again or definitely closed.\nBesides the Targets defined in the RCs OLA, which are enforced to guarantee the permanence of the RC in the infrastructure, the targets promised to the users in the VO SLAs should also be met on a monthly basis: when a violation occurs, the RC is requested to provide a justification and a plan for improving the quality of the provided services.\nIf repeated violations occur, the SLA can be renegotiated with the customer, either by changing the Service Level Targets, or by choosing a different RCs as a provider.\n","categories":"","description":"Guidelines for Resource Centres to join the EGI Infrastructure","excerpt":"Guidelines for Resource Centres to join the EGI Infrastructure","ref":"/providers/joining/federated-resource-centre/","tags":"","title":"Joining as a Federated Resource Centre"},{"body":"In these guidelines, we describe the set of actions a service provider should follow to join the EGI Infrastructure, and to ensure the high-quality delivery of service according to the EGI policies.\nProviders have different options to become members of the EGI Federation, and deliver services for advanced computing within the EGI Infrastructure. The options are documented below, linking to detailed instructions:\nJoining as a federated resource centre, becoming a provider (that we call Resource Centre) delivering one of the following services: Cloud Compute, HTC Compute, Cloud Container Compute, or Online Storage. More than 200 Resource Centres have been integrated in this way. Joining as a software provider, also called Technology Provider, which is providing middleware deployed on the federated resource centres. Joining as a new provider for an existing service in the Compute and Data Federation or the Platform services, such as DataHub, Notebooks, Training Infrastructure, and Workload Manager Joining as Core/Central service provider for services supporting all the other services of the EGI Infrastructure. Contributing a new service, enabling advanced computing in research and education, and not yet present in the EGI service portfolio. ","categories":"","description":"Guidelines for service providers to join the EGI Infrastructure","excerpt":"Guidelines for service providers to join the EGI Infrastructure","ref":"/providers/joining/","tags":"","title":"Joining EGI as a provider"},{"body":"Document control Property Value Title How to publish Site Information Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Publishing site information in the Information Discovery System Owner SDIS team EGI profile for the use of the GLUE 2.0 Information Schema specifies how the GLUE 2.0 information schema should be used in EGI. It gives detailed guidance on what should be published, how the information should be interpreted, what kinds of uses are likely, and how the information may be validated to ensure accuracy.\nConfiguring a site BDII The site BDII needs to be configured to read from every node in the site which publishes information (meaning that it runs a so-called resource BDII). In YAIM this is defined with the BDII_REGIONS variable, which contains a list of node names which in turn refer to variables called BDII_\u003cNODE\u003e_URL which specify the LDAP URL of each resource BDII.\nSome services may have DNS aliases for multiple hosts, but the BDII_REGIONS must contain the real hostnames for each underlying node - the information in the resource BDII is different for each node, so reading it via an alias would produce inconsistent results. However, it will usually be desirable for the published endpoint URLs to contain the alias rather than the real hostname; that can often be defined with a YAIM variable for the service. For the site BDII itself this variable is SITE_BDII_HOST. (If multiple site or top BDIIs are configured identically their content will also be identical, so reading via an alias does not produce any inconsistencies.)\nMost services now publish themselves, so sites should check that all relevant services are included. In particular, VOMS servers have only published themselves comparatively recently so may be missing from the configuration. If the glite-CLUSTER node type is used this must also be included. Publication has been enabled for Argus in EMI 2, so this may also need to be added. Common services which do not currently publish are APEL and Squid. See the table below for more detailed information.\nIt is important to realise that the site BDII itself has a resource BDII, and this must be explicitly included in the configuration, e.g. with something like\nBDII_REGIONS=\"CE SE BDII\" (...) BDII_BDII_URL=\"ldap://$SITE_BDII_HOST:2170/mds-vo-name=resource,o=grid\" In the past it was common for the site BDII to be colocated with the CE so it did not need to be listed explicitly, but if installed on a dedicated node (which is now the recommended deployment) it must be included.\nTo check that all expected services are published the following command can be used:\n$ ldapsearch -x -h $SITE_BDII_HOST -p 2170 -b mds-vo-name=$SITE_NAME,o=grid \\ objectclass=GlueService \\ | perl -p00e 's/\\r?\\n //g' | grep Endpoint: (replacing SITE_BDII_HOST; and SITE_NAME with the values for your site), which should list all the service URLs.\nIn addition, most services should now be published in GLUE 2 format. There is no explicit configuration needed for GLUE 2, but one thing to be aware of is that the site name (and the other parts like o=grid) in the GIIS URL field in the GOCDB must have the correct case as GLUE 2 is case-sensitive.\nTo verify the GLUE 2 publication use the command:\n$ ldapsearch -x -h $SITE_BDII_HOST -p 2170 -b GLUE2DomainID=$SITE_NAME,o=glue \\ objectclass=GLUE2Endpoint \\ | perl -p00e 's/\\r?\\n //g' | grep URL: Some services, notably storage elements, may be missing or incomplete in GLUE 2 if they are older than the EMI 2 release. The following table shows the publishing status for gLite and WLCG node types (ARC and Unicore have a different structure).\nNode type GLUE 1 GLUE 2 Notes LCG-CE Yes No Obsolete CREAM Yes Yes Full publication only in EMI 2 CLUSTER Yes Yes Full publication only in EMI 2 WMS Yes Yes LB Yes Yes DPM Yes EMI 2 dCache Yes EMI 2 StoRM Yes EMI 2 LFC Yes EMI 2 FTS Yes EMI 2 Channels not yet published in GLUE 2 Hydra EMI 2 EMI 2 Not yet released in EMI 2 AMGA Yes EMI 2 VOMS Yes Yes MyProxy Yes Yes Argus No EMI 2 Internal service, publication for deployment monitoring Site BDII Yes Yes Top BDII Yes Yes R-GMA Yes No Obsolete VOBOX Yes Yes Apel No No Internal service, publishing not yet requested Squid No No Configuration exists but not enabled Nagios Yes Yes Service-related documentation Federated Cloud BDII configuration For information about configuration of a Federated Cloud BDII, please look at the EGI Information System.\nGlueSite Object These are the existing well established attributes in the GlueSite object. All of these MUST remain.\nAttribute Example Schema Notes GlueSiteName RAL-LCG2 Free text, no whitespace Same as GOCDB name if in GOCDB, your choice. GlueSiteUniqueID RAL-LCG2 Identical to your !GlueSiteName Same as GlueSiteName GlueSiteWeb https://cern.ch/it Free Text Valid URL about the site. GlueSiteLatitude 52.42 NN.NN Site Latitude. GlueSiteLongitude 16.91 NN.NN Longitude of Site. GlueSiteDescription Rutherford Lab Free Text A long name for the site. GlueSiteLocation Dublin, Ireland Town, City, Country An decreasing resolution ending with Country, agree a country name within a country. i.e UK != United Kingdom. Scotland and the Balkans should write a dynamic provider. !GlueSiteUserSupportContact mailto:helpdesk@example.com Valid URL URL for getting support. A ticket system if available. !GlueSiteSysAdminContact xmpp://admins@jabber.org Valid URL How to contact the admins. !GlueSiteSecurityContact mailto:security@example.com Valid URL How to contact for security related matters. The GlueSite object in the 1.3 Glue Schema contains an attribute GlueSiteOtherInfo. To quote.\nThe attribute is to be used to publish data that does not fit any other attribute of the site entity. A name=value pair or an XML structure are example[s] of usage.\nAll this extra configuration will be with in the static information for the glue site within the Grid Information Provider system.\nGuidelines for GlueSite Object A format for publishing useful information about sites within the !GlueSiteOtherInfo is needed, as shown in the following table.\nKey Example Type Notes GRID EGI [#validgrid List of valid grid names] Multiple ones can be defined. WLCG_TIER 1 Tier level of site in WLCG context. Either 0, 1 , 2 , 3 , 4 WLCG_PARENT UK-T1-RAL Name of the higher (administrative) tier site in WLCG The WLCG_NAME of the site at a higher tier with WLCG WLCG_NAME IT-ATLAS-federation [#lcgnames Valid WLCG Names] An official WLCG name. WLCG_NAMEICON https://example.com/tier2.png Valid URL URL to WLCGNAME icon, ideally 80x80 pixels. EGEE_ROC Russia Valid federated Operations Centre name Only applicable if your site is still part of a federated Operations Centre (“ROC” according to the old EGEE terminology). Name MUST match the Operations Centre name declared in GOCDB. Note. If the site is now part of a NGI, then EGI_NGI MUST be used (see below). EGI_NGI NGI_CZ Valid NGI Must agree with the GOC DB EGEE_SERVICE prod prod, pps or cert Which EGEE grid your site is part of, multiple attributes is okay. Obsolete in EGI. OLDNAME Bristol text If your !GlueSiteName changes at some point please record your old name here. ICON https://example.com/icon.png Valid URL Icon Image for your site, ideally 80x80 pixels BLOG https://scotgrid.blogspot.com/feeds/posts/default Valid RSS or Atom Feed Your site blog if you have one CONFIG yaim yaim, puppet, quattor, … The configuration tool(s) used at the site Note. Keywords starting with one of the grid names are to some extent reserved for that grid.\nExample GlueSiteName: RAL-LCG2 GlueSiteOtherInfo: BLOG=https://example.com/blog/feed GlueSiteOtherInfo: EGI_NGI=NGI_UK GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: GRID=GRIDPP GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: ICON=https://example.com/images/tierOneSmall.png GlueSiteOtherInfo: WLCG_PARENT=CERN-PROD GlueSiteOtherInfo: WLCG_TIER=1 Distributed Tier1s and Tier2s Within an WLCG context for instance there are instances of distributed Tier2s and Tier1s. If separate component sites want to exist as a single WLCG tier then they might contain common values for their WLCGNAME.\nGlueSiteName: CSCS-LCG2 GlueSiteOtherInfo: CONFIG=yaim GlueSiteOtherInfo: EGI_NGI=NGI_CH GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: WLCG_NAME=CH-CHIPP-CSCS GlueSiteOtherInfo: WLCG_PARENT=FZK-LCG2 GlueSiteOtherInfo: WLCG_TIER=2 Note that WLCG_PARENT is an accounting unit defined in the MOU document, as shown in WLCG CRIC.\nEstablished Grid Name Short Name Long Name URL EGI European Grid Initiative https://www.egi.eu EELA Europe and Latin America https://www.eu-eela.eu/ WLCG World LHC Computing Grid https://cern.ch/lcg GRIDPP UK Particle Physics Grid https://www.gridpp.ac.uk UKNGS National UK Grid Service https://www.ngs.ac.uk OSG Open Science Grid (US) https://www.opensciencegrid.org/ NDGF Nordic DataGrid Facility https://www.ndgf.org/ LondonGrid London Grid https://www.gridpp.ac.uk/tier2/london/ NORTHGRID Northern (UK) Grid https://www.gridpp.ac.uk/northgrid/ SCOTGRID Scottish Grid https://www.scotgrid.ac.uk/ SOUTHGRID Southern (UK) Grid https://www.gridpp.ac.uk/southgrid/ Academic Grid Malaysia Malaysian Grid UPM Campus Grid Universiti Putra Malaysia https://www.upm.edu.my/ AEGIS Academic and Educational Grid Initiative of Serbia https://www.aegis.rs/ BIGGRID Dutch e-science Grid https://www.biggrid.nl/ Consorzio Cometa Consorzio Multi-Ente per la promozione e l’adozione di Tecnologie di calcolo Avanzato (Italy) https://www.consorzio-cometa.it/en D-Grid German Grid https://www.d-grid-gmbh.de/index.php?id=1\u0026amp;L=1 EUMED EU/Mediterranean Grid https://www.eumedgrid.eu/ GILDA Grid INFN Laboratory for Dissemination Activities (Italy) https://gilda.ct.infn.it/ GISELA Grid Initiative for e-Science virtual communities in Europe and Latin America https://www.gisela-grid.eu/ GRISU Griglia del Sud (Southern Italy Grid) https://www.grisu-org.it/ NEUGRID Neuroscience Grid https://neugrid4you.eu/background RDIG Russian Data Intensive Grid https://grid-eng.jinr.ru/?page_id=43 SEE-GRID South Eastern European GRid-enabled eInfrastructure Development https://www.see-grid.org/ Important: The EGEE Grid name was decommissioned on [[Agenda-14-02-2011|14-02-2011]]. All sites need to replace this grid name with EGI.\nBeing part of a grid is just a reference that your site is in some way associated with a particular Resource Infrastructure Provider either technically or as part of a collaboration. The list of Grids can be extended. Please contact operations@egi.eu to request changes.\nValid WLCG Names The WLCG names are the site names that appear within the LCG MOU concerning commitments to LHC computing.\nWLCG Name Current GlueSiteName CA-TRIUMF TRIUMF-LCG2 CERN CERN-PROD DE-KIT FZK-LCG2 ES-PIC pic FR-CCIN2P3 IN2P3-CC IT-INFN-CNAF INFN-T1 NDGF NDGF-T1 NL-T1 SARA-MATRIX TW-ASGC Taiwan-LCG2 UK-T1-RAL RAL-LCG2 US-FNAL-CMS USCMS-FNAL-W1 US-T1-BNL BNL-LCG2 For the tier two names please consult WLCG CRIC. The column marked Accounting Name are the WLCG Names which in the case of Tier2s are the GOCDB names. Use your site GOCDB name as your WLCG_NAME.\nAlso some tier2s live under more than 1 tier1 perhaps for different for different VOs. If your tier2 has more that one WLCG_PARENT then just add two distinct records to show this. Also some tier2s do not have a WLCGNAME at all.\nGlueSiteUniqueId: EENet GlueSiteName: EENet GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: EGI_NGI=NGI_NL GlueSiteOtherInfo: WLCG_TIER=2 GlueSiteOtherInfo: WLCG_PARENT=UK-T1-RAL GlueSiteOtherInfo: WLCG_PARENT=NL-T1 Valid EGI NGI Names The valid names are those published on GOCDB.\nYAIM Instructions YAIM will have to be updated for those sites using yaim. This will be done and submitted to sites in the normal way.\nYAIM Variable and Value Resulting Glue Attribute and Value SITE_NAME=RAL_LCG2 GlueSiteName: RAL-LCG2 SITE_DESC=“Rutherford Lab” GlueSiteDescription: Rutherford Lab SITE_EMAIL= steve@example.com GlueSiteSysAdminContact: mailto:steve@example.com SITE_SUPPORT_EMAIL= steve@example.com GlueSiteUserSupportContact: mailto:steve@example.com SITE_SECURITY_EMAIL= steve@example.com GlueSiteSecurityContact: mailto:steve@example.com SITE_LOC=“Soho, London, United Kingdom” GlueSiteLocation: Soho, London, United Kingdom SITE_LONG=52.45 GlueSiteLongitude: 52.45 SITE_LAT=-12.34 GlueSiteLatitude: -12.34 SITE_WEB=“https://example.com/\" GlueSiteWeb: https://example.com/ SITE_OTHER_GRID=“EGI|WLCG” GlueSiteOtherInfo: GRID=EGI\nGlueSiteOtherInfo: GRID=WLCG SITE_OTHER_EGEE_ROC=“UK/I” GlueSiteOtherInfo: EGEE_ROC=UK/I SITE_OTHER_EGI_NGI=“NGI_CZ” GlueSiteOtherInfo: EGI_NGI=NGI_CZ SITE_OTHER_EGEE_SERVICE=“prod” GlueSiteOtherInfo: EGEE_SERVICE=prod SITE_OTHER_WLCG_TIER=2 GlueSiteOtherInfo: WLCG_TIER=2 SITE_OTHER*=\"|” GlueSiteOtherInfo: KEY=GlueSiteOtherInfo: KEY= If multiple values for GlueSiteOtherInfo are needed, then just delimit your values with a |. The character | must be avoided in values.\nCheck your own GlueSite Object The information published can be checked through an ldap search:\n$ ldapsearch -x -H ldap://$SITE_BDII_HOST:2170 \\ -b 'Mds-Vo-Name=$SITE_NAME,o=Grid' \\ '(ObjectClass=GlueSite)' In addition, VAPOR is a tool which provides a GUI for different views of published information, including a LDAP view.\nSite information in GLUE 2 The GLUE 2 equivalent of the GlueSite object is the GLUE2AdminDomain. The same information should be present although in a slightly different format, and there are separate GLUE2Contact and GLUE2Location objects.\n","categories":"","description":"How to publish site information","excerpt":"How to publish site information","ref":"/providers/operations-manuals/man01_how_to_publish_site_information/","tags":"","title":"MAN01 How to publish site information"},{"body":"The more you go in data analysis, the more you understand that the most suitable tool for coding and visualizing is not pure code, or some integrated development environment (IDE), nor data manipulation diagrams (such as workflows or flowcharts). From some point on you just need a mix of all these – that is what notebook platforms are, Jupyter being the most popular of them.\nWhat is it? EGI Notebooks is a service-like environment based on the Jupyter technology, offering a browser-based tool for interactive data analysis.\nThe Notebooks environment provides users with notebooks where they can combine text, mathematics, computations and rich media output. EGI Notebooks is a multi-user service that can scale on demand, being powered by the compute services of EGI.\nEGI Notebooks provides the well-known Jupyter interface for notebooks, with the following added features:\nIntegration with EGI Check-in allows you to login with any eduGAIN or social accounts (e.g. Google, Facebook) Persistent storage associated with each user is available in the notebooks environment Customisable with new notebook environments, expose any existing notebook to your users Can easily use EGI compute and storage services from your notebooks, as your notebooks run on EGI infrastructure Service Modes We offer different service modes depending on your needs\nNotebooks for researchers Individual users can use the centrally operated service from EGI. Users can log in, write and play and re-play notebooks by:\ncreating an EGI account Enrolling to the one of the supported VOs such as vo.notebooks.egi.eu VO accessing https://notebooks.egi.eu/ The central instance supports the following VOs:\nvo.notebooks.egi.eu, enroll here vo.access.egi.eu auger biomed vo.reliance-project.eu eiscat.se eval.c-scale.eu vo.panosc.eu vo.environmental.egi.eu vo.lethe-project.eu vo.cessda.eduteams.org Notebooks for communities User communities can have their customised EGI Notebooks service instance. EGI offers consultancy, support, and can operate the setup as well. A community specific setup allows the community to use the community's own Virtual Organisation (i.e. federated compute and storage sites) for Jupyter, add custom libraries into Jupyter (e.g. discipline-specific analysis libraries) or have fine grained control on who can access the instance (based on the information available to the EGI Check-in AAI service).\nEGI currently operates community instances for:\nD4Science. These instances are accessed through specific Gateways: SoBigData, Blue-Cloud, D4Science Services and EOSC-Pillar. Check with D4Science support for more information. Training instance EGI can provide a custom and temporary instance of the Notebooks service for training events, if you have a specific event where you would like to use EGI Notebooks as platform for your training, let us know.\nNote This instance may not use the same software version as in production and may not be always available, as it is typically configured only for specific training events. ","categories":"","description":"Interactive data analysis with EGI Notebooks","excerpt":"Interactive data analysis with EGI Notebooks","ref":"/users/dev-env/notebooks/","tags":"","title":"Notebooks"},{"body":"Overview Online Storage includes services that allow users to store, share and access data using the EGI infrastructure. Different categories of storage are available, depending on how data is stored, the technology used to access and consume data, and the foreseen usage.\nThree major service offerings are available:\nBlock Storage is block-level storage that can be attached to virtual machines (VMs) as volumes, a simple solution for durable data that does not need to be shared beside a single VM. Grid Storage is file storage for High Throughput Compute (HTC) and/or High Performance Compute (HPC) scenarios. Object Storage is persistent, hierarchical blob storage for cloud native applications, archiving, or when data is shared between different VMs or multiple steps of processing workflows. Comparison of storage types The differences between Block, Grid, and Object Storage are summarized below:\nType Sharing Accounting Usage Block From within VMs, only at the same site the VM is located For the entire block POSIX access, use as local disk Grid From any device connected to the internet For the data stored Grid protocols and HTTP/WebDAV Object From any device connected to the internet For the data stored HTTP requests to REST API The following sections offer a more detailed description of each storage service.\n","categories":"","description":"Data storage services in the EGI infrastructure\n","excerpt":"Data storage services in the EGI infrastructure\n","ref":"/users/data/storage/","tags":"","title":"Online Storage"},{"body":"Introduction This page was created to help you, as a ROD, start with ROD duties.\nThe section How to become a ROD member describes steps which needs to be taken before starting working as a ROD. The section ROD duties documents all the tasks that make up the ROD duties. Section Important to read introduces all documents which concern this activity and which are supposed to be read at the beginning. Section Operational Tools describes all tools used by ROD teams. Finally, Contact section is going to inform you how to contact others.\nHow to become a ROD member There are few actions which needs to be taken before you start your work:\nGet a valid grid certificate delivered by Certificate Authorities (CA) - this step is important because most of the tools used during the shift require certificate. Find EUGRIDPMA members. Register to Dteam VO. Dteam membership will give you possibility to test sites and debug problems. Register into GGUS tool as support staff. GGUS is a ticketing system which is used for operational purpose within EGI. With support staff role you will be able to reply on and update recorded tickets. Register in Configuration Database, a central database which contains all the information about EGI Infrastructure (sites and people). To be ROD members you have to be registered in this database. It will allows you to perform step 5. Request the Regional Staff role in the Configuration Database. Thanks to this role you will be recognized automatically in operations tools as ROD member. It gives you a several privileges in the database as well as in other tools. Contact your NGI manager - you need to contact your NGI manager to be approved as Regional Staff and to be added to ROD mailing list in your NGI (this mailing list is a contact point to the whole ROD team within the NGI). Get familiar with the ROD documentation, a single place where you will find all information relevant to your work as a ROD. To see how to perform all those actions please watch video How to become a ROD member (7 steps which should be done to become a ROD member also).\nROD duties The Regional Operations team is responsible for detecting problems, coordinating the diagnosis, and monitoring the problems through to a resolution. It monitors sites in their region, and react to problems identified by the monitors, either directly or indirectly, provide support to sites as needed, add to the knowledge base, and provide informational flow to oversight bodies in cases of non-reactive or non-responsive sites. ROD is a team responsible for solving problems on the infrastructure according to agreed procedures. They ensure that problems are properly recorded and progress according to specified time lines. They ensure that necessary information is available to all parties. The team is provided by each Operation Centre and requires procedural knowledge on the process (rather than technical skills) for their work.\nAll duties listed are mandatory for ROD team:\nHandling incidents. The main responsibility of ROD is to deal with incidents at sites in the region. This includes making sure that the tickets are opened and handled properly. The procedure for handling tickets is described in EGI Infrastructure Oversight escalation procedure Propagate actions from EGI Operations down to sites. ROD is responsible for ensuring that decisions taken on the EGI Operations level are propagated to sites. Putting a site in downtime or suspend for urgent matters. In general, ROD can place a site in downtime (in the Configuration Database) if it is either requested by the site, or ROD sees an urgent need to put the site into downtime. ROD may also suspend a site, under exceptional circumstances, without going through all the steps of the escalation procedure. For example, if a security hazard occurs, ROD must suspend a site on the spot in the case of such an emergency. It is important to know that EGI Operations can also suspend a site in the case of an emergency e.g. security incidents or lack of response. Notify EGI Operations about core or urgent matters. ROD should create Helpdesk tickets to EGI Operations in the case of core or urgent matters. Important to read Before you start your duties you should get familiar with following documents:\nEGI Infrastructure Oversight escalation procedure. This document defines escalation procedure for operational problems. It describes steps and timelines which ROD team should follow. Dashboard How-Tos and Training Guides. A collection of How-Tos and guides for EGI Operations. It includes a Dashboard How-To, Training Guides which can be used as a presentation for training staff and quick sheets. ROD FAQ. Frequently Asked Questions related to ROD work It is also important to watch video tutorials prepared for ROD teams. They will walk you through several topics which are important for your work.\nOperational Tools ROD uses several operational tools to perform theirs duties (Operations tools video):\nOperations Portal. Dashboard tool on the Operations Portal is a main tool which is used by ROD teams. All actions concerning incidents (alarms and tickets) should be performed using this tool. Service Monitoring (ARGO) is the official EGI monitoring system based on Nagios. It checks the availability of the services and creates alarms visible on the Operations Portal dashboard when a failure occurs. Helpdesk is the EGI central helpdesk system designed for reporting and tracking problems. Configuration Database is a central database which contains all static information about the infrastructure (sites and people). Contact Each ROD teams is supposed to provide own mailing list as a contact point to the team. The list of people responsible for ROD in a given NGI and contact points can be found in the EGI Configuration Database.\nAll ROD mailing list are subscribed to “all-central-operator-on-duty AT mailman.egi.eu” mailing list so to contact other ROD teams you can use this list.\nTo contact EGI Operations team you can:\nsend an Helpdesk ticket and assign it to EGI Operation support unit send an email to “operations AT egi.eu” You are welcome to send us questions in case of any doubts concerning ROD duties.\n","categories":"","description":"Overview of the information and guidelines for ROD","excerpt":"Overview of the information and guidelines for ROD","ref":"/providers/rod/overview/","tags":"","title":"Overview"},{"body":"Deploy cluster Through a “job wizard” interface the user can login to the Elastic Cloud Compute Cluster (EC3) portal and configure the virtual cluster with the related tools and applications to be deployed in the EGI Cloud. Click on the “Deploy your cluster” button to create a cluster in the EGI Cloud.\nThe cluster is composed of a front node, where a batch job scheduler is running, and a number of compute nodes. These compute nodes will be dynamically deployed and provisioned to fit increasing load, and un-deployed when they are in idle status. The installation and configuration of the cluster is performed by means of the execution of Ansible receipts.\nA wizard will guide the user during the configuration process of the cluster, allowing to configure details like the operating system, the characteristics of the nodes, the maximum number of nodes of the cluster or the pre-installed software packages. Specifically, the general wizard steps include:\nLRMS selection: choose Torque from the list of LRMSs (Local Resource Management System) that can be automatically installed and configured by EC3.\nEndpoint: the endpoints of the providers where to deploy the elastic cluster. The endpoints serving the vo.access.egi.eu VO are dynamically retrieved from the EGI Application DataBase using REST APIs.\nOperating System: choose one of the available EGI base OS images available to create the cluster (e.g. CentOS7, or EGI Ubuntu 18.04 LTS).\nInstance details: in terms of CPU and RAM to allocate for the front-end and the working nodes.\nCluster’s size and name: the name of the cluster and the maximum number of nodes of the cluster, without including the frontend. This value indicates the maximum number of working nodes that the cluster can scale. Initially, the cluster is created with the frontend and only one working node: the other working nodes are powered on on-demand.\nResume and Launch: a summary of the chosen cluster configuration. To start the deployment process, click the Submit button.\nNote The configuration of the cluster may take some time. Please wait for its completion before starting to use the cluster! When the frontend node of the cluster has been successfully deployed, the user will be notified with the credentials to access via SSH.\nThe cluster details are available by clicking on the “Manage your deployed clusters” link on the front page.\nAccessing the EC3 cluster To access the frontend of the elastic cluster:\nDownload the SSH private key provided by the EC3 portal; Change its permissions to 600; Access via SSH providing the key as identity file for public key authentication. ]$ ssh -i key.pem cloudadm@\u003cCLUSTER_PUBLIC_IP\u003e Last login: Mon Nov 18 11:37:29 2019 from torito.i3m.upv.es [cloudadm@server ~]$ Both the frontend and the working node are configured by Ansible. This process usually takes some time. User can monitor the status of the cluster configuration using the is_cluster_ready command-line tool:\n]# is_cluster_ready Cluster is still configuring. The cluster is successfully configured when the command returns the following message:\n]# is_cluster_ready Cluster configured! node state enabled time stable (cpu,mem) used (cpu,mem) total ]$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------- wn1 off enabled 00h04'20\" 0,0 1,-1 wn2 off enabled 00h04'20\" 0,0 1,-1 Operate the EC3 cluster To force CLUES to spawn a new node of the cluster, please use the command:\n]$ clues poweron wn1 node wn1 powered on The configuration triggers the execution of several ansible processes to configure the node and may take some time. To monitor the configuration of the node, you can use the is_cluster_ready command.\nTo avoid that clues powers off the node in case of inactivities, once the node is configured, you can disable the node as it follows:\n]$ clues disable wn1 node wn1 successfully disabled Log files Cluster logs files are available in /var/tmp/.im/\u003ccluster_id\u003e IM log files are in /var/log/im/im.log CLUES log files are in /var/log/clues2/clues2.log ","categories":"","description":"Deploy a virtual cluster with EC3 using the portal.\n","excerpt":"Deploy a virtual cluster with EC3 using the portal.\n","ref":"/users/compute/orchestration/im/ec3/portal/","tags":"","title":"EC3 Portal"},{"body":"Resource Centres are free to use any Cloud Management Platform as long as they are able to integrate with the EGI Federation components. At the moment this compliance is supported for OpenStack open-source cloud platform.\nThe general minimum requirements are described below.\nRelevant parts of PROC09 Resource Centre Registration and Certification have been successfully completed\nHardware requirements greatly depend on your cloud infrastructure, EGI components in general do lightweight operations by interacting with your services APIs.\nImage sync requires enough space to store the community images into your local catalogue. The number and size of images which will be downloaded depends on the communities you plan to support. For the operations VO ops, a \u003c 4GB image is used. Servers need X.509 host certificates in order to authenticate to each other or to act as public endpoints in the EGI Federated Cloud context. For accounting purposes one IGTF-accredited X.509 certificate is needed per site, but the other public endpoints can use ordinary certificates (issued by Let’s Encrypt for example).\nFor the IGTF-accredited certificates, a list of national / regional Certificate Authorities is available at EUGridPMA Membership; it is expected that the Resource Centre will obtain the IGTF certificate from a close Certificate Authority For operational purposes, the ops VO needs to be supported by the Resource Centre as per Resource Centre OLA. Also at least one community VO that supports EGI users is expected to be supported by the Resource Centre (i.e. vo.access.egi.eu for piloting and prototyping FedCloud usage).\nThe public endpoints need to have proper firewall configuration (to allow inbound external access)\nA flavor (a preset configuration that defines the compute, memory, and storage capacity - min 8 GB - of an instance) needs to be made available by the site for monitoring purposes.\n","categories":"","description":"Requirements for integration","excerpt":"Requirements for integration","ref":"/providers/cloud-compute/requirements/","tags":"","title":"Requirements"},{"body":"Identity card Property Value Name Accounting (Repository and Portal) Description Tracking, in an APEL-based repository, and reporting, via a web portal, of the usage of EGI resources and services. URL https://accounting.egi.eu (portal) Support Email apel-admins \u003cat\u003e stfc.ac.uk Helpdesk Support Unit EGI Services and Service Components I__ Accounting Portal I__ APEL Client and Accounting Repository Configuration Database entries Repository Portal Suppliers UKRI (Repository), CESGA (Portal) Roadmap N/A Release notes N/A Source code APEL, Portal: N/A Issue tracker for developers APEL, Portal: Helpdesk Licence Apache 2.0 Privacy Notice Privacy Notice ","categories":"","description":"EGI Accounting identity card","excerpt":"EGI Accounting identity card","ref":"/internal/accounting/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Collaboration Tools Description Supporting Collaboration across the EGI Federation URL N/A Support Email it-support@egi.eu Helpdesk Support Unit EGI Services and Service Components I__ Collaboration Tools Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=983 Supplier EGI Foundation and CESNET Roadmap N/A Release notes N/A Source code N/A Issue tracker for developers N/A License Every component is having its own licence Privacy Policy Parent policy, components usually have their own policy ","categories":"","description":"Technical details of the Collaboration Tools","excerpt":"Technical details of the Collaboration Tools","ref":"/internal/collaboration-tools/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Configuration Database Description Central registry of the infrastructure topology URL https://goc.egi.eu Support Email gocdb-admin \u003cat\u003e mailman.egi.eu Helpdesk Support Unit EGI Services and Service Components I__ Configuration Database (GOCDB) Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=335 Supplier UKRI Roadmap N/A Release notes Release notes Source code https://github.com/GOCDB Issue tracker for developers https://github.com/GOCDB/gocdb/issues Licence Apache 2.0 Privacy Notice Privacy notice ","categories":"","description":"Configuration Database identity card","excerpt":"Configuration Database identity card","ref":"/internal/configuration-database/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Helpdesk Description Central helpdesk providing a single interface to EGI support URL https://helpdesk.egi.eu Support Email support at ggus.eu Helpdesk Support Unit EGI Services and Service Components I__ Helpdesk (GGUS) Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=247 Supplier KIT Roadmap N/A Release notes https://ggus.eu/index.php?mode=release_notes Source code Not available Issue tracker for developers N/A License BMC Software Inc. Privacy Policy https://ggus.eu/?mode=privacy ","categories":"","description":"Technical details of EGI Helpdesk","excerpt":"Technical details of EGI Helpdesk","ref":"/internal/helpdesk/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Messaging service Description Facilitate messages exchange between EGI services URL N/A Support Email argo-ggus-support at grnet.gr Helpdesk Support Unit EGI Services and Service Components I__ Messaging Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=429 Supplier GRNET, SRCE Roadmap Release notes https://github.com/ARGOeu/argo-messaging/releases Source code https://github.com/ARGOeu/argo-messaging Issue tracker for developers https://github.com/ARGOeu/argo-messaging/issues License Apache 2.0 Privacy Policy ","categories":"","description":"Technical details of Messaging Service","excerpt":"Technical details of Messaging Service","ref":"/internal/messaging/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name ARGO Monitoring Service Description Service Monitoring for Availability and Reliability URL https://argo.egi.eu/ Support Email argo-ggus-support at grnet.gr Helpdesk Support Unit EGI Services and Service Components I__ Monitoring (ARGO) Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=641 Supplier CNRS, GRNET, SRCE Roadmap Release notes Source code https://github.com/ARGOeu Issue tracker for developers https://github.com/ARGOeu License Apache 2.0 Privacy Policy https://argo.egi.eu/egi/Critical/policies ","categories":"","description":"Technical details of EGI Service Monitoring","excerpt":"Technical details of EGI Service Monitoring","ref":"/internal/monitoring/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Operation Portal Description Provision of VO management functions and other capabilities supporting the daily operations of EGI URL https://operations-portal.egi.eu/ Support Email cic-information at in2p3.fr Helpdesk Support Unit EGI Services and Service Components I__ Operations Portal Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=568 Supplier CNRS Roadmap Release notes Release notes Source code https://gitlab.in2p3.fr/opsportal/sf3 Issue tracker for developers https://gitlab.in2p3.fr/opsportal/sf3/-/issues License Apache 2 Privacy Policy https://operations-portal.egi.eu/home/a/aup ","categories":"","description":"Technical details of the Operation Portal","excerpt":"Technical details of the Operation Portal","ref":"/internal/operations-portal/service-information/","tags":"","title":"Service information"},{"body":"Identity card Property Value Name Security coordination Description Enhance local security for a safer global infrastructure URL https://csirt.egi.eu Support Email abuse at egi.eu Helpdesk Support Unit EGI Services and Service Components I__ Security Coordination I__ Security Monitoring Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=968 https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=1127 Supplier UKRI, FOM-Nikhef, CERN, CESNET, GRNET, IJS Roadmap N/A Release notes N/A Source code N/A Issue tracker for developers N/A License N/A Privacy Policy N/A ","categories":"","description":"Technical details of EGI Security Coordination","excerpt":"Technical details of EGI Security Coordination","ref":"/internal/security-coordination/service-information/","tags":"","title":"Service information"},{"body":"Sign up You need to sign up for an account for accessing the EGI services. As part of this process you will be assigned a personal unique EGI ID which will be then used across all EGI tools and services. Follow the instructions below to get started:\nGo to https://aai.egi.eu/signup. This will show you the identity provider discovery page: browse through the list of Identity Providers to find your Home Organisation, or, alternatively, type the name of your Home Organisation in the search box. Note that the names are localised based on the selected language.\nEnter your login credentials to authenticate yourself with your Home Organisation\nAfter successful authentication, you may be prompted by your Home Organisation to consent to the release of personal information to the EGI AAI Service Provider Proxy.\nAfter successful authentication, you will be redirected to the EGI account registration form. On the introductory page, click Begin to start the registration process.\nEGI requires some basic information from you, depending on the attributes released by your Identity Provider, you may need to provide the values of the missing attributes.\nOn the registration form, click Review Terms and Conditions (Acceptable Use Policy and Conditions of Use - EGI AUP)\nIf you agree to the Terms of Use, select the I Agree option.\nImportant You will not be able to agree to the terms until you review them. Finally, click Submit to submit your request.\nImportant You will not be able to submit your request until you agree to the terms. After submitting your request, Check-in will send you an email with a verification link. After you click that link, you’ll be taken to the request confirmation page.\nImportant If you do not find the email in your Inbox, please check your Spam or Junk folder for an email from “EGI AAI Notifications”. If you do find the email in these folders, mark the email as “safe” or “not spam” to ensure that you receive any future notifications about your EGI ID. After reviewing your request, click Confirm and re-authenticate yourself using the Identity Provider you selected before.\nNote: After your registration has been completed, you can manage your profile through the EGI Account Registry portal.\nViewing user profile information The profile includes all the information related to the user. This information can be categorised as follows:\nBasic profile Includes the basic information about your profile:\nName Identifiers Email addresses VO/Group membership and roles Includes information about the Virtual Organisations (VOs) and groups the user is member of and the roles assigned to the user within those VOs. Check the guide about VOs for more details.\nLinked identities Information about linked identities to your account. Check the guide for linking accounts for more information.\nNext steps Once your Check-in account is ready you can check how to link it with different identities or how to join an existing Virtual Organisation (VO).\n","categories":"","description":"Register an account with Check-in to get access to EGI services\n","excerpt":"Register an account with Check-in to get access to EGI services\n","ref":"/users/aai/check-in/signup/","tags":"","title":"Sign up for an EGI Account"},{"body":"An overview of the use cases and possible deployment scenarios of the EGI DataHub.\nBuild Data Spaces for Communities Users can access with their EGI Check-in credentials data available at one or more providers which are abstracting over heterogeneous backends.\nDifferent data access interfaces are available (Web, REST, POSIX, CDMI).\nIntegrate DataHub with EGI Notebooks EGI Notebooks has been integrated with DataHub in order to easily access and store files from the JupyterLab interface.\nOn the central EGI Notebooks instance, a predefined space open to all users and VO specific spaces are available.\nMake your data FAIR Users can assign metadata to files, create shares and mint DOIs.\nThe EGI DataHub offers an OAI-PMH interface which enables standard discoverability and access to datasets.\nSmart caching Site A hosts data and computing resources Site B hosts only data Site X uses data from A and B without pre-staging Pre-staging can also be done using APIs Data is accessed locally via POSIX with FUSE ","categories":"","description":"Use-cases for EGI DataHub\n","excerpt":"Use-cases for EGI DataHub\n","ref":"/users/data/management/datahub/usecases/","tags":"","title":"DataHub Use-Cases"},{"body":"OpenStack providers of the EGI Cloud Compute service offer native OpenStack features via native APIs integrated with EGI Check-in accounts.\nThe extensive OpenStack user documentation includes details on every OpenStack project most providers offer access to:\nKeystone, for identity Nova, for VM management Glance, for VM image management Cinder, for block storage Swift, for object storage Neutron, for network management Horizon, as a web dashboard The Horizon Web-dashboard of the OpenStack providers can be accessed using your EGI Check-in credentials directly. See the Getting Started guide for more information. The rest of this guide will focus on CLI/API access.\nInstallation The OpenStack client is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure.\nLinux / Mac Windows Installation of the OpenStack client can be done using:\npip install openstackclient As there are non-pure Python packages needed for installation, the Microsoft C++ Build Tools is a prerequisite, please make sure it’s installed with the following options selected:\nC++ CMake tools for Windows C++ ATL for latest v142 build tools (x86 \u0026 x64) Testing tools core features - Build Tools Windows 10 SDK (\u003clatest\u003e) In case you prefer to use non-Microsoft alternatives for building non-pure packages, please see here.\nInstallation of the OpenStack client can be done using:\npip install openstackclient Add IGTF CA to python's CA store:\ncat /etc/grid-security/certificates/*.pem \u003e\u003e $(python -m requests.certs) Authentication Check the documentation at the authentication and authorisation section on how to get the right credentials for accessing the providers.\nOpenStack token for other clients Most OpenStack clients allow authentication with tokens, so you can easily use them with EGI Cloud providers just doing a first step for obtaining the token. With the OpenStack client you can use the following command to set the OS_TOKEN variable with the needed token:\n$ OS_TOKEN=$(openstack --os-auth-type v3oidcaccesstoken \\ --os-protocol openid --os-identity-provider egi.eu \\ --os-auth-url \u003ckeystone url\u003e \\ --os-access-token \u003cyour access token\u003e \\ --os-project-id \u003cyour project id\u003e token issue -c id -f value) You can easily obtain an OpenStack token with the FedCloud client:\nfedcloud openstack --site \u003cNAME_OF_THE_SITE\u003e --vo \u003cNAME_OF_VO\u003e token issue -c id -f value Useful commands with OpenStack CLI Usage of the OpenStack client is described in detail here.\nPlease refer to the nova documentation for a complete guide on the VM management features of OpenStack. We list in the sections below some useful commands for the EGI Cloud.\nRegistering an existing ssh key It’s possible to register an ssh key that can later be used as the default ssh key for the default user of the VM (via the --key-name argument to openstack server create):\nopenstack keypair create --public-key ~/.ssh/id_rsa.pub mykey Creating a VM openstack flavor list FLAVOR=\u003cFLAVOR_NAME\u003e openstack image list IMAGE_ID=\u003cIMAGE_ID\u003e openstack network list # Pick FedCloud network NETWORK_ID=\u003cNETWORK_ID\u003e openstack security group list openstack server create --flavor $FLAVOR --image $IMAGE_ID \\ --nic net-id=$NETWORK_ID --security-group default \\ --key-name mykey oneprovider # Creating a floating IP openstack floating ip create \u003cNETWORK_NAME\u003e # Assigning floating IP to server openstack server add floating ip \u003cSERVER_ID\u003e \u003cIP\u003e # Removing floating IP from server openstack server show \u003cSERVER_ID\u003e # Deleting server openstack server remove floating ip \u003cSERVER_ID\u003e \u003cIP\u003e openstack server delete \u003cSERVER_ID\u003e # Deleting floating IP openstack floating ip delete \u003cIP\u003e OpenStack: launch an instance on the provider network OpenStack: Managing IP addresses Using cloud-init openstack server create --flavor m3.medium \\ --image d0a89aa8-9644-408d-a023-4dcc1148ca01 \\ --user-data userdata.txt --key-name My_Key server01.example.com OpenStack: providing user data (cloud-init) cloudinit documentation Shell script data as user data #!/bin/sh adduser --disabled-password --gecos \"\" clouduser cloud-config data as user data #cloud-config hostname: mynode fqdn: mynode.example.com manage_etc_hosts: true Official cloud-config examples Cloud-init example Creating a snapshot image from running VM You can create a new image from a snapshot of an existing VM that will allow you to easily recover a previous version of your VM or to use it as a template to clone a given VM.\nopenstack server image create \u003cyour VM\u003e --name \u003cname of the snapshot\u003e Once the snapshot is ready openstack image show \u003cname of the snapshot\u003e will give your the details you can use it as any other image at the provider:\nopenstack server create --flavor \u003cflavor\u003e \\ --image \u003cname of the snapshot\u003e \\ \u003cname of the new VM\u003e You can override files in the snapshot if needed, e.g. changing the SSH keys:\nopenstack server create --flavor \u003cflavor\u003e \\ --image \u003cname of the snapshot\u003e \\ --file /home/ubuntu/.ssh/authorized_keys=my_new_keys \\ \u003cname of the new VM\u003e Terraform Terraform supports EGI Cloud OpenStack providers by using valid access tokens for Keystone. For using this, just configure your provider as usual in Terraform, but do not include user/password information. Instead, use the FedCloud client client to configure environment variables as follows:\n# export OS_AUTH_URL and OS_PROJECT_ID with $ eval \"$(fedcloud site show-project-id --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e)\" # now get a valid token $ export OS_TOKEN=$(fedcloud openstack --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e \\ token issue -c id -f value) Here is a sample main.tf configuration file for Terraform:\nterraform { required_providers { openstack = { source = \"terraform-provider-openstack/openstack\" } } } # Create a server resource \"openstack_compute_instance_v2\" \"vm\" { name = \"testvm\" image_id = \"...\" flavor_id = \"...\" security_groups = [\"default\"] } Initialize Terraform with:\n$ terraform init Now check the deployment plan:\n$ terraform plan If you are happy with the plan, perform the deployment with:\n$ terraform apply For more information on how to use Terraform with OpenStack please check the OpenStack provider documentation.\nlibcloud Apache libcloud supports OpenStack and EGI authentication mechanisms by setting the ex_force_auth_version to 3.x_oidc_access_token or 2.0_voms respectively. Check the libcloud docs on connecting to OpenStack for details. See below two code samples for using them\nOpenID Connect import requests from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver refresh_data = { 'client_id': '\u003cyour client_id\u003e', 'client_secret': '\u003cyour client_secret\u003e', 'grant_type': 'refresh_token', 'refresh_token': '\u003cyour refresh_token\u003e', 'scope': 'openid email profile', } r = requests.post(\"https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token\", auth=(client_id, client_secret), data=refresh_data) access_token = r.json()['access_token'] OpenStack = get_driver(Provider.OPENSTACK) # first parameter is the identity provider: \"egi.eu\" # Second parameter is the access_token # The protocol 'openid' is specified in ex_tenant_name # and tenant/project cannot be selected :( driver = OpenStack('egi.eu', access_token, ex_tenant_name='openid', ex_force_auth_url='https://keystone_url:5000', ex_force_auth_version='3.x_oidc_access_token') VOMS from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver OpenStack = get_driver(Provider.OPENSTACK) # assume your proxy is available at /tmp/x509up_u1000 # you can obtain a proxy with the voms-proxy-init command # no need for username driver = OpenStack(None, '/tmp/x509up_u1000', ex_tenant_name='EGI_FCTF', ex_force_auth_url='https://sbgcloud.in2p3.fr:5000', ex_force_auth_version='2.0_voms') ","categories":"","description":"How to interact with the OpenStack providers APIs in the EGI Cloud\n","excerpt":"How to interact with the OpenStack providers APIs in the EGI Cloud\n","ref":"/users/compute/cloud-compute/openstack/","tags":"","title":"Using OpenStack Providers"},{"body":"According to AARC-G002 the information about the groups a user is a member of is commonly used by Service Providers in order to authorise user access to protected resources.The entity responsible for disseminating this information is the EGI Check-in AAI proxy and the format used is that of a URN namespace, called eduPersonEntitlement, that is uniformly interpreted across infrastructures.\nThe general form of the eduPersonEntitlement string is:\n\u003cNAMESPACE\u003e:group:\u003cVO\u003e[:\u003cGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e\nAs a result, an eduPersonEntitlement string informing the Service Provider that the user has the role Associate in the vo.example.eu VO (modelled as a COU) is:\nurn:mace:egi.eu:group:vo.example.eu:role=associate#aai.egi.eu\nEntitlement Construction For the case of the CO Person with a profile/canvas, like the one provided above, we expect to get entitlements for all the entries listed under the tab Role Attributes. Additionally, we will get entitlements for all the General Purpose(GP) Groups enlisted under the tab Groups. These GP Groups have no prefix, neither CO: nor CO:COU, and no postfix, neither active nor all.\nVO(COU) For each entry in the table Role Attributes, that is in status Active or Grace Period, we create one eduPersonEntitlement for each different Role and for the Affiliation. For example, the CO Person from above is affiliated as Member to the VO vo.example.eu and has been assigned the role of an Associate. This will generate two entitlements as:\nurn:mace:egi.eu:group:vo.example.eu:role=associate#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.eu:role=member#aai.egi.eu\nVO Groups (sub COUs) There are occasions where we need a VO to be organized in subgroups. For example vo.example.eu contains the sub-COU vo.example-sub.eu.\nThe CO Person is affiliated as member and with the Role of Support in the VO sub-group vo.example-sub.eu:\nIn such occasions the eduPersonEntitlement will have the following structure:\nurn:mace:egi.eu:group:vo.example.eu:vo.example-sub.eu:role=support#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.eu:vo.examples-sub.eu:role=member#aai.egi.eu\n","categories":"","description":"Expressing VO group membership and role information\n","excerpt":"Expressing VO group membership and role information\n","ref":"/users/aai/check-in/vos/expressing-vo-information/","tags":"","title":"VO Membership Information"},{"body":"Although the recommended way to access and use EGI Secrets Store is via the command-line interface, the service can also be accessed via its web interface.\nTip Tutorials about using the web interface are available on the Hashicorp Vault site. Using the web interface Follow the steps below to access the web interface of EGI Secrets Store:\nOpen https://secrets.egi.eu in your browser. Choose the OIDC authentication method in the pulldown menu, then click Sign in with OIDC provider. Login via EGI Check-in and authorize the Vault GUI. The main page will open, with a top folder secrets that contains a space for each user.\nBrowse secrets To browse and manage your secrets, click on Secrets in the top navigation bar, then click on folder secrets.\nEach user has a private secret space, or a “home directory”, under the root folder secrets, with the user’s EGI Check-in ID as the folder name. You can only create secrets in your own secret space.\nNote Your “home directory” is not created automatically, you have to create it the first time you login using the web interface. Create a secret with your EGI Check-in ID as the path. Tip You can find your EGI Check-in ID via the user registry or by clicking the down arrow next to the user icon in the top right corner. Once you identified your secret space, click on it to browse your secrets.\nCreate secrets To create your first secret, click on Secrets in the top navigation bar, then click on folder secrets, then click Create secret on the right.\nUse your EGI Check-in ID (e.g. e0b6...@egi.eu in the image below) as the path, followed by the name of your secret. Add at least one key to your secret, together with a value.\nClick Save and your “home directory” will be created together with your first secret.\nOnce you created your first secret, you can create the subsequent secrets without having to enter your Check-in ID in the path. Just navigate to your “home folder” and create a secret there.\n","categories":"","description":"The web interface of EGI Secrets Store\n","excerpt":"The web interface of EGI Secrets Store\n","ref":"/users/security/secrets-store/gui/","tags":"","title":"Secrets Store Web Interface"},{"body":"To access the web interface of the EGI Configuration Database (GOCDB), users can either:\nUse EGI Check-in with an institutional account, or Use an X.509 digital certificate installed in the internet browser, or the local machine’s certificate store. Users can access the system as soon as they are authenticated. However, they will only be able to update information based on their roles, and only once they will have registered a new user account.\nMore information about roles and associated permission is available in the Users and roles section of the documentation.\nApplications requesting a specific role have to be validated by parent roles or administrators. Once granted, users can access and/or modify relevant information, according to the roles granted to them.\nUsing institutional account via EGI Check-in In order to be able to access the Configuration Database with their institutional account, users need to:\nHave their Identity Provider (IdP) federated in EGI Check-in (via eduGAIN or directly). Have created an EGI Check-in account. Important In the case the user cannot use an IdP compliant with REFEDS R\u0026S and REFEDS Sirtfi, the user will have to request joining a specific group, by performing the steps below. Using a compliant IdP is the preferable solution.\nUser should ask to join the GOCDB user group. The access request will be managed by the EGI Operations team. Using an X.509 digital certificate To access the Configuration Database using a digital certificate, first obtain a certificate from one of the recognised EU-Grid-PMA Certification Authorities (CAs), then install it in your browser of choice (or import it into the certificate store of your local machine, on Windows).\nNote X.509 certificates do not support single or double quotes in the certificate’s Distinguished Name (DN). The DN below is rejected because of the single quote:\n/C=UK/O=STFC/OU=SomeOrgUnit/CN=David Mc'Donald\nThis is in accordance with RFC1778, which also disallows single quotes in all Relative Distinguished Name (RDN) components, and the OGF Certificate Authority Working Group (CAOPS) who strongly discourage any type of quote in a certificate DN as specified by their Grid Certificate Profile document.\nRegistering a new user account Being authenticated in one of the two ways described above is enough to have read-only access to all the public features of the EGI Configuration Database. If you need to edit data in and request roles, you will need to fill in the registration form.\nTo Register:\nGo to the EGI Configuration Database web portal In the left sidebar, look out for the User status panel click on the “Register” link fill in the form and validate Recovering access to an existing account Note If you were registered but are not recognised anymore (e.g. because your certificate DN changed), do not register again! Instead, follow the steps Lost access to your Configuration Database account section. ","categories":"","description":"Accessing the Configuration Database","excerpt":"Accessing the Configuration Database","ref":"/internal/configuration-database/access/","tags":"","title":"Access"},{"body":"Overview This page provides an introduction of connecting from a local computer to a cloud host via SSH. It provides general guidelines, SSH options, tips, and examples for setting up the OpenStack environment.\nSSH Keys The recommended method to access a cloud virtual machine is via ssh using SSH keys. You may inject your public key into the virtual machine, at deployment time, and use your private key to connect via ssh without any password.\nTip If you are using ssh keys in GitHub your public keys are available at: https://github.com/${github_username}.keys. i.e.: wget https://github.com/github_username.keys SSH username The username to use to connect to a virtual machine is dependent on the virtual machine image and is generally different in each operating system image:\nFor images available in the EGI AppDB Cloud Marketplace, you should be able to find the username in the image description. For official OS virtual machine images you can use the general OpenStack reference documentation on obtaining images. For custom virtual machine images you need to refer to your virtual machine image provider (i.e. it could be something specific like cloudadm). For virtual machines deployed with Infrastructure Manager the default username is cloudadm. It is also possible to change the username using cloud-init with a user-data configuration (i.e. see the cloud config examples) or inject some code to add additional users (i.e. with Ansible).\nLocal ssh key configuration The private ssh-key stored on your local computer is required to have restrictive file permissions. Depending on your local operative system you may need to run:\n$ chmod 600 ~/.ssh/id_rsa (with id_rsa being the name of the private key associated with the public key in use).\nUsername and password Warning Username and password access to cloud virtual machine images is usually disabled for security reasons and it is strongly suggested not to be used. In case you have no other option, and are conscious of the risks, in order to enable SSH password authentication, the destination virtual machine needs to have /etc/ssh/sshd_config configuration changed from PasswordAuthentication no to PasswordAuthentication yes.\nIf really needed, a custom image with PasswordAuthentication enabled can be used or that can be injected when the virtual machine is deployed.\nDepending on your deployment method it could be done with Ansible, Terraform, Salt, Puppet, Chef, cloud-init, or your own deployment tool if supported (i.e. the Infrastructure Manager and a custom TOSCA template).\nWarning If you enable PasswordAuthentication, be sure to generate a strong and unique password or passphrase for your account, otherwise you virtual machines will be compromised, and your access may be suspended. OpenStack networking The OpenStack environment needs to be populated with the necessary configurations and virtual hardware. To access the virtual machine from outside the OpenStack project you have to associate a floating IP to the virtual machine (which will provide a public IP to the virtual machine), you also have to open the necessary ports and add or edit the security groups, (more details on that in the specific section below).\nDepending on the default configuration of the OpenStack project in order to associate a floating IP to a virtual machine in a private network it may be necessary to set up a virtual router in OpenStack and attach it with an interface to the private network. This step is usually not required as the OpenStack router is usually pre-configured by the cloud provider.\nSecurity Groups Rules The Virtual Machine that you want to connect needs to have the SSH port (22) reachable by your local machine. For that, it is necessary that a specific Rule is set up in one of the Security Groups associated with the virtual machine. The rule has to open port 22 either to any IPs (with CIDR 0.0.0.0/0) or to a specific IP (or subnet) matching the IP of the local machine used to connect with the virtual machine.\nSites are often providing a default security group, that may already contain this rule. You can check this using openstack security group rule list default.\nPrivate IP vs public IP Virtual machines in OpenStack are configured in a private network (like in the subnet 192.168.0.0/24) but you can directly SSH-connect with them from the internet only using a Public IP, which has to be associated with a virtual machine in the private network.\nAccessing virtual machines in the private network In general, to reach all the virtual machines in a private network, only a single public IP is needed.\nThe virtual machine associated with a public IP is often referred to as a Bastion host, once you connect with the bastion host, you can connect with the other virtual machine in the same private network using the private IPs. Alternatively, it is also possible to set up a JumpHost configuration in your local ssh configuration to do that with a single command.\nExample: ssh configuration for Jump host $ cat ~/.ssh/config # Bastion Host bastion 193.1.1.2 User ubuntu Hostname 193.168.1.2 IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes # with ProxyJump Host private_vm HostName 192.168.1.2 ProxyJump bastion # old-style with ProxyCommand and additional settings Host private_vm 192.168.1.2 Hostname 192.168.1.2 ProxyCommand ssh -q -A bastion nc %h %p User ubuntu ServerAliveInterval 60 TCPKeepAlive yes ControlMaster auto ControlPath ~/.ssh/mux-%r@%h:%p ControlPersist 8h IdentityFile ~/.ssh/dev CheckHostIP=no StrictHostKeyChecking=no General considerations related to setting up the ssh configuration are valid also for the connection between hosts in the private network (i.e. the ssh destination host needs to have a public key in the ~/.ssh/known_hosts file of the destination user, matching the private key used for the connection).\nSSH connection practical example Network configuration of two virtual machines A and B :\nA private IP 192.168.1.2, public IP 193.168.1.2 B private IP 192.168.1.3 Connecting from a local machine to A # ssh VM_OS_username@PUBLIC_IP $ ssh centos@193.1.1.2 If the ssh local key is not the default ~/.ssh/id_rsa it needs to be specified with:\n# ssh -i /path_of_your_private_ssh_key VM_OS_username@PUBLIC_IP $ ssh -i ~/private_key centos@193.1.1.2 Connecting from a local machine to B # from your computer # connect to A $ ssh centos@193.1.1.2 # from the shell opened in 193.1.1.2 # connect from A to B $ ssh centos@192.168.1.3 Infrastructure Manager (IM) The Infrastructure Manager (IM) provides the SSH key that can be used to connect to the virtual machine in the VM info page of the IM-Dashboard.\nThe page shows the information related with the virtual machine: the IP, the username (usually cloudadm), and the SSH key.\nToken-based authentication If supported by your virtual machine, you can also use ssh-oidc which implements the authentication consuming under-the-hood tokens from a local demon installed on your local machine.\nMore details on that soon.\nThe Infrastructure Manager (IM) can Enable SSH OIDC access to the VM in virtual machines by selecting the related Optional Features.\n","categories":"","description":"Accessing virtual machines with SSH","excerpt":"Accessing virtual machines with SSH","ref":"/users/tutorials/adhoc/accessing-vm-with-ssh/","tags":"","title":"Accessing virtual machines with SSH"},{"body":"Access the EGI Helpdesk service Users can access the EGI Helpdesk service GGUS with an existing EGI Check-in account.\nWhen you access the service for the first time, you are automatically assigned a role with minimal privileges by default: in such a situation you can only create user tickets and can’t access tickets created by other users.\nIn order to create your first ticket, please use the green “+” button in the lower left part of your screen.\nAccess to the previous EGI Helpdesk The previous EGI Helpdesk is still available in read-only mode for historical reasons: users can access to the tickets that were closed before the migration to the new helpdesk. Either an X509 personal certificate or a federated identity are required for the access.\nRoles management Roles in GGUS define the permissions and capabilities each user has when interacting with tickets. Proper role assignment ensures that users have access to the appropriate tools and information they need for their specific tasks.\nNote: Currently the roles are managed within GGUS; we are planning to move the roles management to EGI Check-in.\nHow to check your roles You can check your assigned role by following these steps (not available for users with the default role):\nClick on your User Logo in the lower left corner of the screen. Navigate to Profile and select Roles. Role Hierarchy and Capabilities Standard Roles User Default role assigned upon registration. Permissions: Basic ticket submission interface (only group and site fields are available). Ticket submission only to TPM and NGIs. Visibility of only your own tickets. GGUS User Users who need read-only access to others’ tickets, but with limited editing rights. Permissions: Enhanced ticket submission interface (category, ticket area, priority, affected vo, notified groups). Ticket submission to TPM, NGIs, Second Level and Other Infrastructures. Read-only access to all tickets. (in development) Limited access to personal information such as names and email addresses. Common Supporters who manage and update tickets throughout the system. Permissions: Full read and write access to all tickets. Roles with Specific Capabilities These roles provide additional capabilities beyond the standard roles and are assigned based on specific needs. The prerequisite for obtaining any of these roles is having at least the Common role, which is usually sufficient for most users.\nGGUS Expert Advanced supporters who can submit tickets to the Third Level and Product Teams. VO Indicator of Membership in the Virtual Organization (VO). Enables VO specific features such as overviews and ticket areas. VO_team Permissions: Ability to submit Team tickets on behalf of a VO. Use Case: Users representing teams and submitting team-related tickets. TPM (Ticket Process Manager) Permissions: Additional section in the Overview showing tickets submitted to the TPM. Use Case: Users who are involved in the First Level Support activity. Alarm Permissions: Ability to create Alarm tickets. Use Case: Users who need to notify WLCG Tier-0 and Tier-1 administrators about serious problems of the site at any time, independent from usual office hours. Multisites Permissions: Ability to create tickets to many sites in one go. Use Case: EGI and WLCG Operations to conduct middleware related campaigns. Mini-Admins Permissions: Ability to grant particular roles, e.g. VO role to other users Use Case: Management of VO users in the helpdesk Contact If you have any issues with roles or general questions regarding the helpdesk, please submit a ticket to the group “Second Level › Services › EGI Services and Service Components › Helpdesk (GGUS)” (start typing “ggus” in the group field for quick access).\n","categories":"","description":"Registration and roles management","excerpt":"Registration and roles management","ref":"/internal/helpdesk/access-and-roles/","tags":"","title":"Access and roles"},{"body":"What is it? EGI Accounting tracks and reports usage of EGI services, offering insights and control over resource consumption. EGI Federation members can use it to account for the resource usage of their own services.\nEGI Accounting consists of two main components:\nThe Accounting Repository is where all accounting data is collected by a network of message brokers that transfer usage data from hosts and services. The Accounting Portal allows filtering and displaying resource usage information. Publishing Accounting HTCondor-CE Accounting. Storage Accounting. Note Documentation for the Accounting Repository is also available in the EGI Wiki, documentation for the Accounting Portal is also available in the EGI Wiki. ","categories":"","description":"Resource usage accounting for EGI services\n","excerpt":"Resource usage accounting for EGI services\n","ref":"/internal/accounting/","tags":"","title":"Accounting"},{"body":"There are two different processes handling the accounting integration:\ncASO, which connects to the OpenStack deployment to get the usage information, and, ssmsend, which sends that usage information to the central EGI accounting repository. They should be run by cron periodically, settings below run cASO every hour and ssmsend every six hours.\nUsing the VM Appliance cASO configuration is stored at /etc/caso/caso.conf. Most default values should be OK, but you must set:\nsite_name (line 12), with the name of your site as defined in GOCDB.\nprojects (line 20), with the list of projects you want to extract accounting from.\ncredentials to access the accounting data (lines 28-47, more options also available). Check the cASO documentation for the expected permissions of the user configured here.\nThe mapping from EGI VOs to your local projects /etc/caso/voms.json, following this format: :\n{ \"vo name\": { \"projects\": [ \"project A that accounts for the vo\", \"project B that accounts for the VO\" ] }, \"another vo\": { \"projects\": [\"project C that accounts for the VO\"] } } cASO will write records to /var/spool/apel from where ssmsend will take them.\nSSM configuration is available at /etc/apel. Defaults should be OK for most cases. The cron file uses /etc/grid-security for the CAs and the host certificate and private keys (/etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem).\nRunning the services Both caso and ssmsend are run via the root user crontab. For convenience there are two scripts /usr/local/bin/caso-extract.sh and /usr/local/bin/ssm-send.sh that run the docker container with the proper volumes.\n","categories":"","description":"Accounting integration\n","excerpt":"Accounting integration\n","ref":"/providers/cloud-compute/openstack/accounting/","tags":"","title":"Accounting"},{"body":"Authentication OpenID Connect (OIDC) is the main authentication protocol used on the EGI Cloud. It replaces the legacy VOMS-based authentication for all OpenStack providers.\nAuthentication to web based services (like the AppDB) will redirect you to the EGI Check-in authentication page. Just select your institution or social login and follow the regular authentication process.\nAccess to APIs or via command-line interfaces (CLI) requires the use of OAuth2.0 tokens and interaction with the OpenStack Keystone OS-FEDERATION API. The process for authentication is as follows:\nObtain a valid OAuth2.0 access token from Check-in. Access tokens are short-lived credentials that can be obtained by recognised Check-in clients once a user has been authenticated. Interchange the Check-in access token for a valid unscoped Keystone token. Discover available projects from Keystone using the unscoped token. Use the unscoped Keystone token to get a scoped token for a valid project. Scoped tokens will allow the user to perform operations on the provider. Authorisation Cloud Compute service is accessed through Virtual Organisations (VOs). Users that are members of a VO will have access to the providers supporting that VO: they will be able to manage VMs, block storage and object storage available to the VO. Resources (VMs and storage) are shared across all members of the VO, please do not interfere with the VMs of other users if you are not entitled to do so (specially do not delete them).\nSome users roles have special consideration in VOs:\nUsers with VO Manager, VO Deputy or VO Expert Role have extra privileges in the AppDbB for managing the Virtual Appliances to be available at every provider. Check the Virtual Machine Image Management documentation for more information. Pilot VO The vo.access.egi.eu Virtual Organisation serves as a test ground for users to try the Cloud Compute service and to prototype and validate applications. It can be used for up to 6 month by any new user.\nWarning After the 6-month long membership in the vo.access.egi.eu VO, you will need to move to a production VO, or establish a new VO. The resources are not guaranteed and may be removed without notice by providers. Back-up frequently to avoid losing your work! For joining this VO, please click on the enrollment URL using your EGI account.\nOther VOs Preexisting VOs of EGI can be also used on IaaS cloud providers. Consult with your VO manager or browse the existing VOs at the EGI Operations Portal.\nCheck-in and access tokens Access tokens can be obtained via several mechanisms, usually involving the use of a web server and a browser. Command-line clients/APIs without access to a browser or interactive prompt for user authentication can use refresh tokens. A refresh token is a special token that is used to generate additional access tokens. This allows you to have short-lived access tokens without having to collect credentials every single time one expires. You can request this token alongside the access and/or ID tokens as part of a user’s initial authentication flow.\nIf you need to obtain these kinds of tokens for using it in command-line tools or APIs, you can easily do so with the EGI Check-in Token Portal. You can access the EGI Check-in Token Portal and click on 'Authorise' to log in with your Check-in credentials to obtain:\na client ID (token-portal) a refresh token Refresh tokens Refresh tokens MUST be treated with care! This is a secret that can be used to impersonate you in the infrastructure. The life time of refresh tokens is up to one year! It is recommended not to store them in plain text. There are more secure alternatives for handling refresh tokens:\nFrom your personal computer: use oidc-agent It is a tool that manages your tokens locally in a secure way refresh tokens are even encrypted in RAM). fedcloud client executed inside EGI Notebooks. mytoken Securely stores refresh tokens on the mytoken-server. Users obtain mytokens, which can be used to obtain access tokens. mytokens are more secure, because they can carry capabilities and restrictions to fine tune their power for specific use cases. Details here: https://mytoken-docs.data.kit.edu Discovering projects in Keystone The access token will provide you access to a cloud provider, but you may have access to several different projects within that provider (a project can be considered equivalent to a VO allocation). In order to discover which projects are available you can do that using the Keystone API.\nYou can use the fedcloud client to simplify the discovery of projects.\n# Get a list of sites (also available in [AppDB](https://appdb.egi.eu)) $ fedcloud site list # Get list of projects that you are allowed to access # You can either specify the name of the account in your oidc-agent configuration # or directly a valid access token $ fedcloud endpoint projects --site=\u003cname of the site\u003e \\ [--oidc-agent-account \u003caccount name\u003e|--oidc-access-token \u003caccess token\u003e] # You can also use environment variables for the configuration $ export OIDC_ACCESS_TOKEN=\u003cyour access token\u003e $ fedcloud endpoint projects # or with oidc-agent $ export OIDC_AGENT_ACCOUNT=\u003caccount name\u003e $ fedcloud endpoint projects Using the OpenStack API Once you know which project to use, you can use your regular openstack cli commands for performing actual operations in the provider:\n$ fedcloud openstack image list --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e For third-party tools that can use token based authentication in OpenStack, use the following command:\n$ export OS_TOKEN=$(fedcloud openstack --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e \\ token issue -c id -f value) Using ssh-oidc ssh-oidc is a set of tools that allows ssh with OIDC.\n","categories":"","description":"Authentication and Authorisation in EGI Cloud\n","excerpt":"Authentication and Authorisation in EGI Cloud\n","ref":"/users/compute/cloud-compute/auth/","tags":"","title":"Authentication and Authorisation"},{"body":"EGI Foundation is having access to a GÉANT Trusted Certificate Service subscription.\nSectigo is the current Certificate Authority (CA) providing certificates to GÉANT TCS.\nThis can be used to issue IGTF-trust (AKA eScience certificates) and public-trust certificates, for the domains managed by EGI Foundation, like egi.eu.\nOperators of central services can request two types of certificates:\nhost certificates, with public-trust, and optionally IGTF trust. robot email certificates, to be used as client certificate, to authenticate using X509 as a service. In some specific cases, like for Cloud Compute providers not having access to an IGTF CA, it’s possible for them to request a robot certificate, as an IGTF certificate is required for sending accounting records.\nYou can contact scs-ra@egi.eu (or operations@egi.eu) if you need support.\nRequesting a new certificate Open a ticket to Collaboration Tools SU in EGI Helpdesk, providing:\nJustification of the request. Type of certificate (host or robot). FQDN of the service. Mailing list to be used as contact address that will receive renewal notifications. For host certificates: a Certificate Signing Request, or mentioning the desire to use the ACME protocol. An operator will follow with the request, and help you getting the certificate.\nRetrieving certificates Host certificates will be sent by email, you will receive a notification with links allowing to download it. For robot certificates, you will receive an invitation by email allowing to generate and retrieve it. Renewing certificates Certificates are usually valid for one year. Auto-renewal of certificates is enabled by default, 30 days before expiration, notifications will be sent to the contact address provided when requesting the certificate.\nThe notifications contain links allowing to renew the certificate.\nCreating a Certificate Signing Request In order to get a certificate, Service Providers may be requested to send a Certificate Signing Request (CSR).\nIn the CSR only the Common Name (CN) is important, it should be the (FQDN) of the service Most of other fields will be replaced by the CA while generating the certificate.\nIt can be done via different ways, some are documented below.\nUsing a web application DigiCert provides an online web application.\nUsing CloudFlare cfssl tool It can be done with the cfssl tool from CloudFlare.\n# Replace #FQDN# by the FQDN of the service $ cfssl genkey \u003c(echo '{\"hosts\":[\"#FQDN#\"],\"CN\"#FQDN#\",\"key\":{\"algo\":\"rsa\",\"size\":4096}}') | cfssljson -bare ##FQDN##.rsa Using OpenSSL It can be done using the following OpenSSL command (This will generate a password-protected key.\nYou will be asked for various questions, but the only important ones are the Common Name (CN) and Subject Alternative Names (SAN) (in case you want to request a certificates covering different FQDNs), as other values will be overwritten by the CA.\n$ openssl req -out CSR.csr -new -newkey rsa:4096 -keyout privateKey.key Adding the -nodes option will disable password protection for the key, beware if using it.\nUsing ACME protocol It is possible to automate the certificate request and renewal using the ACME protocol via certbot or similar tools.\nTwo things should be considered:\nit’s not yet possible to get eScience/IGTF-trusted certificates, it’s on the roadmap but without any firm ETA. it’s using a different intermediate CA from Sectigo, not the usual GEANT one used for TCS, but this shouldn’t have much impact for generic/non-eScience public services. The EGI SDIS team will have to create and register an ACME client ID for you.\nOnce you will have the credential it should be possible to request a certificate, using the standard certbot client, as documented below.\nRegistering the client and saving the credentials locally This will interactively register your certbot client. Even if the ACME credentials are shared, only a given client is able to manage the certificates it requested. Email address is used for urgent renewal and security notices. This is preferred way if you need to get notifications on certificate expiration and if you are doing some manual management or testing.\n$ sudo certbot register --no-eff-email \\ --server https://acme.sectigo.com/v2/GEANTOV \\ --eab-kid \u003cEAB_KID\u003e \\ --eab-hmac-key \u003cEAB_HMAC_KEY\u003e \\ --email \u003cCONTACT_EMAIL\u003e # Checking existing account $ sudo certbot show_account --server https://acme.sectigo.com/v2/GEANTOV # Unregistering an account # Beware: you won't any more be able to revoke certificate issued with the account $ sudo certbot unregister --server https://acme.sectigo.com/v2/GEANTOV Requesting a certificate $ sudo certbot certonly --standalone --non-interactive \\ --server https://acme.sectigo.com/v2/GEANTOV \\ --domain fakedomaindonotexist.egi.eu Revoking a certificate $ sudo certbot revoke \\ --server https://acme.sectigo.com/v2/OV \\ --cert-name fakedomaindonotexist.egi.eu Registering and requesting a certificate all at once This is useful when you don’t want interactive registration, like for one shot scripts. Email address is used for urgent renewal and security notices.\nApparently the notification to use this email is not visible in cert-manager, and the first option with explicit registration may be safer to get the notifications, if it’s a required feature.\n$ sudo certbot certonly --standalone --non-interactive --agree-tos \\ --server https://acme.sectigo.com/v2/GEANTOV \\ --eab-kid \u003cEAB_KID\u003e --eab-hmac-key \u003cEAB_HMAC_KEY\u003e \\ --rsa-key-size 4096 \\ --email \u003cCONTACT_EMAIL\u003e \\ --domain fakedomaindonotexist.egi.eu Other usual certbot options should also work. You may also be able to use other tools that can speak the ACME protocol, it should be standard.\n","categories":"","description":"Certificate provisioning","excerpt":"Certificate provisioning","ref":"/internal/collaboration-tools/certificates/","tags":"","title":"Certificates"},{"body":"","categories":"","description":"Integration with Check-in for IdPs and SPs","excerpt":"Integration with Check-in for IdPs and SPs","ref":"/providers/check-in/","tags":"","title":"Check-in"},{"body":" Overview The FTS3 service offers a command-line client to ease the interaction with the service.\nPrerequisites The client software is available for RHEL and derivatives.\nUsers from other distributions should refer to the RESTFul API section.\nInstallation The CLI can be installed from the EPEL repositories with the following package:\nyum install fts-rest-cli -y Commands This section describes some of the commands that can be issues via the FTS CLI. As per the API, in order to authenticate to the FTS REST server you need an X.509 User certificate or an EGI Check-in token, please refer to this section for more information.\nPlease note that when authenticating via EGI-Check-in only https/s3 transfers are supported.\nCheck the full documentation about the FTS CLI\nfts-rest-whoami This command can be used to check, as the name suggests, who are we for the server.\nUsage fts-rest-whoami [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example $ fts-rest-whoami --access-token $TOKEN -s https://fts-egi.cern.ch:8446/ User DN: 2dee939532f16e482748b6c25f6ebbf2cac57abd28ca98bee06a114393d14a89@egi.eu VO: aai.egi.eu VO id: 2b4ace55-1b2e-5bf3-837d-03e3b08777d9 Delegation id: bd9a59d81b2c37ab Base id: 01874efb-4735-4595-bc9c-591aef8240c9 fts-rest-transfer-submit This command can be used to submit new jobs to FTS3. It supports simple and bulk submissions. The bulk format is as follows:\n{ \"files\": [ { \"sources\": [ \"gsiftp://source.host/file\" ], \"destinations\": [ \"gsiftp://destination.host/file\" ], \"metadata\": \"file-metadata\", \"checksum\": \"ADLER32:1234\", \"filesize\": 1024 }, { \"sources\": [ \"gsiftp://source.host/file2\" ], \"destinations\": [ \"gsiftp://destination.host/file2\" ], \"metadata\": \"file2-metadata\", \"checksum\": \"ADLER32:4321\", \"filesize\": 2048, \"activity\": \"default\" } ] } Usage fts-rest-transfer-submit [options] SOURCE DESTINATION [CHECKSUM] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) -b/--blocking : Blocking mode. Wait until the operation completes. -i/--interval : Interval between two poll operations in blocking mode. -e/--expire : Expiration time of the delegation in minutes. --delegate-when-lifetime-lt : Delegate the proxy when the remote lifetime is less than this value (in minutes) -o/--overwrite : Overwrite files. -r/--reuse : Enable session reuse for the transfer job. --job-metadata : Transfer job metadata. --file-metadata : File metadata. --file-size : File size (in bytes) -g/--gparam : Gridftp parameters. -t/--dest-token : The destination space token or its description. -S/--source-token : The source space token or its description. -K/--compare-checksum : Deprecated: compare checksums between source and destination. -C/--checksum-mode : Compare checksums in source, target, both or none. --copy-pin-lifetime : Pin lifetime of the copy in seconds. --bring-online : Bring online timeout in seconds. --timeout : Transfer timeout in seconds. --fail-nearline : Fail the transfer is the file is nearline. --dry-run : Do not send anything, just print the json message. -f/--file : Name of configuration file --retry : Number of retries. If 0, the server default will be used. If negative, there will be no retries. -m/--multi-hop : Submit a multihop transfer. --cloud-credentials : Use cloud credentials for the job (i. E. Dropbox). --nostreams : Number of streams --ipv4 : Force ipv4 --ipv6 : Force ipv6 Example fts-rest-transfer-submit -s https://fts-egi.cern.ch:8446 \\ --access-token=$TOKEN \\ https://source.host/file https://destination.host/file Job successfully submitted. Job id: 83ef08a6-1ac7-11f0-ae40-fa163ebe1520 $ fts-rest-transfer-submit -s https://fts-egi.cern.ch:8446 \\ --access-token=$TOKEN -f test.json Job successfully submitted. Job id: 9a28d204-d568-11ea-9c80-02163e018681 fts-rest-transfer-status This command can be used to check the current status of a given job.\nUsage fts-rest-transfer-status [options] JOB_ID Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example fts-rest-transfer-status -s https://fts-cern.cern.ch:8446 --access-token=$TOKEN 83ef08a6-1ac7-11f0-ae40-fa163ebe1520 Request ID: 83ef08a6-1ac7-11f0-ae40-fa163ebe1520 Status: FAILED Client DN: 2dee939532f16e482748b6c25f6ebbf2cac57abd28ca98bee06a114393d14a89@egi.eu Reason: One or more files failed. Please have a look at the details for more information Submission time: 2025-04-16T13:34:26 Priority: 3 VO Name: aai.egi.eu fts-rest-transfer-cancel This command can be used to cancel a running job. It returns the final state of the cancelled job. Please, mind that if the job is already in a final state (FINISHEDDIRTY, FINISHED, FAILED), this command will return this state. You can additionally cancel only a subset appending a comma-separated list of file IDs.\nUsage fts-rest-transfer-cancel [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example fts-rest-transfer-cancel -s https://fts3-public.cern.ch:8446 \\ --access-token=$TOKEN 9a28d204-d568-11ea-9c80-02163e018681 CANCELED ","categories":"","description":"Clients for accessing EGI Data Transfer\n","excerpt":"Clients for accessing EGI Data Transfer\n","ref":"/users/data/management/data-transfer/clients/","tags":"","title":"Data Transfer Clients"},{"body":"EGI DataHub spaces can be accessed via web interface, the oneclient component or the API.\nThe official documentation for oneclient is hosted on the Onedata homepage, and a specific tutorial on how to install and use it from a Virtual Machine is also available.\nUsing the web interface Using EGI Check-in allows you to connect with your institute’s credentials.\nOn this page it’s possible to have an overview of all the spaces and their supporting providers.\nOn this capture, the information about the spaces supported by a specific provider is displayed.\nThe data space can be managed (i.e. uploading/downloading/managing files and metadata, managing space access) using the web browser.\nGenerating tokens for using Oneclient or APIs Important In order to be able to access your spaces using oneclient or the API, it is required to generate an access token. Tokens have to be generated from the EGI DataHub (Onezone) interface.\nThe access tokens can be created and managed using the EGI DataHub web interface.\nEnvironment variables The sections below assume you have defined the following variables in your environment:\nONECLIENT_ACCESS_TOKEN: access token allowing to access all the spaces ONECLIENT_PROVIDER_HOST: name or IP of the Oneprovider the client should connect to. Installing and testing Oneclient in a docker container Important In order to be able to use FUSE, the container should run in privileged mode. A quick and simple solution for testing is to install the client on demand in a container for a supported Operating System flavor (mainly various CentOS and Ubuntu releases).\n$ docker run -it --privileged centos:7 /bin/bash root@81dbd7e84438 /]# curl -sS https://get.onedata.org/oneclient.sh | bash # (...) Complete! Installation has been completed successfully. Run 'oneclient --help' for usage info. root@81dbd7e84438 /]# export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e root@81dbd7e84438 /]# export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu root@81dbd7e84438 /]# mkdir /tmp/space root@81dbd7e84438 /]# oneclient /tmp/space root@81dbd7e84438 /]# ls /tmp/space Here the data is mounted in /tmp/space, creating a file into it will push it to the Oneprovider and it will be accessible in the web interface and from other providers supporting the space.\nFor a real production usage it's preferable to use the Oneclient container as a source for a volume mounted into another container.\nTesting Oneclient in a Oneclient docker container with NFS or samba Docker containers for the Oneclient are available, the existing versions can be seen on the Oneclient docker hub.\nIt’s possible to use the most recent version by specifying the latest tag. We also recommend using the same version as shown on the Onezone and Oneprovider pages.\n$ export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e $ export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu $ docker run -it --privileged \\ -e ONECLIENT_ACCESS_TOKEN=$ONECLIENT_ACCESS_TOKEN \\ -e ONECLIENT_PROVIDER_HOST=$ONECLIENT_PROVIDER_HOST \\ onedata/oneclient:21.02.2 Connecting to provider 'plg-cyfronet-01.datahub.egi.eu:443' using session ID: '4138963898952098752'... Getting configuration... Oneclient has been successfully mounted in '/mnt/oneclient' Now the client will run in the background and the data will be available through samba/CIFS or NFS protocols:\n# Identifying the IP of the container $ docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" $(docker ps -ql) 172.17.0.2 So the data can be accessed at\nsmb://172.17.0.2/onedata nfs://172.17.0.2/onedata Testing Oneclient in a Oneclient docker container with local file access Another solution is to mount a local directory as a volume in the container, allowing to access both the working directory as well as the Onedata spaces, thus allowing to easily exchange files between a local directory and a Onedata space.\nIn order to do this we will open a bash shell in the container then we will mount manually the Onedata spaces.\n$ export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e $ export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu $ docker run -it --privileged \\ -e ONECLIENT_ACCESS_TOKEN=$ONECLIENT_ACCESS_TOKEN \\ -e ONECLIENT_PROVIDER_HOST=$ONECLIENT_PROVIDER_HOST \\ -v $PWD:/mnt/src --entrypoint bash onedata/oneclient:21.02.2 root@aca612a84fb4:/tmp# oneclient /mnt/oneclient Connecting to provider 'plg-cyfronet-01.datahub.egi.eu:443' using session ID: '1641165171427694510'... Getting configuration... Oneclient has been successfully mounted in '/mnt/oneclient'. root@aca612a84fb4:/tmp# ls /mnt/oneclient (...) root@aca612a84fb4:/tmp# ls /mnt/src (...) Now it's possible to use the following mount points:\n/mnt/oneclient: the Onedata spaces /mnt/src: the local directory (any absolute path could have been used instead of $PWD that points to the working directory) Testing Oneclient in a Virtual Machine The following variables have to be exported:\nONECLIENT_ACCESS_TOKEN: access token allowing to access all the spaces. ONECLIENT_PROVIDER_HOST: name or IP of the Oneprovider the client should connect to. $ curl -sS https://get.onedata.org/oneclient.sh | bash $ export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e $ export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu $ mkdir /tmp/space $ oneclient /tmp/space Testing Oneclient in a Vagrant box It's possible to quickly test Oneclient using Vagrant.\n$ vagrant init ubuntu/xenial64 $ vagrant up $ vagrant ssh $ curl -sS https://get.onedata.org/oneclient.sh | bash $ export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e $ export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu $ mkdir /tmp/space $ oneclient /tmp/space ","categories":"","description":"Clients for accessing data EGI DataHub\n","excerpt":"Clients for accessing data EGI DataHub\n","ref":"/users/data/management/datahub/clients/","tags":"","title":"DataHub Clients"},{"body":"What is it? Elastic Cloud Compute Cluster (EC3) is a tool to create elastic virtual clusters on top of Infrastructure-as-a-Service (IaaS) providers.\nBeing based on Infrastructure Manager, EC3 supports the same wide choices of backends, either public (such as Amazon Web Services, Google Cloud or Microsoft Azure) or on-premises (such as OpenStack). EC3 can provision clusters running TORQUE, SLURM, HTCondor, Apache Mesos, Nomad, Kubernetes and others, which will be automatically resized to fit the load (e.g. number of jobs at the batch system).\nNote EC3 was presented in one of the EGI Webinars. Please see more details on the Indico page and check out the video recording on YouTube. The following section of the documentation will guide you on how to:\nDeploy a simple EC3 elastic cluster on top of the IaaS providers of the EGI Cloud, either using the web interface or the command-line interface, and Run pre-configured scientific applications in the EC3 elastic cluster. ","categories":"","description":"Working with Elastic Cloud Compute Clusters in the EGI Cloud\n","excerpt":"Working with Elastic Cloud Compute Clusters in the EGI Cloud\n","ref":"/users/compute/orchestration/im/ec3/","tags":"","title":"Elastic Cloud Compute Clusters"},{"body":"The EGI Cloud Container Compute service allows you to run container-based applications on the providers of the EGI Federated Cloud. There are two main ways of executing containers:\nUsing docker (or a similar container runtime) on a VM, so you can just interact directly with the container runtime to run your applications. This fits simpler applications that can easily fit on one node and are composed by a small number of containers.\nUsing a container orchestration platform, e.g. kubernetes on a set of VMs to manage the applications in an automated way for you. This is usually suited for more complex applications that spawn several nodes and are composed of several containers that need to cooperate to deliver the expected functionality.\nFollow the guides below to learn more about them.\nThe EGI Cloud Container Compute service has been presented in the EGI Webinars.\nJanuary 2024: Introduction to the new generation EGI container execution platform. See slides and recording.\nApril 2021: Managing Singularity, Docker and udocker containers, Kubernetes clusters in the EGI Cloud. See slides and recording.\n","categories":"","description":"Run containers on the EGI Cloud\n","excerpt":"Run containers on the EGI Cloud\n","ref":"/users/compute/cloud-container-compute/","tags":"","title":"Cloud Container Compute"},{"body":"What is it? The EGI Collaboration Tools are a set of online services maintained by the EGI Foundation and meant to support collaboration across the EGI Federation, supporting its daily activities (sharing information and documents, organising meetings, communicating in task or project specific groups…).\nNote Component-specific documentation is maintained by the upstream software providers. What are the components? EGI Single Sign On (EGI SSO) The EGI SSO is a central authentication and authorisation service allowing to use a single username and password to access various Collaboration Tools.\nGroup owners can manage group members, and send invitations to new users. Groups are used in Collaboration Tools services for authorisation.\nEGI SSO is now being deprecated in favour of EGI Check-in, providing a federation authentication and authorisation service.\nEGI.eu domain Domain entries can be created for the service as long as they are relevant for the EGI infrastructure. All EGI central services are usually available under the egi.eu domain.\nRequests needs to be discussed with the EGI Operations team.\nCertificate for EGI.eu domain EGI Foundation is having access to a GÉANT Trusted Certificate Service subscription.\nThis can be used to issue IGTF-trust (AKA eScience certificates) and public-trust certificates, for the domains managed by EGI Foundation, like egi.eu.\nPlease refer to documentation on requesting a certificate.\nEGI.eu site Information about the EGI Federation activities is published on the public site.\nWiki A Confluence-based “Wiki” space is available, allowing the EGI members to work collaboratively on documenting many aspects like Service Management, Polices and Procedures or activities from Boards and Groups.\nSeparate instances for EGI and EOSC are operated by EGI Foundation.\nIssues management Jira EGI internal request tracking system, meant to support the Service Management System and projects.\nEGI Foundation is operating dedicated instances for EGI and EOSC.\nRequest Tracking (RT) EGI internal request tracking system, being replaced for many activities by Jira.\nMailing lists Many mailing lists exists to cover specific areas of collaboration. New ones can be crated on request as documented below.\nMembership in mailing lists is usually determined by membership in EGI SSO groups. Only group owners can manage the group membership.\nNote that canonical addresses are list-name@mailman.egi.eu, not list-name@egi.eu.\nSee the dedicated page on mailing list for more information like how to request the creation of a new mailing list.\nDocument server EGI Documentation database, hosting many published document relevant to the EGI Federation.\nDocuments have metadata describing them and can be versioned. EGI SSO groups are used for restricting access to documents.\nAgenda management via Indico Event planner, used for various conferences and meetings, powered by the Indico product.\nAll users with an SSO account can use it.\nIndico also has its own local accounts, which can be used by people who do not want an EGI SSO account, but want to register to some meeting in Indico.\nContact EGI Collaboration Tools support can be contacted via:\nEGI Helpdesk Support Unit “Collaboration Tools” EGI IT Support ","categories":"","description":"Fostering Collaboration across the EGI Federation\n","excerpt":"Fostering Collaboration across the EGI Federation\n","ref":"/internal/collaboration-tools/","tags":"","title":"Collaboration Tools"},{"body":"You can find here documentation covering getting started with IM Command-Line Interface (CLI) on EGI Cloud Compute sites. Full documentation at IM CLI documentation\nGetting started Install with pip You only have to call the install command of the pip tool with the IM-client package.\n$ pip install IM-client IM-Client Docker image The IM Client has an official Docker container image available on Docker Hub that can be used instead of installing the CLI. You can use it by typing:\n$ docker run --rm -ti -v \"$PWD:/tmp/im\" grycap/im-client \\ -r https://server.com:8800 -a /tmp/im/auth.dat list Configuration To avoid typing the parameters in all the client calls, the user can define a config file im_client.cfg in the current directory or a file .im_client.cfg in their home directory. In the config file the user can specify the following parameters:\n[im_client] restapi_url=https://im.egi.eu/im auth_file=auth.dat Authentication data file An authentication file must be created to access the IM service. It must have one line per authentication element. It must have at least one line with the authentication data for the IM service and another one for the Cloud provider(s) the user want to access. Each line of the file is composed by pairs of key and value separated by semicolon, and refers to a single credential. The key and value should be separated by =, that is an equals sign preceded and followed by one whitespace at least. The following lines show the credentials needed to access an EGI Cloud Compute site:\ntype = InfrastructureManager; token = egi_aai_token_value id = egi; type = EGI; host = CESGA; vo = vo.access.egi.eu; token = egi_aai_token_value The value of egi_aai_token_value must be replaced with a valid EGI Check-in access token. Users of EGI Check-in can get all the information needed to obtain access tokens, by visiting EGI Check-in Token Portal.\noidc-agent can be used to get a valid access token:\nid = im; type = InfrastructureManager; token = command(oidc-token OIDC_ACCOUNT) id = egi; type = EGI; host = SCAI; vo = vo.access.egi.eu; token = command(oidc-token OIDC_ACCOUNT) Create and Manage an infrastructure To create a virtual infrastructure you have to describe a file documenting the required resources. IM supports its native language RADL and the OASIS TOSCA Simple Profile in YAML Version 1.0. You can find some examples in the IM GitHub repository.\nFor example, we can use RADL to define a simple VM with 1 CPU, 1 GB of RAM using the EGI Ubuntu 20.04 image.\nnetwork public (outbound = 'yes') system node ( cpu.count\u003e=2 and memory.size\u003e=4g and net_interface.0.connection = 'public' and disk.0.os.name='linux' and disk.0.image.url = 'appdb://SCAI/egi.ubuntu.20.04?vo.access.egi.eu' ) configure wn ( @begin --- - tasks: - debug: msg=\"Configured!\" @end ) deploy node 1 IM also supports TOSCA. For example this is an equivalent TOSCA document to deploy a single VM:\ntosca_definitions_version: tosca_simple_yaml_1_0 imports: - indigo_custom_types: https://raw.githubusercontent.com/indigo-dc/tosca-types/master/custom_types.yaml topology_template: node_templates: simple_node: type: tosca.nodes.indigo.Compute capabilities: endpoint: properties: network_name: PUBLIC host: properties: num_cpus: 2 mem_size: 4 GB os: properties: image: appdb://SCAI/egi.ubuntu.20.04?vo.access.egi.eu outputs: node_ip: value: { get_attribute: [ simple_node, public_address, 0 ] } node_creds: value: { get_attribute: [ simple_node, endpoint, credential, 0 ] } Then we can call the create operation of the IM client tool using a RADL or a TOSCA YAML file:\n$ im_client.py create infra.radl Secure connection with: https://im.egi.eu/im Infrastructure successfully created with ID: 457273ea-85e4-11ec-aa81-faaae69bc911 Then we can call get the current state of infrastructure using the getstate operation of the IM client tool:\n$ im_client.py getstate 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://im.egi.eu/im The infrastructure is in state: pending VM ID: 0 is in state: pending. The valid VM and infrastructure states are the following:\npending, launched, but still in initialization stage; configuring, created successfully and running, but still in the configuration stage; configured, running and contextualized; unconfigured, running but not correctly contextualized; stopped, stopped or suspended; off, shutdown or removed from the infrastructure; failed, an error happened during the launching; or unknown, unable to obtain the status. deleting, in the deletion process. Once the configuration step has started we can get the output of the Ansible process using the getcontmsg operation:\n$ im_client.py getcontmsg 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://im.egi.eu/im Connected with: http://localhost:8800 Msg Contextualizator: 2022-02-11 10:40:12.768523: Copying YAML, hosts and inventory files. VM 0: Contextualization agent output processed successfullyGenerate and copy the ssh key Sleeping 0 secs. Launch task: wait_all_ssh ... Once the VM is booted we can access it via SSH using the ssh operation:\n$ im_client.py ssh 457273ea-85e4-11ec-aa81-faaae69bc911 When using a TOSCA yaml document to create the infrastructure, we can get the TOSCA output values with the getoutputs operation:\n$ im_client.py getoutputs 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://im.egi.eu/im The infrastructure outputs: node_ip = 8.8.8.8 node_creds = {'token': '...', 'user': 'cloudadm', 'token_type': 'private_key'} Once we no more need the Infrastructure, we can destroy it using the destroy operation:\n$ im_client.py destroy 457273ea-85e4-11ec-aa81-faaae69bc911 Secure connection with: https://im.egi.eu/im Infrastructure successfully destroyed ","categories":"","description":"Getting started with the command-line interface of Infrastructure Manager\n","excerpt":"Getting started with the command-line interface of Infrastructure …","ref":"/users/compute/orchestration/im/cli/","tags":"","title":"Infrastructure Manager Command-Line Interface"},{"body":"You can find here documentation on how to deploy a sample SLURM cluster, which you can then adapt to create other kind of clusters easily.\nGetting started We will use docker for running EC3, direct installation is also possible and described at EC3 documentation. First get the docker image:\ndocker pull grycap/ec3 And check that you can run a simple command:\n$ docker run grycap/ec3 list name state IP nodes ------------------------ For convenience we will create a directory to keep the deployment configuration and status together.\nmkdir ec3-test cd ec3-test You can list the available templates for clusters with the templates command:\n$ docker run grycap/ec3 templates name kind summary ---------------------------------------------------------------------------------------------------------------------- blcr component Tool for checkpointing applications. [...] sge main Install and configure a cluster SGE from distribution repositories. slurm main Install and configure a cluster using the grycap.slurm ansible role. slurm-repo main Install and configure a cluster SLURM from distribution repositories. [...] We will use the slurm template for configuring our cluster.\nSite details EC3 needs some information on the site that you are planning to use to deploy your cluster:\nauthentication information network identifiers VM image identifiers We will use the FedCloud client to discover the required information. Set your credentials as shown in the authentication guide and create the authorisation files needed for ec3 (in this case for CESGA with VO vo.access.egi.eu):\nfedcloud ec3 init --site CESGA --vo vo.access.egi.eu This will generate an auth.dat file with your credentials to access the site and a templates/refresh.radl with a refresh token to allow long running clusters to be managed on the infrastructure.\nLet’s get also some needed site information. Start getting the available networks, we will need both a public and private network:\n$ fedcloud openstack --site CESGA --vo vo.access.egi.eu network list +--------------------------------------+----------------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+----------------------+--------------------------------------+ | 12ffb5f7-3e54-433f-86d0-8ffa43b52025 | net-vo.access.egi.eu | 754342b1-92df-4fc8-9499-2ee8b668141f | | 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 | public00 | 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 | +--------------------------------------+----------------------+--------------------------------------+ Then, get the list of images available:\n$ fedcloud openstack --site CESGA --vo vo.access.egi.eu image list +--------------------------------------+----------------------------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+----------------------------------------------------------+--------+ | 9d22cb3b-e6a3-4467-801a-a68214338b22 | Image for CernVM3 [CentOS/6/QEMU-KVM] | active | | b03e8720-d88a-4939-b93d-23289b8eed6c | Image for CernVM4 [CentOS/7/QEMU-KVM] | active | | 06cd7256-de22-4e9d-a1cf-997b5c44d938 | Image for Chipster [Ubuntu/16.04/KVM] | active | | 8c4e2568-67a2-441a-b696-ac1b7c60de9c | Image for EGI CentOS 7 [CentOS/7/VirtualBox] | active | | abc5ebd8-f65c-4af9-8e54-a89e3b5587a3 | Image for EGI Docker [Ubuntu/18.04/VirtualBox] | active | | 22064e93-6af9-430b-94a1-e96473c5a72b | Image for EGI Ubuntu 16.04 LTS [Ubuntu/16.04/VirtualBox] | active | | d5040b3e-ef33-4959-bb88-5505e229f579 | Image for EGI Ubuntu 18.04 [Ubuntu/18.04/VirtualBox] | active | | 79fadf3f-6092-4bb7-ab78-9a322f0aad33 | cirros | active | +--------------------------------------+----------------------------------------------------------+--------+ For our example we will use the EGI CentOS 7 with id 8c4e2568-67a2-441a-b696-ac1b7c60de9c.\nFinally, with all this information we can create the images template for EC3 that specifies the site configuration for our deployment. Save this file as templates/centos.radl:\ndescription centos-cesga ( kind = 'images' and short = 'centos7-cesga' and content = 'CentOS7 image at CESGA' ) network public ( provider_id = 'public00' and outports contains '22/tcp' ) network private (provider_id = 'net-vo.access.egi.eu') system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' and disk.0.os.credentials.username = 'centos' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and ec3_max_instances = 5 and # maximum number of worker nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' and disk.0.os.credentials.username = 'centos' ) Note we have used public00 as public network and opened port 22 to allow ssh access. The private network uses net-vo.access.egi.eu. We have two kind of VMs in almost every deployment: the front, that runs the batch system, and the wn, that will execute the jobs. In our example, both will use the same CentOS image, which is specified with the disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' line: ost refers to OpenStack, fedcloud-osservices.egi.cesga.es is the hostname of the URL obtained above with fedcloud endpoint list and 8c4e2568-67a2-441a-b696-ac1b7c60de9c is the ID of the image in OpenStack. The size of the VM is also specified.\nLaunch cluster We are ready now to deploy the cluster with ec3 (this can take several minutes):\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 launch mycluster slurm ubuntu refresh -a auth.dat Creating infrastructure Infrastructure successfully created with ID: 74fde7be-edee-11ea-a6e9-da8b0bbd7c73 Front-end configured with IP 193.144.46.234 Transferring infrastructure Front-end ready! We can check the status of the deployment:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 list name state IP nodes ---------------------------------------------- mycluster configured 193.144.46.234 0 And once configured, ssh to the front node. The is_cluster_ready command will report whether the cluster is fully configured or not:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 ssh mycluster Warning: Permanently added '193.144.46.234' (ECDSA) to the list of known hosts. Last login: Thu Sep 3 14:07:46 2020 from torito.i3m.upv.es $ bash cloudadm@slurmserver:~$ is_cluster_ready Cluster configured! cloudadm@slurmserver:~$ EC3 will deploy CLUES, a cluster management system that will power on/off nodes as needed depending on the load. Initially all the nodes will be off:\nnode state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn2 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn3 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn4 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn5 off enabled 00h03'55\" 0,0.0 1,1073741824.0 SLURM will also report nodes as down:\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 5 down* wn[1-5] As we submit a first job, some nodes will be powered on to meet the request. You can also start them manually with clues poweron.\ncloudadm@slurmserver:~$ srun hostname srun: Required node not available (down, drained or reserved) srun: job 2 queued and waiting for resources srun: job 2 has been allocated resources wn1.localdomain cloudadm@slurmserver:~$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1 idle enabled 00h07'45\" 0,0.0 1,1073741824.0 wn2 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn3 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn4 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn5 off enabled 00h52'25\" 0,0.0 1,1073741824.0 cloudadm@slurmserver:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 4 down* wn[2-5] debug* up infinite 1 idle wn1 Destroying the cluster Once you are done with the cluster and want to destroy it, you can use the destroy command. If your cluster was created more than one hour ago, your credentials to access the site will be expired and need to refreshed first with fedcloud ec3 refresh:\n$ fedcloud ec3 refresh # refresh your auth.dat $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 list # list your clusters name state IP nodes ---------------------------------------------- mycluster configured 193.144.46.234 0 $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy mycluster -a auth.dat -y WARNING: you are going to delete the infrastructure (including frontend and nodes). Success deleting the cluster! ","categories":"","description":"Getting started with Elastic Cloud Compute Cluster on EGI Cloud with the Command Line Interface\n","excerpt":"Getting started with Elastic Cloud Compute Cluster on EGI Cloud with …","ref":"/users/compute/orchestration/im/ec3/cli/","tags":"","title":"EC3 Command-Line Interface"},{"body":"Multiple tools and command-line interfaces are available for accessing and working with EGI Secrets Store.\nThe FedCloud client is strongly recommended as it is tightly integrated with the service, it works out of the box without additional configuration, has a simple syntax and supports advanced features such as client-side encrypted secrets.\nThe Hashicorp Vault client can be used to access advanced Vault features not exposed via other tools.\nPrerequisites To access the EGI Secrets Store service from the command-line you need a valid EGI Check-in access token. Get it either from the EGI Check-in Token Portal, or from the oidc-agent (see here for details), then set it to an environment variable:\nMac / Linux Powershell Windows $ export OIDC_ACCESS_TOKEN=\u003ctoken\u003e \u003e $env:OIDC_ACCESS_TOKEN=\"\u003ctoken\u003e\" \u003e set OIDC_ACCESS_TOKEN=\u003ctoken\u003e Access via FedCloud client The FedCloud client is integrated with the EGI Secrets Store service, so that users can access the service immediately with simple commands. Below is a quick start to using the service.\nBasic usage Let’s assume you want to create a secret my_app_secrets and store passwords for two services in it:\nMac / Linux Powershell Windows $ fedcloud secret put my_app_secrets \\ mysql_password=123456 \\ redis_password=abcdef \u003e fedcloud secret put my_app_secrets ` mysql_password=123456 ` redis_password=abcdef \u003e fedcloud secret put my_app_secrets ^ mysql_password=123456 ^ redis_password=abcdef Listing all your secrets is very simple:\n$ fedcloud secret list my_app_secrets Using any of the keys (actually, their values) is straightforward too:\nMac / Linux Powershell Windows # Get all keys and their values $ fedcloud secret get my_app_secrets key value -------------- ------- redis_password abcdef mysql_password 123456 # Get the value of a specific key from a secret $ fedcloud secret get my_app_secrets mysql_password 123456 # Using the value of a specific key from a secret $ export ADMIN_PASSWORD=$(fedcloud secret get my_app_secrets admin_password) $ echo $ADMIN_PASSWORD abcdef # Get all keys and their values \u003e fedcloud secret get my_app_secrets key value -------------- ------- redis_password abcdef mysql_password 123456 # Get the value of a specific key from a secret \u003e fedcloud secret get my_app_secrets mysql_password 123456 # Using the value of a specific key from a secret \u003e $env:ADMIN_PASSWORD=$(fedcloud secret get my_app_secrets admin_password) \u003e echo $env:ADMIN_PASSWORD abcdef :: Get all keys and their values \u003e fedcloud secret get my_app_secrets key value -------------- ------- redis_password abcdef mysql_password 123456 :: Get the value of a specific key from a secret \u003e fedcloud secret get my_app_secrets mysql_password 123456 :: Using the value of a specific key from a secret \u003e set ADMIN_PASSWORD=(fedcloud secret get my_app_secrets admin_password) \u003e echo %ADMIN_PASSWORD% abcdef Deleting a secret is easy, but it is irreversible:\n$ fedcloud secret delete my_app_secrets Secret values from small text files If the secret value starts with @ the content of the file with that name is used as the value of the key. The following example creates a secret named certificate for storing the certificate file and its keyfile:\n$ fedcloud secret put certificate cert=@hostcert.pem key=@hostkey.pem You can get the certificate and its keyfile (e.g. when you want to use it on a virtual machine) as follows:\n$ fedcloud secret get certificate cert \u003e hostcert.pem $ fedcloud secret get certificate key \u003e hostkey.pem Note The size of the secret object (all the values in it) is limited to 512kB, which is sufficient for storing tokens, certificates, configuration files and so on. For larger datasets, please use encrypted cloud storage. Secret values from small binary files It is recommended to store secret values as text for compatibility and ease of manipulation. However, the FedCloud client supports storing small binary files as secret values by encoding/decoding the binary data to ASCII via base64.\nAdd option --binary-file or -b when using binary files as the secret value:\n$ fedcloud secret put secret-image image=@secret-image.png -b $ fedcloud secret get secret-image image -b \u003e received-image.png Modifying existing secrets Secret values in secret objects cannot be edited individually. However, you can get the contents of existing secret objects, change them locally, and put the new contents back, overwriting the old secret. Some examples are shown below.\nImportant You will probably want to take care of securely disposing of the local temporary file(s) involved in the update. Omitted here for brevity. Mac / Linux Powershell Windows For a secret named certificate containing two keys named cert and key:\n$ fedcloud secret get certificate key value -------------- ------- cert ... key ... To add new values to an existing secret:\n$ fedcloud secret get certificate -f json \u003e certificate.json $ fedcloud secret put certificate @certificate.json \\ another_cert=@usercert.pem \\ another_key=@userkey.pem To delete values from an existing secret:\n$ fedcloud secret get certificate -f json | jq 'del (.another_cert, .another_key)' \\ \u003e certificate.json $ fedcloud secret put certificate @certificate.json To replace existing values in an existing secret:\n$ fedcloud secret get certificate -f json \u003e certificate.json $ fedcloud secret put certificate @certificate.json \\ cert=@new_hostcert.pem \\ key=@new_hostkey.pem For a secret named certificate containing two keys named cert and key:\n\u003e fedcloud secret get certificate key value -------------- ------- cert ... key ... To add new values to an existing secret:\n\u003e fedcloud secret get certificate -f json \u003e certificate.json \u003e fedcloud secret put certificate @certificate.json ` another_cert=@usercert.pem ` another_key=@userkey.pem To delete values from an existing secret:\n$ fedcloud secret get certificate -f json | ` jq 'del (.another_cert, .another_key)' ` \u003e certificate.json $ fedcloud secret put certificate @certificate.json To replace existing values in an existing secret:\n$ fedcloud secret get certificate -f json \u003e certificate.json $ fedcloud secret put certificate @certificate.json ` cert=@new_hostcert.pem ` key=@new_hostkey.pem For a secret named certificate containing two keys named cert and key:\n\u003e fedcloud secret get certificate key value -------------- ------- cert ... key ... To add new values to an existing secret:\n\u003e fedcloud secret get certificate -f json \u003e certificate.json \u003e fedcloud secret put certificate @certificate.json ^ another_cert=@usercert.pem ^ another_key=@userkey.pem To delete values from an existing secret:\n\u003e fedcloud secret get certificate -f json ^ | jq 'del (.another_cert, .another_key)' ^ \u003e certificate.json \u003e fedcloud secret put certificate @certificate.json To replace existing values in an existing secret:\n\u003e fedcloud secret get certificate -f json \u003e certificate.json \u003e fedcloud secret put certificate @certificate.json ^ cert=@new_hostcert.pem ^ key=@new_hostkey.pem Export and import secrets FedCloud client can output secrets in YAML or JSON format for further processing when using option --output-format or -f:\n$ fedcloud secret get my_secrets -f json $ fedcloud secret get my_secrets -f yaml \u003e my_secrets.yaml The YAML or JSON files created by FedCloud client can be imported back into EGI Secrets Store by using a single key and adding @ before its name, followed by the filename to load the value(s) from:\n$fedcloud secret put my_other_secrets @my_other_secrets.yaml As the YAML format is a superset of JSON, it is expected by default, unless the filename has .json extension. Try to export your secrets to both formats to see the differences between them.\nImporting secret objects from files in text format with key=value lines is not supported, as the format is error-prone, especially for multi-line secret values or values with special characters. You can replace = with : for converting simple text files to YAML files.\nTip Do not forget that in YAML files a blank space is required after the : separating keys and values. Note There is a difference between cert=@hostcert.pem for reading the content of the file hostcert.pem as the value for the key cert, and @my_secrets.yaml for reading the whole secret object with all key:value pairs from the YAML file. Client-side encrypted secrets EGI Secret Store encrypts secret objects both in transit and at rest. For highly-sensitive secrets, you can opt to also encrypt your secret values on the client-side, before storing them in EGI Secrets Store.\nThe client-side encryption is done on the fly by the FedCloud client if an encryption key (passphrase) is provided via option --encrypt-key or -e:\n$ fedcloud secret put sensitive data=@sensitive-data.txt -e password Decryption is done in a similar way, just by providing the passphrase via option --decrypt-key or -d. The secret values will be decrypted on the fly if the passphrase is correct:\n$ fedcloud secret get sensitive data -d password Note Only secret values are encrypted, not the (names of) the keys. Verifying what is actually stored in a secret can be done without providing the passphrase:\n$ fedcloud secret get sensitive data gAAAAAB............................... Note The encryption/decryption is done with the standard Python cryptography library. Security experts are invited to review the code (available on GitHub) and provide feedback and suggestions for improvements. Reading data from standard inputs Reading data from stdin may help in creating shorter scripts and to avoid storing secrets in intermediate files on disk, for security reasons. The symbol - in input parameters means the data will be read from the standard input in the same way as @ can be used to read from files. For example:\nTo read entire secret from standard input, which must to be in JSON or YAML format:\n$ echo '{\"mysql_pwd\":\"123\"}' | fedcloud secret put my_secrets - To read only a secret value from standard input:\n$ echo \"abcdef\" | fedcloud secret put my_secrets admin_password=- To copy a secret object export it to JSON, then import it as a new copy:\n$ fedcloud secret get my_secrets -f json | fedcloud secret put my_secret_copy - To add new values to an existing secret, without using intermediate files:\nMac / Linux Powershell Windows $ fedcloud secret get certificate -f json | fedcloud secret put certificate - \\ another_cert=@usercert.pem \\ another_key=@userkey.pem \u003e fedcloud secret get certificate -f json | ` fedcloud secret put certificate - ` another_cert=@usercert.pem ` another_key=@userkey.pem \u003e fedcloud secret get certificate -f json | ^ fedcloud secret put certificate - ^ another_cert=@usercert.pem ^ another_key=@userkey.pem Access via Vault client To access EGI Secrets Store using the Vault client, visit the Vault project’s site, download the correct version for your operating system, and install it.\nMac Ubuntu / Debian CentOS / RHEL Amazon Linux Fedora To install the Vault client:\n$ brew tap hashicorp/tap $ brew install hashicorp/tap/vault To install the Vault client:\n$ wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg $ echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list $ sudo apt update \u0026\u0026 sudo apt install vault To install the Vault client:\n$ sudo yum install -y yum-utils $ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo $ sudo yum -y install vault To install the Vault client:\n$ sudo yum install -y yum-utils shadow-utils $ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo $ sudo yum -y install vault To install the Vault client:\n$ sudo dnf install -y dnf-plugins-core $ sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo $ sudo dnf -y install vault To create/read secrets and to manage the EGI Secrets Store service, you need a Vault token. You can get one from your EGI Check-in access token.\nMac / Linux Powershell Windows To get a Vault token:\n$ export VAULT_ADDR=https://secrets.egi.eu $ export VAULT_TOKEN=$(vault write auth/jwt/login jwt=$OIDC_ACCESS_TOKEN | grep -Po 'token\\s+\\K[^\\s]+$') To get a Vault token:\n\u003e $env:VAULT_ADDR=\"https://secrets.egi.eu\" \u003e $env:VAULT_TOKEN=$(vault write auth/jwt/login jwt=$env:OIDC_ACCESS_TOKEN ` | Select-String -Pattern \"(?\u003c=token\\s+)[^\\s]+(?=$)\" ` | %{$_.Matches.value}) To get a Vault token:\n\u003e set VAULT_ADDR=https://secrets.egi.eu \u003e for /f \"delims=\" %a in ('vault write auth/jwt/login \"jwt=%OIDC_ACCESS_TOKEN%\" ^| findstr /r /c:\"token[ ][ ]*[^^ ]*\"') do @set VAULT_TOKEN=%a:token=% \u003e set VAULT_TOKEN=%VAULT_TOKEN: =% For convenience, add the path to your secret space to an environment variable:\nMac / Linux Powershell Windows $ export VAULT_HOME=/secrets/$(curl -X POST https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo -H \"Authorization: Bearer $OIDC_ACCESS_TOKEN\" | jq -r .voperson_id) \u003e $env:VAULT_HOME=/secrets/$(curl -X POST https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo -H \"Authorization: Bearer $env:OIDC_ACCESS_TOKEN\" | jq -r .voperson_id) \u003e for /f \"delims=\" %a in ('curl -X POST https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo -H \"Authorization: Bearer %OIDC_ACCESS_TOKEN%\" ^| jq -r .voperson_id') do set VAULT_HOME=\"/secrets/%a\" List secrets After setting the environment variables VAULT_TOKEN and VAULT_HOME, you can list the secrets in your personal secret space:\nMac / Linux Powershell Windows To list your secrets:\n$ vault list $VAULT_HOME To list your secrets:\n\u003e vault list $env:VAULT_HOME To list your secrets:\n\u003e vault list %VAULT_HOME% Create secret To create a new secret named test in your personal secret space, containing a key my-key having the value test value use the command below.\nImportant The command below will replace, without warning, a previous secret named test, including all its keys, with a new one that will only include the keys you provide as part of the command. Mac / Linux Powershell Windows To create or update a secret:\n$ vault write $VAULT_HOME/test \"my-key=test value\" db-pass=1234 To create or update a secret:\n\u003e vault write $env:VAULT_HOME/test \"my-key=test value\" db-pass=1234 To create or update a secret:\n\u003e vault write %VAULT_HOME%/test \"my-key=test value\" db-pass=1234 Tip Use quotes if the key or value includes white spaces. You can include in the same quote both the key and the value. Note You can add as many key to a secret as needed, but keep in mind that they will always be handled (read, written, or deleted) atomically. Read secret To read a secret named test from your personal secret space use the commands below.\nMac / Linux Powershell Windows To read all keys from a secret:\n$ vault read $VAULT_HOME/test Key Value --- ----- refresh_interval 768h my-key test value db-pass 1234 To read specific keys from a secret:\n$ vault read -field=\"my-key\" $VAULT_HOME/test test value To read all keys from a secret:\n\u003e vault read $env:VAULT_HOME/test Key Value --- ----- refresh_interval 768h my-key test value db-pass 1234 To read specific keys from a secret:\n$ vault read -field=\"my-key\" $env:VAULT_HOME/test test value To read all keys from a secret:\n\u003e vault read %VAULT_HOME%/test Key Value --- ----- refresh_interval 768h my-key test value db-pass 1234 To read specific keys from a secret:\n$ vault read -field=\"my-key\" %VAULT_HOME%/test test value Tip Alternative commands kv put and kv get exist for vault write and vault read. Be sure to check out the full list of Vault commands for more information. ","categories":"","description":"Command-line interfaces for accessing EGI Secrets Store\n","excerpt":"Command-line interfaces for accessing EGI Secrets Store\n","ref":"/users/security/secrets-store/cli/","tags":"","title":"Secrets Store Command-Line Interface"},{"body":"The EGI documentation is a static site built using Hugo from Markdown source files. Hugo uses goldmark to parse and render markdown, which is compliant with CommonMark and GitHub Flavored Markdown (also based on CommonMark).\nThe EGI documentation is organized into sections and pages. Read below to understand when to use each of these, and how to create new sections and add new pages to a section.\nImportant Avoid using pages in the documentation for now, create a distinct section for every page, because currently there is no way to automatically validate links in pages. Sections Sections are those pages that can have subpages. They always appear in bold in the left-side navigation tree: Sections are also pages, meaning that selecting them in the navigation tree will show their content.\nCreating sections To create a new section, create a folder under /content/\u003clanguage\u003e/, add a file named _index.md to it, then author content for it as described below.\nNote Sections immediately under /content/\u003clanguage\u003e/ can show up in the top-level navigation bar. See below for details on how to control this. Pages Pages are Markdown files that contain the documentation about a specific topic. They hold the content for a section (in which case are named _index.md and the containing folder is the section), or a stand-alone page that is immediately under a section (the containing folder is the section).\nThis is how stand-alone pages appear in the left-side navigation tree: Creating pages Creating a documentation page is done by creating a Markdown file (with .md extension) under the relevant section (in the section’s folder).\nNote When authoring pages please observe and adhere to the Style Guide. Page metadata Each page needs some metadata that controls where the page appears and how its content will be rendered. The beginning of the Markdown file contains a front matter in YAML, holding the metadata of the page:\n--- title: \"Style\" linkTitle: \"Style Guide\" description: \"Style guide for EGI documentation\" type: docs weight: 30 --- The parameter weight controls the order of the pages on the same level in the left-side navigation tree. Pages will appear in ascending order of their weight.\nThe above metadata produces the following result (relevant elements highlighted): Add page to top navigation bar Pages can be added to the top navigation bar by using a front matter like this:\n--- title: \"About\" description: \"About EGI Documentation\" type: docs menu: main: weight: 50 --- Pages will be added to the top navigation bar in ascending order of their menu weight, from left to right.\nIf you also want to add an icon to the entry in the top navigation bar:\n--- title: \"About\" description: \"About EGI Documentation\" type: docs menu: main: weight: 50 pre: \u003ci class='fa fa-info'\u003e\u003c/i\u003e --- Embedding images (or other content) Hugo organizes content for each page into a subfolder with the same name as the page’s filename. This allows authors to easily keep track of the resources used by each page.\nLet’s assume we have a section named About with a subpage Concepts, using the following hierarchy of files:\n/documentation /content /en /about _index.md concepts.md map.png /concepts metadata.png Embedding an image in the page of the section (the file _index.md) can be done with:\n![Image title](map.png) Embedding an image in a subpage can be done by editing the Markdown file of the page (concepts.md in our case):\n![Image title](metadata.png) ![Image title](../map.png) Note For subpages, the (relative) path to the image already includes a folder with the same name as the file’s base name. Linking to pages You can include hyperlinks in the documentation that will link to any documentation page, or to external resources.\nAssuming we have the same section named About with a subpage Concepts as used above, and the file _index.md contains this:\nThis is a link to a [page](concepts) This is another link to the same [page](concepts.md) while the page concepts.md contains this:\nThis is link to a [section](../) This another link to a [section](../../about) the links in these pages exemplify how to link to a different page, be it a section or a stand-alone page.\nNote When linking to a stand-alone page, if you omit the file extension, .md will be assumed. Linking to documentation headings You can also include hyperlinks in the documentation that will link to any heading (aka chapter) from any documentation page.\nTo link to a heading in a documentation page, you need to point the hyperlink to the anchor created automatically by Markdown for the targeted heading.\nNote You can derive the anchor from the name of the target heading, by converting it to lowercase, removing non-alphanumeric characters, replacing spaces with dashes, and keeping exactly one dash separating each subsequent word pair in the anchor. Below you have an example of a page with sample headings and links to those headings:\nThis is a link to a [chapter](#some-chapter-title) This is a link to another [subchapter](#some-subchapter-title) ## Some chapter title ... ### Some subchapter title ... Assuming we have the same section named About with a subpage Concepts as used above, and the file _index.md contains this:\n## Section chapter This is a link to a [page chapter](concepts#page-chapter) This is another link to the same [page chapter](concepts.md#page-chapter) while the page concepts.md contains this:\n## Page chapter This s link to a [section chapter](../#section-chapter) This s link to a [section chapter](../../about#section-chapter) the links in these pages exemplify how to link to headings in a different page or section.\nGlossary The EGI Glossary is available on the EGI Glossary space.\n","categories":"","description":"Concepts used when writing documentation","excerpt":"Concepts used when writing documentation","ref":"/about/concepts/","tags":"","title":"Concepts"},{"body":"Overview The data management services of EGI comprises two groups of services:\nServices that provide data management capabilities to enhance the raw storage available in the EGI infrastructure Specialized services that offer advanced organisation of data during ongoing research projects, as an integrated environment with data management and digital lab notebook The EGI data management services offer both application programming interfaces (APIs) and command-line interfaces (CLIs) that are integrated with the advanced EGI services and platforms (such as development environments, machine learning, or cloud orchestrators), and can be accessed from most compute services.\nGeneric data management This higher-level data management service is available to researchers:\nEGI DataHub is a high-performance data management solution that offers unified data access across multiple types of underlying storage, allowing users to share, collaborate and easily perform computations on the stored data. Specialized data management The following specialized data management service is also available:\nEGI Data Transfer is a low-level service to move data from one Grid or Object storage to another. Use-cases for storing research data Depending on the type of the employed compute services and the use-cases being addressed, users might need to choose different data service to store, access, and manage data.\nUser Data storage Cloud user Block and Object storage HTC user Grid storage HPC user High-performance parallel file systems or Object storage The following sections offer detailed descriptions for each data management service.\n","categories":"","description":"Data management services in the EGI infrastructure\n","excerpt":"Data management services in the EGI infrastructure\n","ref":"/users/data/management/","tags":"","title":"Data Management"},{"body":"What is it? EGI DataHub is a high-performance data management solution that offers unified data access across globally distributed environments and multiple types of underlying storage. It allows researchers to share, collaborate and perform computations on the stored data easily.\nUsers can bring data close to their community or to the compute facilities they use, in order to exploit it efficiently. This is as simple as selecting which (subset of the) data should be available at which supporting provider.\nThe main features of DataHub are:\nDiscovery of data spaces via a central portal Policy based data access Replication of data across providers for resiliency and availability purposes Integration with EGI Check-in allows access using community credentials, including from other EGI services and components File catalog to track replication of data and manage logical and physical files EGI DataHub supports multiple access policies:\nUnauthenticated, open access Access after user registration or Access restricted to members of a Virtual Organization (VO) Data replication in EGI DataHub may take place either on-­demand or automatically. Replication uses a file catalogue to enable tracking of logical and physical copies of data.\nNote EGI DataHub is based on the Onedata technology. Motivations Putting up a (scalable) distributed data infrastructure needs specific expertise, resources and knowledge No easy way to discover and transfer data No easy way of making data (publicly) accessible without transferring it with a sharing service No easy way of combining multiple datasets from different data providers Users need to access data locally and from compute resources Components and concepts The following concepts (components) will help you understand how EGI DataHub works.\nSpace Virtual volume where users will organize their data. A space is supported by one or more Oneproviders that provide the actual storage resources.\nOneprovider Data management component deployed in the data centres, provisioning data and managing transfers. A Oneprovider is typically deployed at a site near the local storage resources, and can access local storage resources over multiple connectors (e.g. CEPH, POSIX). A default one is operated for EGI by CYFRONET.\nOnezone Central component for federating providers. It takes care of Authentication and Authorization and other management tasks (like space creation). EGI DataHub is a Onezone instance.\nEGI DataHub The central Onezone instance of the EGI Federation. Single Sign On (SSO) with all the connected storage providers (Oneprovider) is guaranteed through EGI Check-in\nOneclient Client application providing access to the spaces through a Linux FUSE mount point (local POSIX access), as if they were part of the local file system. Oneclient can be used from VMs, containers, desktops, etc.\nNote Web interfaces and APIs are also available. Highlighted features Using the EGI DataHub web interface it's possible to manage the space.\nUsing Oneclient it's possible to mount a space locally, and access it over a POSIX interface, using files as they were stored locally. The file's blocks are downloaded on demand.\nIn Onedata the file distribution is done on a block basis, blocks will be replicated on the fly, and it's possible to instrument the replication between Oneproviders.\nThree different formats of metadata can be attached to files: basic (key-value), JSON and RDF. The metadata can be managed using the Web interface and the APIs. It's also possible to create indexes and query them.\nIt's possible to view the popularity of a file and manage smart caching.\nHow to get access The EGI DataHub Web interface can be accessed by any user authenticated via EGI Check-in. Users authenticated have access to the PLAYGROUND space, with a 30 GB quota, where they can tests some of the available features. Users have also read-only access to the open-datasets space where some example datasets are stored.\nIn order to access via oneclient or API please check the related documentation.\nAdvanced users willing to install their own Oneprovider can check the dedicated installation instructions.\n","categories":"","description":"Discover, manage, and replicate data with EGI DataHub","excerpt":"Discover, manage, and replicate data with EGI DataHub","ref":"/users/data/management/datahub/","tags":"","title":"EGI DataHub"},{"body":"The EGI Docker VA is a ready-to-use Virtual Machine Image with docker and docker-compose pre-installed.\nYou can start that image as any other VA available from AppDB:\nGo to the EGI Docker image entry in AppDB.\nCheck the identifiers of the endpoint, image and flavor you want to use at the provider.\nUse a ssh key when, so you can log into the VM once it's instantiated.\nOnce up, just ssh in the VM and start using docker as usual.\nUsing docker from inside the VM You can log in with user ubuntu and your ssh key:\n$ ssh -i \u003cyourprivatekey\u003e ubuntu@\u003cyour VM ip\u003e Once in, you can run any docker command, e.g.:\n$ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b901d36b6f2f: Pull complete 0a6ba66e537a: Pull complete Digest: sha256:8be990ef2aeb16dbcb9271ddfe2610fa6658d13f6dfb8bc72074cc1ca36966a7 Status: Downloaded newer image for hello-world:latest Hello from Docker. This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker Hub account: https://hub.docker.com For more examples and ideas, visit: https://docs.docker.com/userguide/ Docker-compose can be also used to execute applications with more than one container running together, follow this documentation to learn more.\nKnown issues MTU and docker Depending on the cloud provider you may encounter unexpected network connectivity issues when working inside the docker container. If that is the case, please try reconfiguring the MTU for the docker daemon:\n# check current MTU value $ sudo docker network inspect bridge | awk '/mtu/ {print $2}' # the default 1500 value does not work properly # edit docker configuration $ sudo vi /etc/docker/daemon.json # ensure MTU value is 1376 { \"mtu\": 1376 } # then restart docker $ sudo systemctl restart docker We experienced this issue when trying to install a pip dependency using continuumio/miniconda3 container from docker hub.\n","categories":"","description":"Run containers on the EGI Cloud on a single VM with Docker\n","excerpt":"Run containers on the EGI Cloud on a single VM with Docker\n","ref":"/users/compute/cloud-container-compute/docker/","tags":"","title":"Docker VM"},{"body":"Introduction A ROD team’s duties can be split into three main areas: handling alarms and tickets, handling downtimes, and communicating urgent issues to the EGI Operations and CSIRT teams.\nHandling alarms and tickets The main responsibility of ROD is to deal with alarms and tickets issued for sites in the region. This includes making sure that the tickets are created and handled properly.\nThe ROD on duty is required to:\ncheck alarm notifications in the Dashboard at least twice a day; close alarms which are in the OK state; handle non-OK alarms less than 24 hours old (notify the site administrators according to your NGI’s procedures); create tickets for alarms older then 24 hours that are not in an OK state; escalate tickets to NGI Management/EGI Operations if necessary (in the Dashboard); monitor and update any GGUS tickets up to the solved status (preferably via the Dashboard); handle the final state of GGUS tickets not opened from the Dashboard by changing their status to verified. Putting a site in downtime for urgent matters ROD can place a site or a service endpoint (there can be multiple services running on a single host) in downtime in the GOCDB if it is either requested by the site, or if ROD sees an urgent need to do it.\nNote: This is actually optional; an NGI may decide on a different policy if the site admins are not happy with ROD setting downtimes for them. However, it should be considered mandatory in case of urgent security incidents.\nROD may also suspend a site, under exceptional circumstances, without going through all the steps of the escalation procedure. For example, if a security hazard occurs, ROD must suspend a site on the spot in the case of such an emergency. It is important to know that EGI Operations can also suspend a site in the case of an emergency, for example as a result of a security incident or lack of response.\nIn both scenarios, it is important that ROD communicates their actions to all involved parties.\nNotifying EGI Operations and EGI CSIRT about urgent matters ROD should create tickets to EGI Operations in the case of urgent matters. For security related issues, ROD should also notify the CSIRT duty contact.\nROD is also responsible for propagating actions from EGI Operations down to sites (this occurs rather infrequently, though).\n","categories":"","description":"Description of duties of regional operators","excerpt":"Description of duties of regional operators","ref":"/providers/rod/duties/","tags":"","title":"Duties"},{"body":"EGI SSO is a legacy central Identity Provider mainly intended to access the EGI Collaboration Tools.\nFor most services people should look into using federated authentication via EGI Check-in. Still, for some services not yet integrated with Check-in, an EGI SSO account should be used, in that case you can create an EGI SSO account.\nFeatures Central Identity Provider allowing to access EGI Collaboration Tools Group management Synchronisation of groups with other EGI Collaboration Tools Linking of X.509 certificate Distinguished Name (DN) to authenticate using a client certificate Linking an X.509 certificate to an EGI SSO account Open the EGI SSO user management page Select your new certificate to authenticate with it Complete the login process using your username and password Once logged, your new DN will be linked to your user account Updating the DN of a certificate linked to a given EGI SSO account On purpose a user cannot manually edit a certificate DN\nYou usually just need to link your new DN to your EGI SSO account, as documented above.\nYou may have to do this by in a private browser session to ensure that your previous certificate is used.\nOnce you have done this, you can request us to remove the former DN by opening an Helpdesk ticket to the Collaboration Tools Support Unit.\nLegacy DNs shouldn’t prevent you accessing any service with a newer certificate, as long as its DN is linked to your account.\n","categories":"","description":"EGI SSO usage","excerpt":"EGI SSO usage","ref":"/internal/collaboration-tools/sso/","tags":"","title":"EGI SSO"},{"body":"Welcome to the home page of Service Providers. This section provides documentation aimed at Service Providers across the EGI Federation.\nTip See the users section for user-centric documentation. Joining the EGI Infrastructure Any service provider can join the EGI Federation and in doing so benefit from being part of the major worldwide e-infrastructure that is EGI. From a technical point of view, the Federation may be joined by integrating your resources with federation services such as Check-in, the EGI Federation Authentication and Authorisation Infrastructure (AAI) service. This may be done at no cost, and resources can immediately benefit from EGI federation services.\nIn order to make a difference in helping to define the strategic direction of the EGI Federation and its federation services, countries or international organisations can join the EGI Council. More details may be found on the EGI site.\nProviders have different options to become members of the EGI Federation, and deliver high-quality services for advanced computing within the EGI Infrastructure. The options are documented below, linking to detailed instructions:\nJoining as a federated resource centre, becoming a provider (that we call Resource Centre) delivering one of the following services: Cloud Compute, HTC Compute, Cloud Container Compute, or Online Storage. More than 200 Resource Centres have been integrated in this way. Joining as a software provider, also called Technology Provider, which is providing middleware deployed on the federated resource centres. Joining as a new provider for an existing service in the Compute and Data Federation or the Platform services, such as DataHub, Notebooks, Training Infrastructure, and Workload Manager Joining as Core/Central service provider for services supporting all the other services of the EGI Infrastructure. Contributing a new service, enabling advanced computing in research and education, and not yet present in the EGI service portfolio. Tip See the operations manuals to get started with your EGI Operations duties. Integrating a service with EGI Check-in If you are interested in integrating your service with Check-in, head to the Check-in for service providers! If you are willing to connect your Identity Providers and allowing your users to access services via Check-in, look at the documentation for Check-in for Identity Providers! ","categories":"","description":"Documentation for service providers","excerpt":"Documentation for service providers","ref":"/providers/","tags":"","title":"Service Provider Guides"},{"body":"Individual Job Records and Messages Header APEL-individual-job-message: v0.3\nThe header only appears once at the top of each message (that is once at the top of each file). It defines the type of record and the schema version.\nRecord Fields The table shows the equivalent field in the CAR, under the container element urf:UsageRecord. If not specified, it refers to the text value of urf:Key, where the element is a direct child of urf:UsageRecord.\nKey Value Description Mandatory CAR equivalent (if different) Site String GOCDB sitename Yes SubmitHost String The CE-ID (see example) Yes MachineName String LRMS hostname Queue String Batch system queue LocalJobId String Batch System Job ID Yes urf:JobIdentity/urf:LocalJobId LocalUserId String Local username urf:UserIdentity/urf:LocalUserId GlobalUserName String User’s X509 DN urf:UserIdentity/urf:GlobalUserName FQAN String User’s VOMS attributes urf:UserIdentity/urf:GroupAttribute[@type=“FQAN”] WallDuration int Wallclock time for the job (seconds) Yes CAR has ISO 8601 time duration CpuDuration int CPU time for the job (seconds) Yes CAR has ISO 8601 time duration Processors int Number of processors urf:Processors[@metric=“max”] NodeCount int Number of nodes StartTime int Start time of the job (epoch time) Yes CAR has ISO 8601 datetime EndTime int Stop time of the job (epoch time) Yes CAR has ISO 8601 datetime InfrastructureDescription String \u003caccounting client\u003e-\u003cCE type\u003e-\u003cbatch system type\u003e eg. “APEL-CREAM-PBS” InfrastructureType String grid OR local MemoryReal int Memory consumed by job (kbytes) urf:Memory[@metric=“max” and @type=“Physical” and @storageUnit=“KB”] MemoryVirtual int Virtual memory consumed by job (kbytes) urf:Memory[@metric=“max” and @type=“Shared” and @storageUnit=“KB”] ServiceLevelType String Si2k OR HEPSPEC Yes urf:ServiceLevel[@type] ServiceLevel double Value of either HepSpec06 or SpecInt2000 Yes urf:ServiceLevel Message End of record %%\nExample Message APEL-individual-job-message: v0.2 Site: ExampleSite SubmitHost: host.domain:port/queue LocalJobId: 11111111 LocalUserId: User1 GlobalUserName: /C=whatever/D=someDN FQAN: /voname/Role=NULL/Capability=NULL WallDuration: 234256 CpuDuration: 2345 Processors: 2 NodeCount: 2 StartTime: 1234567890 EndTime: 1234567899 MemoryReal: 1000 MemoryVirtual: 2000 ServiceLevelType: Si2k ServiceLevel: 1000 %% ...another job record... %% ... %% Notes If GlobalUserName or UserFQAN is not published, the value for these fields on the server will be set to ‘None’.\nJobs are assumed to be grid jobs. To specify local jobs, use:\nInfrastructureType: local SubmitHostType: LRMS SubmitHost: LRMS-hostname The Group value specified for local jobs must be different to equivalent grid jobs, or you will not be able to differentiate them in the accounting portal. Suggestion:\nGroup: ExampleVO - grid job Group: local-ExampleVO - local job Changes since version 0.2 InfrastructureType field (optional) InfrastructureDescription field (optional) SubmitHostType field (optional) Changes from version 0.1 to version 0.2 LocalJobID has changed to LocalJobId LocalUserID has changed to LocalUserId UserFQAN has changed to FQAN ScalingFactorUnit has changed to ServiceLevelType The possible values of ScalingFactorType have changed from [“SpecInt2000”, “HepSpec06”, “custom”] to [“Si2k”], [“HEPSPEC”] ScalingFactor has changed to ServiceLevel Summary Job Records and Messages Header APEL-summary-job-message: v0.3\nThe header only appears once at the top of each message. It defines the type of record and the schema version.\nRecord Fields The table shows the equivalent field in the AUR, under the container element aur:SummaryRecord. If not specified, it refers to the text value of urf:Key, where the element is a direct child of aur:SummaryRecord.\nKey Value Description Mandatory AUR equivalent Site String GOCDB sitename Yes Month int Month of summary (see notes) Yes Year int Year of summary (see notes) Yes GlobalUserName String User’s X509 DN aur:UserIdentity/urf:GlobalUserName VO String User’s VO aur:UserIdentity/urf:Group VOGroup String User’s VOMS group aur:UserIdentity/urf:GroupAttribute[@type=“vo-group”] VORole String User’s VOMS role aur:UserIdentity/urf:GroupAttribute[@type=“vo-role”] SubmitHost String The CE-ID or LRMS hostname Infrastructure String grid OR local Processors int Number of processors NodeCount int Number of nodes EarliestEndTime int End time of the first job in the month (epoch time) AUR has dates in ISO 8601 format LatestEndTime int End time of the last job in the month (epoch time) AUR has dates in ISO 8601 format WallDuration int Sum of wall clock times for all jobs in the month (in seconds) Yes AUR has durations in ISO 8601 format CpuDuration int Sum of CPU time for all jobs in the month (in seconds) Yes AUR has durations in ISO 8601 format NormalisedWallDuration int Sum of normalised wall clock time for all jobs (in seconds; normalised by HEPSPEC06) Yes AUR has durations in ISO 8601 format NormalisedCpuDuration int Sum of normalised CPU times for all jobs (in seconds; normalised by HEPSPEC06) Yes AUR has durations in ISO 8601 format NumberOfJobs int Total number of jobs Yes Message End of record %%\nExample Message APEL-summary-job-message: v0.3 Site: ExampleSite Month: 3 Year: 2010 GlobalUserName: /C=whatever/D=someDN VO: ExampleVO VOGroup: /ExampleVO VORole: Role=production SubmitHost: host.domain:port/queue Infrastructure: grid Processors: 1 NodeCount: 1 EarliestEndTime: 1267527463 LatestEndTime: 1269773863 WallDuration: 23425 CpuDuration: 2345 NormalisedWallDuration: 244435 NormalisedCpuDuration: 2500 NumberOfJobs: 100 %% ...another summary job record... %% ... %% Notes If GlobalUserName, VO, Group or Role are not published, the value for these fields on the server will be set to ‘None’.\nThe job records are included in months according to the month and year of their EndTime. The month and year should be in UTC. Only completed jobs are accounted for by APEL.\nAll durations are in hours. Normalised durations should be multiplied by HEPSPEC06. All figures should be rounded to the nearest integer.\nSummary Sync Records and Messages The Summary Sync records are used for the creation of the “APEL Pub/Sync” tests. It is a mechanism for the central APEL server to know the number of records that each site is storing locally. It is in general only used by sites which publish via the standard APEL client.\nHeader APEL-sync-message: v0.1\nRecord Fields Key Value Description Mandatory Site String GOCDB sitename Yes SubmitHost String CE ID Yes NumberOfJobs int Total number of jobs for that month Yes Month int Month Yes Year int Year Yes Message End of record %%\nExample Message APEL-sync-message: v0.1 Site: ExampleSite SubmitHost: host.domain:port/queue NumberOfJobs: 3479 Month: 1 Year: 2010 %% ...another sync record... %% ... %% ","categories":"","description":"EGI Grid Accounting record and message formats","excerpt":"EGI Grid Accounting record and message formats","ref":"/internal/accounting/record-and-message-formats/grid-accounting/","tags":"","title":"Grid Accounting"},{"body":"What is it? Grid storage enables storage of files in a fault-tolerant and scalable environment, and sharing it with distributed teams. Your data can be accessed through multiple protocols, and can be replicated across different providers to increase fault-tolerance. Grid storage gives you complete control over what data you share, and with whom you share the data.\nThe main features of grid storage:\nAccess highly-scalable storage from anywhere Control the data you share Organise your data using a flexible, hierarchical structure Grid storage file access is based on the gridFTP and WebDav/HTTP protocols, together with XRootD and legacy SRM (under deprecation at some of the endpoints).\nSeveral grid storage implementations are available in the EGI Infrastructure, the most common being:\ndCache DPM (N.B. under decommissioning) StoRM EOS Endpoint discovery The grid storage endpoints that are available to a user’s Virtual Organizations are discoverable via the EGI Information System (BDII).\nThe lcg-infosites command can be used to obtain VO-specific information on existing grid storages, using the following syntax:\n$ lcg-infosites --vo voname -[v] -f [site name] [option(s)] [-h| --help] [--is BDII] For example, to list the Storage Elements (SEs) available to the biomed VO, we could issue the following command:\n$ lcg-infosites --vo biomed se Avail Space(kB) Used Space(kB) Type SE ------------------------------------------ 280375465082 n.a SRM ccsrm.ihep.ac.cn 10995116266 11 SRM cirigridse01.univ-bpclermont.fr Access from the command-line Access to grid storage via a command-line interface (CLI) requires users to obtain a valid X.509 user VOMS proxy. Please refer to the Check-in documentation for more information.\nNote Integration via OpenID Connect to the EGI Check-in service is under piloting at some of the endpoints of the EGI Cloud infrastructure , but it has not yet reached the production stage. The CLI widely used to access grid-storage is gfal2, which is available for installation both on RHEL and Debian compatible systems.\nIn particular, gfal2 provides an abstraction layer on top of several storage protocols (XRootD, WebDAV, SRM, gsiftp, etc), offering a convenient API that can be used over different protocols.\nThe gfal2 CLI can be installed as follows (for RHEL compatible systems):\n$ yum install gfal2-util gfal2-all where gfal2-all will install all the plug-ins (to deal with all the available protocols).\nBelow you can find examples of the usual commands needed to access storage via gfal2. For a complete list of available commands, and the guide on how to use them, please refer to the gfal2-util documentation.\nNote In the examples below, the used gsiftp protocol can be replaced by any other supported protocol. List files on a given endpoint $ gfal-ls gsiftp://dcache-door-doma01.desy.de/dteam 1G.header-1 domatest gb SSE-demo test tpctest Create a folder $ gfal-mkdir gsiftp://dcache-door-doma01.desy.de/dteam/test Copy a local file $ gfal-copy test.json gsiftp://dcache-door-doma01.desy.de/dteam/test Copying file:///root/Documents/test.json [DONE] after 0s Copy files between storages $ gfal-copy gsiftp://prometheus.desy.de/VOs/dteam/public-file gsiftp://dcache-door-doma01.desy.de/dteam/test Copying gsiftp://prometheus.desy.de/VOs/dteam/public-file [DONE] after 3s Download a file to a local folder $ gfal-copy gsiftp://prometheus.desy.de/VOs/dteam/public-file /tmp Copying gsiftp://prometheus.desy.de/VOs/dteam/public-file [DONE] after 0s Delete a file $ gfal-rm gsiftp://dcache-door-doma01.desy.de/dteam/test/public-file gsiftp://dcache-door-doma01.desy.de/dteam/test/public-file DELETED Access via EGI Data Transfer The EGI Data Transfer service provides mechanisms to optimize the transfer of files between EGI Online Storage endpoints. Both a graphical user interface (GUI) and command-line interfaces (CLI) are available to perform bulk movement of data. Please check out the related documentation for more information.\nIntegration with Data Management frameworks Grid storage access, most of the time, is hidden from users by the integration with the Data Management Frameworks (DMFs) used by Collaborations and Experiments.\nFor example, EGI Workload Manager provides a way to efficiently access grid storage endpoints in order to read/store files, and to catalogue the existing file and related metadata.\nWhen running computation via the EGI Workload Manager, users do not actually access the storage directly. However, users can retrieve the output of the computation once it has been stored on the grid.\n","categories":"","description":"Grid Storage offered by EGI HTC providers\n","excerpt":"Grid Storage offered by EGI HTC providers\n","ref":"/users/compute/high-throughput-compute/grid-storage/","tags":"","title":"Grid Storage"},{"body":"Templates We will build a torque cluster on one of the EGI Cloud providers using EC3. Create a directory to store EC3 configuration and init it with the FedCloud client:\nmkdir -p torque cd torque fedcloud ec3 init --site \u003cyour site\u003e --vo \u003cyour vo\u003e We will use the following templates:\ntorque (from ec3 default templates) nfs (from ec3 default templates), ubuntu-1604 (user’s template), cluster_configure (user’s template) You can find the content below (make sure that you adapt them to your needs):\ntemplates/ubuntu-1604.radl specifies the VM image to use in the deployment:\ndescription ubuntu-1604 ( kind = 'images' and short = 'Ubuntu 16.04' and content = 'FEDCLOUD Image for EGI Ubuntu 16.04 LTS [Ubuntu/16.04/VirtualBox]' ) system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 4 and memory.size \u003e= 8196 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://\u003curl\u003e/\u003cimage_id\u003e' and disk.0.os.credentials.username = 'ubuntu' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048m and ec3_max_instances = 10 and # maximum number of working nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://\u003curl\u003e/\u003cimage_id\u003e' and disk.0.os.credentials.username = 'ubuntu' ) templates/cluster_configure.radl customises the torque deployment to match our needs:\nconfigure front ( @begin --- - vars: - USERS: - { name: user01, password: \u003cPASSWORD\u003e } - { name: user02, password: \u003cPASSWORD\u003e } [..] tasks: - user: name: \"{{ item.name }}\" password: \"{{ item.password }}\" shell: /bin/bash append: yes state: present with_items: \"{{ USERS }}\" - name: Install missing dependences in Debian system apt: pkg={{ item }} state=present with_items: - build-essential - mpich - gcc - g++ - vim become: yes when: ansible_os_family == \"Debian\" - name: SSH without password include_role: name: grycap.ssh vars: ssh_type_of_node: front ssh_user: \"{{ user.name }}\" loop: '{{ USERS }}' loop_control: loop_var: user - name: Updating the /etc/hosts.allow file lineinfile: path: /etc/hosts.allow line: 'sshd: XXX.XXX.XXX.*' become: yes - name: Updating the /etc/hosts.deny file lineinfile: path: /etc/hosts.deny line: 'ALL: ALL' become: yes @end ) configure wn ( @begin --- - vars: - USERS: - { name: user01, password: \u003cPASSWORD\u003e } - { name: user02, password: \u003cPASSWORD\u003e } [..] tasks: - user: name: \"{{ item.name }}\" password: \"{{ item.password }}\" shell: /bin/bash append: yes state: present with_items: \"{{ USERS }}\" - name: Install missing dependences in Debian system apt: pkg={{ item }} state=present with_items: - build-essential - mpich - gcc - g++ - vim become: yes when: ansible_os_family == \"Debian\" - name: SSH without password include_role: name: grycap.ssh vars: ssh_type_of_node: wn ssh_user: \"{{ user.name }}\" loop: '{{ USERS }}' loop_control: loop_var: user - name: Updating the /etc/hosts.allow file lineinfile: path: /etc/hosts.allow line: 'sshd: XXX.XXX.XXX.*' become: yes - name: Updating the /etc/hosts.deny file lineinfile: path: /etc/hosts.deny line: 'ALL: ALL' become: yes @end ) Create the cluster Deploy the cluster using ec3 docker image:\n$ docker run -it -v $PWD:/root/ -w /root \\ grycap/ec3 launch torque_cluster \\ torque nfs ubuntu-1604 refresh cluster_configure \\ -a auth.dat Creating infrastructure Infrastructure successfully created with ID: 529c62ec-343e-11e9-8b1d-300000000002 Front-end state: launching Front-end state: pending Front-end state: running IP: 212.189.145.XXX Front-end configured with IP 212.189.145.XXX Transferring infrastructure Front-end ready! To access the cluster, use the command:\n$ docker run -ti -v $PWD:/root/ -w /root grycap/ec3 ssh torque_cluster Warning: Permanently added '212.189.145.140' (ECDSA) to the list of known hosts. Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 3.13.0-164-generic x86_64) * Documentation: https://help.ubuntu.com/ Last login: Tue Feb 19 13:04:45 2019 from servproject.i3m.upv.es Configuration of the cluster Enable Password-based authentication Change settings in /etc/ssh/sshd_config\n# Change to no to disable tunnelled clear text passwords PasswordAuthentication yes and restart the ssh daemon:\nsudo service ssh restart Configure the number of processors of the cluster $ cat /var/spool/torque/server_priv/nodes wn1 np=XX wn2 np=XX [...] To obtain the number of CPU/cores (np) in Linux, use the command:\n$ lscpu | grep -i CPU CPU op-mode(s): 32-bit, 64-bit CPU(s): 16 On-line CPU(s) list: 0-15 CPU family: 6 Model name: Intel(R) Xeon(R) CPU E5520 @ 2.27GHz CPU MHz: 2266.858 NUMA node0 CPU(s): 0-3,8-11 NUMA node1 CPU(s): 4-7,12-15 Test the cluster Create a simple test script:\n$ cat test.sh #!/bin/bash #PBS -N job #PBS -q batch #cd $PBS_O_WORKDIR/ hostname -f sleep 5 Submit to the batch queue:\nqsub -l nodes=2 test.sh Destroy the cluster To destroy the running cluster, use the command:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy torque_cluster WARNING: you are going to delete the infrastructure (including frontend and nodes). Continue [y/N]? y Success deleting the cluster! ","categories":"","description":"Using Elastic Cloud Computing Cluster (EC3) platform to create elastic virtual clusters on the EGI Cloud.\n","excerpt":"Using Elastic Cloud Computing Cluster (EC3) platform to create elastic …","ref":"/users/compute/orchestration/im/ec3/apps/htc/","tags":"","title":"HTC"},{"body":"Notebooks running on EGI can access other existing computing and storage services from EGI or other e-Infrastructures. For data services, check data section of the documentation\nEGI services: access tokens Most services integrated with EGI Check-in can handle valid access tokens for authorising users. These are short-lived (normally less than 1-hour) and need to be renewed for longer usage. EGI Notebooks provides a ready to use access token that can be accessed from your notebooks and is automatically refreshed so you can always have a valid one.\nThe token is available at /var/run/secrets/egi.eu/access_token and you can use it for example to access cloud providers of the EGI cloud. See the following sample code where a list of VMs is obtained for CESGA:\nfrom keystoneauth1.identity import v3 from keystoneauth1 import session from novaclient import client with open(\"/var/run/secrets/egi.eu/access_token\") as f: access_token = f.read() auth = v3.OidcAccessToken(auth_url=\"https://fedcloud-osservices.egi.cesga.es:5000/v3\", identity_provider=\"egi.eu\", protocol=\"openid\", project_id=\"3a8e9d966e644405bf19b536adf7743d\", access_token=access_token) sess = session.Session(auth=auth) nova = client.Client(session=sess, version=2) nova.servers.list() A valid ID token is also available at /var/run/secrets/egi.eu/id_token.\nfedcloud client A direct benefit of the integration with access tokens in EGI Notebooks is that you can easily work with the fedcloud client. Once logged into the EGI Notebooks open a terminal and run:\nexport OIDC_ACCESS_TOKEN=`cat /var/run/secrets/egi.eu/access_token` fedcloud token check If the fedcloud command is not available, please follow the getting started guide to get it.\nEGI Workload Manager (DIRAC) If you are using a Notebooks instance integrated with EGI Workload Manager, you may access the DIRAC client via CVMFS with the following two command lines:\n$ source /cvmfs/dirac.egi.eu/dirac/bashrc_egi_dev $ dirac-login --issuer=https://dirac.egi.eu/auth D4Science If you are using a Notebooks instance integrated with D4Science, you can easily invoke DataMiner or any other D4Science functionality as the service will provide the GCUBE_TOKEN environment variable with a valid token.\nThis code will print the list of DataMiner methods available within your VRE:\nimport os from owslib.wps import WebProcessingService # init http header parameter and base folders for gCube REST API gcube_vre_token_header = {'gcube-token': os.environ[\"GCUBE_TOKEN\"]} # init WPS access for DataMiner algorithms dataminer_url = 'https://dataminer-prototypes.d4science.org/wps/WebProcessingService' wps = WebProcessingService(dataminer_url, headers=gcube_vre_token_header) for process in wps.processes: print('- Name: ', process.title) DataMiner algorithms can be invoked also from Notebooks, this code shows a sample:\nfrom owslib.wps import ComplexDataInput, monitorExecution # define processid and inputs processid = 'org.gcube.dataanalysis.wps.statisticalmanager.synchserver.mappedclasses.transducerers.WOFOST_CLOUD_V0_2_1' inputs = [ ('ClassToRun', 'nl.wur.wofostsystem.App'), ('FileInput', ComplexDataInput( 'https://data.d4science.org/shub/E_eVhZTzBWWktOaVJxQjJkdTUxR3FHaTFFdE9BTDYrZkZxQnFWcGMyaVVJbXptejdDOEFpSVNmam82RllkRUJ6cA==', mimeType=\"text/xml\") ) ] # execute the process execution = wps.execute(processid, inputs) monitorExecution(execution, sleepSecs=5, download=True) print(execution.status) Note that inputs that point to a URL should be specified using the ComplexDataInput class as shown above.\nOther third-party services We are open for integration with other services that may be relevant for your research. Please contact support \u003cat\u003e egi.eu with your request so we can investigate the best way to support your needs.\n","categories":"","description":"Access new services from the Notebooks\n","excerpt":"Access new services from the Notebooks\n","ref":"/users/dev-env/notebooks/integration/","tags":"","title":"Notebooks Integration with Other Services"},{"body":" Target Audience: Researchers, and IT-service providers who support research and education. \"Webinar: EGI Cloud Services: Updates, Innovations, and Future Directions\" (February, 2025) Agenda, slides and recording: https://www.egi.eu/event/webinar-egi-cloud-services-updates-innovations-and-future-directions/ About: This webinar highlights the latest developments in EGI Cloud Service, designed to support cutting-edge research and innovation. It provides an overview of the service features, recent updates to the EGI Cloud infrastructure, and new capabilities introduced to address the evolving needs of the scientific community. Suggested tutorials Create your first Virtual Machine (VM): Step by step guide to get your first Virtual Machine up and running Accessing virtual machines with SSH: Accessing virtual machines with SSH Target Audience: This presentation is intended for site administrators, site managers, security professionals, EGI management, and anyone who could be affected by a security incident and required to manage or respond to it. \"EGI CSIRT IRTF 2024 in Review: Incidents, Learnings, and Plans for 2025\" (January, 2025) Agenda, slides and recording: https://www.egi.eu/event/webinar-egi-irtf-2024-in-review-incidents-learnings-and-plans-for-2025/ About: We will start by presenting the role of the EGI CSIRT Incident Response Task Force (IRTF) in strengthening security across our community. Discover who we are, what drives our mission, and how we collaborate with key partners to protect the computing grid. Then, we’ll explain the initiatives from 2024, sharing the results of the Communications Challenges, the metrics of security monitoring, and present the various incidents that occurred during the year, along with the lessons that can be learned from them. Looking ahead, we will present our initiatives for 2025, designed to enhance visibility into the security status of sites, strengthen collaboration with VOs, and foster greater engagement from sites to improve their overall security posture. Target Audience: Scientific communities, developers, IT service providers, and end users. \"Webinar: EGI DataHub: an innovative platform for ingestion, management and publishing of distributed scientific data\" (September, 2024) Agenda, slides and recording: https://www.egi.eu/event/egi-datahub-platform-ingestion-management-publishing-distributed-scientific-data/ About: Watch our webinar on the latest advancements in EGI DataHub, an innovative distributed data access and management platform powered by Onedata technology. EGI DataHub unites 18 data providers across Europe, offering seamless data access and management capabilities. Target Audience: e-Infrastructure platform/technology providers Domain scientists and research Data Scientists \"Webinar: Managing Secrets in the EGI Infrastructure with EGI Secrets Store\" (July, 2024) Agenda, slides and recording: https://www.egi.eu/event/webinar-managing-secrets-in-the-egi-infrastructure-with-egi-secrets-store/ About: In this session, we will explore the EGI Secrets Store, a powerful solution designed to help you securely manage credentials, tokens, passwords, and other sensitive information required by your applications and services during deployment and operation. Traditional methods of storing these secrets in clear text within code repositories or configuration files pose significant security risks and make rotation cumbersome. EGI Secrets Store addresses these challenges by providing a secure, efficient, and standardised way to handle secrets. Target Audience: Scientific communities, developers, IT service providers, and end users. \"Webinar: Reproducible Open Science with EGI Replay\" (April, 2024) Agenda, slides and recording: https://www.egi.eu/event/reproducible-open-science-with-egi-replay/ About: In this webinar we showcase EGI Replay, a managed service for the EGI Community based on Binder. EGI Replay offers users the ability to effortlessly reproduce intricate calculations, simulations, and visualizations by importing Jupyter Notebooks and their respective runtime environments. With a single link, users can easily share their work with collaborators, making Replay an invaluable tool for workshops, scientific workflows, and team collaboration. Target Audience: Scientific communities, developers, integrators and end users. \"Data Management in EGI with Rucio and FTS\" (October, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5711/ About: Rucio is a data management software, originally developed for ATLAS at CERN to supersede their previous data management software 10 years ago. Since then Rucio has been constantly developed by ATLAS and other communities that have come to use Rucio, ensuring that it is a feature rich, and well maintained open software. Multi-VO Rucio implemented by the STFC in the UK hosts Rucio as a service for many communities. This is to provide communities the opportunity to use Rucio for their data management solution, without having to learn about, and host their own instance of Rucio. FTS is a low level data movement service, responsible for reliable bulk transfer of files between storages. It's responsible for globally distributing the majority of the LHC data across the WLCG infrastructure and it supports many communities is EGI. In this webinar we introduce the main functionalities and show how to interact with the services in order to schedule transfers. Suggested tutorials Data transfer with grid storage: Use EGI Data transfer to handle data in grid storage Target Audience: Scientific communities, developers, integrators and end users. \"Using EGI Cloud infrastructure with fedcloudclient\" (September, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5694/ About: The FedCloud client is a high-level Python package for a command-line client designed for interaction with the OpenStack services in the EGI infrastructure. The client can access various EGI services and can perform many tasks for users including managing access tokens, listing services, and mainly execute commands on OpenStack sites in EGI infrastructure. The webinar will provide tutorial and demonstration of using fedcloudclient in EGI Cloud infrastructure. Suggested tutorials Automate with oidc-agent, fedcloudclient, terraform and Ansible: Step by step guide to automating the deployment using Ansible with Terraform, oidc-agent and fedcloudclient Target Audience: Scientific communities, developers, IT service providers, and end users. \"Using Dynamic DNS service in the EGI Cloud infrastructure\" (June, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5559/ About: Nowadays, more and more services are dynamically deployed in Cloud environments. Usually, the services hosted on virtual machines in Cloud are accessible only via IP addresses or pre-configured hostnames given by the target Cloud providers, making it difficult to provide them with meaningful domain names. The Dynamic DNS service provides a unified, federation-wide Dynamic DNS support for VMs in EGI infrastructure. Users can register their chosen meaningful and memorable DNS host names in given domains (e.g. my-server.vo.fedcloud.eu) and assign to public IPs of their servers. By using Dynamic DNS, users can host services in EGI Cloud with their meaningful service names, can freely move VMs from sites to sites without modifying server/client configurations (federated approach), and can request valid server certificates in advance (critical for security). The webinar will provide demonstration and tutorial, also practical advice on using Dynamic DNS service in realistic user scenarios. Target Audience: Developers and administrators of relying parties that want to connect to Check-in for authenticating users and managing their access rights. \"Providing controlled access to distributed resources and services with EGI Check-in: the provider perspective\"(May, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5494/ About: This webinar will help new services to integrate with Check-in, the EGI Authentication \u0026 Authorisation Infrastructure enabling secure access to relying parties. The target group of the training are developers and administrators of services that want to connect to Check-in for user authentication and authorisation. The training will showcase the use of the EGI Check-In Federation Registry tool for managing the lifecycle of a relying party, i.e. registration, reconfiguration and de-registration. The training will include hands-on sessions for the participants to integrate their own relying party to Check-In. Target Audience: Scientific communities and IT-service providers who support research and education. \"Access and analyze data from the EGI DataHub with Jupyter notebooks and MATLAB\" (May, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5499/ About: Good, clean data is hard to come by. The EGI provides scientists and researchers access to a large collection of public datasets from data centres globally. This data can be accessed using the EGI Jupyter Notebook service. MATLAB users can now analyse this data using the familiar MATLAB desktop, via a web browser, on the EGI’s resources. In this webinar, you will learn how to: Use your MATLAB licence to login to the EGI MATLAB installation Access data from the EGI DataHub Read in scientific data into MATLAB Analyse and visualise data using computational notebooks called MATLAB Live Scripts. Share your MATLAB code with your peers. Suggested tutorials Access DataHub from a VM: Use data in EGI DataHub from a virtual machine. Create a VM with Jupyter and DataHub: Step by step guide to get a Virtual Machine for Jupyter and DataHub in your cloud provider Target Audience: Scientific communities and IT-service providers. \"Monitoring services with ARGO\" (May, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5496/ About: ARGO is a lightweight service for Service Level Monitoring designed for medium and large sized Research Infrastructures. Services are monitored with probes compatible with flexible and widely adopted Nagios plugin format. Besides basic availability checks, services can be monitored by emulating typical user scenarios that allows to derive the quality of service the actual user gets. ARGO offers near real-time status updates which allow both end-users and site admins to have an overview of the services offered at any given point in time via a web user interface and via enriched email notifications. ARGO generates custom Availability and Reliability reports based on the aggregated monitoring data. The rich monitoring data collected in ARGO service is actually stored in a highly flexible big data friendly form using state-of-the-art computational pipelines and formats. This provides the ability to reuse \u0026 analyse the data in different ways such as to highlight service usage patterns and provide a number of trends and insights. In this training session we are going to show the process we follow to monitor a new service with ARGO. In addition, the real time computations and the results via the alerts, API and UI will be shown. ARGO is a service jointly developed and maintained by CNRS, GRNET and SRCE. Target Audience: Scientific communities, and programmers who support research and education. \"Managing Singularity, Docker and udocker containers, Kubernetes clusters in the EGI Cloud\" (April, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5492/ About: Containers provide a streamlined way to build, test, deploy, and redeploy applications on different environments: from the developer’s local machine to any cloud provider. Containers make it easy for developers to package applications and for operators to manage and deploy those applications on the infrastructure. Container orchestrators like Kubernetes facilitate the management of containerized workloads and services, using declarative configuration and automation. In this webinar we will introduce the different runtimes available for executing containers in EGI infrastructure and will show how to manage Kubernetes clusters to get your containers under control executed on EGI cloud providers. Target Audience: Users and application experts of the EGI communities. \"DIRAC Services for EGI users\" (October, 2020) Agenda, slides and recording: https://indico.egi.eu/event/5267/ About: DIRAC is a complete framework for building distributed computing systems of any level of complexity. Initially developed for the LHCb High Energy Physics experiment at the LHC collider at CERN, the framework was generalised for the use by multiple scientific communities in various domains. Services based on the DIRAC software are offered by several grid infrastructure projects such as France-Grilles or GridPP/UK. Since 2014, the DIRAC services have also been provided for the EGI users. During the webinar, an overview of the DIRAC framework will be presented together with a number of services offered to the users by EGI: how to manage user jobs in the EGI infrastructure, how to connect custom computing and storage resources, how to manage user data as well as automate regular tasks. Extending DIRAC with community custom services will also be discussed. Target Audience: IT service providers, site and NGI operation managers (new member of staff). \"EGI Operations and responsibilities of an NGI\" (October 2020) Agenda, slides and recording: https://indico.egi.eu/event/5268/ About: This webinar will give an overview of tried and tested approaches to federated operations, both at the level of the infrastructure as well as at the national level. It will cover the most important aspects and day-to-day work covered by staff - both at the international infrastructure level at EGI as well as at an example National Grid Initiative (NGI). Example scenarios will be presented along with the tools used to deal with the scenarios. Finally there will be an opportunity for questions and discussions arising from the topics covered. Target Audience: Site administrators and cluster administrators; CVMFS power users. \"CernVM-FS for Containers\" (October 2020) Agenda, slides and recording: https://indico.egi.eu/event/5251/ About: Delivering complex software stacks across a worldwide distributed system is a challenge in high-throughput scientific computing. The global-scale virtual file system CernVM-FS distributes more than a billion software binaries to hundreds of thousands of machines around the world. In this webinar, we will present the latest developments with regard to CernVM-FS container integration. Containers and CernVM-FS team up nicely: containers provide the isolation capabilities that decouple the application stack from the underlying platform and CernVM-FS provides efficient distribution means for the containerized software binaries. Containers are an enabling technology to harness opportunistic resources and HPC facilities. CernVM-FS enables the use of such resources at scale. In this webinar, we will show how existing repositories can be used with several popular container runtimes, such as docker, podman, singularity, and kubernetes. We will also show how operating system containers themselves can be efficiently distributed through CernVM-FS. Lastly, we will highlight an upcoming new way of publishing content from within a container. This makes it easy to set up, build and test and deploy-to-cvmfs pipelines on kubernetes. Target Audience: Scientific communities and IT-service providers who operate IdP for them. \"The EGI AAI Check-In service for scientific communities\" (May 2020) Agenda, slides and recording: https://indico.egi.eu/event/5088/ About: The EGI Check-in service (also called EGI AAI proxy) enables access to EGI services and resources using federated authentication mechanisms. Specifically, the proxy service is operated as a central hub between federated Identity Providers (IdPs) residing ‘outside’ of the EGI ecosystem, and Service Providers (SPs) that are part of EGI. The main advantage of this design principle is that all entities need to establish and maintain technical and trust relation only to a single entity, the EGI AAI proxy, instead of managing many-to-many relationships. In this context, the proxy acts as a Service Provider towards the Identity Providers and as an Identity Provider towards the Service Providers. Through the EGI AAI proxy, users are able to authenticate with the credentials provided by the IdP of their Home Organisation (e.g. via eduGAIN), as well as using social identity providers, or other selected external identity providers (support for eGOV IDs is also foreseen). To achieve this, the EGI AAI has built-in support for SAML, OpenID Connect and OAuth2 providers and already enables user logins through Facebook, Google, LinkedIn, and ORCID. In addition to serving as an authentication proxy, the EGI AAI provides a central Discovery Service (Where Are You From – WAYF) for users to select their preferred IdP. The EGI AAI proxy is also responsible for aggregating user attributes originating from various authoritative sources (IdPs and attribute provider services) and delivering them to the connected EGI service providers in a harmonised and transparent way. Service Providers can use the received attributes for authorisation purposes, i.e. determining the resources the user has access to. During this webinar we will give an overview about the service and provide guidelines to support the resource providers’ and communities' needs for federated access through the EGI AAI Check-In service. The webinar will also cover more advanced workflows for addressing non-web-based access use cases (e.g. command line and API). Target Audience: Scientific communities, for programmers and IT-service providers who support research and education. \"The EGI Notebooks service: Support for analytics and big data visualisation in the cloud\" (May 2020) Agenda, slides and recording: https://indico.egi.eu/event/5087/ About: The EGI Notebooks service is an environment based on Jupyter and the EGI cloud service that offers a browser-based, scalable tool for interactive data analysis. The notebooks environment provides users with notebooks where they can combine text, mathematics, computations and rich media output. The service, in production since late 2019, is offered in two options: (i) Notebooks for researchers: EGI offers a basic instance of the Notebooks as an open service. Any researcher can access this automatically to write and play notebooks on limited capacity cloud servers. (ii) Notebooks for communities: EGI offers customised Notebooks service to scientific communities. Such customised instances can be hosted on special hardware (for example with fat nodes and GPUs), can offer special libraries, data import/export and user authentication systems. During the webinar Enol will go through the main features of the EGI Notebooks service and he will explain how to use it with Binder and other open-source solutions to implement Open Science. Target Audience: Scientific communities, for programmers and IT-service providers who support research and education. \"Introduction to Slurm\" (March 2023) About: Slurm is an open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. In this tutorial, we briefly discuss the benefits of using batch schedulers, the motivations to use Slurm and provide a list of commands to get started with Slurm. Suggested material Slides Target Audience: Scientific communities, for programmers and IT-service providers who support research and education. \"Introduction to Snakemake\" (December 2022) About: The Snakemake workflow management system is a tool for creating reproducible and scalable data analyses. Workflows are described via a human-readable, Python-based language. They can be seamlessly scaled to server, cluster, grid and cloud environments without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of the required software, which will be automatically deployed to any execution environment. Slides and code https://github.com/c-scale-community/c-scale-tutorial-snakemake Target Audience: Scientific communities, for programmers and IT-service providers who support research and education. \"Leveraging the Onedata Platform for Long-Term Data Archiving\" (June 2023) About: In this presentation, we will discuss the latest advancements in the Onedata platform, focusing on its new features for long-term data archiving and processing. We will demonstrate how the platform has been optimized to meet the Open Archival Information System (OAIS) standards, ensuring the reliable preservation and accessibility of archived information over time. Furthermore, we will explore the integration of Function as a Service (FaaS) capabilities in the platform, allowing for seamless and scalable data processing on demand. By combining the robust archiving capabilities of the OAIS standard with the flexibility of FaaS, the Onedata platform emerges as a powerful solution for organizations seeking efficient and reliable management of their long-term data storage and processing needs. ","categories":"","description":"Tutorials for individual services from the EGI owned service portfolio, as well as services that are offered by the broader EGI community to complement the EGI services towards certain types of advanced computing use cases.\n","excerpt":"Tutorials for individual services from the EGI owned service …","ref":"/users/tutorials/intermediate/","tags":"","title":"Intermediate level"},{"body":"Introduction Technology Providers (TP) develop or deliver technology and software for specific user communities or customisation for specific requirements. In EGI case, they maintain the middleware which the RCs install and allowing the users to exploit the compute, storage, data, and cloud resources.\nIntegration of middleware stack To maintain an appropriate quality of service of the EGI production infrastructure, every middleware stack (Compute, Storage, etc.) installed on and delivered by the RCs must fulfil a number of requirements.\nNote Related procedure: PROC19 Integration of new cloud management framework or middleware stack in the EGI Infrastructure) PROC19 was defined to ensure that any single aspect of the integration of the new piece of middleware with the infrastructure is reviewed and assessed.\nAfter the creation of the request in the EGI Helpdesk, with details about the technology, the contacts, the expected customers, and the motivation, the integration steps cover the following areas (where possible, steps can be done in parallel):\nUnderpinning Agreement (UA) between EGI Foundation and the technology provider it could be either the Corporate-level Technology Provider Underpinning Agreement or a customised version. Configuration Management: mapping of the new technology in the Configuration Management Database (CMDB) Information System: evaluating if the new technology should publish information in the Information System according to the GLUE Schema. Monitoring: the new technology should allow external monitoring. If particular aspects of the technology need to be monitored, specific monitoring probes should be provided by the TPs and deployed on the EGI Monitoring service. Support: the Support Unit where incidents and service requests will be addressed needs to be defined in the EGI Helpdesk, associated to the Quality of Support defined in the UA. Accounting: the need to gather usage data, which depends on the technology itself and on the infrastructure requirements and will be published in the EGI Accounting Portal. Integration in UMD or CMD: the Unified Middleware Distribution (UMD) and Cloud Middleware Distribution (CMD) are the integrated sets of software components contributed by Technology Providers and packaged for deployment as production quality services in EGI. Documentation: exhaustive documentation for RC administrators and users should be provided and may be added to the EGI Documentation. Security: a security assessment of the software is required according to a number of guidelines defined by the EGI Security team. ","categories":"","description":"Guidelines for Technology Providers to join the EGI Infrastructure","excerpt":"Guidelines for Technology Providers to join the EGI Infrastructure","ref":"/providers/joining/technology-provider/","tags":"","title":"Joining as a Technology Provider"},{"body":"There are several container management tools available, on the EGI Cloud we use Kubernetes as the default platform for our service. This guide explains how to get a running scalable Kubernetes deployment for your applications with EC3.\nGetting started Before getting your kubernetes cluster deployed, you need to get access to the Cloud Compute service, check the Authentication and Authorisation guide for more information. You should also get the FedCloud client installed to get EC3 templates needed to start deployment.\nYour kubernetes deployment needs to be performed at an specific provider (site) and project. Discover them using fedcloud as described in the EC3 tutorial.\nEC3 Templates EC3 relies on a set of templates that will determine what will be deployed on the infrastructure. fedcloud helps you to get an initial set of templates for your kubernetes deployment:\nmkdir k8s cd k8s fedcloud ec3 init --site \u003cyour site\u003e --vo \u003cyour vo\u003e You will also need a base image template for the deployment. Please refer to the EC3 tutorial to create such file. Below you can see an example for IFCA-LCG2 site with project related to vo.access.egi.eu:\ndescription ubuntu-ifca ( kind = 'images' and short = 'ubuntu18-ifca' and content = 'Ubuntu 18 image at IFCA-LCG2' ) network public ( provider_id = 'external' and outports contains '22/tcp' ) network private (provider_id = 'provider-2008') system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://api.cloud.ifca.es/723171cb-53b2-4881-ae37-a7400ce0665b' and disk.0.os.credentials.username = 'cloudadm' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and ec3_max_instances = 5 and # maximum number of working nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://api.cloud.ifca.es/723171cb-53b2-4881-ae37-a7400ce0665b' and disk.0.os.credentials.username = 'cloudadm' ) Now you are ready to deploy the cluster using launch command of ec3 with:\nthe name of the deployment, k8s in this case\na list of templates: kubernetes for configuring kubernetes, ubuntu for specifying the image and site details, refresh to enable credential refresh and elasticity\nthe credentials to access the site with -a auth.dat\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 \\ launch k8s kubernetes ubuntu refresh -a auth.dat Creating infrastructure Infrastructure successfully created with ID: b9577c34-f818-11ea-a644-2e0fc3c063db Front-end configured with IP 193.144.46.249 Transferring infrastructure Front-end ready! Your kubernetes deployment is now ready, log in with the ssh command of ec3:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 ssh k8s Warning: Permanently added '193.144.46.249' (ECDSA) to the list of known hosts. Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-109-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage * Kubernetes 1.19 is out! Get it in one command with: sudo snap install microk8s --channel=1.19 --classic https://microk8s.io/ has docs and details. * Canonical Livepatch is available for installation. - Reduce system reboots and improve kernel security. Activate at: https://ubuntu.com/livepatch Last login: Wed Sep 16 14:35:35 2020 from 158.42.104.204 $ bash cloudadm@kubeserver:~$ You can interact with the kubernetes cluster with the kubectl command:\ncloudadm@kubeserver:~$ sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kubeserver.localdomain Ready master 23h v1.18.3 The cluster will only have one node (the master) and will start new nodes as you create pods. Alternatively you can poweron nodes manually:\ncloudadm@kubeserver:~$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn2.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn3.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn4.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn5.localdomain off enabled 00h32'22\" 0,0 1,1073741824 cloudadm@kubeserver:~$ clues poweron wn1.localdomain node wn1.localdomain powered on cloudadm@kubeserver:~$ sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kubeserver.localdomain Ready master 24h v1.18.3 wn1.localdomain Ready \u003cnone\u003e 6m49s v1.18.3 Exposing services outside the cluster Kubernetes uses services for exposing an applications via the network. The services can rely on Load Balancers supported at the underlying cloud provider, which is not always feasible on the EGI Cloud providers. As an alternative solution we use an ingress controller which allows us to expose services using rules based on hostnames.\nHelm allows you to quickly install the nginx based ingress. Add the helm repos:\ncloudadm@kubeserver:~$ sudo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx cloudadm@kubeserver:~$ sudo helm repo add stable https://kubernetes-charts.storage.googleapis.com/ cloudadm@kubeserver:~$ sudo helm repo update Create a configuration file (ingress.yaml), get the externalIP using ip addr:\ncontroller: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master service: type: NodePort externalIPs: - 192.168.10.3 defaultBackend: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master and install:\ncloudadm@kubeserver:~$ sudo helm install ingress -n kube-system -f ingress.yaml ingress-nginx/ingress-nginx Now you are ready to expose your services using a valid hostname. Use the Dynamic DNS service for getting hostnames if you need. Assign as IP the public IP of the master node. Once you have a hostname assigned to the master IP, the ingress will be able to reply to requests already:\n$ curl ingress.test.fedcloud.eu \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e404 Not Found\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e404 Not Found\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003enginx/1.19.2\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e The following example yaml creates a service and exposes at that ingress.test.fedcloud.eu host:\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: my-nginx labels: app: nginx spec: ports: - port: 80 protocol: TCP selector: app: nginx --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test spec: rules: - host: ingress.test.fedcloud.eu http: paths: - pathType: Prefix path: \"/\" backend: serviceName: my-nginx servicePort: 80 Now the ingress will redirect request to the NGINX pod that we have just created:\n$ curl ingress.test.fedcloud.eu \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Volumes Volumes on these deployments can be supported with NFS volume driver. You can either manually configure the server on one of the nodes or use EC3 to deploy it and configure it for you. Create a templates/nfs.radl to do so:\ndescription nfs ( kind = 'component' and short = 'Tool to configure shared directories inside a network.' and content = 'Network File System (NFS) client allows you to access shared directories from Linux client.' ) system front ( ec3_templates contains 'nfs' and disk.0.applications contains (name = 'ansible.modules.grycap.nfs') ) configure front ( @begin - tasks: - name: Create /volume for supporting NFS file: path: /volumes state: directory mode: '0777' - roles: - role: grycap.nfs nfs_mode: 'front' nfs_exports: - path: \"/volumes\" export: \"wn*.localdomain(rw,async,no_root_squash,no_subtree_check,insecure) kubeserver.localdomain(rw,async,no_root_squash,no_subtree_check,insecure)\" @end ) system wn (ec3_templates contains 'nfs') configure wn ( @begin - tasks: - name: Install NFS common apt: name: nfs-common state: present @end ) if you have a running cluster, you can add the NFS support by reconfiguring the cluster:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 \\ reconfigure k8s -a auth.dat -t nfs Reconfiguring infrastructure Front-end configured with IP 193.144.46.249 Transferring infrastructure Front-end ready! And then install the NFS driver in kubernetes with helm:\ncloudadm@kubeserver:~$ sudo helm install nfs-provisioner \\ stable/nfs-client-provisioner \\ --namespace kube-system \\ --set nfs.server=192.168.10.9 \\ --set storageClass.defaultClass=true \\ --set nfs.path=/volumes \\ --set tolerations[0].effect=NoSchedule,tolerations[0].key=node-role.kubernetes.io/master Now you are ready to create a PVC and attach it to a pod, see this example:\n--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: nfs-client accessModes: - ReadWriteOnce resources: requests: storage: 3Gi --- apiVersion: v1 kind: Pod metadata: name: pvc-pod namespace: default spec: restartPolicy: Never volumes: - name: vol persistentVolumeClaim: claimName: test-pvc containers: - name: test image: \"busybox\" command: [\"sleep\", \"1d\"] volumeMounts: - name: vol mountPath: /volume Once you apply the yaml, you will see the new PVC gets bounded to a PV created in NFS:\ncloudadm@kubeserver:~$ sudo kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-39f970de-eaad-44d7-b49f-90dc9de54a14 3Gi RWO nfs-client 9m46s Destroying the cluster Once you don’t need the cluster anymore, you can undeploy with the destroy command of EC3:\n$ fedcloud ec3 refresh # refresh your credentials to interact with the cluster $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy k8s -y -a auth.dat WARNING: you are going to delete the infrastructure (including frontend and nodes). Success deleting the cluster! ","categories":"","description":"Run containers on the EGI Cloud with Kubernetes\n","excerpt":"Run containers on the EGI Cloud with Kubernetes\n","ref":"/users/compute/cloud-container-compute/k8s/","tags":"","title":"Kubernetes"},{"body":" Linking new identities to your EGI Account Identity linking allows you to access EGI resources with your existing personal EGI ID, using any of the login credentials you have linked to your account. You can use any of your organisational or social login credentials for this purpose. To link a new organisational or social identity to your EGI account:\nEnter the following URL in a browser: https://aai.egi.eu/registry\nClick Login and authenticate using any of the login credentials already linked to your EGI account\nNavigate to My EGI User Community Identity page in one of the following ways:\nhover over your display name next to the gear icon on the top right corner of the page; or, alternatively, select EGI User Community from the list of available Collaborations and then click My EGI User Community Identity from the People menu Under the Organisational Identities section of your profile page, expand Actions menu and click Link New Identity.\nOn the introductory page for Identity Linking, click Begin\nYou will need to sign in using the login credentials from the institutional/social identity provider you want to link to your account.\nWarning It is very important to escape the identity provider selection, cached in the discovery page, before picking the new one. After successful authentication, the new Identity Provider will be available under the Organizational Identities tab and you’ll be able to access EGI resources with your existing personal EGI ID using the login credentials of the identity provider you selected in Step 6.\nLinking your certificate to your EGI Account Certificate linking allows you to add the subject DN of your certificate to your existing personal EGI ID. For this you need to import your certificate to your browser.\nTo link a subject DN to your EGI account:\nEnter the following URL in a browser: https://aai.egi.eu/registry\nClick Login and authenticate using any of the login credentials already linked to your EGI account\nNavigate to My EGI User Community Identity page in one of the following ways:\nhover over your display name next to the gear icon on the top right corner of the page; or, alternatively, select EGI User Community from the list of available Collaborations and then click My EGI User Community Identity from the People menu Under the Organisational Identities section of your profile page, expand Actions menu and click Link New Identity.\nOn the introductory page for Identity Linking, click Begin\nContinuously, you will need to sign in using the IGTF Certificate Proxy.\nWarning It is very important to escape the identity provider selection, cached in the discovery page, before picking the new one. Then select the certificate you want to link to your account from the popup window.\nAfter successful authentication you will be redirected back to your EGI Account. Also, you’ll be able to access EGI resources with your existing personal EGI ID using IGTF Certificate Proxy and your certificate.\nTo verify that the subject DN is added to your EGI account scroll down to Organisational Identities and click on view button in the row where the source is https://edugain-proxy.igtf.net/simplesaml/saml2/idp/metadata.php.\nThen scroll down to Certificates and you should see the subject DN of your certificate.\n","categories":"","description":"Linking additional organisational/social identities to your EGI Account\n","excerpt":"Linking additional organisational/social identities to your EGI …","ref":"/users/aai/check-in/linking/","tags":"","title":"Linking Identities"},{"body":"Mailing lists are managed by software called Mailman.\nThe software provides a web interface and a public list of mailing lists is available.\nRequesting the creation of a new mailing list By default all mailing lists are managed using groups in EGI SSO. Only for very specific they may be managed directly in mailman.\nOpen a ticket to Collaboration Tools SU in EGI Helpdesk, providing:\nList name List Description List administrator contact After validation of those information the list will be created.\nName and description of the mailing list will be validated, you may be recommended to change name or description to follow already existing best practices for naming, or to improve description.\nList administrators EGI mailing lists are usually managed by EGI SSO, with some exceptions for lists that include members without SSO accounts.\nList administrators of every SSO-managed list are set to the owners of the corresponding EGI SSO group when a new list is created.\nNote For SSO managed-lists: a change to the list of list administrators through the Mailman interface will not be permanent. Only the EGI IT support can change who are the owners of an SSO group, please contact us if you want to add a new list administrator.\nEach list can be administered over the web interface by following its link from the page Admin links. You can use your EGI SSO password to access the administration pages of the lists for which you are the administrator.\nList members Members of lists are synchronized with members of same named groups in the EGI SSO system.\nNote For SSO managed-lists: Changes made to list membership through the Mailman interface will not be permanent. Managing list members must be done by managing members of corresponding groups in EGI SSO. Log into your EGI SSO account, and you will see a list of groups of which you are the owner. Click on the “»manage” link, it will display a page where you can\nremove members from the group add existing users as members of thew group invite new users to the group If you manage more then one list, and you need to add many new users to several lists at once, it may be helpful to follow the link titled “»Create new users in groups”. It directs to a page where you can enter many addresses at once, and specify the groups to which they should be added. Existing users will be added immediately, non-existing users will be invited to create an account and they will be assigned to the groups when they create the account.\nDocumentation Mailman documentation Mailman Frequently Asked Questions List administrator tasks ","categories":"","description":"Mailing lists usage\n","excerpt":"Mailing lists usage\n","ref":"/internal/collaboration-tools/mailing-lists/","tags":"","title":"Mailing lists"},{"body":"Document control Property Value Title Service Intervention Management Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Managing Service interventions Owner SDIS team Service Intervention A service intervention is defined as an action which will involve or lead to the possibility of a loss, or noticeable degradation of a service. Depending on the planning of the outage, we have two types of intervention:\nScheduled interventions: planned and agreed in advance Unscheduled interventions: unplanned, usually triggered by an unexpected failure How to manage an intervention Interventions are recorded through the Configuration Database. For more information, have a look at the description.\nScheduled interventions Scheduled interventions MUST be declared at least 24 h in advance, specifying reason and duration. Existing scheduled interventions CAN be extended, provided that it’s done 24 hours in advance. Unscheduled interventions Any intervention declared less than 24 h in advance will be considered unscheduled. Sites MUST declare unscheduled interventions as soon as they are detected to inform the users. Unscheduled interventions CAN be declared up to 48 hours in the past (retroactive information to the user community). Required information The required information to fill in when declaring an intervention is:\nSeverity (Outage or Warning) Description Timezone Starting and ending dates Affected site / Affected services and endpoints Recommendations For interventions that impact end users, the downtime SHOULD be declared 5 working days in advance, specifying reason and duration. A post−mortem SHOULD be included in the downtime report. Notifications Intervention notifications (through broadcasts, RSS feeds, etc) as specified in the following procedures are automatically sent when declaring a downtime in Configuration Database: at declaration time, 24 h in advance and 1 h before the intervention.\nSuspension policy Sites on downtime for more than 1 month will be suspended/uncertified. AT_RISK downtime declarations are only for providing warnings to users, and are ignored for calculating site availability (actual status will be used).\n","categories":"","description":"How to service interventions","excerpt":"How to service interventions","ref":"/providers/operations-manuals/man02_service_intervention_management/","tags":"","title":"MAN02 Service intervention management"},{"body":"EGI offers MATLAB on the cloud using the MATLAB Integration for Jupyter and shares data and code with other EGI users. MATLAB is also available directly on compute services offered by the members of the EGI Federation.\nMATLAB on the EGI infrastructure can be used by scientists and engineers for Open Science by enabling them to share their data and code using computational notebooks.\nBefore getting started MATLAB Licenses To use the MATLAB Integration on the EGI Notebooks service, a supported MATLAB license is required. You can also use the toolboxes linked to your MATLAB license.\nIf you are unsure of your MATLAB license type, contact your System Administrator or MathWorks support.\nTutorials If you are not familiar or have limited experience with MATLAB, MATLAB Onramp provides a free, self-paced tutorial. Additional Onramps are available for other topics using MATLAB.\nGetting started Once your server has started up (after selecting the MATLAB environment), click on the MATLAB icon\nA dialog for handling MATLAB licensing will appear.\nIndividual and Campus Wide MATLAB Licenses\nIf you have access to a MATLAB Individual License or a MATLAB Campus Wide License, use the Online License Manager tab to log in using your MathWorks account (does not need to match the email used on the EGI Identity Provider) and click on next.\nConcurrent/Network Licenses\nIf you have Network Licenses, enter the port@hostname of your License Server on the Network License Manager tab of the login screen. Before doing so, please check with your system administrator about allowing the EGI access to your on-prem Network License Manager (License Server).\nFree trial MATLAB Licenses If you do not have a MATLAB license and would like to try it out, you can download a free MATLAB trial license here. Note that the trial license is for MATLAB only and does not have any toolboxes.\nMore information on MATLAB licensing with this integration can be found here.\nFollowing this, the MATLAB IDE will appear in your browser.\nTo facilitate sharing your research output, you can use MATLAB Live Scripts combining rich text, equations, images, code and inline output all in one document. Live Scripts can be accessed from the Live Editor tab. Here is an example of a tutorial Live Script from the EGI webinar showing steps required to analyze some public COVID-19 data with MATLAB. Some other examples of Live Scripts can be found here.\nLimitations Browser based MATLAB has some differences as compared to MATLAB on the desktop. For more information, see the limitations here. Additionally, Simulink is not supported on the EGI at this time.\nRestarting, stopping or signing out of the MATLAB session At any time, clicking on this symbol will bring up a dialog to stop, restart or sign out of the current MATLAB session.\nOpen Science using MATLAB on EGI Notebooks You can access publicly available datasets via the EGI DataHub and analyze them in-the-cloud directly using MATLAB, without the need for time consuming downloads. Your MATLAB code can also be shared with your community in a variety of interoperable and open formats.\nAnalyzing public datasets using MATLAB on EGI This short video and this webinar explains how to access EGI Data Services from MATLAB in detail. Data from different data providers can be accessed from the DataHub. More information on Data Management from Notebooks and persistent storage can be found here.\nMATLAB support for data formats MATLAB supports several scientific and standard data formats including web data and those from specialized hardware.\nSharing data and code You can share your data and analyses by sharing your EGI provided persistent storage space with others. Users belonging to a specific community can request community Notebooks.\nInteroperability with Python and other languages You can collaborate with users of other languages by calling these languages (eg. Python) from MATLAB for conducting specific analyses. You can also save data in formats compatible with other languages.\nHere are some other ways in which MATLAB enables Open Science\nUsing MATLAB on other EGI services In addition to MATLAB on EGI Notebooks, you can also use your MATLAB license to access MATLAB on any of the Institutions participating in the EGI Federation. To run your compute-intensive MATLAB code faster or to run code in parallel, you will need MATLAB Parallel Server and MATLAB Parallel Computing Toolbox on your MATLAB license. If you have access to a Campus Wide License , you can now scale up to use all available workers on the HPC cluster of your choice.\nTo access MATLAB on any site of the EGI Federation, follow these steps.\nCheck if you have access to MATLAB Parallel Server and Parallel Computing Toolbox\nAsk the system administrator of the site about MATLAB as part of EGI\nContact Shubo Chakrabarti at MathWorks or Enol Fernandez at EGI for setting up access to MATLAB at the site.\n","categories":"","description":"Using MATLAB in EGI Notebooks\n","excerpt":"Using MATLAB in EGI Notebooks\n","ref":"/users/dev-env/notebooks/kernels/matlab/","tags":"","title":"MATLAB"},{"body":"This documentation covers how to install and configure a OneData OneProvider in order to join a new or existing EGI DataHub space. In particular two types of installations are available, depending if the provider wants to support the space with an empty storage or if existing data should be exposed via the Oneprovider.\nRequirements for production installation Oneprovider RAM: 32GB CPU: 8 vCPU Disk: 50GB SSD To be adjusted for the dataset and usage scenario For high Input/Output operations per second (IOPS) High performance backend storage (CEPH) Low latency network Network Requirements The following ports need to be open on the local and site firewall: 80, 443, 9443, 6665 (for data transfer) Warning The Oneprovider installation includes also a Memcached server running on port 11211. Please ensure that this port and other unused ports are not open to the internet by setting up proper local firewall rules. Installation and attach empty storage to the EGI DataHub The installation of a new Oneprovider is performed using the onedatify installation script which will setup the components using Docker and Docker-compose.\nThis simple installation script can be generated from the EGI DataHub interface.\nFirstly, you need to login to the EGI DataHub and using the Data menu you either select an existing space or create a new one.\nSecondly, you can select on the space menu the Providers section and click on the Add Support button on the top right corner.\nYou should then select on the page the tab: Deploy your own provider\nIn order to obtain the PROVIDER_REGISTRATION_TOKEN that is part of the preconfigured command, you need to contact the EGI HelpDesk, selecting DataHub as support unit.\nPlease include in the request the following info:\nUse case for the installation of a Oneprovider connected to the EGI DataHub Name and the email of the Oneprovider admin domain name of the Oneprovider Run the command on the host With the obtained PROVIDER_REGISTRATION_TOKEN, paste the copied command in the terminal on the Oneprovider machine as superuser.\nIf necessary, the Onedatify script will ask for permission to install all necessary dependencies including Docker and Docker Compose.\nAfter the dependency installation is complete, the script will ask several questions and suggest default settings:\nThe progress can be monitored on a separate terminal using the following command:\nonedatify logs After the deployment is complete, a message will be shown, including connection details for the administration panel of the Oneprovider:\nThis administration panel at port 9443 can be used to administer the Oneprovider.\nInstallation and expose existing data to the EGI DataHub The installation of a new OneProvider to expose existing datasets to an EGI DataHub space is similar to the installation with an empty storage.\nWhen adding support to an existing or new space you should select from the EGI DataHub user interface the tab : Expose Existing dataset.\nPlease refer to the steps described in the Installation and attach empty storage to the EGI DataHub section to request a PROVIDER_REGISTRATION_TOKEN.\nOnce obtained, you will have to copy the command already configured with the correct parameters for the Onezone (datahub.egi.eu) and the space to join.\nRun the command on the host Paste the copied command in the terminal on the Oneprovider machine as superuser, and follow the instructions as for the case of an empty storage.\nThe only difference is that at the end of the installation and configuration process the existing files will be automatically imported to the OneProvider.\nYou can monitor the import activity from the administration panel at port 9443.\nAdditional configuration for EGI DataHub After completing the installation it might be necessary to add some specific configuration depending on the use case.\nStorage Import Storage import function is used to import files located on a storage and not added or modified directly by DataHub API, client or web interface. For example if another application access the storage directly as, for example in an NFS share. The file registration process creates the necessary metadata so that files that are added, removed or modified directly on the storage are reflected and accessible in the corresponding space. It is possible to configure the storage to automatically detect changes made on the storage using the continuous scan option or by manually triggering scans.\nIn general, there are three basic space usage scenarios to be considered before deciding to enable the automatic scan of the storage or to perform a single scan:\nAn initially empty space, modified only via DataHub -\u003e no need to enable the auto-scan or to perform a manual scan. A space with some initial data that has been imported once and then the space is only modified via DataHub -\u003e In this case the auto scan is not needed however it needs to somehow guard the storage from external modifications otherwise it will desynchronise with the space. E.g.: all changes should be done trough API, DataHub client or the web interface. A space that exposes a dataset from a storage that later is still dynamically changing outside of DataHub (when it is assumed that some modification can happen at some point) -\u003e in this case continuous scan is needed. Every storage import scan causes additional load on the Oneprovider. If the space is large and changes very often outside of Onedata, it might have a visible impact on the overall performance - but we are talking millions of files and hundreds of changes per second. Otherwise, having the continuous scan run in the background at the default value, with interval of 60 seconds should not cause any issues. Depending on the specific needs however, if the loads becomes too high or if the changes need to be applied timely it is possible to increase or decrease this interval respectively.\nIn the following screenshot is shown how to access the configuration for the storage import:\nThis configuration page is located under the “CLUSTER” tab in the main page. There we should select the cluster that is providing the space that we need to configure, Spaces and select the specific space that need to be configured. In the example shown, the “Auto storage import configuration” menu has been expanded and continuos scan enabled.\n","categories":"","description":"Documentation for installation/configuration of OneProvider to join EGI DataHub spaces","excerpt":"Documentation for installation/configuration of OneProvider to join …","ref":"/providers/datahub/oneprovider/","tags":"","title":"DataHub OneProvider"},{"body":"This section provides information on how to set up a Resource Centre providing cloud resources in the EGI infrastructure. Integration with FedCloud requires a working OpenStack installation as a pre-requirement. EGI supports any recent OpenStack version (tested from OpenStack Mitaka).\nThe following OpenStack services are expected to be available and accessible from outside the site:\nKeystone Nova Cinder Glance Neutron Swift (if providing Object Storage) The integration is performed by a set of EGI components that interact with the OpenStack services APIs:\nAuthentication of EGI users into your system is performed by configuring the native OpenID Connect support of Keystone cASO collects accounting data from OpenStack and uses SSM to send the records to the central accounting database on the EGI Accounting service (APEL). cASO and SSM are operated by the site. EGI runs the cloud-info-provider and cloudkeeper instances to provide information discovery and VM image synchronisation Installation options cASO and SSM releases can be be obtained from GitHub:\ncASO. cASO containers are available in the fedcloud-caso package from fedcloud-catchall-operations SSM. Open Ports Inbound The following services must be accessible to allow access to an OpenStack-based FedCloud site (default ports listed below, can be adjusted to your installation).\nPort Application Note 5000/TCP OpenStack/Keystone Authentication to your OpenStack. 8776/TCP OpenStack/cinder Block Storage management. 8774/TCP OpenStack/nova VM management. 9696/TCP OpenStack/neutron Network management. 9292/TCP OpenStack/glance VM Image management. Outbound The EGI Cloud components require the following outgoing connections open:\nPort Host Note 443/TCP msg.argo.grnet.gr ARGO Messaging System (used to send accounting records by SSM). 8443/TCP msg.argo.grnet.gr AMS authentication (used to send accounting records by SSM). Users Local Users In order to get accounting information from your OpenStack, cASO needs to be run with a user that is a member of the projects to extract accounting information from and it’s allowed to access identity:list_users and identity:list_projects in Keystone. Check cASO documentation for further information.\nFederated Users Regular user accounts will be managed by the Federated Identity features of OpenStack. These users are created into a specific OpenStack domain for every configured identity provider. All users within the egi.eu domain will have a unique username. For users whose community identity is managed by Check-in, this identifier is of the form \u003cuniqueID\u003e@egi.eu. The \u003cuniqueID\u003e portion is an opaque identifier issued by Check-in, for example:\n$ openstack domain list +----------------------------------+----------------------------------+---------+---------------------------------------------------------------+ | ID | Name | Enabled | Description | +----------------------------------+----------------------------------+---------+---------------------------------------------------------------+ | 0125ed0ebc8045a49ed8c34c2a78740d | 0125ed0ebc8045a49ed8c34c2a78740d | True | Auto generated federated domain for Identity Provider: egi.eu | | default | Default | True | The default domain | +----------------------------------+----------------------------------+---------+---------------------------------------------------------------+ $ openstack user list --domain 0125ed0ebc8045a49ed8c34c2a78740d +------------------------------------------------------------------+-------------------------------------------------------------------------+ | ID | Name | +------------------------------------------------------------------+-------------------------------------------------------------------------+ | 2c096b11a1410d44e3936fa40479ad26eaa649cfd6887f06b3c6669e5d6c03d0 | efb8534478028XXXXXXXXXXXXXXXfeed9766fafc@sram.surf.nl | | 933c692b53192e4d893e5ed5c026aa444acb4d75f6ee6c304422861207ce1ea5 | e9c37aa0d1XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX2867bc43581b835c@egi.eu | | d52112709a37975903576f80f37dde4604d1a227c53cb1fef43c45981673640c | 529a87e5ceXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXe714cb1309cc3907@egi.eu | +------------------------------------------------------------------+-------------------------------------------------------------------------+ If you have set the email of the user in the mapping, you will be able to also get this information:\n$ openstack user show d52112709a37975903576f80f37dde4604d1a227c53cb1fef43c45981673640c +---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ | domain_id | 0125ed0ebc8045a49ed8c34c2a78740d | | email | XXXX-redacted@example.com | | enabled | True | | federated | [{'idp_id': 'egi.eu', 'protocols': [{'protocol_id': 'openid', 'unique_id': '529a87e5ceXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXe714cb1309cc3907%40egi.eu'}]}] | | id | d52112709a37975903576f80f37dde4604d1a227c53cb1fef43c45981673640c | | name | 529a87e5ceXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXe714cb1309cc3907@egi.eu | | options | {} | | password_expires_at | None | +---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ Every VO has a VO identity card available via the Operations Portal, where you can also get contact information for the VO managers.\nVMs created by EGI’s Infrastructure Manager have additional metadata properties that can help to identify the workload:\n$ openstack server show 0f3e1420-4480-4bea-95f1-9920a70b324d -c properties -f yaml properties: eu.egi.cloud.orchestrator: es.upv.grycap.im eu.egi.cloud.orchestrator.id: 0afdc7ba-bf5d-11ed-9e89-86ce117c3fcf eu.egi.cloud.orchestrator.url: https://im.egi.eu/im eu.egi.cloud.orchestrator.user: __OPENID__XXXXXXredacted ","categories":"","description":"Integration with OpenStack\n","excerpt":"Integration with OpenStack\n","ref":"/providers/cloud-compute/openstack/","tags":"","title":"OpenStack"},{"body":"OpenStack providers in the EGI infrastructure offer services and features via OpenStack APIs, and the command-line interface (CLI), both integrated with EGI Check-in accounts.\nThe extensive OpenStack user documentation includes details on every OpenStack project, but most providers offer:\nKeystone, for identity Nova, for VM management Glance, for VM image management Cinder, for block storage Swift, for object storage Neutron, for network management Horizon, as a web dashboard The Horizon web-dashboard of the OpenStack providers can be used to manage and use services. It can be accessed using EGI Check-in credentials directly. Select EGI Check-In:\nOr OpenID Connect:\nOr egi.eu:\nIn the Authenticate using drop-down menu of the login screen.\nAdditionally you may need to select aai.egi.eu/auth/realms/egi as well:\nTip You can quickly find the dashboards of all providers in the EGI infrastructure that are accessible to you (use the correct VO) with the FedCloud Client:\n$ fedcloud endpoint list --service-type org.openstack.horizon --site ALL_SITES The same way you can also discover other types of resources, just use the correct resource type:\norg.openstack.horizon for dashboards org.openstack.nova for virtual machines org.openstack.swift for object storage Note For more advanced information discovery, including resources not based on OpenStack deployments, check out the EGI architecture summary. ","categories":"","description":"How to interact with OpenStack providers\n","excerpt":"How to interact with OpenStack providers\n","ref":"/users/getting-started/openstack/","tags":"","title":"Using OpenStack Providers"},{"body":"EGI and the Pangeo community in Europe have forged a collaboration to provide deployments of the Pangeo ecosystem using the computing resources offered by the European Open Science Cloud (EOSC). Read on to learn more about how to get involved.\nAbout the Pangeo community in Europe Many scientific domains have been transformed into data-driven disciplines. Research projects increasingly and crucially depend on the exchange and processing of data and have developed platforms tailored to their needs. Pangeo, a world-wide community driven platform initially developed for Geoscience, has a huge potential to become a common gateway able to leverage a wide variety of infrastructures and data providers for various science fields.\nPublic pangeo deployments that are providing fast access to large amounts of data and compute resources are all USA-based and members from the Pangeo community in Europe do not have a shared platform where scientists or technologists can exchange know-how. The main objective of this collaboration with EGI is to demonstrate how to deploy and use Pangeo on EOSC and underlined the benefits for the European community.\nHow to get involved EGI and the Pangeo community in Europe are interacting openly via a GitHub repository: pangeo-eosc. Visit this repository to find publications coming out of this collaboration and guides on how to get started as a user or as a provider.\n","categories":"","description":"How to to use the EGI infrastructure with Pangeo\n","excerpt":"How to to use the EGI infrastructure with Pangeo\n","ref":"/users/getting-started/communities/pangeo/","tags":"","title":"Pangeo"},{"body":"Grid Accounting Grid accounting can either be sent as Individual or Summary job records. Summary Sync records can be sent in either case, and provide a mechanism to validate that all records stored locally at a site have been published.\nIndividual Job Records and Messages Summary Job Records and Messages Summary Sync Records and Messages ","categories":"","description":"EGI Accounting record and message formats","excerpt":"EGI Accounting record and message formats","ref":"/internal/accounting/record-and-message-formats/","tags":"","title":"Record and Message Formats"},{"body":"What is it? Replay allows the re-creation of a custom computing environment for reproducible execution of notebooks (and potentially many other types of applications). Users who create their own notebooks in the EGI Notebooks to analyze data can easily create a shareable link for those notebooks in the form of a GitHub repository. Based on this link, anyone can then reproduce the same data analysis using the link in the EGI Replay service.\nThe service builds on BinderHub, an Open Source tool that allows to build docker images from a Git repository and then makes them available through your browser.\nEGI Replay offers a service similar to the publicly accessible mybinder.org site. However, EGI Replay has the following additional features:\nAccess with academic user accounts: login via Check-in that’s connected to eduGAIN and social media accounts. Access to scalable storage: selected storage spaces of EGI DataHub are directly available under the datahub folder, simplifying the access to shared data from Binder notebooks. Guaranteed capacity: environments have 2GB of RAM guaranteed and can reach 6GB as maximum. Persistent sessions: There is no hard limit on the session time per user, although sessions will be shut down automatically after 1 hour of inactivity (see session limitations at the public mybinder.org service). Access to the rest of EGI services: a personal access token is available in the Replay session to interact with the rest of the EGI infrastructure. Community Replay environments: User communities can have their customized Replay service instance from EGI, with extra features as requested (such as access to GPUs, integration with community specific data repositories and services). EGI offers consultancy and support the setup of these instances, and provides operational oversight for them. EGI Replay has been presented in the EGI Webinars. See slides and recording.\nReproducible research Replay facilitates the sharing and reproducibility of digital data analysis:\nUsers can define their computational analysis in the EGI Notebooks service. Once the notebook is ready for publishing, it can be shared in a GitHub repository. Optionally, users can use the Zenodo-GitHub integration for generating DOIs that can be cited in publications and can be discovered by fellow researchers Anyone can use the link to the GitHub repository or Zenodo DOI to reproduce the computational analysis in EGI Replay. Access to the service EGI’s Replay has the same access conditions as the centrally operated Notebooks service from EGI. Before using the service, you need to have an EGI account and be a member of one of the supported resource pools (alias Virtual Organisations). Follow the instructions on the EGI Replay login page for access.\nThe default capacity allocated for users includes up to 4 vCPU cores and 6GB of RAM for every user.\nCreating a Binder repository Replay starts from a code repository that contains the code or notebook you’d like to run and a set of configuration files that specify what’s the exact computational environment your code needs to run.\nReplay then creates a reproducible container using repo2docker, and generates a user session to interact with the container in the browser.\nThe configuration for building the container supports specifying conda environments; installing Python, R and Julia environments; installing additional OS packages; and even complete custom Dockerfiles to bring any application to the system. The code repository can be hosted on popular git hosting platforms like GitHub and GitLab and can also be referenced with a DOI from Zenodo, FigShare or Dataverse. You can learn more on the configuration of your repository with Replay at the Binder user documentation.\nYou can start by forking the EGI-Federation/binder-example GitHub repository for creating your own reproducible environment. To run this directly on EGI’s Replay click on the button below:\nYou can create such link to share your notebooks from the Replay interface, as shown in the screenshot below, you can copy the URL shown when the building is in progress:\nThe binder examples organisation on GitHub contains more sample repositories for common configurations that can help you getting started.\nAccessing data Your notebooks running in Replay have outgoing internet connectivity, so you can connect to external services to bring data in for analysis or deposing the notebooks output.\nEvery session that you start will also provide access to your spaces in the DataHub under a folder named datahub. Only those spaces configured to be mounted locally will be made available automatically. Check the documentation for the Notebook’s DataHub support for more information.\nErrors when launching You can only have one notebook server instance running at any given time, if you try to launch a second instance while there is already one running you will get an error like shown in the screenshot:\nIn that case, you can either stop your existing server or connect to it from your Replay JupyterHub home:\n","categories":"","description":"Custom reproducible computing environments for notebooks\n","excerpt":"Custom reproducible computing environments for notebooks\n","ref":"/users/dev-env/replay/","tags":"","title":"Replay"},{"body":"In this section you can find the common operational activities related to keep the service available to our users.\nInitial set-up Notebooks VO The resources used for the Notebooks deployments are managed with the vo.notebooks.egi.eu VO. Operators of the service should join the VO, check the entry at the operations portal and at AppDB.\nClients installation In order to manage the resources you will need these tools installed on your client machine:\nfedcloudclient for discovering sites and managing tokens, terraform to create the VMs at the providers, ansible to configure the VMs and install kubernetes at the providers, terraform-inventory to get the list of hosts to use from terraform. Get the configuration repository All the configuration of the notebooks is stored at a git repository available in GitHub. Start by cloning the repo:\ngit clone https://github.com/EGI-Federation/notebooks-operations Kubernetes We use terraform and ansible to build the cluster at one of the EGI Cloud providers. If you are building the cluster for the first time, create a new directory on your local git repository from the template, add it to the repo, and get terraform ready:\ncp -a template \u003cnew provider\u003e git add \u003cnew provider\u003e cd \u003cnew provider\u003e/terraform terraform init Using the fedcloud you can get set the right environment for interacting with the OpenStack APIs of a given site:\neval \"$(fedcloud site show-project-id --site CESGA --vo vo.notebooks.egi.eu)\" Whenever you need to get a valid token for the site and VO, you can obtain it with:\nOS_TOKEN=$(fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu \\ token issue -c id -f value) First get the network IDs and pool to use for the site:\n$ fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu network list +--------------------------------------+-------------------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-------------------------+--------------------------------------+ | 1aaf20b6-47a1-47ef-972e-7b36872f678f | net-vo.notebooks.egi.eu | 6465a327-c261-4391-a0f5-d503cc2d43d3 | | 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 | public00 | 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 | +--------------------------------------+-------------------------+--------------------------------------+ In this case we will use public00 as the pool for public IPs and 1aaf20b6-47a1-47ef-972e-7b36872f678f as the network ID. Check with the provider which is the right network to use. Use these values in the terraform.tfvars file:\nip_pool = \"public00\" net_id = \"1aaf20b6-47a1-47ef-972e-7b36872f678f\" You may want to check the right flavors for your VMs and adapt other variables in terraform.tfvars. To get a list of flavors you can use:\n$ fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu flavor list +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ | 26d14547-96f2-4751-a686-f89a9f7cd9cc | cor4mem8hd40 | 8192 | 40 | 0 | 4 | True | | 42eb9c81-e556-4b63-bc19-4c9fb735e344 | cor2mem2hd20 | 2048 | 20 | 0 | 2 | True | | 4787d9fc-3923-4fc9-b770-30966fc3baee | cor4mem4hd40 | 4096 | 40 | 0 | 4 | True | | 58586b06-7b9d-47af-b9d0-e16d49497d09 | cor24mem62hd60 | 63488 | 60 | 0 | 24 | True | | 635c739a-692f-4890-b8fd-d50963bff00e | cor1mem1hd10 | 1024 | 10 | 0 | 1 | True | | 6ba0080d-d71c-4aff-b6f9-b5a9484097f8 | small | 512 | 2 | 0 | 1 | True | | 6e514065-9013-4ce1-908a-0dcc173125e4 | cor2mem4hd20 | 4096 | 20 | 0 | 2 | True | | 85f66ce6-0b66-4889-a0bf-df8dc23ee540 | cor1mem2hd10 | 2048 | 10 | 0 | 1 | True | | c4aa496b-4684-4a86-bd7f-3a67c04b1fa6 | cor24mem50hd50 | 51200 | 50 | 0 | 24 | True | | edac68c3-50ea-42c2-ae1d-76b8beb306b5 | test-bigHD | 4096 | 237 | 0 | 2 | True | +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ Finally, ensure your public ssh key is also listed in the cloud-init.yaml file and then you are ready to deploy the cluster with:\nterraform apply Your VMs are up and running, it's time to get kubernetes configured and running with Ansible.\nThe following Ansible role needs to be installed first:\nansible-galaxy install grycap.kubernetes and then:\ncd .. # you should be now in \u003cnew provider\u003e ANSIBLE_TRANSFORM_INVALID_GROUP_CHARS=silently TF_STATE=./terraform \\ ansible-playbook --inventory-file=$(which terraform-inventory) \\ playbooks/k8s.yaml Interacting with the cluster As the master will be on a private IP, you won't be able to directly interact with it, but you can still ssh into the VM using the ingress node as a gateway host (you can get the different hosts with TF_STATE=./terraform terraform-inventory --inventory)\n$ ssh -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -q egi@\u003cingress ip\u003e\" \\ -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null egi@\u003cmaster ip\u003e egi@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 33m v1.15.7 k8s-nfs Ready \u003cnone\u003e 16m v1.15.7 k8s-w-ingress Ready \u003cnone\u003e 16m v1.15.7 egi@k8s-master:~$ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE certs-man 2 Wed Jan 8 15:56:58 2020 DEPLOYED cert-manager-v0.11.0 v0.11.0 cert-manager cluster-ingress 3 Wed Jan 8 15:56:53 2020 DEPLOYED nginx-ingress-1.7.0 0.24.1 kube-system nfs-provisioner 3 Wed Jan 8 15:56:43 2020 DEPLOYED nfs-client-provisioner-1.2.8 3.1.0 kube-system Modifying/Destroying the cluster You should be able to change the number of workers in the cluster and re-apply terraform to start them and then execute the playbook to get them added to the cluster.\nAny changes in the master, NFS or ingress VMs should be done carefully as those will probably break the configuration of the Kubernetes cluster and of any application running on top.\nDestroying the cluster can be done with a single command:\nterraform destroy Notebooks deployments Once the k8s cluster is up and running, you can deploy a notebooks instance. For each deployment you should create a file in the deployments directory following the template provided:\ncp deployments/hub.yaml.template deployments/hub.yaml Each deployment will need a domain name pointing to your ingress host, you can create one at the FedCloud dynamic DNS service.\nThen you will need to create an OpenID Connect client for EGI Check-in to authorise users into the new deployment. You can create a client by going to the EGI Federation Registry. You can find more information about registering an OIDC Client in EGI Check-in guide for SPs Use the following as redirect URL: https://\u003cyour host domain name\u003e/hub/oauth_callback.\nThen add offline_access to the list of scopes and submit the request. After the approval of the Service request, save the client and take note of the client ID and client secret for later.\nFinally, you will also need 3 different random strings generated with openssl rand -hex 32 that will be used as secrets in the file describing the deployment.\nGo and edit the deployment description file to add this information (search for # FIXME NEEDS INPUT in the file to quickly get there)\nFor deploying the notebooks instance we will also use ansible:\nANSIBLE_TRANSFORM_INVALID_GROUP_CHARS=silently \\ TF_STATE=./terraform ansible-playbook \\ --inventory-file=$(which terraform-inventory) playbooks/notebooks.yaml The first deployment trial may fail due to a timeout caused by the downloading of the container images needed. You can retry after a while to re-deploy.\nIn the master you can check the status of your deployment (the name of the deployment will be the same as the name of your local deployment file):\n$ helm status hub LAST DEPLOYED: Thu Jan 9 08:14:49 2020 NAMESPACE: hub STATUS: DEPLOYED RESOURCES: ==\u003e v1/ServiceAccount NAME SECRETS AGE hub 1 6m46s user-scheduler 1 3m34s ==\u003e v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hub ClusterIP 10.100.77.129 \u003cnone\u003e 8081/TCP 6m46s proxy-public NodePort 10.107.127.44 \u003cnone\u003e 443:32083/TCP,80:30581/TCP 6m45s proxy-api ClusterIP 10.103.195.6 \u003cnone\u003e 8001/TCP 6m45s ==\u003e v1/ConfigMap NAME DATA AGE hub-config 4 6m47s user-scheduler 1 3m35s ==\u003e v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hub-db-dir Pending managed-nfs-storage 6m46s ==\u003e v1/ClusterRole NAME AGE hub-user-scheduler-complementary 3m34s ==\u003e v1/ClusterRoleBinding NAME AGE hub-user-scheduler-base 3m34s hub-user-scheduler-complementary 3m34s ==\u003e v1/RoleBinding NAME AGE hub 6m46s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE continuous-image-puller-flf5t 1/1 Running 0 3m34s continuous-image-puller-scr49 1/1 Running 0 3m34s hub-569596fc54-vjbms 0/1 Pending 0 3m30s proxy-79fb6d57c5-nj8n2 1/1 Running 0 2m22s user-scheduler-9685d654b-9zt5d 1/1 Running 0 3m30s user-scheduler-9685d654b-k8v9p 1/1 Running 0 3m30s ==\u003e v1/Secret NAME TYPE DATA AGE hub-secret Opaque 3 6m47s ==\u003e v1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE continuous-image-puller 2 2 2 2 2 \u003cnone\u003e 3m34s ==\u003e v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hub 1 1 1 0 6m45s proxy 1 1 1 1 6m45s user-scheduler 2 2 2 2 3m32s ==\u003e v1/StatefulSet NAME DESIRED CURRENT AGE user-placeholder 0 0 6m44s ==\u003e v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE jupyterhub notebooktest.fedcloud-tf.fedcloud.eu 80, 443 6m44s ==\u003e v1beta1/PodDisruptionBudget NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE hub 1 N/A 0 6m48s proxy 1 N/A 0 6m48s user-placeholder 0 N/A 0 6m48s user-scheduler 1 N/A 1 6m47s ==\u003e v1/Role NAME AGE hub 6m46s NOTES: Thank you for installing JupyterHub! Your release is named hub and installed into the namespace hub. You can find if the hub and proxy is ready by doing: kubectl --namespace=hub get pod and watching for both those pods to be in status 'Running'. You can find the public IP of the JupyterHub by doing: kubectl --namespace=hub get svc proxy-public It might take a few minutes for it to appear! Note that this is still an alpha release! If you have questions, feel free to 1. Read the guide at https://z2jh.jupyter.org 2. Chat with us at https://gitter.im/jupyterhub/jupyterhub 3. File issues at https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues Updating a deployment Just edit the deployment description file and run ansible again. The helm will be upgraded at the cluster.\n","categories":"","description":"Getting the service up and running","excerpt":"Getting the service up and running","ref":"/providers/notebooks/operations/","tags":"","title":"Service Operations"},{"body":"This page contains information about connecting services to EGI Check-in in order to allow user login through Check-in and to receive users’ attributes. Check-in is connected to a wide range of academic and social Identity Providers that users can choose from in order to access your service.\nServices eligible for integration EGI Operations, as owner of the Check-in service, must approve every request for integration of new services with Check-in. The approval (or non-approval) is based on some prerequisites, the relevance of the service for the EGI community and the available resources to support the integration. The prerequisites are described in the following sections.\nEGI at any time can prevent a service provider to access the Check-in service\nServices federated in EGI All the services that are operated by Resource Providers federated in EGI federation and that abide to the RC OLA, and consequently to the relevant security policies of EGI, can be connected with Check-in. Fulfilling all the relevant EGI policies makes the service eligible in receiving attributes released by Check-in.\nServices not federated in EGI A service not part of the EGI federation can be integrated with Check-in if the organisation providing the service complies with the EGI security requirements relevant to the service providers.\nBy accepting the policies a service provider assures that they will operate the service in good faith, without deliberately exposing the user to security risks, without claiming intellectual property on the data owned by the user, and protecting sensitive data generated by the interaction of the user with the service.\nThe policies that the service provider will have to accept are available in the EGI Policies and procedures page and specifically are:\nEGI Security Policy Service Operations Security Policy Traceability and Logging Policy Security Incident Response Policy Policy on the Processing of Personal Data Service Provider integration workflow To integrate your Service Provider with the EGI Check-in service, you need to create a registration request using the EGI Federation Registry Portal. You can also use the Federation Registry portal to request the reconfiguration or deregistration of an existing deployed service. Service registration requests typically require approval by an administrator. Please refer to the Federation Registry Documentation for more information.\nThe integration follows a two-step process:\nRegister your Service Provider and test integration with the demo instance of EGI Check-in by selecting the “Demo” integration environment during registration through the EGI Federation Registry Portal. Service registration requests require approval by an administrator. The review process for the demo environment involves primarily the technical aspects of the service configuration. However, moving the service to production requires compliance with all the eligibility criteria (see Step 2). The demo instance allows for testing authentication and authorisation through the academic and social Identity Providers connected to Check-in without affecting the production Check-in service. Note that while the demo instance has identical functionality to the production instance, no information is shared between the two systems. You can also test new features of Check-in that are not available in production yet, by registering your Service Provider and testing integration with the development instance of Check-in. In the development instance service requests can be self-reviewed without the need to wait for approval from an administrator. As with the demo instance, the development instance allows for testing authentication and authorisation without affecting the production Check-in service. NB: The development environment is intended for testing the latest features of Check-in and may differ from the demo and production environments in terms of stability and functionality. Keep in mind that the supported Identity Providers in the development instance are limited. Therefore, we recommend using any of the social identity providers or the EGI SSO to test the login workflow when using the development instance. Register your Service Provider with the production instance of EGI Check-in by selecting the “Production” integration environment during registration through the EGI Federation Registry Portal. The production instance allows access to your service through the academic and social Identity Providers connected to Check-in. This requires that your service meets all the eligibility criteria and that integration has been thoroughly tested during Step 1. General Information EGI Check-in supports two authentication and authorisation protocols that you can choose from:\nSecurity Assertion Markup Language (SAML) 2.0 OpenID Connect - an extension to OAuth 2.0 Service providers should ensure that a proper authorisation model is put in place: if low assurance accounts, like those coming from social media identity providers, are granted access without any vetting, it may lead to an abuse of their service.\nRegardless of which of the two protocols you are going to use, you need to provide the following information to connect your service to EGI Check-in:\nName of the service (in English and optionally in other languages supported by the service) Short description of the service Site (URL) for localised information about the service; the content found at the URL SHOULD provide more complete information than what provided by the description Contact information of the following types: Helpdesk/Support contact information (for redirecting user) Administrative Technical Security/incident response Privacy statement URL: The privacy policy is used to document the data collected and processed by the service. You can use the Privacy Policy template Logo URL (optional for showing in catalogues); if provided, logos SHOULD: use a transparent background where appropriate to facilitate the usage of logos within a user interface use PNG, or GIF (less preferred), images use HTTPS URLs in order to avoid mixed-content warnings within browsers have a size larger than 40000 and smaller than 50000 characters when encoded in base64 Country of the service Compliance with the EGI Policies and the REFEDS Data Protection Code of Conduct The most important URLs for each environment are listed in the table below but more information can be found in the protocol-specific sections that follow.\nProduction Demo Development Protocol Production environment SAML https://aai.egi.eu/auth/realms/egi/protocol/saml/descriptor OpenID Connect https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration Protocol Demo environment SAML https://aai-demo.egi.eu/auth/realms/egi/protocol/saml/descriptor OpenID Connect https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration Protocol Development environment SAML https://aai-dev.egi.eu/auth/realms/egi/protocol/saml/descriptor OpenID Connect https://aai-dev.egi.eu/auth/realms/egi/.well-known/openid-configuration SAML Service Provider To enable federated access to a web-based application, you can connect to the EGI Check-in IdP as a SAML Service Provider (SP). Users of the application will be redirected to Check-in in order to log in, and Check-in can authenticate them using any of the supported backend authentication mechanisms, such as institutional IdPs registered with eduGAIN or Social Providers. Once the user is authenticated, EGI Check-in will return a SAML assertion to the application containing information about the authenticated user.\nMetadata registration SAML authentication relies on the use of metadata. Both parties (you as a SP and the EGI Check-in IdP) need to exchange metadata in order to know and trust each other. The metadata include information such as the location of the service endpoints that need to be invoked, as well as the certificates that will be used to sign SAML messages. The format of the exchanged metadata should be based on the XML-based SAML 2.0 specification. Usually, you will not need to manually create such an XML document, as this is automatically generated by all major SAML 2.0 SP software solutions (e.g., Keycloak, Shibboleth, SimpleSAMLphp, and mod_auth_mellon). It is important that you serve your metadata over HTTPS using a browser-friendly SSL certificate, i.e. issued by a trusted certificate authority.\nYou can get the metadata of the EGI Check-in IdP Proxy on a dedicated URL that depends on the integration environment being used:\nProduction Demo Development Instance Production environment Keycloak-based EGI Check-in IdP SAML Metadata URL: https://aai.egi.eu/auth/realms/egi/protocol/saml/descriptorSAML entity ID: https://aai.egi.eu/auth/realms/egi Legacy EGI Check-in IdP SAML Metadata URL: https://aai.egi.eu/proxy/saml2/idp/metadata.phpSAML entity ID: https://aai.egi.eu/proxy/saml2/idp/metadata.php Instance Demo environment Keycloak-based EGI Check-in IdP SAML Metadata URL: https://aai-demo.egi.eu/auth/realms/egi/protocol/saml/descriptorSAML entity ID: https://aai-demo.egi.eu/auth/realms/egi Legacy EGI Check-in IdP SAML Metadata URL: https://aai-demo.egi.eu/proxy/saml2/idp/metadata.phpSAML entity ID: https://aai-demo.egi.eu/proxy/saml2/idp/metadata.php Instance Development environment Keycloak-based EGI Check-in IdP SAML Metadata URL: https://aai-dev.egi.eu/auth/realms/egi/protocol/saml/descriptorSAML entity ID: https://aai-dev.egi.eu/auth/realms/egi Legacy EGI Check-in IdP SAML Metadata URL: https://aai-dev.egi.eu/proxy/saml2/idp/metadata.phpSAML entity ID: https://aai-dev.egi.eu/proxy/saml2/idp/metadata.php To register your SAML SP, you must submit a service registration request through the Federation Registry. Your request should include essential information about your service, as outlined in the General Information section, along with your SP’s metadata URL and entity ID.\nMetadata Metadata provided by your SP should contain a descriptive name of the service that your SP represents in at least English. It is recommended to also provide the name in other languages which are commonly used in the geographic scope of the deployment. The name should be placed in the \u003cmd:ServiceName\u003e in the \u003cmd:AttributeConsumingService\u003e container.\nIt is recommended that your SP metadata contains:\nan \u003cmd:SPSSODescriptor\u003e role element containing an AuthnRequestsSigned and an WantAssertionsSigned attribute set to true at least one \u003cmd:AssertionConsumerService\u003e endpoint element at least one \u003cmd:KeyDescriptor\u003e element whose use attribute is omitted or set to encryption an \u003cmd:Extensions\u003e element at the role level containing an \u003cmdui:UIInfo\u003e extension element containing the child elements \u003cmdui:DisplayName\u003e, \u003cmdui:Logo\u003e, and \u003cmdui:PrivacyStatementURL\u003e an \u003cmdattr:EntityAttributes\u003e extension element for signaling Subject Identifier requirements with previously prescribed content an \u003cmd:ContactPerson\u003e element with a contactType of support and/or a \u003cmd:ContactPerson\u003e element with a contactType of technical. The \u003cmd:ContactPerson\u003e element(s) should contain at least one \u003cmd:EmailAddress\u003e. The support address may be used for generic support questions about the service, while the technical contact may be contacted regarding technical interoperability problems. The technical contact must be responsible for the technical operation of the service represented by your SP. If the SP supports the Single Logout profile, then its metadata MUST contain (within its \u003cmd:SPSSODescriptor\u003e role element):\nat least one \u003cmd:KeyDescriptor\u003e element whose use attribute is omitted or set to signing at least one \u003cmd:SingleLogoutService\u003e endpoint element (this MAY be omitted if the SP solely issues \u003csamlp:LogoutRequest\u003e messages containing the \u003caslo:Asynchronous\u003e extension SAML2ASLO) Attributes The EGI Check-in IdP is guaranteed to release a minimal subset of the REFEDS R\u0026S attribute bundle to connected Service Providers without administrative involvement, subject to user consent. The following attributes constitute a minimal subset of the R\u0026S attribute bundle:\nCommunity User Identifier (CUID) which is a globally unique, opaque, persistent and non-reassignable identifier identifying the user (voPersonID). For users whose community identity is managed by Check-in, this identifier is of the form \u003cuniqueID\u003e@egi.eu. The \u003cuniqueID\u003e portion is an opaque identifier issued by Check-in. Email address (mail) Display name (displayName) OR (givenName AND sn) A more extensive list of all the attributes that may be made available to Service Providers is included in the User Attribute section.\nAttribute-based authorisation As mentioned in the General Information, omitting authorisation checks may lead to abuse of the service.\nEGI Check-in provides information about the authenticated user that may be used by Service Providers in order to control user access to resources. This information is provided by the EGI Check-in IdP in the SAML attribute assertion. The table below lists the SAML attributes that are relevant for user authorisation:\nDescription SAML Attribute VO/group membership/roles of the authenticated user eduPersonEntitlement Capabilities eduPersonEntitlement GOCDB roles eduPersonEntitlement Identity Assurance eduPersonAssurance Service Provider Migration to Keycloak The migration guide below applies to SAML Service Providers (SPs) registered in the Development, Demo and Production environments of Check-in.\nDevelopment and Demo: Beginning March 9, 2023, SAML SPs using the legacy Check-in IdP metadata will no longer be supported.\nProduction: Beginning December 12, 2023, SAML SPs using the legacy Check-in IdP metadata will no longer be supported.\nHow to Migrate your Service to Keycloak If your SAML SP is not yet registered in the Federation Registry, you will need to submit a service registration request. This request should include essential information about your service, as described in the General Information section, in addition to your SAML SP’s metadata URL and entity ID.\nIf your SAML SP is already registered, please ensure the accuracy of the information under the General tab, particularly the policy and contact details, as well as the Protocol Specific tab. Pay special attention to the new Requested Attributes section, which allows you to manage the user attributes available through Check-in.\nImportant If your SAML SP relies on experimental features of Check-in which are only available in the Development environment, you will need to re-register your SP through the Federation Registry using the “Copy Service” functionality. New Identity Provider Metadata The first thing you need to do is to update the IdP metadata URL in the SP configuration, according to the Metadata registration section.\nNew Attributes When migrating your SP to the Keycloak-based EGI Check-in IdP, please be aware that some attributes will no longer be supported. These deprecated attributes will be replaced by new ones, as detailed in the table below:\nDeprecated Attributes New Attributes distinguishedName voPersonCertificateDN eduPersonScopedAffiliation voPersonExternalAffiliation eduPersonUniqueId voPersonID Note The values of the deprecated attributes will remain unchanged; only the attribute names will be different. Ensure that you update the attribute mapping in your SP configuration to align with the new attribute names.\nNameID Use of \u003csaml:NameID\u003e elements is intended for the Single Logout profile and is not suitable for long-term identification of users. For user identification, you should use the voPersonID SAML attribute, as detailed in the User attributes section. Your SAML SP should explicitly specify in its metadata the NameID formats it supports from the following options:\nPersistent (urn:oasis:names:tc:SAML:2.0:nameid-format:persistent) Transient (urn:oasis:names:tc:SAML:2.0:nameid-format:transient) Email address (urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress) Unspecified (urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified) Otherwise, Keycloak will assign the unspecified (urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified) NameID format to your SP.\nReferences Keycloak Service Provider Documentation Shibboleth Service Provider Documentation SimpleSAMLphp Service Provider QuickStart Simple SAML 2.0 service provider with mod_auth_mellon Apache module Example SAML Service Provider Configurations Keycloak If you are using Keycloak as an SAML Service Provider, then you need to follow the steps below in order to register EGI Check-in as an Identity Provider:\nAccess the administrator console of your Keycloak instance and navigate to “Identity Providers” and then select “SAML v2.0”\nIn the next page, complete the following fields:\nAlias: egi-check-in-saml Display name: EGI Check-in Scroll down, and complete the rest options:\nSAML entity descriptor: https://aai.egi.eu/auth/realms/egi/protocol/saml/descriptor And then click on the “Add” button.\nAfter adding EGI Check-in IdP, scroll down to the “SAML settings” section and edit the following options:\nPrincipal type: Attribute [Name] Principal attribute: urn:oid:1.3.6.1.4.1.25178.4.1.6 HTTP-POST binding response: On HTTP-POST binding for AuthnRequest: Off HTTP-POST binding logout: Off Want AuthnRequests signed: Off Want Assertions signed: Off Want Assertions encrypted: On Validate Signatures: On Note urn:oid:1.3.6.1.4.1.25178.4.1.6 is the OID presentation of the voPersonID attribute. Then, scroll down to the “Advanced settings” section and enable the “Trust Email” option and click on “Save”.\nNext, you will need to create a mapper for each attribute that your Service Provider will request from EGI Check-in Proxy. Go to the “Mappers” tab and then click on “Add Mapper”.\nFor the voPersonID attribute you will need to add the following options:\nName: voPersonID Sync Mode Override: import Mapper Type: Attribute Importer Attribute Name: urn:oid:1.3.6.1.4.1.25178.4.1.6 Friendly Name: voPersonID Name Format: ATTRIBUTE_FORMAT_URI User Attribute Name: voPersonID And for the eduperson_entitlement claim:\nName: eduPersonEntitlement Sync Mode Override: Inherit Mapper Type: Attribute Importer Attribute Name: urn:oid:1.3.6.1.4.1.5923.1.1.1.7 Friendly Name: eduPersonEntitlement Name Format: ATTRIBUTE_FORMAT_URI User Attribute Name: eduPersonEntitlement Note For other attributes, create a mapper similar to the eduPersonEntitlement mapper. OpenID Connect Service Provider Service Providers can be integrated with EGI Check-in using OpenID Connect (OIDC) as an alternative to the SAML2 protocol. To allow this, the EGI Check-in IdP provides an OpenID Connect (OAuth2) API based on Keycloak, which has been certified by the OpenID Foundation. Interconnection with the EGI Check-in OpenID Provider allows users to sign in using any of the supported backend authentication mechanisms, such as institutional IdPs registered with eduGAIN or Social Providers. Once the user has signed in, EGI Check-in can return OIDC Claims containing information about the authenticated user.\nClient registration Before your service can use the EGI Check-in OpenID Provider for user login, you must submit a service registration request using Federation Registry in order to obtain OAuth 2.0 credentials. The client configuration should include the general information about your service, as described in General Information section.\nObtaining OAuth 2.0 credentials You need OAuth 2.0 credentials, which typically include a client ID and client secret, to authenticate users through the EGI Check-in OpenID Provider.\nYou can specify the client ID and secret when creating/editing your client or let them being automatically generated during registration (recommended).\nTo find the ID and secret of your client, do the following:\nSelect your client from the Manage Services Page. Look for the Client ID in the Protocol tab. Select the Display/edit client secret: option from the Protocol tab. Note You can copy these values using the green copy button next to the desired field. Redirection URIs OpenID Connect Services MUST pre-register one or more Redirection URI(s) to which authentication responses from EGI Check-in will be sent. EGI Check-in utilises exact matching of the redirect URI specified in an authentication request against the pre-registered URIs OAuth2-BCP, with the matching performed as described in RFC3986 (Simple String Comparison). Redirection URIs MUST use the schemata defined in Section 3.1.2.1 of the OIDC-Core specification. Note that the Redirection URI MUST use the https scheme; the use of http Redirection URIs is only allowed in the development environment.\nClaims The EGI Check-in UserInfo Endpoint is an OAuth 2.0 Protected Resource that returns specific information about the authenticated end user as Claim Values. To obtain the requested Claims about the end user, the Client makes a request to the UserInfo Endpoint using an Access Token obtained through OpenID Connect Authentication. The scopes associated with the Access Token used to access the EGI Check-in UserInfo Endpoint will determine what Claims will be released. These Claims are represented by a JSON object that contains a collection of name and value pairs for the Claims.\nThe following scope values can be used to request Claims from the EGI Check-in UserInfo Endpoint:\nScope Claims openid sub voperson_id voperson_id profile namegiven_namefamily_namepreferred_username email emailemail_verifiedvoperson_verified_email aarc namegiven_namefamily_namepreferred_usernameemailemail_verifiedvoperson_verified_emailvoperson_certificate_dnvoperson_certificate_issuer_dnvoperson_external_affiliationvoperson_id eduperson_entitlement eduperson_entitlement voperson_certificate voperson_certificate_dnvoperson_certificate_issuer_dn voperson_external_affiliation voperson_external_affiliation A more extensive list of all the attributes that may be made available to Service Providers is included in the User Attribute section.\nGrant Types Check-in supports the following OpenID Connect/OAuth2 grant types:\nAuthorization Code: used by Web Apps executing on a server. Token Exchange: used by clients to request and obtain security tokens in support of delegated access to resources. Device Code: used by devices that lack a browser to perform a user-agent based OAuth flow. Client credentials: used by clients to obtain an access token outside of the context of a user. Such an access token is typically used by clients to access resources about themselves rather than to access a user’s resources. Authorization Code The Authorization Code Flow returns an Authorization Code to the Client, which can then exchange it for an ID Token and an Access Token directly. This provides the benefit of not exposing any tokens to the User Agent and possibly other malicious applications with access to the User Agent. The Authorization Server can also authenticate the Client before exchanging the Authorization Code for an Access Token. The Authorization Code flow is suitable for Clients that can securely maintain a Client Secret between themselves and the Authorization Server.\nAuthorization Code Flow Steps The Authorization Code Flow goes through the following steps.\nClient prepares an Authentication Request containing the desired request parameters. Client sends the request to the Authorization Server. Authorization Server Authenticates the end user. Authorization Server obtains end user Consent/Authorization. Authorization Server sends the end user back to the Client with an Authorization Code. Client requests a response using the Authorization Code at the Token Endpoint. Client receives a response that contains an ID Token and Access Token in the response body. Client validates the ID token and retrieves the end user’s Subject Identifier. Authentication Request The request parameters of the Authorization Endpoint are:\nclient_id: ID of the client that ask for authentication to the Authorization Server. redirect_uri: URI to which the response will be sent. scope: A list of attributes that the application requires. state: Opaque value used to maintain state between the request and the callback. response_type: value that determines the authorization processing flow to be used. For Authorization Code grant set response_type=code. This way the response will include an Authorization Code. Example request:\nHTTP/1.1 302 Found Location: ${AUTHORIZATION_ENDPOINT}? response_type=code \u0026scope=openid%20profile%20email \u0026client_id=s6BhdRkqt3 \u0026state=af0ifabcd \u0026redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb Note You can find the AUTHORIZATION_ENDPOINT in the Endpoints table. Example response:\nHTTP/1.1 302 Found Location: https://client.example.org/cb? code=SplxlOBeZQQYbYS6WxSbIA \u0026state=af0ifabcd Token Request A Client makes a Token Request by presenting its Authorization Grant (in the form of an Authorization Code) to the Token Endpoint using the grant_type value authorization_code, as described in Section 4.1.3 of OAuth 2.0 RFC6749. If the Client is a Confidential Client, then it MUST authenticate to the Token Endpoint using the authentication method registered for its client_id. The Client sends the parameters to the Token Endpoint using the HTTP POST method and the Form Serialization.\nThe parameters that are present in the token request are described in the table below:\nParameter Presence Values grant_type Required authorization_code code Required The value of the code in the response from Authorization Endpoint redirect_uri Required URI to which the response will be sent (must be the same as the request to Authorization Endpoint) Example request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ -u \"${CLIENT_ID}\":\"${CLIENT_SECRET}\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=SplxlOBeZQQYbYS6WxSbIA\" \\ -d \"redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb\" | python -m json.tool Note You can find the TOKEN_ENDPOINT in the Endpoints table. Example response:\n{ \"access_token\": \"SlAV32hkKG...\", \"expires_in\": 3600, \"id_token\": \"eyabcdGciOiJSUzI1N...\", \"token_type\": \"Bearer\" } Proof Key for Code Exchange (PKCE) The Proof Key for Code Exchange (PKCE, pronounced pixie) extension (RFC 7636) describes a technique for public clients (clients without client_secret) to mitigate the threat of having the Authorization Code intercepted. The technique involves the client first creating a secret, and then using that secret again when exchanging the Authorization Code for an access token. This way if the code is intercepted, it will not be useful since the token request relies on the initial secret.\nClient configuration To enable PKCE you need to go to the Manage Services Page and create/edit a client. In “Protocol” tab under “Token Endpoint Authentication Method” select “No authentication” and in “Crypto” tab under “Proof Key for Code Exchange (PKCE) Code Challenge Method” select “SHA-256 hash algorithm”.\nProtocol Flow Because the PKCE-enhanced Authorization Code Flow builds upon the standard Authorization Code Flow, the steps are very similar.\nFirst, the client creates and records a secret named the code_verifier. The code_verifier is a high-entropy cryptographic random STRING using the unreserved characters [A-Z] / [a-z] / [0-9] / “-” / “.” / “_” / “~”, with a minimum length of 43 characters and a maximum length of 128 characters. Then the client creates a code_challenge derived from the code_verifier by using one of the following transformations on the code verifier:\nplain code_challenge = code_verifier S256 code_challenge = BASE64URL-ENCODE(SHA256(ASCII(code_verifier))) If the client is capable of using S256, it MUST use S256. Clients are permitted to use plain only if they cannot support S256 for some technical reason.\nNote There are various tools that generate these values such as https://tonyxu-io.github.io/pkce-generator/ Then the code_challenge is sent in the Authorization Request along with the transformation method (code_challenge_method).\nExample request:\nHTTP/1.1 302 Found Location: ${AUTHORIZATION_ENDPOINT}? client_id=${CLIENT_ID} \u0026scope=openid%20profile%20email \u0026redirect_uri=${REDIRECT_URI} \u0026response_type=code \u0026code_challenge=${CODE_CHALLENGE} \u0026code_challenge_method=S256 Note You can find the AUTHORIZATION_ENDPOINT in the Endpoints table. The Authorization Endpoint responds as usual but records code_challenge and the code_challenge_method.\nExample response:\nHTTP/1.1 302 Found Location: ${REDIRECT_URI}?code=fgtLHT The client then sends the Authorization Code in the Access Token Request as usual but includes the code_verifier secret generated in the first request.\nExample request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=${CODE}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"redirect_uri=${REDIRECT_URI}\" \\ -d \"code_verifier=${CODE_VERIFIER}\" | python -m json.tool Note You can find the TOKEN_ENDPOINT in the Endpoints table. The Authorization Server transforms code_verifier and compares it to code_challenge from the first request. Access is denied if they are not equal.\nExample response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUlMyNTYifQ...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUlMyNTYifQ...\", \"scope\": \"openid email profile\", \"token_type\": \"Bearer\" } Refresh flow The following request allows obtaining an access token from a refresh token using the grant_type value refresh_token:\nParameter Presence Values client_id Required The identifier of the client. client_secret Required The secret value of the client. grant_type Required refresh_token refresh_token Required The value of the refresh token scope Required This parameter should contain openid at least Example request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -u \"${CLIENT_ID}\":\"${CLIENT_SECRET}\" \\ -d \"grant_type=refresh_token\" \\ -d \"refresh_token=${REFRESH_TOKEN}\" \\ -d \"scope=openid%20email%20profile\" | python -m json.tool; Note You can find the TOKEN_ENDPOINT in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWx...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYW...\", \"refresh_token\": \"eyabcdGciOiJub25...\", \"scope\": \"openid profile email\", \"token_type\": \"Bearer\" } Refresh Request when using PKCE To combine the refresh token grant type with PKCE you need to make the following request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"grant_type=refresh_token\" \\ -d \"refresh_token=${REFRESH_TOKEN}\" \\ -d \"scope=openid%20email%20profile\" | python -m json.tool; Note You can find the TOKEN_ENDPOINT in the Endpoints table. Token Exchange To get a token from client B using a token issued for client A, the parameters of the request are:\nParameter Presence Values grant_type Required urn:ietf:params:oauth:grant-type:token-exchange audience Optional Define the logical name of the service that the token will be used for subject_token Required The value of the access token subject_token_type Required urn:ietf:params:oauth:token-type:access_token (because this feature accepts access tokens only) scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected Example request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -u \"${CLIENT_B_ID}\":\"${CLIENT_B_SECRET}\" \\ -d \"grant_type=urn:ietf:params:oauth:grant-type:token-exchange\" \\ -d \"subject_token=${ACCESS_TOKEN_A}\" \\ -d \"subject_token_type=urn:ietf:params:oauth:token-type:access_token\" \\ -d \"scope=openid%20profile%20offline_access\" | python -m json.tool; Note You can find the TOKEN_ENDPOINT in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"refresh_token\": \"eyabcdGciOiJub25lIn0.eyJleHAiO...\", \"scope\": \"openid profile offline_access\", \"token_type\": \"Bearer\" } Device Code The device code flow enables OAuth clients on (input-constrained) devices to obtain user authorisation for accessing protected resources without using an on-device user-agent, provided that they have an internet connection.\n1. Device Authorization Request The client initiates the authorisation flow by requesting a set of verification codes from the Authorization Server by making an HTTP “POST” request to the device Authorization Endpoint. The client constructs the request with the following parameters:\nParameter Presence Values client_id Required The identifier of the client scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected Example request:\n$ curl -X POST \"${DEVICE_AUTHORIZATION_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"scope=openid%20email%20profile\" | python -m json.tool Note You can find the DEVICE_AUTHORIZATION_ENDPOINT in the Endpoints table. Example response:\n{ \"device_code\": \"HvtHOpSah_Anupq-0dtzvN7cb-wcnwxytiMzpBZBN6E\", \"expires_in\": 600, \"interval\": 5, \"user_code\": \"NMEM-SDPK\", \"verification_uri\": \"https://aai.egi.eu/auth/realms/egi/device\", \"verification_uri_complete\": \"https://aai.egi.eu/auth/realms/egi/device?user_code=NMEM-SDPK\" } 2. User Interaction After receiving a successful Authorization Response, the client displays or otherwise communicates the user_code and the verification_uri to the end user and instructs them to visit the URI in a user agent on a secondary device (for example, in a browser on their mobile phone), and enter the user code.\n3. Device Access Token Request After displaying instructions to the user, the client makes an Access Token Request to the token endpoint. The request contains the following parameters:\nParameter Presence Values grant_type Required urn:ietf:params:oauth:grant-type:device_code device_code Required The device verification code, device_code from the Device Authorization Response client_id Required The identifier of the client client_secret Required The secret value of the client scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected Example request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Adevice_code\" \\ -d \"device_code=${DEVICE_CODE}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"scope=openid%20profile\" | python -m json.tool Note You can find the TOKEN_ENDPOINT in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJhZG1pbiIs...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiI5MDM0Mi...\", \"scope\": \"openid profile\", \"token_type\": \"Bearer\" } Device Code with PKCE To combine Device Code flow with PKCE you need to make the following requests:\n1 - Device Authorization Request:\n$ curl -X POST \"${DEVICE_AUTHORIZATION_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"scope=openid%20email%20profile\" \\ -d \"code_challenge=${CODE_CHALLENGE}\" \\ -d \"code_challenge_method=S256\" | python -m json.tool Note You can find the DEVICE_AUTHORIZATION_ENDPOINT in the Endpoints table. 2 - Device Access Token Request\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Adevice_code\" \\ -d \"device_code=${DEVICE_CODE}\" \\ -d \"client_id=${CLIENT_ID}\" \\ -d \"client_secret=${CLIENT_SECRET}\" \\ -d \"code_verifier=${CODE_VERIFIER}\" | python -m json.tool Note You can find the TOKEN_ENDPOINT in the Endpoints table. Client credentials The parameters that are used in the Client Credentials flow are:\nParameter Presence Values grant_type Required client_credentials scope Optional Define scope(s) to request Example request:\n$ curl -X POST \"${TOKEN_ENDPOINT}\" \\ -u \"${CLIENT_ID}\":\"${CLIENT_SECRET}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=client_credentials\" \\ -d \"scope=openid%20email%20profile%20eduperson_entitlement%20voperson_id\" | python -m json.tool; Note You can find the TOKEN_ENDPOINT in the Endpoints table. Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"not-before-policy\": 0, \"refresh_expires_in\": 0, \"scope\": \"openid eduperson_entitlement voperson_id profile email\", \"token_type\": \"Bearer\" } Example Access Token (decoded payload):\n{ \"azp\": \"1deb9fbd-44a3-4dff-ab4a-49e092e7f566\", \"clientAddress\": \"xxx.xxx.xxx.xxx\", \"clientHost\": \"xxx.xxx.xxx.xxx\", \"client_id\": \"1deb9fbd-44a3-4dff-ab4a-49e092e7f566\", \"exp\": 1674473629, \"iat\": 1674470029, \"iss\": \"https://aai.egi.eu/auth/realms/egi\", \"jti\": \"bdf15737-01ba-4e61-b5bc-3304d637e2b6\", \"scope\": \"openid eduperson_entitlement voperson_id profile email\", \"sub\": \"253b69f3-2325-4fd5-a26d-95e26b42bbaf@egi.eu\", \"typ\": \"Bearer\", \"voperson_id\": \"253b69f3-2325-4fd5-a26d-95e26b42bbaf@egi.eu\" } Endpoints The most important OIDC/OAuth2 endpoints are listed below:\nProduction Demo Development Endpoints Production environment Provider configuration https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration Issuer https://aai.egi.eu/auth/realms/egi Authorization https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/auth Token https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token Device Authorization https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/auth/device JSON Web Key Sets(JWKS) https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/certs UserInfo https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo Introspection https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Logout https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/logout Endpoints Demo environment Provider configuration https://aai-demo.egi.eu/auth/realms/egi/.well-known/openid-configuration Issuer https://aai-demo.egi.eu/auth/realms/egi Authorization https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/auth Token https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/token Device Authorization https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/auth/device JSON Web Key Sets(JWKS) https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/certs UserInfo https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo Introspection https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Logout https://aai-demo.egi.eu/auth/realms/egi/protocol/openid-connect/logout Endpoints Development environment Provider configuration https://aai-dev.egi.eu/auth/realms/egi/.well-known/openid-configuration Issuer https://aai-dev.egi.eu/auth/realms/egi Authorization https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/auth Token https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/token Device Authorization https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/auth/device JSON Web Key Sets(JWKS) https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/certs UserInfo https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/userinfo Introspection https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/token/introspect Logout https://aai-dev.egi.eu/auth/realms/egi/protocol/openid-connect/logout Authorization Endpoint The Authorization Endpoint performs Authentication of the end user. This is done by sending the User Agent to the Authorization Server's Authorization Endpoint for Authentication and Authorisation, using request parameters defined by OAuth 2.0 and additional parameters and parameter values defined by OpenID Connect.\nFor more information please check the Authorization Code Flow.\nToken Endpoint To obtain an Access Token, an ID Token, and optionally a Refresh Token, the Client sends a Token Request to the Token Endpoint.\nThis endpoint is used in the following flows:\nAuthorization Code Refresh Token Token Exchange Device Code Client Credentials Device Authorization Endpoint This endpoint has been introduced in the OAuth 2.0 Device Authorization Grant specification RFC8628 and it is used in Device Code Flow.\nThe OAuth client on the device interacts with the authorization server directly without presenting the request in a user agent, and the end user authorizes the request on a separate device. This interaction is defined as follows.\nFor more information please check the Device Code Flow.\nJSON Web Key Sets Endpoint This URL points to the Authorization Server’s JWK Set JWK document. The referenced document contains the signing key(s) the client uses to validate signatures from the authorization server.\nUserInfo Endpoint The UserInfo Endpoint is an OAuth 2.0 Protected Resource that returns Claims about the authenticated end user. To obtain the requested Claims about the end user, the Client makes a request to the UserInfo Endpoint using an Access Token obtained through OpenID Connect Authentication. These Claims are normally represented by a JSON object that contains a collection of name and value pairs for the Claims.\nUserInfo Request The Client sends the UserInfo Request using either HTTP GET or HTTP POST. The Access Token obtained from an OpenID Connect Authentication Request must be sent as a Bearer Token, per Section 2 of OAuth 2.0 Bearer Token Usage (RFC6750).\nIt is recommended that the request use the HTTP GET method and the Access Token be sent using the Authorization header field.\nExample request:\n$ curl -X GET \"${USERINFO_ENDPOINT}\" -H \"Content-type: application/json\" -H \"Authorization: Bearer ${ACCESS_TOKEN}\" | python -m json.tool; Note You can find the USERINFO_ENDPOINT in the Endpoints table. Example response:\n{ \"eduperson_assurance\": [ \"https://refeds.org/assurance/IAP/low\", \"https://aai.egi.eu/LoA#Substantial\" ], \"eduperson_entitlement\": [ \"urn:mace:egi.eu:group:demo.fedcloud.egi.eu:members:role=member#aai.egi.eu\", \"urn:mace:egi.eu:group:demo.fedcloud.egi.eu:role=member#aai.egi.eu\", \"urn:mace:egi.eu:group:demo.fedcloud.egi.eu:vm_operator:role=member#aai.egi.eu\" ], \"email\": \"jdoe@example.org\", \"email_verified\": true, \"family_name\": \"John\", \"given_name\": \"Doe\", \"name\": \"John Doe\", \"preferred_username\": \"jdoe\", \"sub\": \"1234567890123456789012345678901234567890123456789012345678901234@egi.eu\", \"voperson_id\": \"1234567890123456789012345678901234567890123456789012345678901234@egi.eu\", \"voperson_verified_email\": [\"jdoe@example.org\"] } Introspection Endpoint The introspection endpoint is an OAuth 2.0 endpoint that takes a parameter representing an OAuth 2.0 token and returns a JSON document representing the meta information surrounding the token, including whether this token is currently active.\nIntrospection Request The protected resource calls the introspection endpoint using an HTTP POST request with parameters sent as application/x-www-form-urlencoded.\nExample request:\n$ curl -X POST \"${INTROSPECTION_ENDPOINT}\" \\ -u \"${CLIENT_ID}\":\"${CLIENT_SECRET}\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"token=${ACCESS_TOKEN}\" | python -m json.tool; Note You can find the INTROSPECTION_ENDPOINT in the Endpoints table. Example response:\n{ \"active\": true, \"auth_time\": 1668613335, \"authenticating_authority\": \"https://idp.admin.grnet.gr/idp/shibboleth\", \"azp\": \"token-portal\", \"client_id\": \"token-portal\", \"eduperson_assurance\": [ \"https://refeds.org/assurance/IAP/low\", \"https://aai.egi.eu/LoA#Substantial\" ], \"eduperson_entitlement\": [ \"urn:mace:egi.eu:group:demo.fedcloud.egi.eu:members:role=member#aai.egi.eu\", \"urn:mace:egi.eu:group:demo.fedcloud.egi.eu:role=member#aai.egi.eu\", \"urn:mace:egi.eu:group:demo.fedcloud.egi.eu:vm_operator:role=member#aai.egi.eu\" ], \"email\": \"jdoe@example.org\", \"email_verified\": true, \"exp\": 1668616935, \"iat\": 1668613335, \"iss\": \"https://aai.egi.eu/auth/realms/egi\", \"jti\": \"fecaf906-8578-4155-9783-f2083900b93c\", \"nonce\": \"30ccf6777eb726aae4f71fc72684c07c\", \"scope\": \"openid eduperson_entitlement voperson_id profile email\", \"session_state\": \"dc0feb13-8a3d-4b91-86c6-039ee27503df\", \"sid\": \"dc0feb13-8a3d-4b91-86c6-039ee27503df\", \"sub\": \"1234567890123456789012345678901234567890123456789012345678901234@egi.eu\", \"typ\": \"Bearer\", \"voperson_id\": \"1234567890123456789012345678901234567890123456789012345678901234@egi.eu\", \"voperson_verified_email\": [\"jdoe@example.org\"] } Logout Endpoint The EGI Check-in OpenID Provider supports user logout based on the OpenID Connect RP-Initiated Logout.\nThe Logout Endpoint is normally obtained via the end_session_endpoint element of Check-in’s Provider Configuration (see Endpoints table). Parameters used in the logout request are detailed below:\nid_token_hint: The ID Token previously issued by Check-in to your Relying Party (RP) and provided to the Logout Endpoint as a hint regarding the end user’s current authenticated session with the client. It indicates the identity of the end user that the RP is requesting Check-in to log out. If the id_token_hint parameter is omitted, the user may be prompted to confirm the logout. client_id: This parameter is used to specify the Client Identifier when post_logout_redirect_uri is specified but id_token_hint is not. post_logout_redirect_uri: URI to which the RP is requesting that the end user’s browser be redirected after a logout has been performed. This URI should use the HTTPS scheme and the value must have been previously registered in the configuration of the Service in EGI Federation Registry. Note that you need to include either the client_id or id_token_hint parameter in case the post_logout_redirect_uri is included. You can use either HTTP GET or HTTP POST to send the logout request to the Logout Endpoint.\nExample Request HTTP/1.1 302 Found Location: ${LOGOUT_ENDPOINT}? id_token_hint=${ID_TOKEN} Note You can find the LOGOUT_ENDPOINT in the Endpoints table. Example Request with redirection HTTP/1.1 302 Found Location: ${LOGOUT_ENDPOINT}? post_logout_redirect_uri=${POST_LOGOUT_REDIRECT_URI} \u0026client_id=${CLIENT_ID} Note You can find the LOGOUT_ENDPOINT in the Endpoints table. Claims-based authorisation As mentioned in the General Information, omitting authorisation checks may lead to abuse of the service.\nEGI Check-in provides information about the authenticated user that may be used by Service Providers in order to control user access to resources. This information is provided by the EGI Check-in OpenID Provider in the form of OIDC claims. The table below lists the claims that are relevant for user authorisation:\nDescription OIDC Claim VO/group membership/roles of the authenticated user eduperson_entitlement Capabilities eduperson_entitlement GOCDB roles eduperson_entitlement Identity Assurance eduperson_assurance Example OIDC Client Configurations Keycloak If you are using Keycloak as an OIDC Relying Party, then you need to follow the steps below in order to register EGI Check-in as an Identity Provider:\nAccess the administrator console of your Keycloak instance and navigate to “Identity Providers” and then select “OpenID Connect v1.0”\nIn the next page, complete the following fields:\nAlias: egi-check-in-oidc Display name: EGI Check-in Scroll down, and complete the rest options:\nDiscovery endpoint: https://aai.egi.eu/auth/realms/egi/.well-known/openid-configuration Client ID: \u003cYOUR_CLIENT_ID\u003e Client Secret: \u003cYOUR_CLIENT_SECRET\u003e And then click on the “Add” button.\nAfter adding EGI Check-in IdP, scroll down to the “OpenID Connect settings” section and expand the “Advanced” option and then add the scopes that the Service needs. For example:\nScopes: openid voperson_id email profile eduperson_entitlement\nThen, scroll down to the “Advanced settings” section and enable the “Trust Email” option and click on “Save” button.\nNext, you will need to add two mappers to store the voperson_id and the eduperson_entitlement claims because Keycloak can map only the standard claims. Go to the “Mappers” tab and then click on “Add Mapper”.\nFor the voperson_id claim you will need to add the following options:\nName: voPersonID Sync Mode Override: import Mapper Type: Username Template Importer Template: ${CLAIM.voperson_id} Target: LOCAL And for the eduperson_entitlement claim:\nName: eduPersonEntitlement Sync Mode Override: Inherit Mapper Type: Attribute Importer Claim: eduperson_entitlement User Attribute Name: eduPersonEntitlement Note For other attributes, create a mapper similar to the eduPersonEntitlement mapper. simple-oidc-client-php In this guide we will demonstrate how to install and configure a Simple OIDC Client.\nInstall simple-oidc-client-php This guide assumes the Apache HTTP server has been installed and the document root is /var/www/html\nMove to the apache document root and download and extract simple-oidc-client-php.tar.gz.\nConfigure Client Login to the EGI Federation Registry\nThen create a new service or edit your existing service. In General tab fill all the required fields. For Integration Environment select Demo. In Protocol Specific tab select as Protocol the OIDC Service and then in the Redirect URI(s) insert your simple-oidc-client-php URL (e.g. http://localhost/simple-oidc-client-php/refreshtoken.php). This URL must link to refreshtoken.php which is located in simple-oidc-client-php directory. Next, in Scope select the scopes that your service needs. Then, submit the form and self approve it. Finally you should get a pair of Client ID and Client Secret.\nConfigure simple-oidc-client-php Now that you have everything you need, you can configure your login settings. Go to your terminal and open config.php with your favourite text editor.\nExample:\n$ vi simple-oidc-client-php/config.php Let’s go quickly through the settings:\ntitle required, the title on the navigation bar img required, the source of the logo scope_info optional, a message that informs the user for the application requirements issuer required, the base URL of our IdentityServer instance. This will allow oidc-client to query the metadata endpoint so it can validate the tokens client_id required, the ID of the client we want to use when hitting the Authorization Endpoint client_secret optional, a value the offers better security to the message flow pkceCodeChallengeMethod optional, a string that defines the code challenge method for PKCE. Choose between plain or S256. redirect_url required, the redirect URL where the client and the browser agree to send and receive correspondingly the code scopesDefine required, defines the scopes the client supports refresh_token_note optional, info for the refresh token access_token_note optional, info for the access token manage_token_note optional, message the informs the user where can manage his tokens manageTokens optional, URL of the manage tokens service sessionName required, define the name of the cookie session sessionLifetime required, define the duration of the session. This must be equal to the validity time of the access token. You must change the followings options based on your Service configuration you setup earlier:\nissuer client_id client_secret redirect_url scopesDefine sessionName (based on the installation path of the portal) An example configuration follows:\n\u003c?php // index.php interface configuration $title = \"Generate Tokens\"; $img = \"https://clickhelp.co/images/feeds/blog/2016.05/keys.jpg\"; $scope_info = \"This service requires the following permissions for your account:\"; // Client configuration $issuer = \"https://aai-demo.egi.eu/auth/realms/egi\"; $client_id = \"CHANGE_ME\"; $client_secret = \"CHANGE_ME\"; // comment if you are using PKCE // $pkceCodeChallengeMethod = \"S256\"; // uncomment to use PKCE $redirect_url = \"http://localhost/simple-oidc-client-php/refreshtoken.php\"; // add scopes as keys and a friendly message of the scope as value $scopesDefine = array( 'openid' =\u003e 'log in using your identity', 'email' =\u003e 'read your email address', 'profile' =\u003e 'read your basic profile info', ); // refreshtoken.php interface configuration $refresh_token_note = \"NOTE: New refresh tokens expire in 12 months.\"; $access_token_note = \"NOTE: New access tokens expire in 1 hour.\"; $manage_token_note = \"You can manage your refresh tokens in the following link: \"; $manageTokens = $issuer . \"manage/user/services\"; $sessionName = \"simple-oidc-client-php\"; $sessionLifetime = 60*60; // must be equal to access token validation time in seconds Common issues Error messages referring to missing code_challenge, code_challenge_method or code_verifier HTTP parameter If you get error messages containing the PKCE HTTP parameters, probably the PKCE mode is enabled in your Service Configuration but the Application is not performing the PKCE mode.\nTo solve this, you need to follow the steps below:\nLogin to Federation Registry Open your Service Configuration Click on the “Protocol Specific” tab and scroll down to “Proof Key for Code Exchange (PKCE) Code Challenge Method” and select “PKCE will not be used for this service” Click on “Submit” to apply the reconfiguration request Error messages referring to invalid_code If you try to perform the Authorization Code flow and you get an invalid_code error message, probably the Application sends the Authorization Request to the Authorization Endpoint of the Keycloak based EGI Check-in OP and then sends the code to the Token Endpoint of the MITREid Connect based EGI Check-in OP or vice versa.\nTo fix this you need to verify that you have updated all the OIDC Endpoints with the Keycloak ones. You can find all the OIDC Endpoints of Keycloak in the Endpoint table.\nError messages referring to the redirect_uri If you try to perform the Authorization Code flow and you get an invalid_redirect_uri error, probably the redirect_uri in the Authorization Request mismatches with the Allowed Redirect URIs in the Service Configuration.\nTo solve this, you need to follow the steps below:\nLogin to Federation Registry Open your Service Configuration Click on the “Protocol Specific” tab and in the “Redirect URI(s)” edit the URI. Click on “Submit” to apply the reconfiguration request UserInfo invalid_token or 401 Unauthorized error response If you are trying to make a request to the UserInfo Endpoint and the response contains the invalid_token error message, probably you are using an invalid Token or the UserInfo Endpoint is wrong.\nTo solve this, please make sure the that:\nYou have obtained an Keycloak issued Access Token and you make a request to the Keycloak based UserInfo Endpoint You have added the Access Token to the Authorization header of the request 502 Bad Gateway error after redirecting back to the Service If you are using NGINX as a Reverse Proxy, and you are getting the following error message in the logs:\nupstream sent too big header while reading response header from upstream\nThen you need to increase the size of the buffer by adding the following options in the vhost configuration:\nproxy_buffers 4 256k; proxy_buffer_size 128k; proxy_busy_buffers_size 256k; Size of the Tokens The size of an Access Token is around 1400 characters, depending on the information (claims) included in the payload of the JWT. So make sure that your OIDC implementation can handle large Tokens.\nToken Introspection errors The Token Introspection is available to all the clients that are using any authentication method (client_secret_basic, client_secret_post, client_secret_jwt or private_key_jwt) (Confidential Clients) to the Token Endpoint. Public Clients (clients that do not use any authentication method) will not be able to get a successful response from the Introspection Endpoint.\nPKCE errors If you are not using PKCE (Proof Key for Code Exchange), please make sure to disable the “PKCE Code Challenge Method” in the Service configuration in EGI Federation Registry, otherwise you will get the following HTTP response during the authentication flow:\nerror=invalid_request\u0026error_description=Missing parameter: code_challenge_method Device Code Grant If you are using a confidential client with the Device Code grant, please make sure that the client_secret is present in the request to the Device Code Endpoint either as HTTP Basic or HTTP POST parameter (see Device Authorization Request).\nToken Exchange Grant If you are using the Token Exchange grant, please make sure that the audience (Optional) defines the logical name of the service that the token will be used for; when specified, it must match the client ID of a client registered in Check-in otherwise an invalid_client error is returned (\"description\": \"audience not found\")\nIntegrating Science Gateways with RCauth for obtaining (proxy) certificates In order for Science Gateways (VO portals) to obtain RFC proxy certificates derived from personal end-entity certificates, an EGI Science Gateway can make use of the IGTF-approved IOTA-type RCauth.eu online CA. The actual integration goes via an intermediary service, called a Master Portal. EGI is running two Master Portal instances, one development, one production instance.\nProduction Development Endpoint Production environment Provider configuration https://aai.egi.eu/mp-oa2-server/.well-known/openid-configuration Client registration https://aai.egi.eu/mp-oa2-server/register Authorization https://aai.egi.eu/mp-oa2-server/authorize Token https://aai.egi.eu/mp-oa2-server/token JSON Web Key Sets(JWKS) https://aai.egi.eu/mp-oa2-server/certs UserInfo https://aai.egi.eu/mp-oa2-server/userinfo Endpoint Development environment Provider configuration https://masterportal-pilot.aai.egi.eu/mp-oa2-server/.well-known/openid-configuration Client registration https://masterportal-pilot.aai.egi.eu/mp-oa2-server/register Authorization https://masterportal-pilot.aai.egi.eu/mp-oa2-server/authorize Token https://masterportal-pilot.aai.egi.eu/mp-oa2-server/token JSON Web Key Sets(JWKS) https://masterportal-pilot.aai.egi.eu/mp-oa2-server/certs UserInfo https://masterportal-pilot.aai.egi.eu/mp-oa2-server/userinfo Registering a client at the Master Portal In order to register a new client for your VO portal go to:\nEGI Development instance: https://masterportal-pilot.aai.egi.eu/mp-oa2-server/register EGI Production instance: https://aai.egi.eu/mp-oa2-server/register Note Make sure to store the client_id and client_secret in a secure place In order to get the client approved, send an email to the administrator of the EGI Master Portal using checkin-support \u003cAT\u003e mailman.egi.eu.\nDetailed information For further and detailed instructions on the integration flow, see the generic RCAuth.eu MasterPortal VOPortal integration guide\nSSH key authentication for proxy retrieval The EGI MasterPortal also allows users to authenticate using SSH key pair, in order to retrieve proxy certificates from the MasterPortal. Users need to first upload the public key via a self-service portal, https://aai.egi.eu/sshkeys/. About once a week they need to follow a web-flow to ensure a long-lived proxy certificate is present in MasterPortal, e.g. by going to https://aai.egi.eu/vo-portal/. They can then obtain a proxy certificate by doing\n$ ssh proxy@ssh.aai.egi.eu and storing the output in /tmp/x509up_u$(id -u)\nGeneric information for users on how to do this can be found at Instructions for end users on how to use the SSH key authN for proxy retrieval. Alternatively VO portals could implement such functionality themselves by using the API described at the Master Portal sshkey endpoint description.\nUser attributes This section defines the attributes that can be made available to services connected to Check-in.\n1. Community User Identifier attribute name Community User Identifier description The User’s Community Identifier is a globally unique, opaque, persistent and non-reassignable identifier identifying the user. For users whose community identity is managed by Check-in, this identifier is of the form \u003cuniqueID\u003e@egi.eu. The \u003cuniqueID\u003e portion is an opaque identifier issued by Check-in SAML Attribute(s) urn:oid:1.3.6.1.4.1.25178.4.1.6 (voPersonID) OIDC scope voperson_idaarc OIDC claim(s) voperson_id OIDC claim location ID tokenUserInfo EndpointIntrospection Endpoint origin The Community User Identifier is assigned by Check-in or an external AAI service managing the community identity of the user changes No multiplicity No availability Always example ef72285491ffe53c39b75bdcef46689f5d26ddfa00312365cc4fb5ce97e9ca87@egi.eu notes Use Community User Identifier within your application as the unique-identifier key for the user. Obtaining the Community User Identifier from the sub claim using the openid scope for OIDC Relying Parties or from eduPersonUniqueId for SAML Service Providers has been deprecated. OIDC RPs should request either the voperson_id or aarc scope to obtain the Community User Identifier. SAML PRs should request the voPersonID attribute to obtain the Community User Identifier. status Stable 2. Display Name attribute name Display Name description The user’s full name, in a displayable form SAML Attribute(s) urn:oid:2.16.840.1.113730.3.1.241 (displayName) OIDC scope profileaarc OIDC claim(s) name OIDC claim location UserInfo Endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example John Doe notes - status Stable 3. Given Name attribute name Given Name description The user’s first name SAML Attribute(s) urn:oid:2.5.4.42 (givenName) OIDC scope profileaarc OIDC claim(s) given_name OIDC claim location UserInfo Endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example John notes - status Stable 4. Family Name attribute name Family Name description The user’s last name SAML Attribute(s) urn:oid:2.5.4.4 (sn) OIDC scope profileaarc OIDC claim(s) family_name OIDC claim location UserInfo Endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example Doe notes - status Stable 5. Username attribute name Username description The username by which the user wishes to be referred to SAML Attribute(s) urn:oid:0.9.2342.19200300.100.1.1 (uid) OIDC scope profileaarc OIDC claim(s) preferred_username OIDC claim location ID tokenUserInfo EndpointIntrospection Endpoint origin Check-in assigns this attribute on user registration changes No multiplicity Single-valued availability Always example jdoe notes The Service Provider MUST NOT rely upon this value being unique status Stable 6. Email Address attribute name Email Address description The user’s email address SAML Attribute(s) urn:oid:0.9.2342.19200300.100.1.3 (mail) OIDC scope emailaarc OIDC claim(s) email OIDC claim location UserInfo EndpointIntrospection Endpoint origin Provided by user’s Identity Provider changes Yes multiplicity Single-valued availability Always example john.doe@example.org notes This MAY NOT be unique and is NOT suitable for use as a primary key status Stable 7. Verified email flag attribute name Verified email flag description True if the user’s email address has been verified; otherwise false SAML Attribute(s) See Verified email list OIDC scope emailaarc OIDC claim(s) email_verified OIDC claim location UserInfo EndpointIntrospection Endpoint origin Check-in assigns this attribute on user registration changes Yes multiplicity Single-valued availability Always example true notes This claim is available only in OpenID Connect status Stable 8. Verified email list attribute name Verified email list description A list of user’s email addresses that have been verified SAML Attribute(s) urn:oid:1.3.6.1.4.1.25178.4.1.14 (voPersonVerifiedEmail) OIDC scope emailaarc OIDC claim(s) voperson_verified_email OIDC claim location UserInfo EndpointIntrospection Endpoint origin Check-in or the user’s Identity Provider changes Yes multiplicity Multi-valued availability Not always example john.doe@example.orgjdoe@example.com notes - status Experimental 9. Affiliation attribute name Affiliation description The user’s affiliation within a particular security domain (scope) SAML Attribute(s) urn:oid:1.3.6.1.4.1.25178.4.1.11 (voPersonExternalAffiliation) OIDC scope voperson_external_affiliation OIDC claim(s) voperson_external_affiliation OIDC claim location UserInfo EndpointIntrospection Endpoint origin When available, this information originates from the user’s authenticating Identity Provider changes Yes multiplicity Multi-valued availability Not always example member@example.orgfaculty@example.org notes Service Providers are encouraged to validate the scope of this attribute status Stable 10. Groups attribute name Groups description The user’s group/VO membership/role information expressed as entitlements SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement) OIDC scope eduperson_entitlement OIDC claim(s) eduperson_entitlement OIDC claim location UserInfo EndpointIntrospection Endpoint origin Group memberships are managed by group administrators changes Yes multiplicity Multi-valued availability Not always example urn:mace:egi.eu:aai.egi.eu:member@fedcloud.egi.euurn:mace:egi.eu:aai.egi.eu:vm_operator@fedcloud.egi.eu notes - status Stable 11. Capabilities attribute name Capabilities description This attribute describes the resource or child-resource a user is allowed to access, optionally specifying certain actions the user is entitled to perform, as described in AARC-G027 SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement) OIDC scope eduperson_entitlement OIDC claim(s) eduperson_entitlement OIDC claim location UserInfo EndpointIntrospection Endpoint origin Capabilities are managed by Check-in changes Yes multiplicity Multi-valued availability Not always example urn:mace:egi.eu:res:rcauth#aai.egi.euurn:mace:egi.eu:res:gocdb#aai.egi.eu notes - status Stable 12. GOCDB Roles attribute name GOCDB Roles description The user’s GOCDB role information expressed as entitlements SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement) OIDC scope eduperson_entitlement OIDC claim(s) eduperson_entitlement OIDC claim location UserInfo EndpointIntrospection Endpoint origin The roles are managed in GOCDB changes Yes multiplicity Multi-valued availability Not always example urn:mace:egi.eu:goc.egi.eu:100453G0:GRIDOPS-CheckIn:Site+Administrator@egi.euurn:mace:egi.eu:goc.egi.eu:92503G08:GRIDOPS-MON:Site+Operations+Manager@egi.eu notes - status Stable 13. Assurance attribute name Assurance description Assurance of the identity of the user, following REFEDS Assurance Framework (RAF) and the EGI AAI Assurance Profiles. The following RAF values are qualified and automatically set for all Community identities:$PREFIX$$PREFIX$/ID/unique$PREFIX$/ID/eppn-unique-no-reassign$PREFIX$/IAP/low$PREFIX$/ATP/ePA-1m$PREFIX$/ATP/ePA-1dFollowing RAF values are set if the currently used authentication provider asserts (or otherwise qualifies to) them:$PREFIX$/IAP/medium$PREFIX$/IAP/highThe following compound profiles are asserted if the user qualifies to them$PREFIX$/profile/cappuccino$PREFIX$/profile/espresso SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.11 (eduPersonAssurance) OIDC scope openid OIDC claim(s) eduperson_assurance OIDC claim location UserInfo EndpointIntrospection Endpoint origin Check-in assigns this attribute on user registration changes Yes multiplicity Multi-valued availability Always example https://aai.egi.eu/LoA#Lowhttps://refeds.org/assurance/IAP/low notes - status Stable 14. CertEntitlement attribute name CertEntitlement description Provides information about the user’s certificate subject(s) and the associated VO(s) SAML Attribute(s) Not available OIDC scope cert_entitlement OIDC claim(s) cert_entitlement OIDC claim location UserInfo EndpointIntrospection Endpoint origin VO/group management tools integrated with Check-in changes Yes multiplicity Multi-valued availability Not always example [{\"cert_subject_dn\": \"/C=GR/O=HellasGrid/...\",\"cert_iss\": \"/C=GR/O=HellasGrid/...\",\"eduperson_entitlement\": \"urn:mace:egi.eu:group:checkin-integration:role=VO-Admin#aai.egi.eu\"}] notes This is available only for DIRAC status Stable 15. SSH Public Key attribute name SSH Public Key description Provides information about the user’s SSH public key(s) SAML Attribute(s) urn:oid:1.3.6.1.4.1.24552.500.1.1.1.13 (sshPublicKey) OIDC scope ssh_public_key OIDC claim(s) ssh_public_key OIDC claim location UserInfo Endpoint origin Added SSH public key(s) in user’s Check-in Profile changes Yes multiplicity Multi-valued availability Not always example ssh-rsa AAAAB3NzaC...qxxEEipdnZ jdoe@grnet-hq.admin.grnet.grssh-rsa AAAA4xzdIf...fxgsRDfgAt jdoe@example.org notes - status Experimental 16. ORCID iD attribute name ORCID iD description Provides information about the user’s ORCID iD SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.16 (eduPersonOrcid) OIDC scope orcid OIDC claim(s) orcid OIDC claim location UserInfo Endpoint origin ORCID Identity Provider changes No multiplicity Single-valued availability Not always example https://orcid.org/XXXX-XXXX-XXXX-XXXX notes The attribute is available when logging in using ORCID status Experimental 17. Subject Distinguished Name (DN) of user’s X.509 certificate attribute name Subject Distinguished Name (DN) of user’s X.509 certificate description The Subject Distinguished Name of an X.509 certificate held by the person. SAML Attribute(s) 1.3.6.1.4.1.25178.4.1.3 (voPersonCertificateDN) OIDC scope voperson_certificateaarc OIDC claim(s) voperson_certificate_dn OIDC claim location UserInfo Endpoint origin Check-in or the user’s Identity Provider changes Yes multiplicity Multi-valued availability Not always example /C=XX/O=YYY/OU=ZZZ/CN=John Doe notes - status Experimental 18. Issuer Distinguished Name (DN) of user’s X.509 certificate attribute name Issuer Distinguished Name (DN) of user’s X.509 certificate description The Subject Distinguished Name of the X.509 certificate issuer SAML Attribute(s) 1.3.6.1.4.1.25178.4.1.4 (voPersonCertificateIssuerDN) OIDC scope voperson_certificateaarc OIDC claim(s) voperson_certificate_issuer_dn OIDC claim location UserInfo Endpoint origin Check-in or the user’s Identity Provider changes Yes multiplicity Multi-valued availability Not always example /C=XX/O=YYY/OU=ZZZ/CN=Example CA notes - status Experimental User authorisation The following information about the authenticated user can be provided by EGI Check-in in order to control user access to resources:\nVO/group membership and role information about the authenticated user Capabilities Identity Assurance GOCDB roles VO/group membership and role information Background VO/group membership and role information is encoded in entitlements (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN.\nSyntax An entitlement value expressing group membership and role information has the following syntax (components enclosed in square brackets are OPTIONAL):\nurn:mace:egi.eu:group:\u003cGROUP\u003e[:\u003cSUBGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e where:\n\u003cGROUP\u003e is the name of a VO, research collaboration or a top level arbitrary group. \u003cGROUP\u003e names are unique within the urn:mace:egi.eu:group namespace; zero or more \u003cSUBGROUP\u003e components represent the hierarchy of subgroups in the \u003cGROUP\u003e; specifying sub-groups is optional the optional \u003cROLE\u003e component is scoped to the rightmost (sub)group; if no group information is specified, the role applies to the VO \u003cGROUP-AUTHORITY\u003e is a non-empty string that indicates the authoritative source for the entitlement value. For example, it can be the FQDN of the group management system that is responsible for the identified group membership information Example:\nurn:mace:egi.eu:group:fedcloud.egi.eu:role=vm_operator#aai.egi.eu Capabilities Background The user’s capability information is encoded in entitlements (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN following the syntax defined in AARC-G027.\nSyntax An entitlement value expressing a capability has the following syntax (components enclosed in square brackets are OPTIONAL):\n\u003cNAMESPACE\u003e:res:\u003cRESOURCE\u003e[:\u003cCHILD-RESOURCE\u003e]...[:act:\u003cACTION\u003e[,\u003cACTION\u003e]...]#\u003cAUTHORITY\u003e where:\n\u003cNAMESPACE\u003e is controlled by the e-infrastructure, research infrastructure or research collaboration that manages the capability. The \u003cNAMESPACE\u003e of capabilities managed by Check-in is set to urn:mace:egi.eu, while, generally, it is in the form of urn:\u003cNID\u003e:\u003cDELEGATED-NAMESPACE\u003e[:\u003cSUBNAMESPACE\u003e]... where:\n\u003cNID\u003e is the namespace identifier associated with a URN namespace registered with IANA2, ensuring global uniqueness. Implementers SHOULD use one of the existing registered URN namespaces, such as urn:mace[MACE].\n\u003cDELEGATED-NAMESPACE\u003e is a URN sub-namespace delegated from one of the IANA registered NIDs to an organisation representing the e-infrastructure, research infrastructure or research collaboration. It is RECOMMENDED that a publicly accessible URN value registry for each delegated namespace be provided.\nThe literal string \"res\" indicates that this is a resource-specific entitlement as opposed to, for example, an entitlement used for expressing group membership AARC-G002.\n\u003cRESOURCE\u003e is the name of the resource. Whether the name should be unique is an implementation decision.\nAn optional list of colon-separated \u003cCHILD-RESOURCE\u003e components represents a specific branch of the hierarchy of resources under the identified \u003cRESOURCE\u003e.\nAn optional list of comma-separated \u003cACTION\u003es MAY be included, which, if present, MUST be prefixed with the literal string “act”. This component MAY be used for further specifying the actions a user is entitled to do at a given resource. Note that the list of \u003cACTION\u003es is scoped to the rightmost child-resource; if no child-resource information is specified, actions apply to the top level resource. The interpretation of a capability without actions specified is an implementation detail.\n\u003cAUTHORITY\u003e is a mandatory and non-empty string that indicates the authoritative source of the capability. This SHOULD be used to further specify the exact issuing instance. For example, it MAY be the FQDN of the service that issued that specific capability. The \u003cAUTHORITY\u003e is specified in the f-component RFC8141 of the URN; thus, it is introduced by the number sign (\"#\") character and terminated by the end of the URN. All characters must be encoded according to RFC8141. Hence, the \u003cAUTHORITY\u003e MUST NOT be considered when determining equivalence (Section 3 in RFC8141) of URN-formatted capabilities. The \u003cAUTHORITY\u003e of capabilities managed by Check-in is typically set to aai.egi.eu.\nExample:\nurn:mace:egi.eu:res:rcauth#aai.egi.eu Identity Assurance Based on the authentication method selected by the user, the EGI proxy assigns a Identity Assurance, which is conveyed to the SP through both the eduPersonAssurance attribute and the Authentication Context Class (AuthnContextClassRef) of the SAML authentication response. EGI Check-in uses Assurance Profiles which distinguish between three Identity Assurance levels, similarly to the eID Assurance Framework (eIDAF). Each level is represented by a URI as follows:\nLow: Authentication through a social identity provider or other low identity assurance provider: https://aai.egi.eu/LoA#Low Substantial: Password/X.509 authentication at the user's home IdP: https://aai.egi.eu/LoA#Substantial High: Substantial + multi-factor authn (not yet supported, TBD): https://aai.egi.eu/LoA#High Moreover, EGI Check-in follows the REFEDS Assurance framework (RAF). The EGI Check-in conveys any RAF values provided by the IdP directly to the SP, through the aforementioned methods. Furthermore, Check-in will append into the User’s profile any additional LoA, if the user is eligible for it. For example, a user having a Verified Email is eligible for the RAF value https://refeds.org/assurance/IAP/low\nSome EGI SPs have been configured to provide limited access (or not to accept at all) credentials with the Low LoA.\nNote: When logging in through the EGI SSO IdP, the LoA is determined based on the selected authentication mechanism as follows:\nUsername/password credentials → Low X.509 certification → Substantial GOCDB Roles Background GOCDB roles, as per GOCDB documentations, are encoded (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN.\nSyntax An entitlement value expressing GOCDB roles has the following syntax (components enclosed in square brackets are OPTIONAL):\nurn:mace:egi.eu:goc.egi.eu:\u003cPRIMARY_KEY\u003e:\u003cON_ENTITY\u003e:\u003cUSER_ROLE\u003e@egi.eu where:\n\u003cPRIMARY_KEY\u003e is the primary key for the user role, e.g. “123G0” \u003cON_ENTITY\u003e is the name of the entity on which the user role applies to, e.g. “GRIDOPS-GOCDB” \u003cUSER_ROLE\u003e is the user’s role, e.g. “Site Operations Manager” Example:\nurn:mace:egi.eu:goc.egi.eu:100453G0:GRIDOPS-CheckIn:Site+Administrator@egi.eu ","categories":"","description":"Check-in guide for Service Providers","excerpt":"Check-in guide for Service Providers","ref":"/providers/check-in/sp/","tags":"","title":"Service Providers"},{"body":"Introduction Storage space usage accounting is based on the StAR (Storage Accounting Record) developed during the EMI project in conjunction with the OGF Usage Record Work Group (UR-WG). The format is documented in GFD-I.201.\nEMI delivered StAR solutions for dCache and DPM in EMI-3. In both cases the storage service queries its database at a site and extracts data to populate StAR usage records. The site then uses SSM as a transport method to send the StAR records via the EGI Messaging Service to EGI Accounting repository.\nDeployment instructions Accounting script for dCache Note Find dCache documentation on the dCache site. The configuration follows the same configuration system that is common to all dCache components. As with other dCache components, the default values are exposed and documented.\nIn general, a site should review the star.properties file (in the standard location for configuration defaults) and update their site-specific configuration, if necessary.\nInformation on how to set the star.properties file can be found in the dCache repository.\nMap local groupid to the VO names as appropriate, for example:\nstar.gid-mapping = 12345=/atlas,12346=/cms,12347=/lhcb (EXAMPLE) Running the dcache-star command generates records according to that site’s configuration.\nEdit SSM’s sender.cfg to have path:/var/spool/dcache/star so that it can find and publish the records dCache generated.\nAccounting script for DPM Important DPM/DMLite is supported until June 2023. All the RCs providing DPM have been invited to move to a different storage solution. A decommissioning and migration campaign was started for this purpose. You need to install DPM-DMLITE 1.8.7 or higher. The Storage Accounting is implemented as a puppet module that adds cron configuration to execute the script daily.\nPlease be sure to have installed the star-accounting.py script v1.0.4 at least.\nAccounting script for EOS The script generating the required space accounting information is available in the eos-server package starting with release 5.0.15.\nPlease have a look at the EOS documentation for more information.\nInstall the APEL SSM software The APEL SSM software can be installed from the UMD-4 repository.\nAdd the information to EGI Configuration Database You need to add a new service endpoint for that host to EGI Configuration Database with the service type eu.egi.storage.accounting and the correct host certificate DN. The Accounting Repository takes up to an hour to update its ACL from the configuration Database and the EGI Messaging Service take up to 4 hours. If you get warnings in your SSM log about invalid username or password you can retry again after a delay. If this persists for over 4 hours, then do open a Helpdesk ticket.\nConfigure SSM Set the configuration files as explained in the general documentation and in the migration instructions.\nRunning the Accounting Software Create a cron job to run your accounting script followed by calling the SSM sender. We recommend that you send storage accounting data once per day. There will be a delay of up to 24 hours before you see the data you have sent reflected in the Accounting Portal.\nExample: $ cat /etc/cron.daily/dmlite-StAR-accounting #!/bin/sh set -e mkdir -p /var/spool/apel/outgoing/`date +%Y%m%d` /usr/share/dmlite/StAR-accounting/star-accounting.py --reportgroups \\ --dbhost=[hostname] \\ --dbuser=[username] \\ --dbpwd=[password] \\ --nsdbname=cns_db \\ --dpmdbname=dpm_db \\ --site=[site name] \u003e /var/spool/apel/outgoing/`date +%Y%m%d`/`date +%Y%m%d%H%M%S` ssmsend You can confirm if the Accounting Repository receives your data by looking at the sites publishing storage accounting records\nThe page is updated on a daily basis.\nStorage Accounting Data at the EGI Accounting Portal The storage accounting view is currently available on the development instance of the Accounting Portal.\n","categories":"","description":"Using Storage Accounting Record (StAR)","excerpt":"Using Storage Accounting Record (StAR)","ref":"/providers/high-throughput-compute/storage-accounting/","tags":"","title":"Storage accounting"},{"body":"General recommendations All files and folders should be lower case EGI Services should be named exactly as in the EGI Services Portfolio Acronyms should be used only when it makes sense Service names should never be replaced by acronyms When introducing services, link to the public page of the service, if any: [EGI Cloud Compute](https://www.egi.eu/service/cloud-compute/) Markdown writing guidelines Documentation pages have to be written in Markdown, compliant with CommonMark and GitHub Flavored Markdown.\nBasic rules Headings must start at level 2 (##), as level 1 (#) is the title of the page Lines should be wrapped at 80 characters Sentences must be separated by one space only Indent is made with spaces, not with tabs Bullet lists should be using - not * Numbered lists should be using 1. for each line (automatic numbering) Indent secondary (and following) level lists with 2 spaces Lines must end with a Line Feed character (\\n) Files must end with an empty line Shell examples should include a prompt ($ or \u003e) in front of commands, to make it easy to understand which is the command and which is the output Commands in shell examples should be broken into multiple lines of 80 characters or less, using a trailing backslash character (\\) on each line that continues on the next Never break command output in shell examples to multiple lines, instead use style exceptions when necessary Tip Syntax examples that can be used in the files are documented in the shortcodes section. Automating formatting and checking Style should be enforced via the usage of Prettier. Prettier can be integrated with various editors.\nWith VIM/neovim it can be used via a plugin like ALE as described in the official documentation. With VisualStudio Code please see the official documentation Configuration is provided in .prettierrc, options can be set as follows:\n--print-width 80 --tab-width 2 --prose-wrap always When a contribution is received (via a pull request), the proposed changes are checked using various linters.\nGeneral writing guidelines Follow the guidelines below to ensure readability and consistency of the EGI documentation. These are based on the OpenStack documentation writing style guidelines, released under a Creative Commons license.\nTip Short and simple sentences are easier to read and understand. Use standard English Use standard British English (UK) throughout all technical publications. When in doubt about the spelling of a word, consult the Merriam-Webster’s Collegiate Dictionary and the IBM developerWorks editorial style guide.\nBe clear and concise Follow the principles of minimalism. If you can describe an idea in one word, do not use two words. Eliminate all redundant modifiers, such as adjectives and adverbs.\nWrite objectively Do not use humor, jargon, exclamation marks, idioms, metaphors, and other colloquialisms.\nDescribe the most common use case first Put the most common case in the main clause and at the beginning of a paragraph or section. You can introduce additional use cases by starting a sentence with “however” or “if”.\nWrite in active voice In general, write in active voice rather than passive voice. Active voice identifies the agent of action as the subject of the verb, usually the user. Passive voice identifies the recipient (not the source) of the action as the subject of the verb.\nActive-voice sentences clarify the performer of an action and are easier to understand than passive-voice sentences. Passive voice is usually less engaging and more complicated than active voice. When you use passive voice, the actions and responses of the software can be difficult to distinguish from those of the user. In addition, passive voice usually requires more words than active voice.\nExamples Do not use Use After the software has been installed, the computer can be started. After you install the software, start the computer. The Configuration is saved when you click OK. Click OK to save the configuration. A server is created by you. Create a server. However, passive voice is acceptable in the following situations:\nUsing active voice sounds like you are blaming the user. For example, you can use passive voice in an error message or troubleshooting content when the active subject is the user. Example Do not use Use If the build fails, you probably omitted the flavor. If the build fails, the flavor might have been omitted. The agent of action is unknown, or you want to de-emphasize the agent of action and emphasize the object on which the action is performed. Example Do not use Use The product, OS, or database returns the messages. The messages are returned [by the database]. Recasting the sentence in active voice is wordy or awkward. Example Do not use Use In 2009, engineers developed a software that simplifies the installation. A software that simplifies the installation was developed in 2009. Write in second person Users are more engaged with documentation when you use second person (that is, you address the user as “you”).\nWriting in second person has the following advantages:\nSecond person promotes a friendly tone by addressing users directly. Using second person with the imperative mood (in which the subject you is understood) and active voice helps to eliminate wordiness and confusion about who or what initiates an action, especially in procedural steps. Using second person also avoids the use of gender-specific, third-person pronouns such as he, she, his, and hers. If you must use third person, use the pronouns they and their, but ensure that the pronoun matches the referenced noun in number. Use first person plural pronouns (we, our) judiciously. These pronouns emphasize the writer or EGI rather than the user, so before you use them, consider whether second person or imperative mood is more “user-friendly.” However, use “we recommend” rather than “it is recommended” or “EGI recommends”. Tip You can use “we” in the place of EGI if necessary. Do not use first person to avoid naming the product or to avoid using passive voice. If the product is performing the action, use third person (the product as an actor). If you want to de-emphasize the agent of action and emphasize the object on which the action is performed, use passive voice.\nThe first-person singular pronoun “I” is acceptable in the question part of FAQs and when authors of blogs or signed articles are describing their own actions or opinions.\nImportant Do not switch person (point of view) in the same guide or on the same page. Examples Do not use Use Creating a server involves specifying a name, flavor, and image. To create a server, specify a name, a flavor, and image. To create a server, the user specifies a name, flavor, and image. To create a server, you specify a name, flavor, and image. Use the present simple tense Users read documentation to perform tasks or gather information. For users, these activities take place in their present, so the present tense is appropriate in most cases. Additionally, the present tense is easier to read than the past or future tense.\nExample Do not use Use The product will prompt you to verify the deletion. After you log in, your account will then begin the verification process. The product prompts you to verify the deletion. After you log in, your account begins the verification process. Tip Use the future tense only when you need to emphasize that something will occur later (from the users’ perspective). Do not humanize inanimate objects Do not give human characteristics to non-human subjects or objects.\nExample Do not use Use This guide assumes… This guide describes… Avoid personification Do not express your fears or feelings in technical writing. Avoid the adverbs such as “probably”, “hopefully”, “basically”, and so on.\nAvoid ambiguous titles Each title should include a clear description of the page’s or chapter’s subject.\nExample Do not use Use Update metadata Update object metadata Eliminate needless politeness Do not use “please” and “thank you” in technical documentation.\nWrite positively Write in a positive tone. Positive sentences improve readability. Try to avoid the following words as much as possible:\nExamples Do not use Use damage affect catastrophic serious bad serious (or add explanation) fail unable to kill cancel or stop fatal serious destroy remove or delete wrong incorrect or inconsistent Do not use contractions Generally, do not contract the words.\nExamples Do not use Use can’t cannot don’t do not Do not overuse this, that, these, and it Use these pronouns sparingly. Overuse contributes to readers’ confusion. To fix the ambiguity, rephrase the sentence.\nExample Do not use Use The monitoring system should perform regular checks to verify that the Ceph cluster is healthy. This can be achieved using the Ceph health command. The monitoring system performs regular checks to ensure the Ceph cluster is functioning correctly. Use the Ceph health command to run a health check. Tip You can also fix the ambiguity by placing a noun modifier immediately after the pronoun. Do not split infinitives Do not place modifiers between “to” and the verb. Typically, placing an adverb or an adjective between “to” and a verb adds ambiguity to a sentence.\nAvoid prepositions at the end of sentences As much as possible, avoid trailing prepositions in sentences by avoiding phrasal verbs.\nExample Do not use Use The image registration window will open up. The image registration window opens. To fix the verb-preposition constructions, replace them with active verbs.\nExamples Do not use Use written up composed pop up appear Use consistent terminology Use consistent terms across all content. Avoid multiple variations or spellings to refer to the same service, function, UI element, and so on.\nUse spelling and grammar checking tools Run text through spelling and grammar checking tools, if available. Correcting mistakes, especially to larger sections of new content, helps eliminate rework later.\nLists When reading a document for the first time, users scan through pages stopping only on the content that stands out, such as titles, lists, links, diagrams, and so on. Lists help to organize options, as well as help readers to find information easily.\nWhen listing items, follow these guidelines:\nUse a bulleted list for options. Create a bulleted list when you need to describe more than three options. Use a numbered list for steps. Use a definition list to explain terms or describe command-line parameters, options, or arguments. Use a colon at the end of the sentence that introduces a list. Use the same grammatical structure (aka parallel structure) for all items in a list. Start each option with a capital letter. When listing options in a paragraph, add and or or before the last item in a list. Use a serial (Oxford) comma before these conjunctions if they connect three or more items.\nPunctuation in lists In bulleted lists:\nIf you list individual words or phrases, do not add a period at the end of each list item. If you use full sentences, add a period at the end of each sentence. If your list includes both individual words or phrases and full sentences, be consistent and either add or do not add periods to all items. In numbered lists:\nAdd periods at the end of steps. If an item of a numbered list is followed by a code block that illustrates how to perform the step, use a colon at the end. Adding exceptions for style violations Successfully passing the checks is a firm requirement, but for the following cases it is possible to add exceptions and bypass some checks in Markdown files:\nWhen in-line HTML must be used (e.g. in tables, or when no other proper solution is available) When the same procedure needs to be described for multiple platforms, and the automatic checker flags it as duplicate content Important Exceptions should only be used when there are no other choices, and should be confined to the smallest possible block of Markdown code. Dealing with in-line HTML tags In some specific cases it is impossible to use anything but in-line HTML tags:\nPresentation page leveraging bootstrap CSS classes or other advanced features Using special formatting for the information presented (e.g. a list in a table cell) Blocks with in-line HTML tags should be preceded by a HTML comment starting with markdownlint-disable to disable the no-inline-html check, as in the following example:\nTip When having a table is not absolutely necessary, use a different construct to present the information. \u003c!-- markdownlint-disable no-inline-html --\u003e | Action | OCCI | OpenStack | This is a very long column with important data | | ----------- | ------------------------ | ---------------------- | ---------------------------------------------- | | List images | `occi -a list -r os_tpl` | `openstack image list` | \u003cul\u003e\u003cli\u003eLorem\u003c/li\u003e\u003cli\u003eipsum\u003c/li\u003e\u003c/ul\u003e | \u003c!-- markdownlint-enable no-inline-html --\u003e Tip Do not forget to follow up with a HTML comment starting with markdownlint-enable to re-enable the no-inline-html check. Important Always use the tag that is providing the proper semantic: e.g. for a list use \u003cul\u003e and \u003cli\u003e, not \u003cbr /\u003e. Dealing with duplicate content When the same procedure needs to be described for multiple platforms, or when the same code has to be exemplified for multiple languages, it is possible that the automatic checkers will flag these as duplicates.\nFor example, describing the following procedure will result in duplicates being reported:\n{{\u003c tabpanex \u003e}} {{\u003c tabx header=\"Linux\" \u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/engine/install/#server), then run the following commands: ```shell $ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Mac\" \u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/desktop/mac/install/), then run the following commands: ```shell $ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Windows\" \u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/desktop/windows/install/), then run the following commands: ```shell \u003e docker pull tdviet/fedcloudclient \u003e docker run -it tdviet/fedcloudclient bash ``` {{\u003c /tabx \u003e}} {{\u003c /tabpanex \u003e}} This type of content should be preceded by a HTML comment that disables the check for duplicates, and followed by another HTML comment that enables it again.\n\u003c!-- // jscpd:ignore-start --\u003e ...content with duplicates here... \u003c!-- // jscpd:ignore-end --\u003e ","categories":"","description":"Style guide for EGI documentation","excerpt":"Style guide for EGI documentation","ref":"/about/contributing/style/","tags":"","title":"Style Guide"},{"body":"Overview In this section you can find a series of Tutorials and Guides designed by the members of the EGI Federation with the goal of providing researchers and scientists with initial knowledge from which they can perform common tasks in the EGI federated infrastructure.\nThe content is organised according to four different skill levels:\nFoundation (or entry level): provides an overview of the EGI federation and of the EGI services, with highlights of their typical use cases. Intermediate: provides details on individual services from the EGI owned service portfolio, as well as services that are offered by the broader EGI community to complement the EGI services towards certain types of advanced computing use cases. Advanced (for more expert users): provides a deep-diving on the advanced services to conduct world-class research and innovation will be presented. Scientific: presents examples of domain-specific Thematic Services benefiting from the solutions offered by the EGI Infrastructure. Do you have a specific training request? Contact us and we will be happy to discuss it with you!\nFind out more:\nLearn more about the EGI Webinars \u0026 Training Programme or visit the YouTube channel ","categories":"","description":"Tutorials for common uses cases in the EGI infrastructure\n","excerpt":"Tutorials for common uses cases in the EGI infrastructure\n","ref":"/users/tutorials/","tags":"","title":"Tutorials \u0026 Guides"},{"body":"What is it? VAPOR is a component of the Operations Portal allowing to query the information system, aggregating information from Top BDII and EGI Configuration Database.\nUsing VAPOR to query resources using a graphical interface VAPOR, a component of the Operations Portal, provides a graphical resources explorer.\nIt can be used as an alternative to querying the Top BDII using ldapsearch.\nUsing the left menu you can select a VO and filter the different resources types.\nVAPOR will list the matching resources.\n","categories":"","description":"Using VAPOR","excerpt":"Using VAPOR","ref":"/internal/operations-portal/using-vapor/","tags":"","title":"Using VAPOR"},{"body":"Virtual Organisations (VOs) in Check-in are represented as Collaborative Organisation Units (COUs).\nIn order to join a Virtual Organisation you must have an EGI account. If you don’t have, then first sign up for an EGI account.\nLogin to Check-in registry with your EGI account.\nExpand the People drop down menu and click Enroll.\nClick the Begin link of the Enrollment flow of the VO you want to join\nClick the Begin button to start the Enrollment flow\na. If there are no pending petitions the enrollment flow will continue as usual.\nb. If there is one pending petition:\nb1. If its status is in Pending Approval you will see a page similar to this:\nWhere you can:\nClick the Notify Approver(s) Again button and a reminder email will be sent to approver(s)\nClick Proceed with new enrollment, so a new enrollment flow will start\nClick Delete and proceed with new enrollment. In this case, the pending petition will be deleted and a new enrollment flow will be created.\nb2. If its status is not in Pending Approval, Pending Confirmation or Finalized you will see a page similar to this:\nWhere you can:\nClick the Resume button and continue the enrollment flow\nClick Proceed with new enrollment, so a new enrollment flow will start\nClick Delete and proceed with new enrollment. In this case, the pending petition will be deleted and a new enrollment flow will start.\nc. If there is more than one pending petition related to the enrollment flow:\nWhere you can:\nClick the View Button of each petition and review it. (see (b) use case above)\nClick Proceed with new enrollment and a new enrollment flow will start\n","categories":"","description":"Joining a Virtual Organisation (VO) in Check-in\n","excerpt":"Joining a Virtual Organisation (VO) in Check-in\n","ref":"/users/aai/check-in/joining-virtual-organisation/","tags":"","title":"Joining a Virtual Organisation"},{"body":"For monitoring purposes, each service endpoints registered into the Configuration Database, and having the flags production and monitored should include the endpoint URL information in order to be contacted by the corresponding service-specific nagios probe.\nThe information needed for service type are:\nSRM: the value of the attribute GlueServiceEndpoint published in the Configuration Database or BDII (e.g. httpg://se.egi.eu:8444/srm/managerv2) Cloud: org.openstack.nova: The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3 org.openstack.swift:The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3 eu.egi.cloud.accounting: for the host sending the records to the accounting repository Other service types: the value of the attribute GlueServiceEndpoint published in the BDII It is also possible to register additional endpoints for every services, they will also be monitored if the “Monitored” flag is set.\nFor having more information about managing the Service endpoints in the Configuration Database, please consult the service endpoints documentation.\nRetrieving the information For retrieving the queue URL from the BDII, you can use the command lcg-infosites, to be executed from a UI. Be sure to query a production Top BDII: you can either use the one provided by your Operations Centre or choose one from the Configuration Database\nFor example:\n$ export LCG_GFAL_INFOSYS=egee-bdii.cnaf.infn.it:2170 $ lcg-infosites --vo ops ce | grep nikhef 5680 15 0 0 0 dissel.nikhef.nl:2119/jobmanager-pbs-infra 5680 17 1 1 0 gazon.nikhef.nl:8443/cream-pbs-infra 5680 15 0 0 0 juk.nikhef.nl:8443/cream-pbs-infra 5680 15 0 0 0 klomp.nikhef.nl:8443/cream-pbs-infra 5680 16 0 0 0 stremsel.nikhef.nl:8443/cream-pbs-infra In order to find the GlueServiceEndpoint URL of your SRM service, you can launch a LDAP query to your Site BDII (or directly to the SRM service):\n$ ldapsearch -x -LLL -H ldap://sbdii01.ncg.ingrid.pt:2170 \\ -b \"mds-vo-name=NCG-INGRID-PT,o=grid\" \\ '(\u0026(objectClass=GlueService)(GlueServiceType=SRM))' \\ GlueServiceEndpoint dn: GlueServiceUniqueID=httpg://srm01.ncg.ingrid.pt:8444/srm/managerv2,Mds-Vo-name=NCG-INGRID-PT,o=grid GlueServiceEndpoint: httpg://srm01.ncg.ingrid.pt:8444/srm/managerv2 In a similar way, by just changing the value of GlueServiceType, you can retrieve the endpoint URLs of other services.\nAn alternative way for retrieving the GlueServiceEndpoint URL is using the GLUE2 information browser provided by VAPOR: select your NGI, then your site and hence the Storage service; click on the endpoint details button for finding the URL associated to the SRM interface.\nFilling the information in URLs information are completely missing Editing the services information Site overview This is the home page regarding your site. You need to fill in the URL information.\nClick on a service for displaying its page (e.g. the CREAM-CE).\nEditing a service Click on the EDIT button in the top right corner\nAdding a Service URL fill in the Service URL field with the queue URL\nReviewing the site Now the CREAM-CE service endpoint contains the required queue information.\nProceed in a similar way for the other services.\nAdditional endpoints information In case you need to register an additional endpoint for a service, go on the service summary page and add the proper information. In the example below it is shown the case of a computing element.\nService summary page This is the service summary page.\nYou need to click on the Add endpoint button for registering additional endpoint URLs.\nAdding an endpoint Fill in the proper information and don’t forget to select the “Monitored” flag for making Nagios to detect the new endpoint.\nReviewing the endpoint description The summary page of the endpoint just added should look like this one.\nReviewing the service description And this is the summary page of the service reporting the information about all its endpoints registered: the first one in the Grid Information section and the additional ones in the Service Endpoints section.\nExamples webdav In order to properly monitor your webdav endpoint:\nyou should register a new service endpoint with the webdav service type, separated from the SRM one; the endpoint URL information used for monitoring purposes should be set in the extension properties section. Create the following: Name: ARGO_WEBDAV_OPS_URL Value: webdav URL containing also the VO ops folder, for example: https://darkstorm.cnaf.infn.it:8443/webdav/ops or https://hepgrid11.ph.liv.ac.uk/dpm/ph.liv.ac.uk/home/ops/ it corresponds to the value of GLUE2 attribute GLUE2EndpointURL (containing the used port and without the VO folder); verify that the webdav URL (for example: https://darkstorm.cnaf.infn.it:8443/webdav) is properly accessible. EOS and XrootD service endpoints The EOS service endpoints expose an XrootD interface, so in order to properly monitor them, even in case you provide a plain XrootD endpoint, please do the following:\nyou should register a new service endpoint with the XrootD service type; the endpoint URL information used for monitoring purposes should be set in the extension properties section. Create the following: Name: ARGO_XROOTD_OPS_URL Value: XRootD base SURL to test (the path where ops VO has write access), for example: root://eosatlas.cern.ch//eos/atlas/opstest/egi/, root://recas-se-01.cs.infn.it:1094/dpm/cs.infn.it/home/ops/, root://dcache-atlas-xrootd-ops.desy.de:2811/pnfs/desy.de/ops or similar). Pay attention to the port configured for the interface. GridFTP In order to properly monitor your gridftp endpoint for ops VO\nregister a new service endpoint, associating the storage element hostname to the service type globus-GRIDFTP, with the “production” flag disabled; in the “Extension Properties” section of the service endpoint page, fill in the following fields: Name: SE_PATH Value: /dpm/ui.savba.sk/home/ops (this is an example, set the proper path) check if the tests are OK (it might take some hours for detecting the new service endpoint) and then switch the production flag to “yes” SURL value for SRM The SURL value needed by the SRM monitoring probes is the following structure:\nsrm://\u003chostname\u003e:\u003cport\u003e/srm/managerv2?SFN=\u003cGlueSAPath or GlueVOInfoPath\u003e\nFor example:\nsrm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/pnfs/in2p3.fr/data/dteam/\nAs explained in previous sections, you can retrieve the port number from the GlueServiceEndpoint URL information. If your SE provides GlueSAPath information, use that. To retrieve it: $ ldapsearch -x -LLL -H \u003cldap://sbdii01.ncg.ingrid.pt:2170\u003e \\ -b \"mds-vo-name=NCG-INGRID-PT,o=grid\" \\ '(\u0026(objectClass=GlueSA)(GlueSAAccessControlBaseRule=VO:ops))' \\ GlueSAPath GlueChunkKey dn: GlueSALocalID=opsdisk:replica:online,GlueSEUniqueID=srm01.ncg.ingrid.pt,Mds-Vo-name=NCG-INGRID-PT,o=grid GlueChunkKey: GlueSEUniqueID=srm01.ncg.ingrid.pt GlueSAPath: /gstore/t2others/ops if your SE doesn’t provide GlueSAPath information, use instead the GlueVOInfoPath one: $ ldapsearch -x -LLL -H \u003cldap://ntugrid5.phys.ntu.edu.tw:2170\u003e \\ -b \"Mds-Vo-name=TW-NTU-HEP,o=grid\" \\ (\u0026(objectClass=GlueVOInfo)(GlueVOInfoAccessControlBaseRule=VO:ops)) \\ GlueVOInfoLocalID GlueChunkKey dn: GlueVOInfoLocalID=ops:SRR,GlueSALocalID=SRR:SR:replica:*****,GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw,Mds-Vo-name=TW-NTU-HEP,o=grid GlueVOInfoPath: /dpm/phys.ntu.edu.tw/home/ops GlueChunkKey: GlueSALocalID=SRR:SR:replica:***** GlueChunkKey: GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw GlueVOInfoLocalID: ops:SRR dn: GlueVOInfoLocalID=ops:data01,GlueSALocalID=data01:replica:online,GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw,Mds-Vo-name=TW-NTU-HEP,o=grid GlueVOInfoPath: /dpm/phys.ntu.edu.tw/home/ops GlueChunkKey: GlueSALocalID=data01:replica:online GlueChunkKey: GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw GlueVOInfoLocalID: ops:data01 Pay attention to use the storage path for the ops VO On GOCDB, in the “Extension Properties” section of the SRM service endpoint page, fill in the following fields: Name: SURL Value: the actual SURL value, for example: srm://srm01.ncg.ingrid.pt:8444/srm/managerv2?SFN=/gstore/t2others/ops ","categories":"","description":"Adding service endpoints information into Configuration Database","excerpt":"Adding service endpoints information into Configuration Database","ref":"/internal/configuration-database/adding-service-endpoint/","tags":"","title":"Adding service endpoints"},{"body":" Target Audience: Scientific communities, for programmers and IT-service providers who support research and education. \"Webinar: TOSCA-based cloud orchestration and deployment with Infrastructure Manager (IM)\" (June, 2024) Agenda, slides and recording: https://www.egi.eu/event/tosca-based-cloud-orchestration-and-deployment-with-infrastructure-manager-im/ About: In this webinar we focus on the usage of the IM Dashboard, an easy-to-use web interface, designed for users to deploy a set of predefined and well-tested customisable virtual infrastructures on top of a wide range of cloud providers with a set of mouse clicks. Furthermore, it uses a two-step wizard that enables it to compose more complex templates, as shown in the demo. Target Audience: Scientific communities, developers, integrators and end users. \"Webinar: Introduction to the new generation EGI container execution platform\" (January, 2024) Agenda, slides and recording: https://www.egi.eu/event/webinar-introduction-to-the-container-platform/ About: The new generation EGI container execution platform is a system that delivers fully managed Kubernetes clusters for end users. The webinar covers how to build container images, how to store and share them through the Harbor repository, and how to run them on fully managed Kubernetes clusters with the Rancher service. Target Audience: e-Infrastructure platform and technology providers and those researching or applying (distributed) deep learning at any level. \"Webinar: Distributed Deep Learning with Horovod\" (March, 2023) Agenda, slides and recording: https://www.egi.eu/event/webinar-distributed-deep-learning-with-horovod/ About: This webinar is focused on the Horovod distributed deep learning framework and the reference architecture and service(s) built for supporting it. The aim of the presented works is to enable the efficient utilisation of cloud resources in the heavily resource intensive task of distributed deep learning. The concept of reference architectures is also briefly presented, along with the experiences gained from their continuous development. Target Audience: Scientific communities, developers, integrators and end users. \"FAIR EVA (Evaluator, Validator \u0026 Advisor)\" (April, 2022)” Agenda, slides and recording: https://indico.egi.eu/event/5876/ About: FAIR EVA (Evaluator, Validator and Advisor) has been developed to check the FAIRness level of digital objects from different repositories or data portals. It requires the object identifier and the repository to check and it can be adapted to different contexts and environments. Developed within the EOSC-Synergy project it aims at helping data producers and data managers to evaluate the adoption of the FAIR principles based on the RDA indicators. This webinar will present the tool as well as show how it can be deployed, how the different tests work and how it can be adapted to different data systems. Target Audience: Scientific communities, developers, integrators and end users. \"OpenRDM - FAIR research data management as a service to the scientific community\" (January, 2022) Agenda, slides and recording: https://indico.egi.eu/event/5753/ About: openRDM.eu provides FAIR research data management. It offers research data management as a service to the scientific community, based on the powerful openBIS platform. The service is available as a preview version containing an openBIS instance. Preview is intended for end-users to learn service \u0026 eventually plan on-premise and/or, own cloud based deployment. Alternatively, self-hosting using local IT infrastructure at the respective institution can also be agreed. Consulting \u0026 support for on-premise and/or own cloud based deployment of openBIS is also offered along with user support including data model generation, to be able to import data into openBIS \u0026 training for the use of openBIS as a data management platform. OpenBIS is designed to facilitate robust data management for a wide variety of experiment types and research subjects. It allows tracking, annotating, and sharing of data throughout distributed research projects in different quantitative sciences. Target Audience: User communities that want to use GPUs in Clouds. \"How to train your AI models in EOSC\", (Dec. 2021) Agenda, slides and recording: https://indico.egi.eu/event/5742/ About: Learn how you can train and develop AI models in EOSC using the distributed and federated computing infrastructure of EGI and the services developed during the Deep Hybrid DataCloud project. The webinar, sponsored by the EGI-ACE project, covers: How to prototype, build and train AI applications exploiting resources from EU e-infrastructures. Use and share AI trained models developed by other researchers (from and outside your communities) with the DEEP Marketplace. Target Audience: Scientific communities, developers, integrators and end users. \"How to orchestrate services in the EOSC Compute Platform with the INDIGO PaaS\" (Oct. 2021) Agenda, slides and recording: https://indico.egi.eu/event/5720/ About: The INDIGO PaaS implements an abstraction and federation layer on top of heterogeneous distributed computing environments: it allow to orchestrate and coordinate the provisioning of virtualized compute and storage resources on Cloud Management Frameworks, both private and public (like OpenStack, OpenNebula, AWS, etc.), and the deployment of dockerized long-running services and batch jobs on Container Orchestration Platforms like Apache Mesos and Kubernetes. In this webinar, we will describe the architecture of the INDIGO Platform and its main features. The demo will show how users can easily interact with this orchestration system using both the command line interface and the web dashboard. Target Audience: Scientific communities, developers, integrators and end users. \"Analyze your data using DODAS generated cluster\" (September, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5695/ About: DODAS enables the execution of user analysis code both in batch mode and interactively via the Jupyter interface. DODAS is highly customizable and offers several building blocks that can be combined together in order to create the best service composition for a given use case. The currently available blocks allow to combine Jupyter and HTCondor as well as Jupyter and Spark or simply a jupyter interface. In addition, they allow the management of data via caches to optimise the processing of remote data. This can be done either via XCache or MinIO S3 object storage capabilities. DODAS is based on docker containers and the related orchestration relies on Kubernetes that enables the possibility to compose the building blocks via a web-based user interface thanks to Kubeapps. In this webinar we will explain the DODAS fundamentals and we will provide a user oriented demo. Target Audience: Scientific communities, developers, integrators and end users. \"Running containers in your user space with udocker\" (June, 2021) Agenda, slides and recording: https://indico.egi.eu/event/5541/ About: udocker enables the execution of docker containers in user space without requiring root privileges for installation or use. Udocker implements the pull, load, import and execution of containers by non-privileged users in Linux systems where docker is not available. It can be used in Linux batch systems and interactive clusters that are managed by other entities, such as grid infrastructures or externally managed batch or interactive systems. udocker does not require any type of privileges nor the deployment of services by system administrators. It can be downloaded and executed entirely by the end user. udocker offers several execution modes exploiting system call interception, library call interception and namespaces. udocker integrates several tools to provide a subset of the docker capabilities aimed at container execution. In this webinar we will explain the udocker fundamental, how to use udocker to execute Linux containers and how to best exploit the several execution engines. Target Audience: Site administrators already familiar with an earlier version of ARC CE or planning a migration to ARC from the CREAM platform. \"Rolling out ARC6 CE\" (July 2020) Agenda, slides and recording: https://indico.egi.eu/event/5157/ About: The Advanced Resource Connector (ARC) middleware integrates computing resources (usually, computing clusters managed by a batch system), making them available via a secure common layer. Conceptually, ARC provides an edge service to batch systems. Through this service, called ARC Compute Element (ARC-CE), scientific communities can launch and manage computational tasks in a uniform manner. A bit more than a year ago a non backward compatible major version, the ARC6.0 was released bringing new functionality, enhanced manageability and increased stability to the community. With the availability of the ARC6 release the support for the previous ARC5 deployments are to be discontinued with the end of June 2020. This interactive webinar will introduce the major new features of the ARC6 release and cover the deployment steps of an ARC6 CE both as a new installation and as an upgrade from a previous ARC5-based deployment. Special attention will be given to the accounting system related changes and the new one-shop-stop sysadmin toolbox built around arcctl. Target Audience: Scientific communities, developers, integrators and end users. \"High performance software - Easy gains with simple CUDA\" (April 2023) About: This tutorial provides an introduction to CUDA in high performance software, covering roughly these topics: Best practices for high performance software engineering, such as avoiding premature optimization, ensuring cache alignment, etc. A broad introduction to GPUs, including their hardware and which categories of problems they are/aren't best suited for. Installing and working with GPU frameworks An overview of profiler tools and how to use them A live coding session to implement and diagnose a basic CUDA program, with the level of detail dependent on available time Q\u0026A and stories from the trenches Please note that the training will not cover multi-GPU setups or provide a detailed dive into GPU hardware and CUDA specifics. Participants should have basic knowledge of Python and matrix computation libraries like NumPy. Slides and code https://github.com/c-scale-community/cscale-gpet-workshop ","categories":"","description":"A deep dive on the advanced services to conduct world-class research and innovation.\n","excerpt":"A deep dive on the advanced services to conduct world-class research …","ref":"/users/tutorials/advanced/","tags":"","title":"Advanced level"},{"body":"Overview EGI Data Transfer offers Application Programming Interfaces (APIs) for both regular users and administrator. This page focuses on the user APIs, available in two favors:\nREST API Python Easy Bindings Note Please check out the documentation for more details about the available APIs, their parameters and return values. Authentication \u0026 Authorisation Users have to authenticate before they can call the API.\nImportant Authentication requires an X.509 user certificate or an EGI Check-in token. During the authentication phase, credentials are delegated to the FTS service, which will contact the storages to steer the data transfers on behalf of the users.\nFor authentication and authorisation, the FTS service supports both plain X.509 proxies and X.509 proxies extended with VO information (VOMS) You can learn more about VOMS configuration and proxy creation.\nRESTFul API with X.509 Credentials The User RESTFul APIs can be used to submit transfers jobs (collections of single transfers), monitor and cancel existing transfers. Please check the CERN documentation for the full API details. Here we will provide some examples usage using the Curl client.\nChecking how the server sees the identity of the user curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY https://fts-egi.cern.ch:8446/whoami { \"dn\": [ \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe\", \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe/CN=proxy\" ], \"vos_id\": [ \"6b10f4e4-8fdc-5555-baa2-7d4850d4f406\" ], \"roles\": [], \"delegation_id\": \"9ab8068853808c6b\", \"user_dn\": \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe\", \"level\": { \"transfer\": \"vo\" }, \"is_root\": false, \"base_id\": \"01874efb-4735-4595-bc9c-591aef8240c9\", \"vos\": [ \"dteam\" ], \"voms_cred\": [ \"/dteam/Role=NULL/Capability=NULL\" ], \"method\": \"certificate\" } Getting a list of jobs running Filtered by VO\ncurl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY https://fts-egi.cern.ch:8446/jobs?vo_name=dteam [ { \"cred_id\": \"1426115d1660de4d\", \"user_dn\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=ftssuite/CN=737188/CN=Robot: fts3 testsuite\", \"job_type\": \"N\", \"retry\": -1, \"job_id\": \"94560e74-7ca3-11e9-97dd-02163e00d613\", \"cancel_job\": false, \"job_state\": \"FINISHED\", \"submit_host\": \"fts604.cern.ch\", \"priority\": 3, \"source_space_token\": \"\", \"max_time_in_queue\": null, \"job_metadata\": { \"test\": \"test_bring_online\", \"label\": \"fts3-tests\" }, \"source_se\": \"mock://somewhere.uk\", \"bring_online\": 120, \"reason\": null, \"space_token\": \"\", \"submit_time\": \"2019-05-22T15:09:22\", \"retry_delay\": 0, \"dest_se\": \"mock://somewhere.uk\", \"internal_job_params\": \"nostreams:1\", \"overwrite_flag\": false, \"copy_pin_lifetime\": -1, \"verify_checksum\": \"n\", \"job_finished\": null, \"vo_name\": \"dteam\" } ] Cancelling a job curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY \\ -X DELETE \\ https://fts-egi.cern.ch:8446/jobs/a40b82b7-1132-459f-a641-f8b49137a713 Getting expiration time of delegated credentials curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\ --cacert $X509_USER_PROXY \\ https://fts-egi.cern.ch:8446/delegation/9ab8068853808c6b { \"voms_attrs\": [ \"/dteam/Role=NULL/Capability=NULL\" ], \"termination_time\": \"2020-07-31T22:50:28\" } RESTFul API with EGI Check-in EGI Check-in can be obtained easily via the Token portal https://aai.egi.eu/token/\nChecking how the server sees the identity of the user curl -k -H \"Authorization: Bearer $TOKEN\" https://fts-egi.cern.ch:8446/whoami | jq . { \"user_dn\": \"2dee939532f16e482748b6c25f6ebbf2cac57abd28ca98bee06a114393d14a89@egi.eu\", \"dn\": [ \"2dee939532f16e482748b6c25f6ebbf2cac57abd28ca98bee06a114393d14a89@egi.eu\" ], \"base_id\": \"01874efb-4735-4595-bc9c-591aef8240c9\", \"voms_cred\": [ \"eduperson_entitlement\", \"email\", \"openid\", \"profile\", \"voperson_id\" ], \"vos\": [ \"aai.egi.eu\" ], \"vos_id\": [ \"2b4ace55-1b2e-5bf3-837d-03e3b08777d9\" ], \"roles\": [], \"level\": { \"transfer\": \"vo\" }, \"delegation_id\": \"bd9a59d81b2c37ab\", \"method\": \"oauth2\", \"is_root\": false, \"oauth2_scope\": \"eduperson_entitlement email openid profile voperson_id\", \"wlcg_profile\": false, \"get_granted_level_for_overriden\": {}, \"get_granted_level_for\": {} } Submitting a transfer Example json file to submit a transfer job (e.g. job.json)\n{ \"files\": [ { \"sources\": [ \"https://eospublic.cern.ch/eos/opstest/dteam/test.file\" ], \"destinations\": [ \"https://eospps.cern.ch/eos/opstest/dteam/destination.file\" ], \"source_tokens\": [ \"xxxx\" ], \"destination_tokens\" : [ \"xxxx\" ], \"checksum\": \"ADLER32\" } ], \"params\": { \"overwrite\": true curl -k -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" https://fts-egi.cern.ch:8446/jobs -d @job.json {\"job_id\": \"47b42b78-1ad1-11f0-9cbe-fa163edd14cf\"} Python Bindings The Python bindings for FTS can be installed from the EPEL package repository with python3 being supported.\nyum install python-fts -y Important Authentication requires an *X.509 user certificate. EGI Check-in is not yet supported. For using the bindings, you need to import fts3.rest.client.easy, although for convenience it can be renamed as something else:\nimport fts3.rest.client.easy as fts3 In the following code snippets, an import as above is assumed.\nIn order to be able to do any operation, information about the state of the user credentials and remote endpoint needs to be kept. That’s the purpose of a Context.\ncontext = fts3.Context(endpoint, ucert, ukey, verify=True) The endpoint to use corresponds to the FTS instance REST server and it must have the following format:\nhttps://\\\u003chost\u003e:\\\u003cport\u003e\nfor instance https://fts-egi.cern.ch:8446\nIf you are using a proxy certificate, you can either specify only user_certificate, or point both parameters to the proxy. user_certificate and user_key can be safely omitted, and the program will use the values defined in the environment variables X509_USER_PROXY or X509_USER_CERT + X509_USER_KEY.\nIf verify is False, the server certificate will not be verified.\nHere are some examples about creating a context, submitting a job with a single transfer and getting the job status:\n# pretty print the json outputs \u003e\u003e\u003e import pprint \u003e\u003e\u003e pp = pprint.PrettyPrinter(indent=4) # creating the context \u003e\u003e\u003e context = fts3.Context(\"https://fts-egi.cern.ch:8446\") # printing the whoami info \u003e\u003e\u003e pp.pprint (fts3.whoami(context)) { u'base_id': u'01874efb-4735-4595-bc9c-591aef8240c9', u'delegation_id': u'9ab8068853808c6b', u'dn': [ u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe', u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe/CN=proxy'], u'is_root': False, u'level': { u'transfer': u'vo'}, u'method': u'certificate', u'roles': [], u'user_dn': u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe', u'voms_cred': [u'/dteam/Role=NULL/Capability=NULL'], u'vos': [u'dteam'], u'vos_id': [u'6b10f4e4-8fdc-5555-baa2-7d4850d4f406']} # creating a new transfer and submitting a job \u003e\u003e\u003e transfer = fts3.new_transfer( ... 'gsiftp://source/path', 'gsiftp://destination/path', ... checksum='ADLER32:1234', filesize=1024, ... metadata='Submission example' ... ) \u003e\u003e\u003e job = fts3.new_job([transfer]) \u003e\u003e\u003e job_id = fts3.submit(context, job) \u003e\u003e\u003e print job_id b6191212-d347-11ea-8a47-fa163e45cbc4 # get the job status \u003e\u003e\u003e pp.pprint(fts3.get_job_status(context, job_id)) { u'bring_online': -1, u'cancel_job': False, u'copy_pin_lifetime': -1, u'cred_id': u'9ab8068853808c6b', u'dest_se': u'gsiftp://destination', u'http_status': u'200 Ok', u'internal_job_params': u'nostreams:1', u'job_finished': u'2020-07-31T16:05:55', u'job_id': u'b6191212-d347-11ea-8a47-fa163e45cbc4', u'job_metadata': None, u'job_state': u'FAILED', u'job_type': u'N', u'max_time_in_queue': None, u'overwrite_flag': False, u'priority': 3, u'reason': u'One or more files failed. Please have a look at the details for more information', u'retry': -1, u'retry_delay': 0, u'source_se': u'gsiftp://source', u'source_space_token': u'', u'space_token': u'', u'submit_host': u'fts-public-03.cern.ch', u'submit_time': u'2020-07-31T16:05:54', u'user_dn': u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe', u'verify_checksum': u't', u'vo_name': u'dteam'} ","categories":"","description":"The programmatic interface of EGI Data Transfer\n","excerpt":"The programmatic interface of EGI Data Transfer\n","ref":"/users/data/management/data-transfer/api/","tags":"","title":"Data Transfer API"},{"body":"Hashicorp Vault, the service on which EGI Secrets Store is based, has a REST API with similar inputs like the Vault CLI. See the full documentation for more details.\nNote There is a long list of libraries, clients, and tools for working with Hashicorp Vault. ","categories":"","description":"The programmatic interface of EGI Secrets Store\n","excerpt":"The programmatic interface of EGI Secrets Store\n","ref":"/users/security/secrets-store/api/","tags":"","title":"Secrets Store API"},{"body":"This section documents how to run some applications and use the existing tools with EC3.\nHow to run scientific applications in EC3 NAMD cluster To deploy NAMD clusters, please select one of the available LRMS (Local Resource Management System) and choose NAMD from the list of applications.\nHow to use generic tools/practices in EC3 ECAS cluster Check the dedicated ECAS documentation.\nKubernetes Check the Cloud Container Compute documentation.\nMesos + Marathon + Chronos To deploy a virtual cluster with Marathon, Mesos, and Chronos as an orchestration, please select Mesos + Marathon + Chronos from the list of available LRMS.\nOSCAR cluster To deploy Serverless computing for data-processing applications in EGI, please select OSCAR from the list of LRMS (Local Resource Management System). OSCAR supports data-driven serverless computing for file-processing applications. A file upload, to the object storage backend MinIO, will trigger the execution of a chosen shell script running inside a user-defined container. These will be orchestrated as Kubernetes batch jobs. The output data will be uploaded to any object storage backends support. Synchronous invocations are also available.\nAs external object storage providers, the following services can be used:\nExternal MinIO servers, which may be in clusters other than the platform. Amazon S3, the Amazon’s object storage service that offers industry-leading scalability, data availability, security, and performance in the public Cloud. Onedata, the global data access solution for science used in the EGI DataHub. See the documentation to deploy an elastic Kubernetes cluster with the OSCAR platform with EC3: Deploy OSCAR with EC3\nSee some use cases of applications that use the OSCAR framework for event-driven high-throughput processing of files (you can found it in the GitHub repository examples folder):\nInference of a machine learning model: See full description at OSCAR Blog entry. Mask detection: See full description at OSCAR Blog entry. Plants Classification, an application that performs plant classification using Lasagne/Theano. ImageMagick, a tool to manipulate images. Radiomics, a use case about the handling of Rheumatic Heart Disease (RHD) through image computing and Artificial Intelligence (AI). More information in the OSCAR web page\nSLURM cluster To deploy SLURM clusters, please select SLURM from the list of available LRMS. See also the dedicated guide on HTC clusters\n","categories":"","description":"How to run scientific applications and tools with EC3\n","excerpt":"How to run scientific applications and tools with EC3\n","ref":"/users/compute/orchestration/im/ec3/apps/","tags":"","title":"EC3 Applications and Tools"},{"body":"Approving role and change requests When a registered user applies for a role, the request has to be validated by someone who has the proper permissions to grant such a role. If you request a role on a given entity, any user with a valid role on that entity or above will be able to approve your request.\nExample - If you request a “site administrator” role on site X, then the following users can approve your request:\nsite administrators and security officers of site X regional operations staff, managers and deputies of the Operations Centre to which site X belongs GOCDB admins Role requests you can approve are listed on the Manage roles page (accessible by clicking the Manage roles link in the user status panel in the sidebar).\nIn order to approve or decline role requests, simply click on the accept or deny links in front of each role request.\nRevoking roles If a user within your scope has a role that needs to be revoked, you can do this from the user’s page, where user’s details are listed along with his/her current roles. To revoke a role, simply click on the role name then on the revoke link at the top right of the role’s details page.\nNote: This works for other users within your scope but also for yourself. However just note that if you revoke your own roles you may not have proper permissions to recover them afterwards.\n","categories":"","description":"Approving/revoking accounts, roles and other actions","excerpt":"Approving/revoking accounts, roles and other actions","ref":"/internal/configuration-database/users-roles/approve-revoke/","tags":"","title":"Approving/revoking accounts, roles and other actions"},{"body":"Working with federated resources EGI is a federation of compute and storage resource providers.\nIn order for the providers to be able to contribute their services to the federation and to help scientific user communities take advantage of compute and storage solutions that could potentially span across multiple of these providers, a common Authentication and Authorisation Infrastructure (AAI) is needed.\n","categories":"","description":"Identity and access management in the EGI Cloud\n","excerpt":"Identity and access management in the EGI Cloud\n","ref":"/users/aai/","tags":"","title":"Authentication \u0026 Authorization"},{"body":"What is it? EGI Check-in is a proxy service that allows scientific communities to securely access and control access to resources in the EGI Federated infrastructure. It operates as a central hub that connects federated Identity Providers (IdPs) with EGI service providers.\nUsers can authenticate with their preferred IdP (e.g an eduGAIN account, institutional account, social media account, etc.) to access and use EGI services in a uniform and easy way.\nThe main features of Check-in:\nEnables multiple federated authentication sources using different technologies Has an user registration portal that also allows linking accounts Increases productivity and security by simplifying user authentication and authorization Federated in eduGAIN as a service provider Combines user attributes originating from various authoritative sources (IdPs and attribute provider services) and delivers them to EGI service providers in a transparent way Note EGI Check-In is based on the AARC Blueprint Architecture. The following sections cover how to sign up for and EGI account, and how to use it to access resources.\n","categories":"","description":"Control access to resources in the EGI Infrastructure\n","excerpt":"Control access to resources in the EGI Infrastructure\n","ref":"/users/aai/check-in/","tags":"","title":"EGI Check-in"},{"body":"This documentation covers how to join the EGI Cloud federation (also known as Federated Cloud or FedCloud) as a provider. If you are interested in joining please first contact EGI at support@egi.eu, expressing interest and providing few details about:\nthe projects you may be involved in as a cloud provider the user communities you want to support (i.e. Virtual Organisations or VOs). You can also support the ’long-tail of science’ through the vo.access.egi.eu VO the technologies (Cloud Management platforms) you want to provide details on the current status of your deployment (to be installed or already installed, already used or not, how it is used, who uses the services, etc). Integration of cloud stacks into EGI FedCloud follows a well-defined path, depending on the particularities of the cloud stack in question. By integration, we refer to the proper interoperation with EGI infrastructure services such as accounting, monitoring, authentication and authorisation, etc. These configurations make your site discoverable and usable by the communities you wish to support, and allow EGI to support you in operational and technical matters.\nThe series of actions that needs to be taken to achieve Cloud integration is summarised below:\nRegistration and certification of the site as a Federated Resource Centre. Briefly a site parses several intermediate stages - Registered, Candidate, Uncertified - until fully Certified. Requirements - hardware and software, network configuration Registration of endpoints in the Configuration Database (GOCDB), a service provided by EGI as a central registry to record information about the EGI Infrastructure topology and which contains general information about all participating sites Authentication and Authorization Integration (AAI), including the setup of an ops local project that will be used for monitoring your site Accounting configuration Installation validation If at any time you experience technical difficulties or need support, please open a ticket. Dedicated integration sessions are available also on request.\n","categories":"","description":"IaaS Service providers documentation","excerpt":"IaaS Service providers documentation","ref":"/providers/cloud-compute/","tags":"","title":"Cloud Compute"},{"body":" Command-line tools The various public EGI services can be managed and used/accessed with a wide variety of command-line interface (CLI) tools. The documentation of each service contains a summary of the CLIs that can be used with that service, together with recommendations on which one to use in what context.\nThe FedCloud client The FedCloud client is a high-level Python package for a command-line client designed for interaction with the OpenStack services in the EGI infrastructure.\nTip The FedCloud client is the recommended command-line interface to use with most EGI services. FedCloud client has the following modules (features):\nCheck-in allows checking validity of access tokens and listing Virtual Organisations (VOs) of a token Endpoint can search endpoints in the Configuration Database and extract site-specific information from unscoped/scoped tokens Sites allows management of site configurations OpenStack can perform commands on OpenStack services deployed to sites EC3 allows deploying elastic cloud compute clusters Installation The FedCloud client can be installed with the pip3 Python package manager (without root or administrator privileges).\nLinux / Mac Windows To install the FedCloud client:\n$ pip3 install fedcloudclient This installs the latest version of the FedCloud client, together with its required packages (like openstackclient). It will also create executables fedcloud and openstack, adding them to the bin folder corresponding to your current Python execution environment ($VIRTUAL_ENV/bin for executing pip3 in a Python virtual environment, ~/.local/bin for executing pip3 as user (with --user option), and /usr/local/bin when executing pip3 as root).\nAs there are non-pure Python packages needed for installation, the Microsoft C++ Build Tools is a prerequisite, make sure it’s installed with the following options selected:\nC++ CMake tools for Windows C++ ATL for latest v142 build tools (x86 \u0026 x64) Testing tools core features - Build Tools Windows 10 SDK (\u003clatest\u003e) In case you prefer to use non-Microsoft alternatives for building non-pure packages, please see here.\nTo install the FedCloud client:\n\u003e pip3 install fedcloudclient This installs the latest version of the FedCloud client, together with its required packages (like openstackclient). It will also create executables fedcloud and openstack, adding them to the bin folder corresponding to your current Python execution environment.\nCheck if the installation is correct by executing the client:\n$ fedcloud --version Installing EGI Core Trust Anchor certificates Some sites in the EGI infrastructure use certificates issued by Certificate Authorities (CAs) that are not included in the default OS distribution. If you receive error message “SSL exception connecting to…”, install the EGI Core Trust Anchor Certificates by running the following commands:\n$ wget https://raw.githubusercontent.com/tdviet/python-requests-bundle-certs/main/scripts/install_certs.sh $ bash install_certs.sh Note The above script does not work on all Linux distributions. Change python to python3 in the script if needed, see the README for more details, or follow the official instructions for installing EGI Core Trust Anchor certificates in production environments. Using via Docker container The FedCloud client can also be used without installation, by running it in a Docker container. In this case, the EGI Core Trust Anchor certificates are pre-installed.\nLinux Mac Windows To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n$ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n$ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n\u003e docker pull tdviet/fedcloudclient \u003e docker run -it tdviet/fedcloudclient bash Once you have a shell running in the container with the FedCloud client, usage is the same as from the command-line.\nUsing from EGI Notebooks EGI Notebooks are integrated with access tokens so it simplifies using the FedCloud client. First make sure that you follow the installation steps above. Then, below are the commands that you need to run inside a terminal in JupyterLab:\nexport OIDC_ACCESS_TOKEN=`cat /var/run/secrets/egi.eu/access_token` fedcloud token check Please follow instructions below to learn how to use the fedcloud command.\nUsing from the command-line The FedCloud client has these subcommands:\nfedcloud token for checking access tokens (see token subcommands) fedcloud endpoint for querying the Configuration Database (see endpoint subcommands) fedcloud site for manipulating site configurations (see site subcommands) fedcloud openstack or fedcloud openstack-int for performing OpenStack commands on sites (see openstack subcommands) fedcloud ec3 for provisioning elastic cloud compute clusters (see cluster subcommands) Note See also the complete documentation or read and contribute to the source code. Performing any OpenStack command on any site requires only three options: the site, the VO and the command. For example, to list virtual machine (VM) images available to members of VO fedcloud.egi.eu on the site CYFRONET-CLOUD, run the following command:\n$ fedcloud openstack image list --vo fedcloud.egi.eu --site CYFRONET-CLOUD Authentication Many of the FedCloud client commands need access tokens for authentication. Users can choose whether to provide access tokens directly (via option --oidc-access-token), or generate them on the fly with oidc-agent (via option --oidc-agent-account) or from refresh tokens (via option --oidc-refresh-token, which must be provided together with option --oidc-client-id and option --oidc-client-secret).\nTip Users of EGI Check-in can get a Check-in client ID and refresh token, as well as all the information needed to obtain access tokens for their FedCloud client, by visiting EGI Check-in Token Portal. Tip To provide access tokens automatically via oidc-agent, follow these instructions to register a client, then pass the client name (account name used during client registration) to the FedCloud client via option --oidc-agent-account. Important Refresh tokens have long lifetime (one year in EGI Check-in), so they must be properly protected. Exposing refresh tokens via environment variables or command-line options is considered insecure and will be disabled in the near future in favor of using oidc-agent. If multiple methods of getting access tokens are given at the same time, the FedCloud client will try to get an access token from the oidc-agent first, then obtain one using the refresh token.\nThe default authentication protocol is openid. Users can change the default protocol via the option --openstack-auth-protocol. However, sites may have the protocol fixed in the site configuration (e.g. oidc for the site INFN-CLOUD-BARI).\nThe default OIDC identity provider is EGI Check-in (https://aai.egi.eu/auth/realms/egi). Users can set another OIDC identity provider via option --oidc-url.\nNote Remember to also set the identity provider’s name accordingly for OpenStack commands, by using the option --openstack-auth-provider. Environment variables Most of the FedCloud client options can be set via environment variables:\nTip To save a lot of time, set the frequently used options like access token, VO, etc. using environment variables. Tip When you want commands to work on all sites in the EGI infrastructure, use ALL_SITES for the --site parameter. Environment variable Command-line option Default value OIDC_AGENT_ACCOUNT --oidc-agent-account OIDC_ACCESS_TOKEN --oidc-access-token OIDC_REFRESH_TOKEN --oidc-refresh-token OIDC_CLIENT_ID --oidc-client-id OIDC_CLIENT_SECRET --oidc-client-secret OIDC_URL --oidc-url https://aai.egi.eu/auth/realms/egi OPENSTACK_AUTH_PROTOCOL --openstack-auth-protocol openid OPENSTACK_AUTH_PROVIDER --openstack-auth-provider egi.eu OPENSTACK_AUTH_TYPE --openstack-auth-type v3oidcaccesstoken EGI_VO --vo Getting help The FedCloud client can display help for the commands and subcommands it supports. Try running the following command to see the commands supported by the FedCloud client:\n$ fedcloud --help Usage: fedcloud [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. --help Show this message and exit. Commands: ec3 EC3 related commands endpoint endpoint command group for interaction with GOCDB and... openstack Executing OpenStack commands on site and VO openstack-int Interactive OpenStack client on site and VO site Site command group for manipulation with site... token Token command group for manipulation with tokens Similarly, you can see help for e.g. the openstack subcommand by running the command below:\n$ fedcloud openstack --help Usage: fedcloud openstack [OPTIONS] OPENSTACK_COMMAND... Executing OpenStack commands on site and VO Options: --oidc-client-id TEXT OIDC client id --oidc-client-secret TEXT OIDC client secret --oidc-refresh-token TEXT OIDC refresh token --oidc-access-token TEXT OIDC access token --oidc-url TEXT OIDC URL [default: \u003chttps://aai.egi.eu/auth/realms/egi\u003e] --oidc-agent-account TEXT short account name in oidc-agent --openstack-auth-protocol TEXT Check-in protocol [default: openid] --openstack-auth-type TEXT Check-in authentication type [default: v3oidcaccesstoken] --openstack-auth-provider TEXT Check-in identity provider [default: egi.eu] --vo TEXT Name of the VO [required] -i, --ignore-missing-vo Ignore sites that do not support the VO -j, --json-output Print output as a big JSON object --help Show this message and exit. Note Most commands support multiple levels of subcommands, you can get help for all of them using the same principle as above. Using from Python The FedCloud client can be used as a library for developing other services and tools for EGI services. Most of the functionalities can be called directly from Python code without side effects.\nAn usage example is available on GitHub. Just copy/download the code, add your access token and execute python demo.py to see how it works.\nUsing in scripts The FedCloud client can also be used in scripts for simple automation, either for setting environment variables for other tools, or to process outputs from OpenStack commands.\nSetting environment variables for external tools Some FedCloud commands generate output that contains shell commands to set environment variables with the returned result, as exemplified below.\nLinux / Mac Windows PowerShell Run a command to get details of a project:\n$ export EGI_SITE=IISAS-FedCloud $ export EGI_VO=eosc-synergy.eu $ fedcloud site show-project-id --site $EGI_SITE export OS_AUTH_URL=\"https://cloud.ui.savba.sk:5000/v3/\"; export OS_PROJECT_ID=\"51f736d36ce34b9ebdf196cfcabd24ee\"; Run the same command but set environment variables with the returned values:\n$ eval $(fedcloud site show-project-id) The environment variables will have their values set to what the command returned:\n$ echo $OS_AUTH_URL https://cloud.ui.savba.sk:5000/v3/ $ echo $OS_PROJECT_ID 51f736d36ce34b9ebdf196cfcabd24ee Run a command to get details of a project:\n\u003e set EGI_SITE=IISAS-FedCloud \u003e set EGI_VO=eosc-synergy.eu \u003e fedcloud site show-project-id --site %EGI_SITE% set OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ set OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee If you copy the returned output and execute it as commands in a command prompt:\n\u003e set OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ \u003e set OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee The environment variables will have their values set to what the command returned:\n\u003e set OS_AUTH_URL OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ \u003e set OS_PROJECT_ID OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee Run a command to get details of a project:\n\u003e $Env:EGI_SITE=\"IISAS-FedCloud\" \u003e $Env:EGI_VO=\"eosc-synergy.eu\" \u003e fedcloud site show-project-id --site $Env:EGI_SITE $Env:OS_AUTH_URL=\"https://cloud.ui.savba.sk:5000/v3/\"; $Env:OS_PROJECT_ID=\"51f736d36ce34b9ebdf196cfcabd24ee\"; Run the same command but set environment variables with the returned values:\n\u003e fedcloud site show-project-id --site $Env:EGI_SITE ` | Out-String | Invoke-Expression The environment variables will have their values set to what the command returned:\n\u003e $Env:OS_AUTH_URL https://cloud.ui.savba.sk:5000/v3/ \u003e $Env:OS_PROJECT_ID 51f736d36ce34b9ebdf196cfcabd24ee Processing output from OpenStack commands The fedcloud openstack subcommand’s output can be converted to JavaScript Object Notation (JSON) format by using the --json-output option. This is useful for further machine processing of the command output.\nTip JSON output can be processed with a tool like jq, which can slice, filter, map, and transform structured data. It acts as a filter: it takes an input and produces an output. Check out the tutorial for using it to extract data from JSON sources. $ export EGI_SITE=IISAS-FedCloud $ export EGI_VO=eosc-synergy.eu $ fedcloud openstack flavor list --site $EGI_SITE --json-output [ { \"Site\": \"IISAS-FedCloud\", \"VO\": \"eosc-synergy.eu\", \"command\": \"flavor list\", \"Exception\": null, \"Error code\": 0, \"Result\": [ { \"ID\": \"0\", \"Name\": \"m1.nano\", \"RAM\": 64, \"Disk\": 1, \"Ephemeral\": 0, \"VCPUs\": 1, \"Is Public\": true }, { \"ID\": \"2e562a51-8861-40d5-8fc9-2638bab4662c\", \"Name\": \"m1.xlarge\", \"RAM\": 16384, \"Disk\": 40, \"Ephemeral\": 0, \"VCPUs\": 8, \"Is Public\": true }, ... ] } ] # The following jq command selects flavors with VCPUs=2 and prints their names $ fedcloud openstack flavor list --site IISAS-FedCloud --json-output | \\ jq -r '.[].Result[] | select(.VCPUs == 2) | .Name' m1.medium Note Note that --json-output option can be used only with those OpenStack commands that have outputs. Using this parameter with commands with no output (e.g. setting properties) will generate an unsupported parameter error. ","categories":"","description":"EGI command line interface\n","excerpt":"EGI command line interface\n","ref":"/users/getting-started/cli/","tags":"","title":"Command Line Interface"},{"body":"ROD intra-team communication At the end of a shift the current ROD team should prepare the hand over for internal ROD matters. Each ROD can decide independently on what the hand over should look like and how it should be passed on to the next team. The following list provides mere suggestions for what should be included:\na list of tickets which will continue into the next week. Each item should contain the name of the site in question, EGI Helpdesk ticket number, an optional ROD ticket ID if your NGI uses an internal ticket system, and the current status of the ticket; any tickets opened that are not related to a particular alarm; a summary of problems encountered with core grid services; a report of any problems with operational tools that occurred during the shift; anything else the new team should be aware of. For internal communication ROD can use mailing list(s), instant messengers, etc. Each ROD team is free to choose how the internal communication is established.\nCommunication with EGI Operations and site administrators ROD should provide an email contact to where all ticket information should be sent and register this address into the EGI Helpdesk. Another address (or possibly the same) should be made available to make it possible for EGI Operations, site administrators, or other bodies to contact them directly.\nROD should communicate with EGI Operations through the mailing list “operations (AT) egi.eu”. Urgent matters should be communicated via a Helpdesk ticket assigned to the EGI Operations support unit to make tracking of the case possible.\nThere is also the hand over section in the ROD dashboard which allows for EGI Operations team and RODs to intercommunicate.\n","categories":"","description":"Communication channels for RODs.","excerpt":"Communication channels for RODs.","ref":"/providers/rod/communication/","tags":"","title":"Communication channels"},{"body":"What is it? The EGI Configuration Database (GOCDB) is a central registry that records topology information about all sites participating in the EGI infrastructure.\nThe configuration database also provides different rules and grouping mechanisms for filtering and managing the information associated to resources. This can include entities such as operations and resource centres, service endpoints and their downtimes, contact information and roles of staff responsible for operations at different levels.\nThe configuration database is used by all the actors (end users, site managers, NGI managers, support teams, VO managers), by other tools, and by third party middleware to discover information about the infrastructure topology.\n","categories":"","description":"Topology and configuration registry for sites in EGI infrastructure\n","excerpt":"Topology and configuration registry for sites in EGI infrastructure\n","ref":"/internal/configuration-database/","tags":"","title":"Configuration Database"},{"body":"Thank you for taking the time to contribute to this project. The maintainers greatly appreciate the interest of contributors and rely on continued engagement with the community to ensure this project remains useful. We would like to take steps to put contributors in the best possible position to have their contributions accepted. Please take a few moments to read this short guide on how to contribute.\nNote Before you start contributing to the EGI documentation, please familiarize yourself with the concepts used by documentation authors. When authoring pages, please observe and adhere to the Style Guide. Tip We also welcome contributions regarding how to contribute easier and more efficiently. Feedback and questions If you wish to discuss anything related to the project, please open a GitHub issue or start a topic on the EGI Community Forum.\nNote The maintainers will move issues from GitHub to the community forum when longer, more open-ended discussion would be beneficial, including a wider community scope. Contribution process All contributions have to go through a review process, and contributions can be made in two ways:\nFor simple contributions navigate to the documentation page you want to improve, and click the Edit this page link in the top-right corner (see also the GitHub documentation). You will be guided through the required steps. Be sure to save your changes quickly as the repository may be updated by someone else in the meantime. For more complex contributions or when you want to preview and test changes locally you should fork the repository as documented on the Using Git and GitHub page. Contributing via PRs Note If you need to discuss your changes beforehand (e.g. adding a new section or if you have any doubts), please consult the maintainers by creating a GitHub issue.\nYou can also create an issue by navigating to a documentation page, and clicking the Create documentation issue link in the top-right corner.\nBefore proposing a contribution via the so-called Pull Request (PR) workflow, there should be an open issue describing the need for your contribution (refer to this issue number when you submit the PR). We have a three-step process for contributions:\nFork the project if you have not done so yet, and commit changes to a feature branch. Building the documentation locally is described in the README. Create a GitHub PR from your feature branch, following the instructions in the PR template. Perform a code review with the maintainers on the PR. Tip Rebase your fork’s main branch on the EGI documentation repository’s main branch, before you create new feature branches from it. PR requirements If the PR is not finalised mark it as draft using the GitHub web interface, so it is clear it should not be reviewed yet. Explain your contribution in plain language. To assist the maintainers in understanding and appreciating your PR, please use the template to explain why you are making this contribution, rather than just what the contribution entails. Code review process Code review takes place in GitHub pull requests (PRs). See this article if you’re not familiar with GitHub PRs.\nOnce you open a PR, automated checks will verify the style and syntax of your changes and maintainers will review your code using the built-in code review process in GitHub PRs.\nThe process at this point is as follows:\nAutomated syntax and formatting checks are run using GitHub Actions, successful checks are a hard requirement, but maintainers will help you address reported issues. Maintainers will review your changes and merge it if no changes are necessary. Your change will be merged into the repository’s main branch. If a maintainer has feedback or questions on your changes, they will set request changes in the review and provide an explanation. Release cycle The documentation is using a rolling release model, all changes merged to the main branch are directly deployed to the live production environment.\nThe main branch is always available. Tagged versions may be created as needed following semantic versioning when applicable.\nCommunity EGI benefits from a strong community of developers and system administrators, and vice-versa. If you have any questions or if you would like to get involved in the wider EGI community you can check out:\nEGI Community Forum EGI site ","categories":"","description":"Contributing to EGI documentation","excerpt":"Contributing to EGI documentation","ref":"/about/contributing/","tags":"","title":"Contributing"},{"body":"The IM Dashboard is a graphical interface for the IM Server specially developed for users to access EGI Cloud Compute resources.\nYou can access EGI’s IM Dashboard at: https://im.egi.eu\nFunctionalities:\nOIDC authentication Display user’s infrastructures Display infrastructure details, template and log Delete infrastructure Create new infrastructure Add nodes to an infrastructure Resize VMs Tip More details about installing and configuring the dashboard are available in the IM Dashboard documentation. Usage The dashboard of the IM enables non advanced users to manage their infrastructures by launching a set of predefined TOSCA templates on top of EGI Cloud Compute resources. The dashboard does not provide all the features provided by the IM service. In case you need more advanced features use the IM Web interface or the IM-CLI.\nLogin Users must use EGI Check-in to log into the dashboard. Once authenticated, they will be redirected to the portfolio of available TOSCA templates.\nMain menu bar The main menu bar is located at the top of the pages:\nThe first button IM Dashboard enables the user to go to the portfolio of available TOSCA templates. Second item Infrastructures redirects to the list of current user deployed infrastructures. In the Advanced item the Settings sub-item displays some configuration settings as the URL of the IM service or the OIDC issuer. External Links show a set of configurable information links (documentation, video tutorials, etc.) Finally, on the right top corner, appears the User menu item. This item shows the full name of the logged user, and an avatar obtained from Gravatar. In this menu the user can access their Cloud Credentials with the cloud providers or log out from the application. Cloud Credentials To be able to access any Cloud site the user must specify the credentials to access them. This page allows the user to specify the credentials for accessing any cloud provider. In the list the user can edit, delete and enable or disable the selected cloud credentials.\nEditing or adding the credentials will show a modal form where the user has the ability to specify all the parameters needed to access the supported cloud providers. In particular, for Cloud Compute sites the user only has to select one of the VOs he is member of and one of sites that supports that VO. These drop-down fields are generated using the information available from the sites and the list of VOs the user is member of.\nTOSCA Templates The list of available TOSCA templates enables the user to select the required topology to deploy. Each TOSCA template can be labelled by the TOSCA developer with any tag that will show a ribbon displayed on the right bottom corner. The special elastic tag is used to mark templates that are configured to automatically manage the elasticity of the deployed cluster.\nThe user must click on the Configure button to set the input values of the TOSCA template and select the VO, Site and Image to use for deploying the infrastructure.\nInitially the user can set a name to describe the infrastructure to be deployed. It will simplify identifying infrastructures. In the firsts tabs, the user can introduce the set of input values of the topology. By default there is only one tab called Input Values, but the TOSCA developer can add or rename them to simplify the selection of input values.\nThe final tab will be the Cloud Provider Selection. In this tab the user first has to select one of the Cloud providers that has been previously added (and not disabled) in the Cloud Credentials page, then has to select the base image used to deploy the VMs. In case of EGI Cloud Compute sites, the user has two options: he can select an image from the list of images provided by the EGI AppDB information system or from the list provided directly by the Cloud site.\nOther providers will only show a drop-down list with the available images. Only in the case of AWS Cloud provider the user has to specify manually the AMI ID of the image.\nInfrastructures This page lists the infrastructures deployed by the user. The first column shows the name set by the user on infrastructure creation, then shows the ID assigned by the IM service, the third column shows the current status of the infrastructure, the fourth show the list of VMs with their IDs and finally a button with a set of actions appears.\nList of Actions The following figure shows the list of actions available to manage the existing infrastructures:\nAdd nodes: The Add nodes action enables to add new VMs to the users' deployment. It will show the list of the different types of nodes currently deployed in the infrastructure and the user must set the number of nodes of each type he wants to deploy. It will also show a dropdown list with the available base images. It will enable changing the base image used to deploy the new nodes. In some cases it will be necessary because the original one has been removed.\nShow template: This action shows the original TOSCA template submitted to create the infrastructure.\nLog: Shows the error/contextualization log of the infrastructure.\nStop: Stops/Suspends all the VMs of the infrastructure.\nStart: Starts/Resumes a previously stopped infrastructure.\nOutputs: Shows the outputs of the TOSCA template. Private key of credentials can be downloaded as a file or copied to the clipboard.\nDelete: Delete this infrastructure and all the associated resources. It also has the option to force the deletion. In this case the infrastructure will be removed from the IM service even if some cloud resources cannot be deleted. Only use this option if you know what you are doing.\nDelete \u0026 Recreate: Delete this infrastructure as the previous option, but once it is deleted it will redirect to the infrastructure creation form, with all the input fields filled with the same set of values used to create the deleted infrastructure.\nReconfigure: Starts the reconfiguration of the infrastructure.\nChange User: Add or change the ownership of the infrastructure at IM level. Providing a valid Access Token of another user, the infrastructure can be shared or transferred to them. If Overwrite is checked, the new user will be the unique owner of the infrastructure (transferring), otherwise it will be added to the list of current users (sharing). The new user must be member of the VO used to create the resources, otherwise he will not be able to manage them. The list of current owners of the infrastructure will also be displayed.\nVM Info page The VM Info page shows all the information about the selected VM and enables to manage the lifecycle of it. On the top right corner the Manage VM drop-down menu allows to Stop/Start, Reboot, Resize, Reconfigure and Terminate the VM. Furthermore the user can check the error or contextualisation log of this particular VM.\nThe VM information is split in two different tables, the first one with the main information: State, IPs, HW features and the SSH credentials needed to access it. The second table shows additional fields.\nWhen resizing the VM the user must provide the new size of the VM in terms of number of CPUs and amount of memory as shown in the figure below:\n","categories":"","description":"The dashboard of Infrastructure Manager\n","excerpt":"The dashboard of Infrastructure Manager\n","ref":"/users/compute/orchestration/im/dashboard/","tags":"","title":"Infrastructure Manager Dashboard"},{"body":"Definition A downtime is a period of time for which a service is declared to be inoperable. Downtimes may be scheduled (e.g. for software/hardware upgrades), or unscheduled (e.g. power outages). The Configuration Database stores the following information about downtimes (non exhaustive list):\nThe downtime classification (Scheduled or unscheduled) The severity of the downtime The date at which the downtime was added The start and end of the downtime period A description of the downtime The entities affected by the downtime Manipulating downtimes Viewing downtimes There are different pages in the Configuration Database where downtimes are listed:\nActive \u0026 Imminent, linked from the main menu, that allows users to see currently active downtimes and downtimes planned in the coming weeks. Downtime Calendar, linked from the main menu, that allows users to view and filter all downtimes. Site details, where all the downtimes associated to the site are listed Service endpoint details, where all the downtimes associated to the service endpoint are listed. Service group details, where all the downtimes associated to the service group are listed. Each downtime has its own page providing details, accessible by clicking on the Downtime Id link or similar in downtime listing pages.\nSubscribing to downtimes The EGI Operations Portal provides a publicly-accessible page allowing to view and filter downtimes: Operations Portal.\nAuthenticated users can subscribe to downtimes affecting sites selected using a filter. The downtimes notifications can be sent by email, RSS and iCal, allowing to easily integrate with your calendar.\nAdding downtimes Provided you have proper permissions (check the permissions matrix section), you can add a downtime by clicking on the Add Downtime link in the sidebar.\nThis is done in 2 steps:\nEnter downtime information Specify the full list of impacted services in case there is more than one or select an site to select all the sites associated services. Please note All dates have to be entered in UTC or using the Site Timezone. A downtime can be retrospectively added if its start date is less than 48h in the past (giving a 2 day window to add). downtime classification (scheduled/unscheduled) is determined automatically (see Scheduled or unscheduled section) Editing downtime information To edit a downtime, simply click the edit link on top of the downtime’s details page. A downtime can be retrospectively updated if its start date is less than 48h in the past (giving a 2 day window to modify). Note there are limitations to downtime editing, especially if it has already started, or is due to start in the next 24hrs or is finished. See downtime shortening and extension section for more details. Removing downtimes To delete a downtime, simply click the delete link on top of the downtime’s details page. For integrity reasons, it is only possible to remove downtimes that have not started.\nGood practices and further understanding Scheduled or unscheduled Depending on the planning of the intervention, downtimes can be:\nScheduled: planned and agreed in advance Unscheduled: planned or unplanned, usually triggered by an unexpected failure or at a short term notice EGI defines precise rules about what should be declared as scheduled or unscheduled, based on how long in advance the downtime is declared. These rules are described in MAN02 Service intervention management and are enforced as follows:\nAll downtimes declared less than 24h in advance will be automatically classified as UNSCHEDULED All other downtimes will be classified as SCHEDULED Notes A downtime can be retrospectively declared and/or updated if its start date is less than 48h in the past (giving a 2 day window to add/modify). Although 24h in advance is enough for the downtime to be classified as SCHEDULED, it is good practice to declare it at least 5 working days before it starts. WARNING or OUTAGE? When declaring a downtime, you will be presented the choice of a “severity”, which can be either WARNING or OUTAGE. Please consider the following definitions:\nWARNING means the resource is considered available, but the quality of service might be degraded. Such downtimes generate notifications, but are not taken into account by monitoring and availability calculation tools. In case of a service failure during the WARNING period an OUTAGE downtime has to be declared, cancelling the rest of the WARNING downtime.\nOUTAGE means the resource is considered as unavailable. Such downtimes will be considered as in maintenance by monitoring and availability calculation tools.\nDowntime shortening and extension Limitation rules to downtime extensions are enforced as follows:\nScheduled downtimes due to start in 24 hours cannot be edited in any way, nor deleted. Other downtimes that have not yet started can be edit and deleted. They can be shortened or moved, i.e. They can be edited such that: Both start and end time are still in the future The duration remains the same or is decreased Ongoing downtimes can not be deleted. A downtime cannot be edited once it has finished, nor can a new downtime be added more than 48 hours into the past. If for any reason a downtime already declared needs to be extended, the procedure is to add another adjacent downtime, before or after.\n","categories":"","description":"Managing and consulting downtimes","excerpt":"Managing and consulting downtimes","ref":"/internal/configuration-database/downtimes/","tags":"","title":"Downtimes"},{"body":"What is it? EGI Data Transfer allows scientists to move any type of data files asynchronously from one storage to another. The service includes dedicated interfaces to display statistics of ongoing transfers and manage storage resource parameters.\nEGI Data Transfer is ideal to move large amounts of files or very large files as the service has mechanisms to verify checksums and ensure automatic retry in case of failures.\nThe main features of EGI Data Transfer are:\nSimplicity. Easy user interfaces for submitting transfers (command-line, Python bindings, Web Monitoring). Reliability. Checksums are automatically calculated for each transfer and failed transfers are retried. Flexibility. Multi-protocol support (WebDAV/HTTPS, GridFTP, xrootd, SRM, S3, GCloud). Intelligence. Parallel transfer optimization ensures users get the most from network without burning the storages. Transfers can be classified by priority and activity. Tip Eager to test this service? Have a look at our tutorial on how to transfer data in the grid. Note EGI Data Transfer is based on the FTS3 service, developed at CERN. Components FTS3 Server The service is responsible for the asynchronous execution of the file transfer, checksumming and retries in case of errors\nFTS3 REST The RESTFul server which is contacted by clients via REST APIs, CLI and Python bindings\nFTS3 Monitoring A Web interface to monitor transfers activity and server parameters\nService Instances The following endpoints are available:\nCERN FTS REST FTS Mon N.B. if you access the endpoints via Browser the following CA certificates need to be installed:\nCERN CA certificates ","categories":"","description":"Very large data transfers in the EGI infrastructure","excerpt":"Very large data transfers in the EGI infrastructure","ref":"/users/data/management/data-transfer/","tags":"","title":"EGI Data Transfer"},{"body":"Introduction Sites, Services, service endpoints, and Service Groups can be extended by adding custom key-value pairs (this follows the GLUE2 extensibility mechanism). Extension properties address a number of use cases, such as filtering Sites and/or Services that define particular properties. Selected methods in the Configuration Database API support the ’extensions' URL parameter. This parameter is used to filter resources according to the extensions they define (described below). Properties are rendered in the XML results of the Site/Service/ServiceGroup using the “EXTENSIONS” XML element, for an example see a sample output from get_service_endpoint(link to old EGI Wiki) Note, anyone with permissions over the target entity can add extension properties to that object. This allows ‘Folksonomy’ building: ‘a user-generated system of classifying and organizing content into different categories by the use of metadata such as electronic tags’ A number of use cases can be addressed; e.g. filtering Sites that support a specific property, e.g. ‘P4U_Pilot_Cloud_Wall’ Key-value pairs prevent certain characters from being used in their values. This includes the equals and opening/closing parenthesis chars ‘=()’. This is to simplify lexical parsing of the query. In addition, to guard against cross-site scripting attacks, the quote, double quote, semi-colon and back tick chars are also not allowed. Keys must be unique for a given site, service, or service endpoint, or service group. Extension Properties in the PI Selected PI methods allow results to be filtered by extension properties via the ’extensions’ PI parameter. Supported methods include: get_site, get_site_list, get_service_endpoints and get_service_group, get_downtime, get_downtime_nested, get_site_list. For individual method support please refer to the PI documentation: GOCDB/PI/Technical Documentation (link to old EGI Wiki) The format of the ’extensions’ PI parameter is one or more (key=value) pairs enclosed in brackets. The value part of a (k=v) pair can be omitted if filtering by value is not required (i.e. ‘(somekey=)’ means select all resources that define the ‘somekey’ property with any value. (k=v) pairs can be optionally prefixed with one of following operators: AND, OR, NOT. If no operator is specified before the FIRST (k=v) pair, then AND is assumed. A single operator applies to ALL the (k=v) pairs to the right of the operator until another operator is encountered. An AND forms a logical conjunction with any previously specified conditions. An OR forms a logical disjunction with any previously specified conditions. A NOT forms a logical conjunction with any previously specified conditions (it can be read as ‘AND NOT’) Because an OR always forms a logical disjunction with any previously specified conditions, you can’t OR against a group occurring to the right that contains multiple k=v pairs e.g. the following is not supported (if there is sufficient demand, it could be considered for a future enhancement): - ((k=v1)AND(k=v2)) OR ((k=v3)AND(k=v4)) Examples:\nEg (note no leading AND): (key1=val)(key2=va2)OR(key3=val3)(key4=val4)NOT(key5=val5)(key6=val6) is expanded to: AND(key1=val)AND(key2=va2)OR(key3=val3)OR(key4=val4)NOT(key5=val5)NOT(key6=val6) which is interpreted as: (((key1=val)AND(key2=va2))OR(key3=val3)) OR(key4=val4) NOT(key5=val5) NOT(key6=val6) Eg: (VObing=true)AND(VObaz=true)AND(VObar=true)OR(s1p1=v1) is equal to: ((VObing=true)AND(VObaz=true)AND(VObar=true))OR(s1p1=v1) Eg: (VO=food)OR(VO2=bar)AND(s4p1=v1) is equal to: ((VO=food)OR(VO2=bar))AND(s4p1=v1) Eg: (VO=food)(s4p1=v1)OR(VObar=true)(VObaz=true) is equal to: ((VO=food)AND(s4p1=v1))OR(VObar=true)OR(VObaz=true) Eg: (VO=food)(s4p1=v1)OR(VObaz=true)AND(VObling=true) is equal to: (((VO=food)AND(s4p1=v1))OR(VObaz=true))AND(VObling=true) To return all sites that define VO with a value of Alice:\n?method=get_site\u0026extensions=(VO=Alice) Use no value to define a wildcard search, i.e. all sites that define the VO property regardless of value:\n?method=get_site\u0026extensions=(VO=) NOTE: From version 5.7 (Autumn/Winter 2016) keys must be unique for a given site, service, or service endpoint, or service group. The following section of documentation has not yet been changed to reflect this.\nExtensions also supports OR/AND/NOT operators. This can be used to search against multiple key values eg:\n?method=get_site\u0026extensions=AND(VO=Alice)(VO=Atlas)(VO=LHCB) These can be used together:\n?method=get_site\u0026extensions=AND(VO=Alice)(VO=Atlas)NOT(VO=LHCB) ?method= get_service_endpoint\u0026extensions=(CPU_HS01_HOUR=1)OR(CPU_HS02_HOUR=2) When no operator is specified the default is AND, therefore the following:\n?method= get_service_endpoint\u0026extensions=(CPU_HS01_HOUR=1)(CPU_HS02_HOUR=2) Is the same as:\n?method= get_service_endpoint\u0026extensions=AND(CPU_HS01_HOUR=1)(CPU_HS02_HOUR=2) The extensions parameter can also be used in conjunction with the existing parameters previously supported:\n?method=get_site\u0026extensions=(VO=Alice)NOT(VO=LHCB)\u0026scope=EGI\u0026roc=NGI_UK The site_extensions and service_extensions can also be used on the get_downtime and get_downtime_nested_services methods using same logic described above. Note, the EXTENSIONS element is not rendered in the XML output for these queries.\n?method=get_downtime_nested_services\u0026site_extensions=(eg.2=val.2)\u0026service_extensions=(eg.2=) ?method=get_downtime\u0026site_extensions=(eg.2=val.2)\u0026service_extensions=(eg.2=) Standard Extension Properties HostDN For EGI Services, the Standard Extension property “HostDN” has been defined to allow the fetching the DNs of multiple hosts behind a load balanced service from the endpoint properties of a single GOCDB Service, rather than creating multiple GOCDB Services with different host DNs.\nRecommended Use To supply multiple or alternate DN(s) for a service, for example of the multiple hosts supporting a single service entry, the Service Extension Property (hereafter Ext) “HostDN” SHOULD be used. If Ext “HostDN” is present it MUST contain one or more x.509 DN values. Multiple values MUST be delimited by enclosing each within “\u003c\u003e” characters. If Ext “HostDN” is present, the Service “Host DN” SHOULD contain the x.509 SubjectAltName used in the X509 certificate(s) presented by the hosts identified by the Ext “HostDN” values.\n","categories":"","description":"Extension Properties\n","excerpt":"Extension Properties\n","ref":"/internal/configuration-database/extension-properties/","tags":"","title":"Extension Properties"},{"body":"Web interface In the EGI DataHub, all files are organized in spaces. The Web User interface allows for uploading new files as well as opening existing files.\nIn order to upload a file, open the directory in which the file should be placed and drag the file into the browser window:\nOpening or downloading a file requires double clicking on the file in the file window.\nNote Make sure that the pop-ups for this browser window are not blocked, and unblock them if necessary. Direct access via POSIX Files can also be accessed directly via the POSIX interface, using Oneclient tool. Details on how to use are described in the official Oneclient documentation.\nCDMI (Cloud Data Management Interface) For more advanced use cases, files can be managed using the CDMI protocol, as described in details in the Onedata CDMI documentation.\nFile Permissions You can control access to your data with a combination of:\nclassical (POSIX) file permissions Access Control Lists (ACL) POSIX Permissions DataHub allows you to control access to your data in a POSIX-like fashion for users, group and others in terms of read, write and executable permissions.\nAn important nuance regarding file permissions is that all space members are treated as a virtual group which is the group owner of all files in the space. That means that whenever a file is accessed by a space member who is not the owner of the file, the group permissions are taken into consideration. Permissions for “others” are considered when accessing shares.\nConsider the following example of a file’s POSIX permissions:\nrwx r-- --- | | | | | guests | | | space members | owner user In the above case, the creator of the file, its owner user, has a full access to the file. All space members have read access to the file. Other users, guests, who try to access the file through a share will fail to do so as all permissions are declined for “others”.\nIn order to edit permissions:\nClick on Data on the left menu bar Select the Space you want to access and the Data option Select a file or a directory and right clicking on it Click on the Permissions option Select POSIX type of permissions radio button at the top Enter privileges in octal form (e.g. 770) Click OK in order to save changes Access Control Lists You can also setup permissions using more advanced Access Control Lists option to control permissions for individual users and groups.\nIn order to edit ACLs:\nNavigate to Data tab Select a file or a directory by right clicking on it Click on the Permissions option Click on the ACL radio button Edit permissions by clicking on the appropriate checkbox Click OK in order to save changes Note, that access lists take precedence over POSIX permissions. If access list is set, POSIX are set to octal value of 000.\nThe order in which permissions take precedence is indicated with an arrow.\nTo limit the risk of making data inaccessible to yourself, it is advisable to first add the required ACL if new ones are needed, and if necessary remove the ones not needed. While using ACLs there should be at least one active, or no user might be able to access the data. To regain the access the space owner would then need to modify the permissions.\nFile sharing It is possible to share available DataHub data with other users or externally by generating a unique URL. This can be done, on the web interface, by selecting a directory or file, and right-clicking on it or clicking the three dots on the right like in the following screenshot and selecting “share”:\nThis will open a window allowing you to give a name to the share that you are about to create. This is shown in the following screenshot:\nBy clicking on the “Create” button, the share is created and you are taken to the last window where the URL of the share can be copied for further use. This is shown in the following screenshot:\nTo manage existing shares, in the space that is in use, it is sufficient to click the “Share” section, then a list of the existing ones is shown. By clicking on the three dots on the right of the share, you can rename, remove or copy the URL of the share. This is shown in the following screenshot:\nBy clicking on the share itself, it is possible to edit additional attributes of the share like the description and the possibility to publish it as Open Data trough one of the supported handle service.\nAn API is also available for the creation and administration of the shares.\nData transfer management After uploading some data to DataHub, you can manage the replication and transfer to other DataHub providers supporting the same space. To do so, after uploading some files, select them, right click on the selection, and select “Data distribution”. This will open the following window:\nThis will show the providers available for the space being used, and the current operation being performed if any. By clicking on the three dots, next to the provider, the following operation can be performed:\nMigrate the data if the provider selected contain the data. This will copy the data to the other provider, if not present, and remove them from the selected one. Replicate the data, if the data is not present in the provider selected Evict the data, if the data is present in the current provider, and al least in another one Data transfer management API The DataHub REST API structure gives an overview of the REST calls available. In particular the section on Transfer focus on the transfer operation which include the operation previously described using the web interface and include option to schedule transfers, a feature which is not available trough the web interface. Furthermore the status of the transfer can also be checked from the section Get transfer status of the same page.\nThe /transfers/ operations provide basic transfer management functionality based on the ID of transfer returned by /transfers [POST] operation.\nWith the API it is possible to get information about a specific transfer by simply querying the following resource:\n$ curl -X GET -H \"X-Auth-Token: $ACCESS_TOKEN\" \\ https://$ONEPROVIDER_HOST/api/v3/oneprovider/transfers/\u003cTRANSFER_ID\u003e or request all transfers for given space:\n$ curl -X GET -H \"X-Auth-Token: $ACCESS_TOKEN\" \\ https://$ONEPROVIDER_HOST/api/v3/oneprovider/spaces/$SPACE_ID/transfers Each transfer can be cancelled using the HTTP DELETE method:\n$ curl -X DELETE -H \"X-Auth-Token: $ACCESS_TOKEN\" \\ https://$ONEPROVIDER_HOST/api/v3/oneprovider/transfers/\u003cTRANSFER_ID\u003e ","categories":"","description":"File Management in DataHub","excerpt":"File Management in DataHub","ref":"/users/data/management/datahub/file-management/","tags":"","title":"DataHub File Management"},{"body":"The pages of this section detail multiple features of the EGI Helpdesk.\n","categories":"","description":"Helpdesk features","excerpt":"Helpdesk features","ref":"/internal/helpdesk/features/","tags":"","title":"Helpdesk features"},{"body":"What is it? High Throughput Compute (HTC) is a computing paradigm that focuses on the efficient execution of a large number of loosely-coupled tasks (e.g. data analysis jobs). HTC systems execute independent tasks that can be individually scheduled on many different computing resources, across multiple administrative boundaries. Users submit these tasks to the infrastructure as jobs. After a job have been scheduled and executed, the output can be collected from the service(s) that executed the job.\nTarget users The target customers for EGI High Throughput Compute are research communities that need to share, store, process, and produce large sets of data. Typically, their research collaborations involve organizations across Europe and the World.Some may already have local resources (e.g. universities, research institutions) that can only be accessed by local users in accordance to the respective organisation’s access policies.\nIn case of local compute resources researchers can request access to the local compute cluster from their IT department. However, when researchers join collaborations that need to share their research activities, data collections, and repositories, they need a homogeneous and coordinated operation of the compute resources, which are not uniformly accessible. In addition, nowadays many research collaborations generate large amounts of data, and managing such data volumes is time consuming and error-prone.\nThe EGI High Throughput Compute service provides access to compute resources, and offers a set of high-level tools that allow managing large amounts of data in a collaborative way (e.g authorization and access control tools can be regulated by the research collaboration in a central manner, data can be uniformly distributed in the EGI Cloud, etc.).\nFeatures EGI High Throughput Compute provides easy, uniform access to shared computing and data services of EGI service providers. Most software deployed in the distributed resource centres is based on open standards and open source middleware services.\nThe main features of the EGI High Throughput Compute are:\nAccess to high-quality computing resources Integrated monitoring and accounting tools provide information about the availability and resource consumption Workload and data management tools to manage all computational tasks Large amounts of processing capacity over long periods of time Faster results for your research Shared resources enable collaborative research The EGI HTC infrastructure The EGI High Throughput Compute infrastructure is the federation of GRID resources provided by EGI providers. Its aim is to share in a secure way the distributed IT resources that are part of the EGI Cloud. It comprises of:\nCompute Resources – execution environment for computing tasks, organized into clusters distributed across multiple resource centres in Europe and the World. Data Infrastructure – storage servers from different resource centres where users can store their data/files in a distributed manner. Federated Operations – global operational tasks (e.g., AAI, Accounting, Helpdesk) needed to federate the heterogeneous resources of resource centres and their operational activities. User Support – EGI provides the central user support and coordinates support activities of EGI providers, who offer user support for the services/resources they contribute to the EGI ecosystem. Architecture and service components The key components of the EGI High Throughput Compute architecture are:\nData Transfer service (FTS) Online Storage services Computing Elements (CEs) are compute resources made available through GRID interfaces. The most common implementations of CEs in the EGI infrastructure are HTCondor-CE and ARC-CE. Access model Access to HTC resources in the EGI infrastructure is based on X.509 certificates and Virtual Organisations (VOs).\nVOs are fully managed by research communities, allowing communities to manage their users and grant access to their services and resources. This means communities can either own their resources and use EGI services to share (federate) them, or can use the resources available in the EGI infrastructure for their scientific needs.\nBefore users can access EGI HTC services, they have to:\nObtain an X.509 certificate. The certificates are issued by Certification Authorities (CAs) part of the European Policy Management Authority for Grid Authentication (EUGridPMA), which is also part of the International Global Trust Federation (IGTF). Enrol into a VO having access to HTC resources, as users are not individually granted access to resources. Add the certificate to their internet browser of choice, or import it into the appropriate certificate store of their local machine (on Windows). Proceed to Workload Manager to submit HTC jobs or retrieve job results, login using EGI Check-in when prompted. If you are interested in using command-line and direct submission to Compute Elements, there is a tutorial on HTC job submission.\n","categories":"","description":"EGI High Throughput Compute service\n","excerpt":"EGI High Throughput Compute service\n","ref":"/users/compute/high-throughput-compute/","tags":"","title":"High Throughput Compute"},{"body":"Users of the EGI Cloud create Virtual Machines (VMs) on the providers. Those VMs are started from images: templates for the root volume of the running instances, i.e. operating system and applications available initially on a VM. The AppDB collects the Virtual Machine Images available on the service as Virtual Appliances (VA).\nAny user can register new Virtual Appliances at the AppDB, these are then managed by special VO members that curate which appliances are available to their VO.\nAppDB Cloud Marketplace The AppDB is a browsable catalogue of Virtual Appliances that users can start at the providers. You can find below a set of reference guides for the catalogue:\nHow to register a VA?: any registered user can register VAs in AppDB for anyone to download or for making them available at the EGI Cloud providers once a VO adds it to the VO-wide image list. Once registered, VAs can be managed as described in the VA management guide. VO managers select VAs to be available at the providers following the VO-wide image list management. Check the full list of Cloud marketplace guides and Cloud marketplace FAQ for more information about the AppDB features.\nCustom images Packaging your application in a custom VM image is a suggested solution in one of the following cases:\nyour particular OS flavor is not available at AppDB; installation of your application is very complex and time-consuming for being performed during contextualization; or you want to reduce the number of 'moving-parts' of your software stack and follow an immutable infrastructure approach for deploying your application. Custom VM images can be crafted in different ways. The two main possibilities are:\nstart from scratch, creating a virtual machine, installing an OS and the software on top of it, then taking the virtual machine OS disk as custom image; or dump an existing disk from a running VM or physical server and modify it, if needed, to run on a virtualisation platform. In this guide we will focus on the first option, because it tends to produce cleaner images and reduces the risks of hardware conflicts. Snapshotting may be also restricted by the cloud providers or by security policies.\nAdvantages:\nPossibility to build the virtual disk directly from a legacy machine, dumping the contents of the disk. Possibility to speed-up the deployment for applications with complex and big installation packages. This because you do not need to install the application at startup, but the application is already included in the machine. Disadvantages:\nBuilding a virtual disk directly from a legacy machine poses a set of compatibility issues with hardware drivers, which usually differs from a virtual and physical environment and even between different virtual environments. You need to keep your machine updated. Outdated VM disk images may take a long time to startup due to the need to download and install the latest OS updates. If you are using special drivers or you are not packaging correctly the disk, your custom VM image may not run (or run slowly) on different cloud providers based on different virtualisation technologies. VM images on public clouds are sometimes public, thus be aware of installing proprietary software on custom images, since other users may be able to run the image or download it. In general, the effort to implement this solution is higher than the basic contextualization. Image size and layout The larger the VM image, the longer it will take to be distributed to the providers and the longer it will take to be started on the infrastructure. As a general rule, always try to make images as smaller as possible following these guidelines:\nDO NOT include (big) data in your image. There are other mechanisms for accessing data from your VM (block/object storage, CVMFS)\nDO NOT include (big) empty space or swap in your image. Extra space for your computation or swap can be added with block storage once the VM is booted or using VM flavors that have extra disk allocated for your VM.\nDO NOT install un-needed software. Tools like GUI are of no-use in most cases since you will have no access to the graphical console of the VM.\nDO adjust the size of the images as much as possible. As stated above, empty space can be allocated on runtime easily.\nDO use compressed image formats, like qcow2 or vmdk (used in OVA) to minimize the size of the image. Preferred format for images in EGI is OVA as it's standardised.\nDO fill with 0 the empty disk space of your image so when compressed it can be significantly reduced, e.g. using:\ndd if=/dev/zero of=/bigemptyfile bs=4096k rm -rf /bigemptyfile DO use a single partition (no /boot, no swap) for the disk layout and avoid LVM. This will allow the cloud provider to easily resize your partition when instantiated and to modify files in it if needed.\nContextualization and credentials Danger Do NOT include any credentials on your images. You should never include any kind of credentials on your images, instead you should use contextualization. cloud-init is a tool that will simplify the contextualization process for you. This is widely available as packages in major OS distributions and is supported by all the providers of the EGI Cloud and most of the commercial providers.\ncloud-init documentation contains detailed examples on how to create users, run scripts, install packages and several other actions supported by the tool.\nFor complex setups, especially when applications involve multiple VMs it may be useful to use cloud-init to bootstrap some Configuration Management Software that will manage the configuration of the VMs during runtime.\nSecurity Always remove all default passwords and accounts from your VM. Disable all services unless necessary for the intended tasks. Make sure the firewall configuration (iptables for Linux, also on IPv6) is minimally open. Put no shared credentials (passwords) in any image. You should also follow the best practice guides for each service that's exposed to the outside world. See for example guides for:\nssh tomcat See also AWS security Best Practices\nTools Whenever possible, automate the process of creating your images. This will allow you to:\nGet reproducible results Avoid tedious manual installation steps Quickly produce updated versions of your images EGI uses packer as a tool for automating the creation of our base images. This tool can use VirtualBox as a hypervisor for the creation of the images and guarantees identical results under different platforms and providers.\nCheck out the fedcloud-vmi-templates GitHub repository for all the packer recipes used to build our images, re-use them as needed for your images.\n","categories":"","description":"Managing VM images in the EGI Cloud\n","excerpt":"Managing VM images in the EGI Cloud\n","ref":"/users/compute/cloud-compute/images/","tags":"","title":"Virtual Machine Images"},{"body":"Information discovery provides a real-time view about the actual images and flavors available at the OpenStack for the federation users. It runs as a single python application cloud-info-provider that pushes information through the Messaging Service.\nBDII is deprecated Cloud providers no longer need to provide BDII as the Argo Messaging Service is used instead for transferring information You can either run the service by yourself or rely on central operations of the cloud-info-provider. In both cases you must register your host DN in the GOCDB entry for the org.openstack.nova.\nCatch-all operations EGI can manage the operation of the cloud-info-provider for the site so you don’t need to do it. In order for your site to be included in the centrally operated cloud-info-provider, you need to create a Pull Request at the EGI-Federation/fedcloud-catchall-operations repository adding your site configuration in the sites directory with a file like this:\nendpoint: \u003cyour endpoint as declared in GOCDB\u003e gocdb: \u003cyour site name as declared in GOCDB\u003e vos: # a list of VOs you support in your deployment as follows - auth: project_id: \u003clocal OpenStack project identifier\u003e name: \u003cname of the vo\u003e - auth: project_id: \u003clocal OpenStack project identifier for second VO\u003e name: \u003cname of another vo\u003e Once PR is merged, the service will be reconfigured and your site should start publishing information.\nLocal operations You can operate by yourself the cloud-info-provider. The software can be obtained as RPMs, debs and python packages from the GitHub releases page.\nThe cloud-info-provider needs a configuration file where your site is described, see the sample OpenStack configuration for the required information. The authentication parameters for your local OpenStack and the AMS are passed as command-line options:\ncloud-info-provider-service --yaml-file \u003cyour site description.yaml\u003e \\ --middleware openstack \\ --os-auth-url \u003cyour keystone URL\u003e \\ [ any other options for authentication ] --format glue21 \\ --publisher ams \\ --ams-cert \u003cyour host certificate\u003e \\ --ams-key \u003cyour host secret key\u003e \\ --ams-topic \u003cyour endpoint topic\u003e For authentication, you should be able to use any authentication method supported by keystoneauth, for username and password use: --os-password and --os-username. Check the complete list of options with cloud-info-provider-service --help.\nThe AMS topic has the format: SITE_\u003cSITE_NAME\u003e_ENDPOINT_\u003cGOCDB_ID\u003e, where \u003cSITE_NAME\u003e is the name of the site as declared in GOCDB and \u003cGOCDB_ID\u003e is the ID of the endpoint in GOCDB. For example, this endpoint would have a topic like: SITE_IFCA-LCG2_ENDPOINT_7513G0.\nYou should periodically run the cloud-info-provider (e.g. with a cron every 5 minutes) to push the information for consumption by clients.\nUsing the EGI FedCloud Appliance The appliance provides a ready-to-use cloud-info-provider configuration if you want to operate it by yourself. Once you have downloaded the appliance check the following files:\n/etc/cloud-info-provider/openstack.rc: the configuration of the account used to log into your OpenStack and the location of the host certificate that will be used to authenticate to the AMS.\n/etc/cloud-info-provider/openstack.yaml: the cloud-info-provider configuration. You need to enter the details about the VOs/projects that the site is supporting.\nThe appliance has a cron job that will connect to the configured OpenStack API and send messages every 5 minutes.\n","categories":"","description":"cloud info provider configuration\n","excerpt":"cloud info provider configuration\n","ref":"/providers/cloud-compute/openstack/cloud-info/","tags":"","title":"Information System"},{"body":"This section documents how to integrate a new application in EC3.\nAbout The process to integrate a new application in EC3 is described by the following process:\nDescribe the application to be integrated with Ansible, the open-source automation engine that automates software provisioning, configuration management, and application deployment. Use the Ansible receipt to create a new RADL template Documentations Ansible documentation EC3 documentation RADL Note For more information please contact\nMiguel Caballer: micafer1 \u003cat\u003e upv.es Amanda Calatrava: amcaar \u003cat\u003e i3m.upv.es ","categories":"","description":"How to integrate a new application in EC3\n","excerpt":"How to integrate a new application in EC3\n","ref":"/users/compute/orchestration/im/ec3/developers/","tags":"","title":"Instructions for Developers"},{"body":"This section contains documentation about the internal EGI services. These services are being operated centrally on behalf of EGI, and are supporting the coordination of the EGI Federation.\nNote See the User Guides section for documentation about public EGI services, and the Service Providers section for details on how to integrate providers into the EGI Federation. Guidelines for software development We provide Guidelines for software development to be considered when developing a product for the EGI Federation.\nRequest for information You can ask for more information about the internal EGI services on our site.\n","categories":"","description":"Documentation for internal EGI services","excerpt":"Documentation for internal EGI services","ref":"/internal/","tags":"","title":"Internal Services"},{"body":"USER tickets // jscpd:ignore-start\nFor generic user tickets selectable issue types are explained in the table below.\nIssue type Description Accounting All kind of issues related to accounting tools (APEL, Accounting Portal and so on). AppDB All kind of issues related to the AppDB or to the interactions with the AppDB. Authorization/Authentication All issues related to authorization and authentication e.g. certificates COD Operations WLCG Coordinator On Duty Operations Catalogue Issues related to file catalogues, like LFC Computing Services All issues related to execution of jobs. It combines two other type of problems (‘Workload management’ and ‘Local batch system’). The ‘Computing Services’ should be used when it is not clear to which subcategory the ticket refersConfiguration Issues of system configuration. Data Management - generic All kind of issues related to data management tools (GFAL, LCG_Util and so on). Databases Issues related to Databases. Deployment - other Issues of software deployment that do not fit any other category. Documentation Issues of missing, wrong, outdated documentation. File Access File access issues. File Transfer File transfer issues, e.g. related to FTS GGUS Bugs and feature requests related to GGUS system. IaaS Operations Issues related to VMs and clients (OCCI, rOCCI, etc.), block storage, networking. Information System Issues related to the BDII. Installation Installation issues. Local Batch System Issues related to the local batch systems. Middleware Issues of middleware stacks like gLite, globus and others. Monitoring Issues of infrastructure and service monitoring. Network problem Issues of network connectivity. Operations Issues of general processes, procedures, information and so on of the entire infrastructure. Other Requests that do not match to any other issue type. Security Issues of infrastructure and operation security. Storage Systems Issues concerning storage systems, like EOS, dCache, DPM VO Specific Software Issues of VO specific software tools and packages. Virtual Appliance Management Issues related to vmcatcher/vmcaster tools, and VA management in general. Workload Management Issues related to workload management system and tools. TEAM tickets Team and alarm tickets have a reduced number of issue types in the drop-down menu.\nIssue type Description Databases Issues related to Databases. File Access File access issues. File Transfer File transfer issues, e.g. related to FTS Local Batch System Issues related to the local batch systems. Middleware Issues of middleware stacks like gLite, globus and others. Monitoring Issues of infrastructure and service monitoring. Network problem Issues of network connectivity. Other Requests that do not match to any other issue type Storage Systems Issues related to storage systems, like EOS, dCache or DPM ALARM tickets Issue type Description Databases Issues related to Databases. File Access File access issues. File Transfer File transfer issues, e.g. related to FTS Local Batch System Issues related to the local batch systems. Middleware Issues of middleware stacks like gLite, globus and others. Monitoring Issues of infrastructure and service monitoring. Network problem Issues of network connectivity. Other Requests that do not match to any other issue type Storage Systems Issues related to storage systems, like EOS, dCache or DPM CMS tickets The CMS VO has an own ticket submit form in the GGUS system. Although this form provides CMS specific issue types no special privilege is required to use it. This does not apply for the TEAM ticket submit form: Here the CMS specific issue types are only selectable for registered CMS TEAM members.\nIssue type Description CMS_AAA WAN Access Issues around WAN CMS_CAF Operations Issues around CAF operations at CERN, incl EOS space requests CMS_Central Workflows Issues around centrally managed MC production and processing CMS_Data Transfers Issues around data transfers, e.g. via Phedex or ASO CMS_Facilities Typically issues at CMS sites CMS_HammerCloud Issues around CMS HammerCloud CMS_Register New CMS Site Chosen when a new site gets registered with CMS CMS_SAM tests Issues around CMS SAM tests CMS_Submission Infrastructure Issues around CMS_Submission Infrastructure CMS_Tier-1 Tape Families For creation of Tape families/groups at Tier-1 archives ATLAS tickets The ATLAS specific issue types are only selectable for registered ATLAS TEAM members.\nIssue type Description ATLAS_ADC Central Issues around ADC in general ATLAS_Databases Issues around databases ATLAS_Deletion Issues around file deletions ATLAS_File Access/Transfer Issues around file access or file transfers ATLAS_Frontier-Squid Issues around frontier or squid ATLAS_Local Batch System Issues around batch systems ATLAS_Middleware Issues related to middleware ATLAS_Monitoring Issues around monitoring ATLAS_Network Problem Issues with network infrastructure ATLAS_Staging Issues around file staging ATLAS_Storage Systems Issues around storage // jscpd:ignore-end\n","categories":"","description":"List of the values of the Issue Type field for the several ticket types\n","excerpt":"List of the values of the Issue Type field for the several ticket …","ref":"/internal/helpdesk/features/issue-type-values/","tags":"","title":"Issue type values"},{"body":"To become a provider of one of the Services for Research already included in the EGI Portfolio, you first have to become provider of a federated resource centre. This first step typically requires you to become either an EGI High Throughput Compute (HTC) provider, by deploying Compute and Storage capabilities, or a Cloud service provider, by deploying OpenStack and federating it into the EGI Cloud Compute service.\nOnce such a foundational role is fulfilled you can configure/deploy the additional service within your resource centre.\n","categories":"","description":"Guidelines for new providers for existing EGI services","excerpt":"Guidelines for new providers for existing EGI services","ref":"/providers/joining/new-provider/","tags":"","title":"Joining as a provider for existing EGI services"},{"body":" Property Value Title Tool Intervention Management Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement How to manage central operational tool unscheduled downtimes Owner SDIS team The purpose of this document is to describe the intervention in case of unscheduled failure of central operational tool.\nScope This manual only applies to unscheduled downtimes of central operational tools. The list of central operational tool is available here.\nNote: Scheduled downtimes are management according to existing procedures (MAN02).\nAnnouncements All announcements should be sent with the Operations Portal Broadcast tool.\nWhen using Operations Portal Broadcast tool the following groups should be included:\nLCG Rollout Mailing List Operators Mailing lists OSG Mailing list Tool Admins Mailing List WLCG Tier 1 contacts NGI managers VO managers VO users Site administrators Operation tools Notice: Individual notification templates together with targets are predefined in Operations Portal Broadcast tool. Administrators are advised to use such predefined templates.\nProcedure In the following sections several relevant scenarios are covered.\nCase 1: short “undetected” downtime Description: Service fails and recovers before administrator manages to react (e.g. short power or network outage).\nAction: Administrator announces the failure by using the following template:\nSubject: [SERVICE_NAME] unscheduled downtime Message: Dear all, [SERVICE_NAME] experienced unscheduled downtime between [START] and [END]. [DETAILED_FAILURE_DESCRIPTION] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] Case 2: long “detected” outage Service fails and administrator detects the problem. The problem takes at least 1 hour time to recover. In the sections below individual situations are described.\n1. Outage Description: Service failure is detected.\nAction: Administrator announces the failure by using the following template:\nSubject: [SERVICE_NAME] outage Message: Dear all, [SERVICE_NAME] is experiencing unscheduled downtime. [ADDITIONAL_INFO] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] 2. Extended downtime Description: Service recovery is delayed. Update should be sent at least every 24h.\nAction: The administrator announces that recovery is taking longer by using the following template:\nSubject: [SERVICE_NAME] extended outage Message: Dear all, Outage of [SERVICE_NAME] is extended. [ADDITIONAL_INFO] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] Note: In this template [ADDITIONAL_INFO] should indicate the estimated time of recovery.\n3. Recovery Description: Service is recovered.\nActions: At the time of recovery the administrator announces the recovery by using the following template:\nSubject: [SERVICE_NAME] recovery Message: Dear all, [SERVICE_NAME] is back online. [ADDITIONAL_INFO] Best Regards [SERVICE_TEAM] 4. Post mortem analysis Description: Service failure required further time to investigate the source of the problem. This action is required only if the post mortem analysis is needed.\nActions: The administrator announces the post mortem analysis of failure by using the following notification template:\nSubject: [SERVICE_NAME] outage analysis Message: Dear all, [SERVICE_NAME] experienced unscheduled downtime between [START] and [END]. [DETAILED_FAILURE_DESCRIPTION] Best Regards [SERVICE_TEAM] ","categories":"","description":"How to manage central operational tool unscheduled downtimes","excerpt":"How to manage central operational tool unscheduled downtimes","ref":"/providers/operations-manuals/man04_tool_intervention_management/","tags":"","title":"MAN04 Tool Intervention Management"},{"body":"This page contains information about using Check-in for managing your Virtual Organisation (VO). For joining a VO please look at Joining Virtual Organisation.\nBackground In simple terms a Virtual Organisation (VO) is just a group of users. In EGI VOs are created to group researchers who aim to share resources across the EGI Federation to achieve a common goal as part of a scientific collaboration. For a more formal definition of VO please look at the EGI Glossary.\nYou can browse existing VOs in the EGI Operations Portal. For each VO you can click on the Details link to get more information. You can join an existing VO either using the enrollment URL or emailing VO managers.\nIf you are interested in creating your own VO, please see instructions in the section below.\nVO management VOs in Check-in are represented as groups that go beyond simple collections of users, providing structured membership management and advanced enrollment workflows tailored for complex access needs. VOs can also be organised in a hierarchical structure for creating groups or subgroups within a VO.\nRegistering your VO Any person who can authenticate to the Operations Portal using their EGI Check-in account can register a new VO.\nThe person initiating the registration is called the VO manager. After the VO is set up and operational, the VO manager is the person who is primarily responsible for the operation of the VO and for providing sufficient information about VO activities for EGI and for VO members (to both people and sites).\nA step-by-step guide for the VO registration process is provided in the procedure PROC14 VO Registration.\nVO Group Management Group Admins Groups are managed by Group Admins, who have several key responsibilities:\nManaging roles for specific permissions within the Group. Managing member roles to users based on their needs or requests. Extending memberships for continued access. Suspending or activating memberships to control user access as required. Managing enrollment configurations to define how users can join the Group. Creating/Deleting Sub Groups within the Group hierarchy. Note: Group Admin is not a role within the Group; it is a separate administrative designation. Group Admins have the ability to manage all aspects of the Group, as well as any sub-groups in the hierarchy, including roles, memberships, and configurations.\nGroup Roles Members of Groups are assigned roles upon joining. Users can join a Group in one of two ways:\nBy accepting an invitation: Users receive the roles specified by the inviting administrator. By submitting an enrollment request: Users can select their preferred roles from the options available, as defined by the Group’s enrollment configuration. Each assigned role includes an entitlement attribute, which grants authorization to specific resources. This flexible approach to role assignment allows Group administrators to control access while offering users the ability to select roles when available.\nNote: Entitlement values can be found on the Group Details Tab\nMembership Status Members of a Group can have different statuses that affect their access and entitlements:\nActive: The membership is fully active, and the user receives all entitlements associated with the roles they hold in the Group.\nSuspended: Administrators can suspend a user’s membership for security reasons, such as suspicious activity. While suspended, the user retains membership but loses all entitlements tied to their roles. Administrators can later revoke the suspension and reactivate the membership.\nPending: A user’s membership can have a future start date based on the enrollment configuration used during joining. This scheduled membership will activate automatically on the specified start date. Administrators also have the option to activate the membership manually if needed.\nNote: Suspension/Activation of a member will also affect all memberships in Sub Groups of target group\nEach status provides Group administrators with flexible control over user access and helps ensure security within the Group.\nMembership Expiration Memberships in Groups come with a defined duration, which may be set to indefinite if allowed by the Group’s configuration settings. However, the duration of any membership is also affected by the Group’s position within the Group hierarchy. Membership in a higher-level Group imposes a duration limit on all memberships in its subordinate Groups. As a result, the expiration date for any membership in a lower-level Group cannot exceed the duration limit set by the higher-level Group.\nUnderstanding Expiration Dates in Group Memberships When viewing members within a Group, you will encounter two types of expiration dates:\nDirect Membership Expiration: This date indicates the expiration of membership specifically for the Group you are currently viewing. It applies only to that Group and not to any other Group within the hierarchy. Effective Membership Expiration: If relevant, this reflects the actual expiration date imposed by a higher-level Group. If a higher-level Group has an earlier expiration date than the Direct Membership Expiration, the Effective Membership Expiration will take precedence, overriding the direct expiration date for the current Group. For example, if a user’s Direct Membership Expiration in a lower-level Group is set to indefinite (or a date beyond 2024), but the Effective Membership Expiration from a higher-level Group is November 12, 2024, the user’s membership will expire on November 12, 2024, in line with the higher Group’s restrictions.\nThis hierarchical approach to managing memberships allows for simplified administration and ensures consistent access policies are maintained across different Group levels.\nPending Memberships with Future Start Dates Some enrollment flows or invitations may specify a starting date in the future. In these cases, users who accept the invitation or submit an enrollment request will have a pending membership status until the specified start date. Once the start date arrives, the membership will automatically activate, changing their status from pending to active.\nView Group Details and Manage Group Roles The Group Details Tab provides essential information and management options, including:\nAvailable Group Roles: Lists roles within the Group and displays the entitlements granted to users with these roles. Group Path: Shows the hierarchical path of the Group within the overall structure. Enrollment Discovery Page URL: Provides a link to the Enrollment Discovery Page, allowing users to access relevant enrollment options. Log in to the Keycloak Account Console using any of your login credentials linked to your account. Go to the Group Management Page and select the target group to access its Group Configuration Page. View Group Details Create Group Role To add a new role, enter the role name in the text input field and click the plus button to create it.\nDelete Group Role Note: A role cannot be deleted from a group if it is assigned to any members.\nLocate the role you want to remove and use the minus button next to it to delete it. Create Sub Group There are two ways to create a subgroup within the platform:\nA) Through the Group Management Page Log in to the Keycloak Account Console using any of your login credentials linked to your account. Go to the Group Management Page to locate your target group. Click on the more options menu next to your group, then select “Create Subgroup” from the available options. B) Using the Sub Groups Tab in the Group Configuration Page Log in to the Keycloak Account Console using any of your login credentials linked to your account. Go to the Group Management Page and select the target group to access its Group Configuration Page. Navigate to the Sub Groups tab to view existing subgroups within this Group. To create a new subgroup, click the plus button (+) at the top of the Sub Groups tab. Alternatively, you can click the more options menu next to an existing subgroup and select the option to create a subgroup within that subgroup. Delete Sub Group Note: Top-level Groups and Sub-Groups that contain additional Sub-Groups cannot be deleted.\nA) Through the Group Configuration Page Log in to the Keycloak Account Console using any of your login credentials linked to your account. Go to the Group Management Page and select the target group to access its Group Configuration Page. Click the trash icon to delete group B) Through the Higher Level Group Log in to the Keycloak Account Console using any of your login credentials linked to your account. Go to the Group Management Page and select the target group to access its Group Configuration Page. Navigate to the Sub Groups tab to view existing subgroups within this Group. To delete a subgroup, click the more options menu next to the subgroup you wish to delete and select the option to delete that. Membership Management View Group Members Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. To view the existing members, select the Group Members tab. Group Members tab view Add Group Members By Invitation / Direct Add Users can be added to a group either by invitation or direct addition:\nBy Invitation: Admins can send an email invitation to users. Upon receiving the invitation, users can log in to the Keycloak Account Console with their Check-in account to accept or reject the invitation. Direct Add: Admins can directly add new members by selecting users who are already registered and belong to a group where they have admin rights. Sending an Invitation to a User or directly adding them the group can be achieved by:\nLogin to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Members tab. Click the Add Member button that opens the pop-up window. Select an Enrollment Configuration Select the role(s) that you want the user have in the Group and click Next Select the user from the drop down selection input or enter an email address to send an Send Invitation. Select whether you want to send an invitation or to add User directly (if option is available) and Confirm. NOTE: Once a user accepts or reject an invitation email notification will be sent to admins of the group\nBy Enrollment Request Users can be added to a group by creating an Enrollment Request. Enrollment Requests can be created through the Enrollment Discovery Page or a Direct Enrollment Link. The Enrollment Discovery has available all the visible and active enrollment flows and the Direct Enrollment Link points to a single Enrollment Flow that must be active.\nΑ) Sharing the Enrollment Discovery Page Link following these steps:\nLogin to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page. Select the more options and from the available options select the “Copy enrollment link to this group”admin-group-subgroups-main.png. Share the copied Enrollment URL with the User. NOTE: Once a user submits an enrollment request admins of the group will receive an email notification.\nB) Sharing an Direct Enrollment Link to a specific Enrollment\nLogin to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Enrollment Tab Locate the desired Enrollment making sure it is active Select the more options and from the available options select the “Copy enrollment link to this group”. Share the copied Enrollment URL with the User. NOTE: Once a user submits an enrollment request admins of the group will receive an email notification.\nRemove Member from Group NOTE: Removing a member from a group will also remove them from all Sub Groups.\nLogin to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Members tab. Locate the User you want to remove from the group Click the X button and then the Yes button at the confirmation pop-up window NOTE: Once a group member is removed admins of the group and the removed user will receive an email notification\nManage Group Member Roles Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Members tab. Locate the User you want to alter their roles. Click the edit button. Alter their roles by selecting the desired ones from the available options. To save edited member roles click the Save button. Extend Group Member Membership Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Members tab. Locate the User you want to alter their roles. Click the edit button. Alter the expiration date using the date picker. To save edited membership details click the Save button. Suspend or Activate Group Member User memberships can be suspended or activated by a group admin by following these steps:\nLogin to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Members tab. Locate the User you want to suspend or activate their membership. Click the suspend/activate button to open the confirmation pop-up window. Optionally provide a justification for your action that will be included in the notification sent to the User and the group Admins. Click the YES button to submit your action NOTE: Once a group member is activated/suspended, admins of the group\nand the user will receive an email notification.\nAdmin Management View Group Admins Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Admins Tab. Group Admin details are available in list form. Add Group Admin Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Admins Tab. Use the input located in the Add New Group Admin section to search for a user to add as a group admin, or type a valid email address to send an invitation.\nNOTE: Selecting a user discovered in the select input and will add the user immediately. Once a User accepts or rejects an invitation and when a user is added directly to a group, group admins receive email notification\nRemove Group Admin Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Admins Tab. Locate User you want to remove from being an admin and click the X button Click the YES button in the confirmation pop-up window NOTE: Once a group admin is removed from a group he and all other admins are sent an email notification\nManage Enrollment Configurations User enrols to a group using a specific enrollment, each enrollment has a configuration that defines the following things:\nEnrollment Name: The identifying name of the enrollment.\nMembership Expiration: The duration of the memberships of users enrolled with this enrollment. See more\nStart Date: Allows for memberships to be activated in future time and not directly after an enrollment is completed.\nRequires Approval: When enabled, enrollment requests submitted by users need to be approved by an administrator; otherwise, requests will be automatically approved.\nComments: If activated Users that are submitting an enrollment request need to also provide additional information.\nAcceptable Use Policy (AUP): Acceptable Use Policy in the form of a URL.\nAvailable Roles: Available roles to users using this enrollment.\nMultiselect Roles: If activated users using this enrollment can select multiple roles.\nVisible to non-members: If activated the enrollment will be available in the Group Enrollment Discovery Page.\nIs Active: Only active enrollments can be used for user enrollments.\nEnrollment Discovery Page Each group has a group enrollment discovery page where users can view all the available (visible) enrollment flows. Selecting an enrollment flow and using the submit button after filling the form creates an enrollment request. Enrollment requests can be used to create a new membership to a group or update an existing one. Always preselected is the default enrollment flow.\nThe Enrollment Discovery Page is accessed through a URL using the group path of a group following this format:\nhttps://aai.egi.eu/auth/realms/id/account/#/enroll?groupPath=/group/path/example\nCreate Enrollment Configuration Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Enrollment Tab Click on the + button located in the table header to open the creation window Fill the form with the necessary information and click the Create button to create the Enrollment Configuration Update Enrollment Configuration Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Enrollment Tab Locate the Enrollment Configuration you want to update in the list. Select the Enrollment Configuration you want to update by clicking on it. Edit the fields you want to update and click the SAVE button to update the Enrollment Configuration NOTE: When updating an enrollment configuration, all ‘pending approval’ and ‘Waiting for reply’ enrollment requests with this configuration are archived.\nDelete Enrollment Configuration Login to Keycloak Account Console using any of the login credentials already linked to your account. Locate your group in the Group Management Page and access the Group Configuration Page by clicking it. Select the Group Enrollment Tab Locate the Enrollment Configuration you want to delete in the list. Select the Enrollment Configuration you want to delete by clicking on it. Click the trash icon next to the Enrollment Configuration name. Click the YES button to delete the Enrollment Configuration in the confirmation pop-up window. NOTE: When deleting an enrollment configuration, all ‘pending approval’ and ‘Waiting for reply’ enrollment requests with this configuration are archived.\nReview Enrollment Request All enrollment requests—no matter for the status—are accessible through the Account Console for Group admins. When a user submits an enrollment request to join a Group and the request requires approval, Group admins are notified via email. These notifications include a direct link to the request that needs to be reviewed, streamlining the approval process. Admins can view and manage these enrollment requests directly from their Account Console, making it easy to keep track of pending requests and process approvals in a timely manner.\nTo review an enrollment request follow these steps:\nLogin to Keycloak Account Console using any of the login credentials already linked to your account. Access the Review Enrollment Requests page available in the Group Management Section. Locate the enrollment request In the list of all pending requests and click on the Review Button to open the Review Page. Check all the information about the User and his Membership. Optionally Leave a justification comment for your Review Action. Approve the request by clicking the green Approve button or reject it by clicking the red Reject button. NOTE: After approving or denying an enrollment request email notifications are sent to the requesting user and other administrators of the group\nEnrollment Request Details Information Available when reviewing an enrollment request:\nGeneral Details Submission Date: Date and time of the submission of the request by the user.\nEnrollment Request State: State of the request. (Pending Approval, Approved, Rejected, Self Reviewed, Archived)\nUser Details The User Details at the time the enrollment request was created:\nFull Name: Full name of the user when the request was submitted.\nEmail: Email address of the user when the request was submitted.\nAuthentication Providers: Authentication provider(s) used by the user for submitting the request.\nAssurance: Information for assessing the confidence level in the identity of the user when the request was submitted.\nShow Current User Details Full Name: Current full name in the user profile\nEmail: Current email address in the user profile\nLinked Identity Providers: Authentication providers linked to the user’s profile.\nMembership Details Group Name: Name of the group\nEnrollment Name: Name of the enrollment configuration used\nGroup Roles: List of the roles that the user will acquire from this enrollment request\nAcceptable User Policy (AUP): Link of the AUP that the user has approved\nMembership Expiration Days: The duration of the membership in days. See more\nComments (or custom name): Additional information from the user submitting the request.\n","categories":"","description":"Managing a Virtual Organisation (VO) in Check-in\n","excerpt":"Managing a Virtual Organisation (VO) in Check-in\n","ref":"/users/aai/check-in/vos/","tags":"","title":"Managing a Virtual Organisation"},{"body":"Introduction An NGI forms a grouping of Sites in EGI Configuration Database. The Configuration Database stores the following information about these groups. The main page listing groups actually shows NGIs/ROCs, and is available from “List of NGIs/ROCs and associated contacts”, linked from the main menu.\nEach NGI has its own listing page, accessible by clicking on the “view” link in group listing pages. A group details page shows users with a role on that group, as well as member sites and associated contacts and roles.\nAdding NGIs Adding groups is not possible through the Input System web interface. If you want to start the registration process of a new NGI, please follow the procedure described on:\nPROC02: Operations Centre creation\nIntegration of the new group in the EGI Configuration Database is part of the procedure but has to be done by the Configuration Database admins.\nEditing Groups To edit a group, simply click on the “edit” link at the top of the group’s details page.\nDeleting Groups This operation is not allowed.\n","categories":"","description":"Managing NGIs entities","excerpt":"Managing NGIs entities","ref":"/internal/configuration-database/ngis/","tags":"","title":"Managing NGIs entities"},{"body":"NGI Core Services NGIs can register a number of ‘NGI-Core’ services in the Configuration Database. A core NGI service is one that is used to calculate the availability and reliability of the NGI. These services fall under the responsibility of the NGI and provide production quality (no testing instances). NGIs can distinguish/flag their core services from their other (non-core) services using one of two ways (see A and B below).\nCore Service Requirements The service instance MUST:\nBe flagged as ‘Production’ (see Production Flag) Not be flagged as ‘Beta’ (see Beta Flag) Monitored flag set to true (see Monitored Flag) Be hosted under a ‘NGI’ scoped Site that has a certification status of ‘Certified’ Required Service Types The following service types are mandatory to support the central operations and all NGIs in the EGI scope should define instances of these services:\nemi.ARGUS (Mandatory) (NGI ARGUS) Top-BDII (Mandatory) Other Mandatory services, depending on middleware deployed by sites under NGI responsibility, are the following:\nMyProxy VOMS NGIs should also register their custom core services like accounting, helpdesk if provided.\nRegistering NGI Core Services NGI core services can be grouped/flagged in one of two ways:\nA) By creating a ‘NGI_XX_SERVICES’ Site and adding their core services under this site. This site must be scoped as ‘NGI’ and define a certification status of ‘Certified’. B) By creating a ‘NGI_XX_SERVICES’ ServiceGroup and adding their core services to this ServiceGroup. It is important that these core service Sites/ServiceGroups adhere to the ‘NGI_XX_SERVICES’ naming scheme. The list of existing Service Groups is available on GOCDB.\n","categories":"","description":"Managing the NGI Core Services entries in Configuration Database","excerpt":"Managing the NGI Core Services entries in Configuration Database","ref":"/internal/configuration-database/ngi-core-services/","tags":"","title":"NGI Core Services"},{"body":" What is it? Object storage is a standalone service that stores data as individual objects, organized into containers. It is a highly scalable, reliable, fast, and inexpensive data storage. It has a simple web services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web.\nThe main features of object storage:\nStorage containers and objects have unique URLs, which can be used to access, manage, and share them. Data can be accessed from anywhere, using standard HTTP requests to a REST API (e.g. VMs running in the EGI Cloud or in other cloud provider’s cloud, from any browser/laptop, etc.) Access can be public or can be restricted using access control lists. There is virtually no limit to the amount of data you can store, only the space used is accounted for. Concepts To use object storage effectively, you need to understand the following key concepts and terminology:\nStorage containers Storage containers (aka buckets) are the fundamental holders of data. Every object is stored in a storage container. You can store any number of objects in a storage container.\nStorage containers have an unique name and act as the root folders of the storage space.\nEach storage container has a unique URL (that includes the name) by which anyone can refer to it.\nObjects Objects are the fundamental entities stored in object storage. Objects consist of object data and metadata. The data portion is opaque to object storage. The metadata is a set of name-value pairs that describe the object. These include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type. You can also specify custom metadata at the time the object is stored.\nAn object is uniquely identified within a storage container by a key (name) and a version.\nEach object has a unique URL, based on the storage container’s URL (that includes the key, and optionally the version) by which anyone can refer to it.\nPermissions Storage containers and objects can be shared by sharing their URLs. However, access to a storage container or to an object is controlled by access control lists (ACLs). When a request is received against a resource, object storage checks the corresponding ACL to verify that the requester has the necessary access permissions.\nNote It is possible to set permissions so that the storage container or object can be accessed publicly. Usage from your application The object storage in the EGI Cloud is offered via OpenStack deployments that implement the Swift service.\nUsers can manage object storage using the OpenStack Horizon dashboard of a provider or from the command-line (CLI). More advanced usage include access via the S3 protocol, via the OpenStack Object Store API, or using the EGI Data Transfer service.\nNote Available object storage resources can be discovered in the Configuration Database (GOCDB). Access from the command-line Multiple command-line interfaces (CLIs) are available to manage object storage:\nThe OpenStack CLI The FedCloud Client is a high-level CLI for interaction with the EGI Federated Cloud (recommended) The Swift CLI has some advanced features that are not available through the OpenStack CLI Access with the FedCloud CLI The main FedCloud commands for managing storage containers and storage objects are described below.\nNote See here for documentation on all storage container-related commands, and here for all object-related commands. List storage containers For example, to access to the SWIFT endpoint at IFCA-LCG2 via the Pilot VO (vo.access.egi.eu), and list the available storage containers, use the FedCloud command below:\nLinux / Mac Windows PowerShell To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n$ export EGI_SITE=IFCA-LCG2 $ export EGI_VO=vo.access.egi.eu $ fedcloud openstack container list --site $EGI_SITE +------------------+ | Name | +------------------+ | test-egi | +------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e set EGI_SITE=IN2P3-IRES \u003e set EGI_VO=vo.access.egi.eu \u003e fedcloud openstack container list --site %EGI_SITE% +------------------+ | Name | +------------------+ | test-egi | +------------------+ To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e $Env:EGI_SITE=\"IN2P3-IRES\" \u003e $Env:EGI_VO=\"vo.access.egi.eu\" \u003e fedcloud openstack container list --site $Env:EGI_SITE +------------------+ | Name | +------------------+ | test-egi | +------------------+ Create new storage container To create a new storage container named test-egi, use the following FedCloud command:\n$ fedcloud openstack container create test-egi +---------+-----------+------------------------------------------------------+ | account | container | x-trans-id | +---------+-----------+------------------------------------------------------+ | v1 | test-egi | tx000000000000000000afc-005f845160-2bb3ed4-RegionOne | +---------+-----------+------------------------------------------------------+ Create new object by uploading a file To upload a file as a new object into a storage container named test-egi, use the following FedCloud command:\nTip The newly created object can have a different name than the file being uploaded, use the --name command flag for this. Tip Multiple files can be uploaded at once, but in that case the resulting objects will have the same names as the uploaded files. $ fedcloud openstack object create test-egi file1.txt +-----------+-----------+----------------------------------+ | object | container | etag | +-----------+-----------+----------------------------------+ | file1.txt | test-egi | 5bbf5a52328e7439ae6e719dfe712200 | +-----------+-----------+----------------------------------+ List objects in a storage container To list the objects in a storage container use the FedCloud command below:\n$ fedcloud openstack object list test-egi +-----------+ | Name | +-----------+ | file1.txt | +-----------+ Download (the content of) an object To download an object named file1.txt located in storage container test-egi, and save its content to a file use the FedCloud command below:\nTip The object can be saved into a file named differently than the object itself, by using the --filename command flag. Tip Multiple files can be downloaded at once, but in that case the resulting files will have the same names as the downloaded objects. $ fedcloud openstack object save test-egi file1.txt Add metadata to an object You can add/update object metadata, stored as key-value pairs among the object properties. E.g. to add a property named key1 with the value value2 to an object named file1.txt located in the storage container named test-egi, you can use the FedCloud command below:\n$ fedcloud openstack object set \\ --property key1=value2 test-egi file1.txt Remove metadata from an object You can also remove metadata from objects. E.g. to remove the property named key1 from the object named file1.txt located in the storage container named test-egi, you can use the FedCloud command below:\nNote Only metadata added by users can be removed (system properties cannot be removed). $ fedcloud openstack object unset \\ --property key test-egi file1.txt Remove an object from a storage container To delete an object named file1.txt from the storage container test-egi, use the following FedCloud command:\nCaution Deleting object from storage containers is final, there is no way to recover deleted objects. Unlike in AWS S3, objects in OpenStack storage containers cannot be protected against deletion. $ fedcloud openstack object delete test-egi file1.txt Removing an entire container To delete a storage container, including all objects in it, use the FedCloud command below.\nTip You can add the -r option to recursively remove sub-containers. Caution Deleting all objects from a storage container is final, there is no way to recover deleted objects. Unlike in AWS S3, objects in OpenStack storage containers cannot be protected against deletion. $ fedcloud openstack container delete test-egi Access via Rclone Rclone is a command-line program to manage files on cloud storage. This section explains how to use rclone to interact with OpenStack Swift available in the EGI Federated Cloud.\nAs a prerequisite, we need to configure the following environment variables: OS_AUTH_URL, OS_AUTH_TOKEN, OS_STORAGE_URL. Use the FedCloud Client to get their values:\n# explore sites with swift storage $ fedcloud endpoint list --service-type org.openstack.swift --site ALL_SITES # get OS_AUTH_URL $ fedcloud openstack --site \u003csite\u003e --vo \u003cvirtual-organisation\u003e catalog show keystone # get OS_AUTH_TOKEN $ fedcloud openstack --site \u003csite\u003e --vo \u003cvirtual-organisation\u003e token issue \\ -c id \\ -f value # get OS_STORAGE_URL for your site and Virtual Organisation $ fedcloud openstack --site \u003csite\u003e --vo \u003cvirtual-organisation\u003e catalog show swift Now configure rclone to work with the environment variables:\n$ rclone config create egiswift swift env_auth true Finally, check that you have access to swift:\n$ export OS_AUTH_TOKEN=\u003ctoken\u003e $ export OS_AUTH_URL=\u003ckeystone-url\u003e $ export OS_STORAGE_URL=\u003cswift-url\u003e $ rclone lsd egiswift: For more information, please see Rclone documentation for Swift.\nAccess via the S3 protocol The OpenStack Swift service is compatible with the S3 protocol, therefore when properly configured, it can be accessed as any other S3-compatible object store.\nNote The S3 protocol was created by Amazon Web Services (AWS) for their object storage, called Simple Storage Service (S3), but it was adopted as the de-facto standard to access object storage offered by other providers. In order to access the storage via S3, an EGI Federated Cloud site admin needs to create and associate to your EGI credentials both access and secret keys which could then be used by clients to have access to the storage.\nAWS CLI The AWS CLI can be used to manage object storages having S3 interface.\nFirst of all the configuration of the access and secret keys need to be done:\n$ aws configure then it offers many commands to list, create buckets, objects, e.g.:\n$ aws s3 ls --no-sign-request \\ --endpoint-url https://object-store.cloud.muni.cz \\ s3://test-egi-public Note In order to access public buckets the --no-sign-request is needed Minio Client The MinIO CLI supports filesystems and Amazon S3 compatible cloud storage services.\nIt offers a modern alternative to UNIX commands like ls, cat, e.g.:\n# key and secret are not mandatory in case of public buckets $ ./mc alias set cesnet https://object-store.cloud.muni.cz $ ./mc ls cesnet/test-egi-public $ ./mc cat cesnet/test-egi-public/file1.txt Davix The Davix Client, developed at CERN for RHEL and Debian environments, is another alternative for working with S3-compatible object storage.\nFor example, to list containers/objects via the S3 protocol, use the command:\n$ davix-ls --s3accesskey 'access' --s3secretkey 'secret' \\ --s3alternate s3s://s3.cl2.du.cesnet.cz/\u003cbucket-name\u003e davix-get, davix-put and davix-del are also available to download, store and delete objects from the storage.\nNote The Davix Client does not support access to public buckets Access via Python The possibility to access programmatically via S3 object storage is also quite important, for instance in the case of interactive computing via EGI Notebooks.\nWhen using Python for instance, S3Fs is a practical Pythonic file interface to S3.\nThe top-level class S3FileSystem holds connection information and allows typical file-system style operations like cp, mv, ls, du, glob, etc., as well as put/get of local files to/from S3.\nimport s3fs fs = s3fs.S3FileSystem(anon=True, client_kwargs={ 'endpoint_url': 'https://object-store.cloud.muni.cz' }) print(fs.ls('s3://test-egi-public')) s3path = 's3://test-egi-public/file1.txt' with fs.open(s3path, 'rb') as f: print(f.read()) There is a good collection of examples on the S3Fs GitHub repository.\nAccess via EGI Data Transfer The EGI Data Transfer service can move files to and from object storages that are compatible with the S3 protocol. You will have to upload the access keys to the EGI Data Transfer service, which will be able to generate properly signed URLs for the objects in the storage.\nNote Please contact support at support \u003cat\u003e egi.eu for more details. You can then refer to this tutorial to see how to transfer to/from an Object storage endpoint.\n","categories":"","description":"Object Storage offered by EGI Cloud providers\n","excerpt":"Object Storage offered by EGI Cloud providers\n","ref":"/users/data/storage/object-storage/","tags":"","title":"Object Storage"},{"body":"Overview This page includes all the available ways to obtain OAuth tokens from EGI Check-in.\n","categories":"","description":"Obtaining Access/Refresh Tokens from Check-in\n","excerpt":"Obtaining Access/Refresh Tokens from Check-in\n","ref":"/users/aai/check-in/obtaining-tokens/","tags":"","title":"Obtaining Access/Refresh Tokens"},{"body":"Pakiti Pakiti is a client-server tool to collect and evaluate data about packages installed on Linux machines, primarily meant to identify vulnerable SW that have not been properly updated. The EGI CSIRT operates the EGI Pakiti instance that is used to monitor the state of the EGI sites.\nPakiti client The pakiti-client can be used to send package informations to pakiti.egi.eu.\nIf you have the proper credentials in the Configuration Database and submit your report with the correct SITE_NAME, you, your NGI-CSIRT and the EGI-CSIRT will be able to monitor the packages installed on your hosts and potentially vulnerabilities. The results can be accessed on the EGI Pakiti central instance.\nRunning the Pakiti client from CVMFS for EGI If you have CVMFS installed and configured to mount grid.cern.ch, you can run pakiti by simply running:\n$ /cvmfs/grid.cern.ch/pakiti/bin/pakiti-client \\ --url \"https://pakiti.egi.eu/feed/\" \\ --site SITE_NAME Please remember to replace SITE_NAME by your actual site name\nManual installation Installing the Pakiti client The pakiti-client is now available from EPEL. If your machine already has EPEL enabled, the following command is enough to install it:\n$ yum install pakiti-client Running the Pakiti client for EGI With the package and the configuration, the following commands will run the\npakiti-client and transmit all its data to the EGI CSIRT pakiti instance!\n$ pakiti-client --url \"https://pakiti.egi.eu/feed/\" --site SITE_NAME Please remember to replace SITE_NAME by your actual site name\nPuppet Installation The simplest way to configure and run the pakiti-client on a cluster is to use puppet: You just need to create a file and a manifest.\npackage { 'pakiti-client': ensure =\u003e 'present', } cron { 'pakiti-egi': ensure =\u003e 'present', command =\u003e 'pakiti-client --url \"https://pakiti.egi.eu/feed/\" --site SITE_NAME', user =\u003e 'nobody', hour =\u003e fqdn_rand(24), minute =\u003e fqdn_rand(60), } ","categories":"","description":"Monitoring patch status","excerpt":"Monitoring patch status","ref":"/internal/security-coordination/monitoring/pakiti/","tags":"","title":"Pakiti"},{"body":"About the information system Information about resources is documented according to the GLUE Schema.\nInformation in the Top BDII is provided for two GLUE schema versions:\nGLUE 1.3: the legacy version of the specification, under the LDAP base Mds-Vo-Name=local,o=grid. GLUE 2.0: the most recent version of the specification, under the LDAP base GLUE2GroupID=grid,o=glue. Some resources may not yet be exposed via GLUE 2.0, and it may be required to use GLUE 1.3 for those ones.\nTip You can use -o ldif-wrap=no to disable wrapping the results. Those examples are relying on the Top BDII service maintained by EGI Foundation: ldap://lcg-bdii.egi.eu:2170.\n# Dumping all the information from GLUE 2.0 $ ldapsearch -x -H ldap://lcg-bdii.egi.eu:2170 -b \"GLUE2GroupID=grid,o=glue\" In order to retrieve information required to contact a given CE, you can look for GLUE2EndpointURL objects. In order to retrieve information about the batch systems, you can look for GLUE2Manager objects. In order to look for information about a specific CE, you can look into GLUE2Service. In order to look for information about the resources available to VOs, you can look into GLUE2Share and GLUE2Policy. The following queries can be used to retrieve information about all the Computing Elements of a given type. You will likely be able to use only a subset of them, only the ones supporting the Virtual Organisation you are a member of, and for which you have a valid VOMS proxy.\nComputing Elements Nowadays mainly two Computing Element (CE) “flavours” are used in production:\nHTCondorCE, a Compute Entrypoint (CE) based on HTCondor. ARC-CE, the ARC Compute Element (CE). The CREAM CE is a legacy and no more supported middleware.\nQuerying for HTCondorCE compute resources Most, if not all the HTCondorCE Computing Elements should be discoverable via GLUE 2.0.\n# Querying GLUE2EndpointURL for all HTCondorCE compute endpoints $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Endpoint)(GLUE2EndpointImplementationName=HTCondor))' \\ GLUE2EndpointInterfaceName \\ GLUE2EndpointImplementationVersion \\ GLUE2EndpointURL # Querying GLUE2Manager for the batch systems' versions and number of CPUs $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Manager)(GLUE2ManagerProductName=HTCondor))' \\ GLUE2ManagerProductName \\ GLUE2ManagerProductVersion \\ GLUE2ComputingManagerTotalLogicalCPUs \\ GLUE2ComputingManagerComputingServiceForeignKey Once you have selected a given CE, you can look into getting more information about this one.\n# Querying GLUE2Service for information about a specific CE $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Service)(GLUE2ServiceID=*condorce1.ciemat.es*))' # Querying GLUE2Share for a specific CE, filtering for details on the queues $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Share)(GLUE2ShareID=*condorce1.ciemat.es*))' \\ GLUE2ComputingShareMappingQueue \\ GLUE2ShareIDGLUE2ComputingShareMaxWallTime \\ GLUE2ComputingShareMaxVirtualMemory \\ GLUE2ComputingShareMaxUserRunningJobs \\ GLUE2ComputingShareMaxRunningJobs \\ GLUE2ComputingShareMaxCPUTime \\ GLUE2ComputingShareWaitingJobs \\ GLUE2ComputingShareUsedSlots \\ GLUE2ComputingShareTotalJobsGLUE2ComputingShareRunningJobs # Querying GLUE2Policy for a specific CE, filtering for supported VOs $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Policy)(GLUE2PolicyID=*condorce1.ciemat.es*))' \\ GLUE2PolicyRule \\ GLUE2PolicyID \\ GLUE2MappingPolicyShareForeignKey It’s also possible to look into GLUE 1.3.\nQuerying for ARC-CE compute resources Most, if not all the ARC-CE should be discoverable via GLUE 2.0.\n# Querying for all ARC-CE compute resources $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Endpoint)(GLUE2EndpointImplementationName=nordugrid-arc))' \\ GLUE2EndpointInterfaceName \\ GLUE2EndpointImplementationVersion \\ GLUE2EndpointURL # Querying GLUE2Manager for the batch systems' versions and number of CPUs $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Manager)(GLUE2ComputingManagerComputingServiceForeignKey=*urn:ogf:ComputingService*))' \\ GLUE2ManagerProductName \\ GLUE2ManagerProductVersion \\ GLUE2ComputingManagerTotalLogicalCPUs \\ GLUE2ComputingManagerComputingServiceForeignKey Once you have selected a given CE, you can look into getting more information about this one.\n# Querying GLUE2Service for information about a specific CE $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Service)(GLUE2ServiceID=*alex4.nipne.ro*))' # Querying GLUE2Share for a specific CE, filtering for details on the queues $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Share)(GLUE2ShareID=*alex4.nipne.ro*))' \\ GLUE2ComputingShareMappingQueue \\ GLUE2ShareIDGLUE2ComputingShareMaxWallTime \\ GLUE2ComputingShareMaxVirtualMemory \\ GLUE2ComputingShareMaxUserRunningJobs \\ GLUE2ComputingShareMaxRunningJobs \\ GLUE2ComputingShareMaxCPUTime \\ GLUE2ComputingShareWaitingJobs \\ GLUE2ComputingShareUsedSlots \\ GLUE2ComputingShareTotalJobsGLUE2ComputingShareRunningJobs # Querying GLUE2Policy for a specific CE, filtering for supported VOs $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Policy)(GLUE2PolicyID=*alex4.nipne.ro*))' \\ GLUE2PolicyRule \\ GLUE2PolicyID \\ GLUE2MappingPolicyShareForeignKey It’s also possible to look into GLUE 1.3.\nUsing GLUE 1.3 GLUE 1.3 is legacy.\nQuerying for information about HTCondor CE using GLUE 1.3.\n# Querying for all HTCondorCE compute resources, using GLUE 1.3 $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEInfoJobManager=HTCondorCE))' \\ GlueCEImplementationVersion GlueCEImplementationName \\ GlueCEStateStatus GlueCEUniqueID GlueServiceEndpoint GlueServiceType # Querying information about the HTCondor CE via GLUE 1.3 $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEUniqueID=condorce1.ciemat.es:9619/condorce1.ciemat.es-condor))' # Limiting output for the Condor CE via GLUE 1.3 $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEUniqueID=condorce1.ciemat.es:9619/condorce1.ciemat.es-condor))' \\ GlueCEInfoHostName GlueCEUniqueID \\ GlueCEInfoJobManager GlueCEImplementationName GlueCEImplementationVersion \\ GlueCEInfoLRMSType GlueCEInfoLRMSVersion \\ GlueCEAccessControlBaseRule \\ GlueCEInfoTotalCPUs \\ GlueCEStateStatus Querying for information about ARC-CE using GLUE 1.3.\n# Querying for all ARC CE compute resources, using GLUE 1.3 $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEInfoJobManager=arc))' \\ GlueCEImplementationVersion GlueCEImplementationName \\ GlueCEStateStatus GlueCEUniqueID GlueServiceEndpoint GlueServiceType # Querying information about the ARC CE via GLUE 1.3 $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEUniqueID=alex4.nipne.ro:2811/nordugrid-SLURM-dteam))' # Limiting output for the ARC CE via GLUE 1.3 $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEUniqueID=alex4.nipne.ro:2811/nordugrid-SLURM-dteam))' \\ GlueCEInfoHostName GlueCEUniqueID \\ GlueCEInfoJobManager GlueCEImplementationName GlueCEImplementationVersion \\ GlueCEInfoLRMSType GlueCEInfoLRMSVersion \\ GlueCEAccessControlBaseRule \\ GlueCEInfoTotalCPUs \\ GlueCEStateStatus Using legacy tools for GLUE 1.3 Using lcg-info it’s possible to easily do more targeted queries, like focusing on a specific VO.\nlcg-info and lcg-infosites are only taking into account the GLUE 1.3 schema, and will lack information that is only published according to the GLUE 2 schema, like for most HTCondorCE Computing Elements.\n# Identify compute resources available for dteam VO $ lcg-info --list-ce --vo dteam --bdii ldap://lcg-bdii.egi.eu:2170 # Identify storage resources available for dteam VO $ lcg-info --list-se --vo dteam --bdii ldap://lcg-bdii.egi.eu:2170 $ lcg-info --list-ce --vo dteam --bdii ldap://lcg-bdii.egi.eu:2170 \\ --attrs CEImpl --query 'CEImpl=*HTCondorCE' (...) - CE: ce01.knu.ac.kr:9619/ce01.knu.ac.kr-condor - CEImpl HTCondorCE - CE: ce13.pic.es:9619/ce13.pic.es-condor - CEImpl HTCondorCE (...) - CE: condorce1.ciemat.es:9619/condorce1.ciemat.es-condor - CEImpl HTCondorCE - CE: condorce2.ciemat.es:9619/condorce2.ciemat.es-condor - CEImpl HTCondorCE (...) We can see that the dteam VO should be able to access Computing Elements from the various types:\n$ lcg-info --list-ce --vo dteam --bdii ldap://lcg-bdii.egi.eu:2170 \\ --attrs CEImpl --query 'CEImpl=*' | grep CEImpl | sort | uniq -c 123 - CEImpl ARC-CE 22 - CEImpl CREAM 7 - CEImpl HTCondorCE Identifying all the resources accessible by a given VO In GLUE 2.0, the access granted to a given VO to a compute or storage resource, is published using the GLUE2Share and GLUE2Policy objects. There are also GLUE2ComputingShare and GLUE2StorageShare to specifically document sharing of compute or storage resources.\n# Querying GLUE2Share for all the resources available to dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2Share)(GLUE2ShareID=*dteam*))' # Querying GLUE2ComputingShare for all the computing resources available to dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*))' # Querying GLUE2StorageShare for all the storage resources available to dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2StorageShare)(GLUE2ShareID=*dteam*))' It is possible to filter for the different types of Computing Element, and select only specific attributes.\nLooking for a HTCondorCE for dteam # Information about the HTCondorCE supporting dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*)(GLUE2ComputingShareComputingEndpointForeignKey=*HTCondorCE*))' \\ GLUE2ShareEndpointForeignKey \\ GLUE2ShareID \\ GLUE2ComputingShareTotalJobs \\ GLUE2ComputingShareRunningJobs \\ GLUE2ComputingShareWaitingJobs Assuming it was decided, based on the site location, available resources, prior experience, or any other reason, to go for condorce1.ciemat.es, the information about the CE can be requested using the following request, filtering on the GLUE2ShareID from the previous query: grid_dteam_condorce1.ciemat.es_ComputingElement.\n# condor_submit needs CE (condorce1.ciemat.es) and pool (condorce1.ciemat.es:9619) $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*grid_dteam_condorce1.ciemat.es_ComputingElement*))' \\ GLUE2ShareID \\ GLUE2ShareDescription \\ GLUE2ComputingShareExecutionEnvironmentForeignKey \\ GLUE2EntityOtherInfo Looking for an ARC-CE for dteam # Information about the ARC-CE supporting dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*)(GLUE2ComputingShareComputingEndpointForeignKey=*urn:ogf*))' \\ GLUE2ComputingShareComputingEndpointForeignKey \\ GLUE2ShareEndpointForeignKey \\ GLUE2ComputingShareTotalJobs \\ GLUE2ComputingShareRunningJobs \\ GLUE2ComputingShareWaitingJobs Assuming it was decided, based on the site location, available resources, prior experience, or any other reason, to go for condorce1.ciemat.es, the information about the CE can be requested using the following request, filtering on the GLUE2ShareID from the previous query: grid_dteam_condorce1.ciemat.es_ComputingElement.\n# arcsub needs CE name (alex4.nipne.ro) $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*urn:ogf:ComputingShare:alex4.nipne.ro:dteam_dteam*))' \\ GLUE2ShareID \\ GLUE2ShareDescription \\ GLUE2ComputingShareComputingServiceForeignKey \\ GLUE2ComputingShareExecutionEnvironmentForeignKey ","categories":"","description":"Querying the Information System","excerpt":"Querying the Information System","ref":"/users/compute/high-throughput-compute/querying-information-system/","tags":"","title":"Querying the Information System"},{"body":"Introduction Scope tags are used to group entities such as Sites, Services and ServiceGroups into flexible categories. A single entity can define multiple scope tags, allowing the resource to be associated with different categories without duplication of information. This is essential to maintain the integrity of topology information across different infrastructures and projects. The Configuration Database admins control which scope tags are made available to avoid proliferation of tags (user defined tags are reserved for the extensibility mechanism). As an example, a site’s scope list could aggregate all of the scopes defined by its child services. In doing this, the site scope list becomes a union of its service scopes plus any other site specific tags defined by the site. By defining scope tags, resources can be ‘filtered-by-scope-tag’ when querying for data in the PI using the ‘scope’ and ‘scope_match’ parameters, see GOCDB Programmatic Interface for details. Clear Separation of Concerns It is important to understand that scopes and Projects are distinct:\nProjects are used to cascade roles and permissions over child objects Scope tags are used to filter resources into flexible categories/groupings Scope tags can be created to mirror the projects. For example, assuming two projects (e.g. EGI.eu and EUDAT), two corresponding tags may be defined. In addition, it is also possible define additional scopes for finer grained resource filtering e.g. ‘SubGroupX’ and ‘EGI_TEST’. The key benefit: A clear separation of concerns between cascading permissions and resource filtering. EGI Scopes To make a Site, Service or ServiceGroup visible to EGI, the resource’s ‘EGI’ scope tag checkbox must be ticked. EGI scoped resources are exposed to the central operational tools for monitoring and will appear in the central operations portal. Un-ticking the EGI checkbox and selecting the ‘Local’ scope makes the selected object invisible to EGI; it will be hidden from the central operation tools (it will not show in the central dashboard and it will not be monitored centrally). This can be useful if you wish to hide certain parts of your infrastructure from EGI but still have the information stored and accessed from the same Configuration Database instance. A use-case for non-EGI sites/services is to hide those entities from central EGI tools, but to include those sites/services for use by regional versions of the operational tools (such as regional monitoring). Note that exposing a site / service endpoint as EGI does not override the production status or certification status fields. For example if a site isn’t marked as production it won’t be monitored centrally even if it’s marked as visible to EGI. You can submit your request for new scope tags via EGI Helpdesk to the “Configuration and Topology Database (GOCDB)” support unit. Reserved Scope Tags Some tags may be ‘Reserved’ which means they are protected - they are used to restrict tag usage and prevent non-authorised sites/services from using tags not intended for them. Reserved tags are initially assigned to resources by the Configuration Database admins, and can then be optionally inherited by child resources (tags can be initially assigned to NGIs, Sites, Services and ServiceGroups). When creating a new child resource (e.g. a child Site or child Service), the scopes that are assigned to the parent are automatically inherited and assigned to the child. Reserved tags assigned to a resource are optional and can be de-selected if required. Users can reapply Reserved tags to a resource ONLY if the tag can be inherited from the parent Scoped Entity (parents include NGIs/Sites). For Sites: If a Reserved tag is removed from a Site, then the same tag is also removed from all the child Services - a Service can’t have a reserved tag that is not supported by its parent Site. For NGIs: If a Reserved tag is removed from an NGI, then the same tag is NOT removed from all the child Sites - this is intentionally different from the Site-\u003eService relationship. To request a reserved scope tag, an approval is required from the operators of the relevant resources. Details on who to contact are listed below. Once authorisation is given, please contact the Configuration Database admins with details of the approval (e.g. link to an EGI Helpdesk ticket that approves the tag assignment). Users with a suitable role over a Site, or a Site’s parent NGI can remove reserved scope tags from their Site and its child Services. As part of maintaining their Site information in GOCDB, Users should remove reserved scope tags from their Sites and/or their Services when they cease to be relevant. FedCloud Reserved Tag Tag for resources that contribute to the EGI Federated Cloud. To request this tag, please contact the FedCloud operators / EGI Operations. Elixir Reserved Tag Tag for resources that contribute to the EGI Federated Cloud. To request this tag, please contact the operators of the ‘ELIXIR’ NGI in the EGI Configuration Database. GreenDIGIT Reserved Tag Tag for resources that contribute to the GreenDIGIT project. To request this tag, please contact EGI Operations. WLCG Reserved Tags A number of reserved scope tags have been defined for the WLCG: The ‘tierN’ tags should be requested for WLCG sites that are defined in CRIC. To request a ‘tierN’ tag, raise a ticket against the CRIC support unit in GGUS. For the experiment VO tags (alice, atlas, cms, lhcb), raise a ticket with the relevant VO support unit. The wlcg tag is a generic catch-all tag for sites/services with either tierN and VO tags and is used to gain an overall view of the WLCG infrastructure. SLA Reserved Tag Entities covered by an EGI VO SLA This Tag will only be applied at the request of EGI operations EOSCCore Tag Tag for resources that contribute to core services of the EOSC. To request this tag, please raise an EGI Helpdesk ticket against the Operations SU. EGICore Tag Tag for resources that are part of the EGI Core services. To request this tag, please raise an EGI Helpdesk ticket against the Operations SU.\n","categories":"","description":"Understanding and manipulating scopes","excerpt":"Understanding and manipulating scopes","ref":"/internal/configuration-database/scopes/","tags":"","title":"Scoping"},{"body":"What is it? EGI is an interconnected federation where a single vulnerable place may have a huge impact on the whole infrastructure. In order to recognise the risks and to address potential vulnerabilities in a timely manner, the EGI Security Monitoring provides an oversight of the infrastructure from the security standpoint.\nAlso, sites connected to EGI differ significantly in the level of security and detecting weaknesses exposed by the sites allows the EGI security operations to contact the sites before the issue leads to an incident.\nInformation produced by security monitoring is also important during assessment of new risks and vulnerabilities since it enables to identify the scope and impact of a potential security incident.\nTechnical description This service includes the following components.\nSecmon A Nagios-based service provided to monitor a range of assets like CRLs, file system permissions, vulnerable file permissions etc.\nAd-hoc probes are deployed to support incident management, to assess the vulnerability of the infrastructure with regards to specific security issues and for proactive security management.\nThe results produced are available to the EGI Security dashboard of the Operations Portal for visualisation.\nPakiti Pakiti is the monitoring and notification service which is responsible for checking the patching status of systems.\nThe results produced are available to the EGI Security dashboard of the Operations Portal for visualisation.\nIncident reporting tool Ticketing system for tracking of incident.\nTools for Security Service Challenge support Security challenges are a mechanism to check the compliance of sites/NGIs/EGI with security requirements. Runs of Security Service Challenges need a set of tools that are used during various stages of the runs.\n","categories":"","description":"Security Monitoring for EGI Resources Providers and Services","excerpt":"Security Monitoring for EGI Resources Providers and Services","ref":"/internal/security-coordination/monitoring/","tags":"","title":"Security Monitoring"},{"body":"Definition A Service entity is formed by a hostname, a hosted service and a URL.\nThe EGI Configuration Database stores the following information about Service entities (non exhaustive list):\nThe fully qualified hostname of the machine The hosted service (see ServiceTypes below) The URL to reach the entities The IP address of the machine The machine’s host certificate DN A description of the node As a machine can host many services, there can be many Service entities per machine.\nExample The machine myhost.domain.org runs a CE, an UI and a UnicoreX service. This will show up in the EGI Configuration Database as 3 Service entities:\nfully qualified hostname ServiceType myhost.domain.org CE myhost.domain.org UI myhost.domain.org unicore6.Gateway Note that a single host can also specify multiple services of the same ServiceType.\nManipulating Service entities Viewing Service entities There are different pages in GOCDB where Service entities are listed:\nA full Service entities listing page, that shows a listing of all the entities in the database, with controls to page through the listing. The table headers can be clicked to set the ordering. Site details page, see GRIDOPS-GOCDB for an example, where all the Service entities belonging to this site are listed Each Service entity also has its own listing page. By clicking the link to view it, you can see all associated information.\nAdding Service entities Provided you have proper permissions (check the permissions matrix in the Permissions_associated_to_roles section), you can add a Service entity by:\nclicking on the Add a New Service link in the sidebar. Simply select parent site, fill the form and validate. By clicking on the Add Service link from a given site’s details page (the link will only appear if you have proper permissions). This will lead you to the same form as above. Editing Service entities information The editing process will show you the same form as the adding process. To edit Service entities, simply click the “edit” link on top of the entities' details page.\nRemoving a Service entity from a site To delete a Service entity you have permissions on, simply click on the “delete” link on top of the entities’ details page. The interface asks for confirmation before proceeding.\nService Endpoint entities A Service entity may optionally define Service Endpoint entities which model network locations for different service functionalities that can’t be described by the main ServiceType and URL alone.\nFor example: The Service entity goc.egi.eu (of ServiceType egi.GOCDB) defines the following Service Endpoint entities:\nName URL Interface Name ProductionPortalInstance https://goc.egi.eu/portal egi.GOCDB.Portal Production PI base URL https://goc.egi.eu/gocdbpi egi.GOCDB.PI Specific Service entities fields and their impact “beta” flag (t/f) This indicates whether the Service entity is a beta service or not (part of the staged rollout process).\nHost DN This is the DN of the host certificate for the service. The format of the DN follows that defined by the OGF Interoperable Certificate Profile which restricts allowed chars to a PrintableString that does NOT contain characters that cannot be expressed in printable 7-bit ASCII. For a list of allowed chars.\nTo supply multiple or alternate DN(s) for a service, for example of the multiple hosts supporting a single Service entity, see standard extension properties.\n“production” flag (t/f) The Service entities’ Production flag indicates if this service delivers a production quality service to the infrastructure it belongs to (EGI).\nNon-production Service entities can be either Monitored or Not Monitored, depending on the Administrator’s choice. Even if this flag is false, the service is still considered part of the EGI and so shows up in the ROD dashboard. If true, then the Monitored flag must also be true: All production resources MUST be monitored (except if the ServiceType is a VOMS or emi.ARGUS) This flag is not to be confused with PRODUCTION_STATUS, which is a Site level flag that shows if the site delivers to the production or Test infrastructure. “monitoring” flag (t/f) This flag is taken into account by monitoring tools.\nCan only be set to “N” (false) if Production flag is also false. If set to “N” the entities won’t be tested. Usage of PRODUCTION and MONITORED flags for EGI Service entities All production Service entities MUST be monitored (except for emi.ARGUS and VOMS ServiceTypes).\nProduction and Monitored Operations Dashboard: A failing test of production Service entities generates an alarm in the ROD Operations Dashboard. Availability calculation: The Service entities test results are considered for Availability computation (if and only if the ServiceType associated to the entities is one of those included in Availability computation) Non-Production and Monitored: YES/NO Availability calculation: If Monitored is set to YES, the Monitoring Service will test the Service entity, but the test results are ignored by the Availability Computation Engine (ACE). Availability calculation: Non-production Service entities are not considered for site availability calculations. Operations Dashboard: If Monitored is set NO, the Service entities is ignored by the Monitoring Service, and no alarms are raised in the Operations Dashboard in case of CRITICAL failure. Monitoring tests for non-production Service entities generate alarms into the ROD Operations Dashboard in case of CRITICAL failure of the test. These alarms are visible in the Operations Dashboard and are tagged as “non production”. ","categories":"","description":"Managing Service entities","excerpt":"Managing Service entities","ref":"/internal/configuration-database/service-entities/","tags":"","title":"Service Entities"},{"body":"Service Groups A service group is an arbitrary grouping of existing service endpoints that can be distributed across different physical sites and users that belong to the SG (SGs were previously known as ‘Virtual Sites’):\nEach service that appears in a group must already exist and be hosted by a physical site. A service group role does not extend any permissions over its child services. This means that you cannot declare a downtime on the services that you group together or modify the service attributes. Any GOCDB user can create their own service group and as the ‘Service Group Administrator’ you can control subsequent user membership requests to the SG (everything is logged, including who created the service group). GOCDB users can request to join an existing service group by finding the target SG and requesting a role on that SG. Service groups are typically used for monitoring a particular collection of services and/or users using the GOCDB ‘get_service_group’ and ‘get_service_group_role’ PI methods. SG members can be listed using the get_service_group_role PI method. PI doc: get_service_group(link to old EGI Wiki) get_service_group_role(link to old EGI Wiki) If you have any further use-cases or suggestions, please submit a GGUS ticket. ","categories":"","description":"Understanding and manipulating service groups","excerpt":"Understanding and manipulating service groups","ref":"/internal/configuration-database/service-groups/","tags":"","title":"Service groups"},{"body":"Introduction In the EGI Configuration Database, a service type is a technology used to provide a service. Each service endpoint is associated with a service type. Service types are pieces of software while service endpoints are a particular instance of that software running in a certain context.\nService Type Naming Scheme Service types include grid and cloud middleware, and operational services. This attribute corresponds to the Glue2 Service.Type attribute and is defined as the “Type of service according to a namespace based classification (the namespace MAY be related to a middleware name, an organisation or other concepts)”. The naming scheme for new service types therefore follow a reverse DNS style syntax, usually naming the technology provider/project followed by technology type in lowercase, i.e. ‘provider.type’ (e.g. org.openstack.swift). Please note, this syntax does not necessarily indicate ownership, the main objective is to avoid name clashes between services. For example, different projects may have similar services but these may be modified/customised just enough to merit a different prefix or service type name. Glue2 defines a service type list at: Glue2 Enums Glue2 service types. The Glue2 and GOCDB recommendation is to use lowercase (legacy enum values do exist that use camelCase). These service types are used at some grid sites within EGI but aren’t EGI operational tools or a part of the core middleware distributions.\nService Type List To request a new service type, please submit a request for a new service type (see the section “Adding a new service type”).\nIn the following section there is the list of “middleware agnostic” service types. You can obtain the whole list of service types by browsing Poem, or by launching the following query to the GOCDBPI interface:\nget_service_type Operational Components (middleware agnostic) Site-BDII: (Site service) This service collects and publishes site’s data for the Information System. All grid sites MUST install one Site-BDII. For cloud sites eu.egi.cloud.information.bdii MUST be installed. Top-BDII: (Central service) The “top-level BDII”. These collect and publish the data from site-BDIIs. Only a few instances per region are required. MyProxy: [Central service] MyProxy is part of the authentication and authorization system. Often installed by sites installing the WMS service. egi.APELRepository: (Central service) The central APEL repository egi.AccountingPortal: (Central service) The central accounting portal egi.GGUS: (Central service) The central GGUS egi.GOCDB: (Central service) The central GOCDB egi.MSGBroker: (Central service) The central message broker egi.Portal: (Central Service) for monitoring generic web portals who dont have a specific service type (deprecated) MSG-Broker: (Central service) A broker for the backbone messaging system. egi.MetricsPortal: (Central service) The central metrics portal egi.OpsPortal: (Central service) The central operations portal egi.GRIDVIEW: (Central service) The central gridview portal egi.GSTAT: (Central service) The central GStat portal egi.SAM: (Central service) The central SAM monitoring ngi.SAM: (Regional Service) NGI-level SAM monitoring box vo.SAM: (Regional Service) VO-level SAM monitoring box site.SAM: (Regional Service) Site-level SAM monitoring box ngi.OpsPortal: (Regional service) NGI-level regional operations portal instance argo.poem: POEM is system for managing profiles of probes and metrics in ARGO system. argo.mon: ARGO Monitoring Engine gathers monitoring metrics and publishes to messaging service. argo.consumer: ARGO Consumer collects monitoring metrics from monitoring engines. argo.computeengine: ARGO Compute Engine computes availability and reliability of services. argo.api: ARGO API service for retrieving status and A/R results. argo.webui: ARGO web user interface for metric A/R visualization and recalculation management. egi.aai.saml: EGI Check-in SAML interface. Enables federated access to EGI services and resources using Security Assertion Markup Language (SAML). Provided by GRNET. egi.aai.oidc: EGI Check-in OpenID Connect interface. Enables federated access to EGI services and resources using OpenID Connect (OIDC). Provided by GRNET. egi.aai.tts: EGI Check-in token translation service. Enables the translation between different authentication and authorisation protocols. Provided by GRNET. Adding new services types Please feel free to make a request for a new service type. All new service type requests need to be assessed by EGI via lightweight review process (by EGI OMB and EGI Operations) so that only suitable types are added, and to prevent duplication.\nYou can submit your request via EGI Helpdesk to the “Configuration and Topology Database (GOCDB)” support unit.\nPlease specify the following information as part of your request:\nname of service type (lowercase): high-level description of the service functionality (255 characters max): project/community/organization maintaining the software: scale of deployment (number of instances and by which organizations): contact point (name/email address): Note: please provide a suggested service type name following the naming scheme described above (technology provider’s reversed domain . software name) and a brief sentence to describe the service type.\n","categories":"","description":"Description of Service types.","excerpt":"Description of Service types.","ref":"/internal/configuration-database/service-types/","tags":"","title":"Service Types"},{"body":"Definition A site (also known as a Resource Centre) is a grouping of grid resources collating multiple Service Endpoints (SEs). Downtimes are recorded on selected SEs of a site. GOCDB stores the following information about sites (non exhaustive list). Note, when editing values in the portal, mandatory fields are marked with ‘*’:\nA unique (short) name - case sensitive (GOCDB and GoCDB are considered different) An official (long) name A domain name for the Site/Resource Centre The home web URL of the Site/Resource Centre A contact email address and telephone number Emergency email for a fast response time in case of urgent problem Alarm email is WLCG Tier1 site specific (used as part of a WLCG workflow for dealing with specific monitoring alarms) A security contact email address and telephone number The site timezone The site’s GIIS URL (Case Sensitive - Please ensure you enter your Site name which is usually encoded in the URL in the correct case!). e.g. ldap://bdii-rc.some-site.uk:2170/mds-vo-name=SITE-NAME,o=grid (if your GOCDB site name site name is upper case) A mandatory human readable description of the site The site’s latitude, longitude and location Production Infrastructure: The site’s intended target infrastructure. This specifies the infrastructure that the site’s services deliver to. This has one of the following values: Production (with this target infrastructure, the EGI site certification transition rules apply) Test (in future, if the site delivers to this infrastructure, then its Certification status will be fixed to ‘Candidate’). ROC [GROUP] - The NGI or Region of the site Country IP address range within which the Site/Resource Centre’s services run IP/netmask (x.x.x.x/x.x.x.x). To specify multiple IP/netmask values, use a comma or semi-colon separated list with no spaces, e.g. 1.2.3.4/255.255.255.0, 1.2.3.5/255.255.255.0 Manipulating sites Viewing sites A site listing page shows a listing of all the sites in the database, with controls to page through the listing. The table headers can be clicked to set the ordering (ascending or descending).\nEach site also has its own listing page. By clicking the link to view a site, you can see all of the site’s information\nSite listing page is available from the sidebar by clicking on the Browse Sites link. sites belonging to a given Operations Centre are also listed from the group details pages (see below) Adding a site Provided you have proper permissions (check the permissions matrix in the related section , you can add a site by clicking on the Add a New Site link in the sidebar. Simply fill the form and validate.\nNote: If you just registered as site admin and want your new site to be registered in GOCDB, please contact your NGI representative.\nEditing site information The editing process will show you the same form as the adding process. To edit a site, simply click the “edit” link on top of the site’s details page.\nRenaming a site Provided you have permissions, you can change the Short Name, Official Name and GIIS URL to the new Resource Center details. For more information regarding the site renaming procedure please see PROC15\nRemoving a site Site deletion is not allowed in GOCDB. If a site stops operation, its certification status should be set to “closed” (see the next section for more information).\nChanging Site Certification Status For each site that delivers to the ‘Production’ Target Infrastructure, GOCDB stores and shows information about its certification status. This reflects the different steps of the official SA1 site certification procedure which typically follows:\nCandidate -\u003e Uncertified -\u003e Certified.\nThe different possible certification statuses are:\nCandidate: the Resource Centre is in under registration according to the registration process described in the RC registration certification procedure. A site will have CANDIDATE status only during certification. Uncertified: site information has been validated by the Operations Centre and is ready to be moved to certified status (again). The certification status of a site can only be changed by a user with a higher level ‘Regional’ (or EGI ‘Project’) level role. This usually means that only regional managers/deputies/staff can update the status of a site that belongs to that region, see the permissions associated to the roles in the related section. Certified: the Operations Centre has verified that the site has all middleware installed, passes the tests and appears stable. Suspended: Site does temporarily not conform to production requirements (e.g. minimum service targets - see the Resource Centre OLA, security matters) and requires Operations Centre attention. A site can be suspended for a maximum of 4 months after which it must be re-certified or closed. Closed: Site is definitely no longer operated by EGI and is only shown for historic reasons. Clarifications:\nThe uncertified status would generally be an information that a site is ready to start certification procedure (again). “uncertified” can also be used as a timewise unlimited state for sites having to keep an old version of the middleware for the absolute needs of an important international VO or to flag a site coping with Operations Centre requirements but not with EGI availability/reliability thresholds. Suspended is always having a temporary meaning. It is used to flag a site temporarily not coping with EGI availability/reliability thresholds or security requirements, and which should be closed or uncertified by its Operations Centre within 4 months. When being suspended, sites can express that they want to pass certification again. The suspended status is useful to EGI and to the Operations Centre themselves to flag the sites that require attention by the Operations Centre. The closed status should be the terminal one. Suspended is not a terminal state. The following site state transitions are allowed:\ncandidate -\u003e uncertified candidate -\u003e closed uncertified -\u003e certified certified -\u003e suspended certified -\u003e closed (on site request) suspended -\u003e uncertified suspended -\u003e closed The following transitions are explicitly forbidden:\nsuspended -\u003e certified candidate -\u003e something else but uncertified and closed closed -\u003e anything else Going with the definition of the suspended status, Operations Centre managers have to regularly give their attention to all their suspended sites, so that they are processed within the given maximum time of four months. Sites being in suspended should either be set to closed or brought back in production via the uncertified status.\nMore information about site certification statuses can be found in the EGI Federation Procedures:\nPROC09 RC Registration and Certification Procedure PROC11 Resource Centre Decommissioning Procedure PROC12 Production Service Decommissioning Procedure Note: Site certification status cannot be changed by site administrators, and requires intervention of Operations Centre staff.\n","categories":"","description":"Managing Sites entities","excerpt":"Managing Sites entities","ref":"/internal/configuration-database/sites/","tags":"","title":"Sites"},{"body":"Authentication The EGI Configuration Database UI attempts to authenticate you in one of two ways (the REST style API applies X.509 only):\nFirst, by requesting an IGTF accredited user certificate from your browser. If a suitable certificate is detected, you will be asked to confirm selection of your certificate in your browser. Note: if a client certificate has been provided, it will take precedence over any IdP based authentication. Second, if you do not have a user certificate or you hide your certificate from (e.g. by starting a new/anonymous private browser session or pressing ‘Cancel’ when prompted for a certificate), you will be redirected to the landing page where you can authenticate with the EGI Identity Provider Service (IdP) and your chosen institution (if available). If authentication is successful, you will be redirected back to the Configuration Database. Please note, not all logins available in the EGI IdP provide a sufficient level of assurance (LoA) to login (the LoA must be ‘Substantial’). Editing your user account The editing process is the same as the registration process. To edit your user account, simply follow these steps:\nclick on the “view details” link in the “User Status” panel on the sidebar. You should get a page showing your user account information. Click on the “edit” link on top of it. Viewing users Each user account has its own user details page which is accessible to anyone with a valid certificate.\nThere is currently no facility for listing all users in the database. List of users that have a role on a given site appears on site details pages (see section about sites). It is also possible to search for a user’s account using the search feature on the sidebar.\nDeleting your user account If you wish to unregister from the Configuration Database, follow these steps:\nclick on the view details link in the “User Status” panel on the sidebar. You should get a page showing your user account information. Click on the “delete” link on top of it. Confirm your choice. Your account will then be deleted along with any roles the account has.\nLost access to your account Under the following circumstances it is possible to lose access to an account:\nYou use your IGTF X.509 certificate to access and renew or change certificate, it is possible that the certificate’s distinguished name (DN) has also changed. This is what the Configuration Database uses to identify your account. You have authenticated with EGI Check-in, but via a different underlying IdP (i.e. You usually log in with your institutional credentials, but today you logged in with EGI SSO). You have changed the way you log in (i.e. X.509 to EGI Check-in) In these situations, it is usually possible to regain access using to your certificate based account by following one of the following procedures:\nIf for any reason you were unable to complete the relevant procedure (e.g. mail confirmations problems) please open an EGI Helpdesk ticket addressed to the “Configuration and Topology Database (GOCDB)” support unit.\nYou have a new certificate and have lost access to your account Install your new certificate in your browser. Go to EGI Configuration Database. If you are already logged in, then clear your caches and restart your browser or start a new private browser session. When prompted, select your new certificate. You should be able to access, but since you are authenticated with your new certificate, it is as if you had no user account. In the User Status panel in the sidebar, click on the Link Identity/Recover Account link. Specify in the form: Authentication type: X.509 The DN of your old certificate previously used to authenticate to your X.509 based account. The email address associated to your account. Submit and, upon validation, an email will be sent to the specified address, which has to match the one registered with your account. This is to avoid identity theft. The email contains a validation link. Click on the validation link or copy/paste in your browser. Once validated, changes are immediate. You can only associate one X.509 DN with your account at any given time.\nYou have authenticated with EGI Check-in, but via a different Identity Provider It’s possible to link this identity with your “other” EGI Check-in identity at the level of the EGI Check-in, see account linking documentation.\nOnce your identity is linked at the EGI Check-in level, if you are still having problems accessing, please reassign the ticket to “Configuration and Topology Database (GOCDB)”\nYou have changed the way you log in (i.e. X.509 to EGI Check-in) You can link these identities at the EGI Configuration Database level by following these steps. The steps assume an existing X.509 based account and that you are currently authenticated via EGI Check-in, though the steps should hold for any pair of supported authentication methods.\nIn the User Status panel in the sidebar, click on the Link Identity/Recover Account link. Specify in the form: Authentication type: X.509 The DN of your certificate used to authenticate to your X.509 based account The email address associated to your X.509 based account. Submit and, upon validation, an email will be sent to the specified address, which has to match the one registered with your X.509 based account. This is to avoid identity theft. The email contains a validation link. Click on the validation link or copy/paste in your browser. Authenticate with your EGI Check-in identity. Once authenticated/validated, changes are immediate and you will be able to access your account with both your X.509 and EGI Check-in identities. ","categories":"","description":"Understanding and manipulating user accounts","excerpt":"Understanding and manipulating user accounts","ref":"/internal/configuration-database/users-roles/managing-accounts/","tags":"","title":"Understanding and manipulating user accounts"},{"body":"Introduction In the sub-pages there is an explanation over the EGI Configuration Database user accounts, how to manage them, and the roles defined.\n","categories":"","description":"Guide about user accounts and the roles","excerpt":"Guide about user accounts and the roles","ref":"/internal/configuration-database/users-roles/","tags":"","title":"Users and roles"},{"body":"Roles definition Registered users with a user account will need at least one role in order to perform any useful tasks.\nRole Types A role: Unregistered users B role: Registered users with no role C role: Users with a role at site level (site admin) C’ role: Users with a management role at site level (site operations manager, site security officer…) D role: Users with a role at regional level (regional staff support staff, ROD, 1st Line Support) D’ role: Users with a management role at regional level (NGI manager or deputy, security officer) E role: Users with a role at project level The only difference between C and C’ users is that:\nC can NOT approve/reject role requests. C’ can only approve/reject role requests for their SITE. The difference between D and D’ users is that:\nD can NOT add/delete sites to/from their NGI. D can NOT update the certification status of member sites. D can NOT approve or reject role requests. Roles At Site level Site Administrator - person responsible of maintaining a site and associated information in the EGI Configuration Database (C Level) Site Security officer - official security contact point at site level (C' Level) Site Operations Deputy Manager - The deputy manager of operations at a site (C’ Level) Site Operations Manager - The manager of site operations (C’ Level) At NGI/Regional level Regional First Line Support - Staff providing first line support for an NGI (D Level) Regional Staff (ROD) - staff involved in Operations Centre activities such as user/operations support (D Level) NGI Security officer - official security contact point at regional level (D' Level) NGI Operations Deputy Manager - Deputy manager of NGI operations (D’ Level) NGI Operations Manager - Manager of NGI operations (D’ Level) At Project level COD staff - COD staff (E Level) COD administrator - People administrating Central COD roles (E Level) EGI CSIRT Officer - official security contact point at project level (E Level) Chief Operations Officer (COO) - The EGI Chief Operations Officer (E Level) Permissions associated to roles Roles and permissions are based on whether the considered object is owned or not. In the table below the following definitions apply:\nOwned group: a group on which the role applies (ROC, NGI, project) Owned site: a site on which the role applies, or belonging to an owned group Owned service endpoint: a service endpoint belonging to an owned site Each role has a set of associated permissions which apply on the role’s scope (site, region or project). Main permissions are summarised in the table below\nAction A) Unregistered users B) Registered users with no role C) Site level users C’ ) Site Management Level Users D) NGI level users D’ ) NGI Management Level Users E) Project level users Add a site to an owned group irr. irr. irr. irr. no yes irr. Add a site to a non owned group no no no no no no no Add a service endpoint to an owned site irr. irr. yes yes yes yes irr. Add a service endpoint to a non owned site no no no no no no no Add a downtime to an owned service endpoint irr. irr. yes yes yes yes irr. Add downtime to a non owned service endpoint no no no no no no no Update information of an owned site irr. irr. yes yes yes yes irr. Update information of a non owned site no no no no no no no Update certification status of an owned site irr. irr. no no no yes yes Update certification status of a non owned site no no no no no no yes Update information of a owned service endpoint irr. irr. yes yes yes yes irr. Update information of a non owned service endpoint no no no no no no no Update information of an owned group irr. irr. irr. irr. yes yes irr. Update information of a non owned group no no no no no no no Update own user account details irr. yes yes yes yes yes yes Update other user’s account no no no no no no no Update a downtime on an owned service endpoint irr. irr. yes yes yes yes irr. Update a downtime on a non owned service endpoint no no no no no no no Delete an owned site irr. irr. no no no no no Delete a non owned site no no no no no no no Delete an owned service endpoint irr. irr. yes yes yes yes irr. Delete a non owned service endpoint no no no no no no no Delete an owned group irr. irr. irr. no no no irr. Delete a non owned group no no no no no no no Delete a downtime on an owned service endpoint irr. irr. yes yes yes yes irr. Delete a downtime on a non owned service endpoint no no no no no no no Delete your own user account irr. yes yes yes yes yes yes Delete other user’s account no no no no no no no Register a new user account yes irr. irr. irr. irr. irr. irr. Request a new role no yes yes yes yes yes yes Approve a role request on an owned group irr. irr. no no no yes yes Approve a role request on an owned site no no no yes no yes irr Approve a role request on a non owned site or group no no no no no no no Reject a role request on an owned group no no no no no yes irr. Reject a role request on an owned site no no no yes no yes irr Reject a role request on a non owned site or group no no no no no no no Revoke an existing role on an owned object irr. irr. no yes no yes irr. Revoke an existing role on a non owned object no no no no no no no Retrieve an existing account/ change certificate DN yes yes yes yes yes yes yes Requesting roles for your account There are 2 ways to request new roles.\nBy clicking on the manage role link (sidebar, user status panel) the first form allows you to choose the entity (site or group) on which you want to request a role the second form lets you choose the role you want to apply for By clicking on the request role link from site detail pages or group detail pages. displayed form lets you choose the role you want to apply for Once made, role requests have to be validated before the role is granted to you. This part of the process is described in the next section.\n","categories":"","description":"Understanding and manipulating roles","excerpt":"Understanding and manipulating roles","ref":"/internal/configuration-database/users-roles/managing-roles/","tags":"","title":"Using roles"},{"body":"SITES field The list of sites included in the drop-down menu of the “SITES” field is taken from GOCDB (EGI sites) and from OIM DB (OSG sites). Sites registered in OIM are only visible if they have certification status “enabled”. Sites registered in GOC DB are only visible if they have certification status “certified”. Sites in other statuses (e.g. “suspended”) are not visible in the “SITES” drop-down list. the sites information is synchronized once per night. In case a site’s status changes, the site disappears from the “SITES” drop-down list. Existing GGUS tickets related to this site get the “SITES” field flushed. The NGI to which the site belongs inherits the ticket from the site and is in charge of further processing this ticket. Tickets to multiple sites option: the “Multisite” ticket The Multisite ticket is a feature for submitting tickets with the same topic against an unlimited number of sites in GGUS. After the submission, the ticket is cloned and sent as separate ticket to each of the sites selected during the ticket creation. All the sites displayed in GGUS can be notified using this option. This feature can be used only by users owning the “Multisites” role. There is a specific submit form linked from GGUS ticket submit area. In the “Notify SITE” drop-down menu sites can be checked for receiving a ticket. Creation of a Multisite ticket When the users with the “Multisites” role click on the “+” button to create a ticket, they will see the option “New Multisite Ticket”.\nBy clicking on that option, they will end up in an interface allowing the submission to multiple sites.\nIn addition to the usual fields, the users can also select proper values for:\nSITE TYPE: EGI or OSG TIER LEVELS NGIs Then the “NOTIFIED SITES” field will be filled automatically based on the selections made for the aforementioned fields (it is also possible to further edit manually the values of this field).\nWhen all the necessary fields of the ticket creation form have been filled in properly, the users can click on the “Create” button to submit the tickets.\nAt this point, a notification email for each submitted ticket is sent to the submitter (to be changed in the future to a single email containing all the created tickets in order to reduce the spam to the submitter). At the same time, all the selected sites in the list are notified and the tickets are assigned to the parent NGIs.\nThe list of submitted tickets is available through the link “Search related tickets”:\n","categories":"","description":"Selecting a Resource Centre as recipient and submitting tickets to multiple RCs\n","excerpt":"Selecting a Resource Centre as recipient and submitting tickets to …","ref":"/internal/helpdesk/features/tickets-to-multiple-sites/","tags":"","title":"'SITES' field and tickets to multiple sites"},{"body":"Overview This tutorial describes the access to EGI DataHub spaces from a virtual machine. In the following paragraphs you will learn how to access data remotely stored in EGI DataHub like if they were local, using traditional POSIX command-line commands, by:\ninstalling the oneclient component configuring access to an EGI DataHub Oneprovider via oneclient Prerequisites In order to access the EGI DataHub data you need an EGI Check-in account. If you don’t have one yet you can Sign up for an EGI account.\nOneclient installation The installation of oneclient package is currently supported for:\nUbuntu 18.04 LTS (Bionic Beaver) Ubuntu 20.04 LTS (Focal Fossa) CentOS 7 CentOS 8 Stream Alternatively a docker based installation is also provided.\nOneclient installation via packages Use the following command in order to install the oneclient package in a supported OS:\n$ curl -sS https://get.onedata.org/oneclient.sh | bash This will also install the needed dependencies.\nOneclient installation via docker In order to use the Dockerized version of oneclient (provided that you have docker installed), you can run the following command:\n$ docker run -it --privileged -v $PWD:/mnt/src --entrypoint bash onedata/oneclient:20.02.15 This command will also expose the current folder to the container (as /mnt/src) to ease the transfer of data.\nGetting the token to access data In order to access data stored in EGI DataHub via oneclient, you need to get an API access token.\nUsing oneclient Once you have acquired a token valid for oneclient you can configure it on the environment as follows:\n$ export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e You must also configure in the environment the provider you would like to connect to. The EGI DataHub offers a PLAYGROUND space hosted by the Oneprovider plg-cyfronet-01.datahub.egi.eu which is accessible for testing by anyone with a valid EGI Check-in account.\nTherefore the access to that particular space can be configured as follows:\n$ export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu Now in order to access data from a local folder you need to run the following commands:\n$ mkdir /tmp/space $ oneclient /tmp/space and then all usual file and folder operations (POSIX) will be available:\n$ root@222d3ceb86df:/tmp/space# ls -l total 0 drwxrwxr-x 1 root root 0 Jan 28 16:56 PLAYGROUND Creating a file into the folder will push it to the Oneprovider and it will be accessible in the web interface and from other providers supporting the space.\nBy using the default settings you can see all the spaces you have access to, but it will also be possible to specify a specific space to access using the option --space \u003cspace\u003e.\nOneclient offers a lot of other options for configuration (e.g. buffer size, direct I/O, etc) which are listed when you type the oneclient command without any argument.\n","categories":"","description":"Use data in EGI DataHub from a virtual machine\n","excerpt":"Use data in EGI DataHub from a virtual machine\n","ref":"/users/tutorials/adhoc/vm-datahub/","tags":"","title":"Access DataHub from a VM"},{"body":"Introduction: purpose and conditions The purpose of ALARM tickets is to notify WLCG Tier-0 and Tier-1 administrators about serious problems of the site at any time, independent from usual office hours. Only experts, nominated by the WLCG VOs are allowed to submit alarm tickets. They need to have the appropriate permissions in GGUS user database. The involved VOs are: Alice Atlas Cms Lhcb Only the Tier-0 \u0026 Tier-1 sites are involved in the alarm tickets process. The WLCG Tier-0/Tier-1 site names can be used as well as the relevant GOC DB site names. Alarm tickets are routed to the NGI/ROC the tier site belongs to automatically. They do not need a routing by the TPM. The NGI/ROC is notified about the ticket in the usual way. In parallel the site receives an alarm email signed with a GGUS certificate. This alarm email is processed at the Tier-0/Tier-1 and notifies the relevant people at any time. Alarm email addresses are taken from GOC DB for the EGI sites and from OIM for the OSG sites. VOMS is used by GGUS as the information source for authorised alarmers. Becoming an Alarm-ticket member People who want to become an alarmer have to\nregister in GGUS first be added to the appropriate group in a VOMS server. GGUS system synchronizes its user database once per night with the VOMS servers. The synchronization is based on the DN string. Please make sure the DN of your GGUS account is the same than the one registered in VOMS.\nTier-1 site admins can become alarmers for testing the alarm process for their site. They have to fill in the registration form for supporters. Please add an appropriate comment (e.g. “I’m a xxx tier-1 site admins and I want to become an alarmer for testing purposes.”) in the registration form.\nTechnical description This section describes the workflows of alarm tickets from a technical point of view.\nALARM ticket submission Alarm tickets can be submitted using the GGUS web portal. On top of the ticket submit form in GGUS web portal there is a link to the submit form for alarm tickets.\nAs alarm ticket submitters are experts who will hopefully provide all necessary information, the number of fields on the alarm ticket submit form is reduced to a minimum compared to the number of fields on the user ticket submit form.\nThree fields on this form are mandatory:\nSubject MoU Area Notified Site All other fields are optional.\nALARM ticket processing The processing of a team ticket consists of two main parts: the notification of the notified site and the routing of the ticket to the NGI/ROC the site belongs to.\nTier-1 site notification In parallel to the creation of an alarm ticket, the GGUS system sends an alarm email directly to the tier 1 site specified in field “Notify SITE”. This email is sent to a specific site alarm mail address and signed with the GGUS certificate. The tier-0 alarm mail address is based on the VO name. It is “voname”-operator-alarm\"atnospam\"cern.ch. Tier-1 site alarm mail addresses are taken from the “Emergency Email” field in GOC DB. For tier-1 sites registered in the OSG OIM DB the alarm email address is taken from field “SMSAddress” of the “Administrative Contact” in OIM DB. The DN of the GGUS certificate is /C=DE/O=GermanGrid/OU=KIT/CN=ggusmail/ggus.eu. The alarm mail looks like the following figure.\nTicket routing Alarm tickets are bypassing the TPMs and routed to the appropriate NGI/ROC automatically. The decision to which NGI/ROC a ticket has to be routed is done automatically, based on the value of the “Notify SITE” field. The “Notify SITE” drop-down menu shows both the tier 1 site names from GOC DB and the tier 1 site names used by WLCG.\nALARM confirmation Once the tier 1 site received the alarm email the receipt should be confirmed. Sending a reply mail containing the typical GGUS identifier and the ticket ID “GGUS-Ticket-ID: #00000000” in the subject is sufficient. Such a reply will be added to the alarm ticket.\nWorking on ALARM tickets For working on alarm tickets and resolving them please use the GGUS portal. A reference link to the alarm ticket is given in the alarm email notification.\nPeriodic ALARM ticket testing rules Alarm ticket testing is documented in WLCG twiki.\nSchema of the Alarm tickets process ","categories":"","description":"Definition and usage of the Alarm tickets\n","excerpt":"Definition and usage of the Alarm tickets\n","ref":"/internal/helpdesk/features/alarm-tickets/","tags":"","title":"Alarm tickets"},{"body":"Most operations in EGI DataHub can be performed using one of the OneData Application Programming Interfaces (APIs).\nImportant In order to be able to access the Onedata APIs, an access token is required. See below for instructions on how to generate one. Getting an API access token Tokens have to be generated from the EGI DataHub (Onezone) interface as documented in Generating tokens for using Oneclient or APIs or using a command-line call as documented hereafter.\nBear in mind that a single API token can be used with both Onezone, Oneprovider and other Onedata APIs.\nIt’s possible to retrieve the CLIENT_ID and REFRESH_TOKEN using the EGI Check-in Token Portal. See Check-in documentation for more information.\n$ CLIENT_ID=\u003cCLIENT_ID\u003e $ REFRESH_TOKEN=\u003cREFRESH_TOKEN\u003e # Retrieving an OIDC token from Check-in $ curl -X POST \\ -d \"client_id=$CLIENT_ID\u0026grant_type=refresh_token\u0026refresh_token=$REFRESH_TOKEN\u0026scope=openid%20email%20profile%20eduperson_entitlement\" \\ 'https://aai.egi.eu/auth/realms/egi/protocol/openid-connect/token' | python -m json.tool; # Token is in the access_token field of the response The following variables should be set:\nOIDC_TOKEN: OpenID Connect Access token. ONEZONE_HOST: name or IP of the Onezone host (to use Onezone API). $ ONEZONE_HOST=https://datahub.egi.eu $ OIDC_TOKEN=\u003cOIDC_ACCESS_TOKEN\u003e $ curl -H \"X-Auth-Token: egi:$OIDC_TOKEN\" -X POST \\ -H 'Content-type: application/json' \\ \"$ONEZONE_HOST/api/v3/onezone/user/tokens/named\" \\ -d '{ \"name\": \"REST and CDMI access token\", \"type\": { \"accessToken\": {} }, \"caveats\": [ { \"type\": \"interface\", \"interface\": \"rest\" } ] }' Data access via CDMI and REST API Below are example commands to learn how to access DataHub files and folders via CDMI and REST API using the command-line interface.\nFor more information please check the Onedata CDMI documentation and the Onedata Oneprovider REST API\nCommon configuration Follow instructions above to get an API access token, and configure environment variables:\n$ export DATAHUB_TOKEN=\u003cDATAHUB_ACCESS_TOKEN\u003e $ export ONEPROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu Having jq installed is useful for better formatting of the JSON output.\nCDMI Configure a header to be passed in some operations.\n$ export CDMI_VSN_HEADER='X-CDMI-Specification-Version: 1.1.1' See examples on how to list a folder, and file download/upload using CDMI:\n# List files in a folder $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -H \"$CDMI_VSN_HEADER\" \\ \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/?children\" | jq . # Download \"helloworld.txt\" from DataHub to \"downloadtest.txt\" on your computer $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/helloworld.txt\" \\ -o downloadtest.txt # Upload \"helloworld.txt\" from your computer to \"uploadtest.txt\" on DataHub $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -H \"$CDMI_VSN_HEADER\" \\ -X PUT \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/uploadtest.txt\" \\ -T helloworld.txt REST API See examples on how to list a folder, and file download/upload using REST API:\n# Get base folder ID $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X POST \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/lookup-file-id/PLAYGROUND\" # Add the folder ID to an environment variable $ export DIR_ID=\u003cID_FROM_PREVIOUS_COMMAND\u003e # List files inside the folder with DIR_ID $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X GET \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$DIR_ID/children\" \\ | jq . # Add the ID of the file that you want to download $ export FILE_ID=\u003cID_FROM_PREVIOUS_COMMAND\u003e # Download file with FILE_ID from DataHub to \"helloworld.txt\" on your computer $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X GET \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$FILE_ID/content\" \\ -o helloworld.txt # Upload \"helloworld.txt\" on your local computer to \"uploadtest.txt\" on DataHub $ curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\ -X POST \\ \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$DIR_ID/children?name=uploadtest.txt\" \\ -H \"Content-Type: application/octet-stream\" -d \"@helloworld.txt\" Data access from Python If your application is written in Python please check the documentation for the OnedataFS Python library\nTesting the API with the REST client A docker container with clients acting as wrappers around the API calls is available: onedata/rest-cli. It's very convenient for discovering and testing the Onezone and Oneprovider API.\n$ docker run -it onedata/rest-cli # Exporting env for Onezone API $ export ONEZONE_HOST=https://datahub.egi.eu $ export ONEZONE_API_KEY=\u003cACCESS_TOKEN\u003e # Checking current user $ onezone-rest-cli getCurrentUSer | jq '.' # Listing all accessible spaces $ onezone-rest-cli listEffectiveUserSpaces | jq '.' $ docker run -it onedata/rest-cli # Exporting env for Oneprovider API $ export ONEPROVIDER_HOST=https://plg-cyfronet-01.datahub.egi.eu $ export ONEPROVIDER_API_KEY=\u003cACCESS_TOKEN\u003e # Listing all spaces supported by the Oneprovider $ oneprovider-rest-cli getAllSpaces | jq '.' # Listing content of a space $ oneprovider-rest-cli listFiles path='EGI Foundation/' $ oneprovider-rest-cli listFiles path='EGI Foundation/CS3_dataset' Printing the raw REST calls of a wrapped command Raw REST calls (used with curl) can be printed using the --dry-run switch.\n$ docker run -it onedata/rest-cli $ export ONEZONE_HOST=https://datahub.egi.eu $ export ONEZONE_API_KEY=\u003cACCESS_TOKEN\u003e # Listing all accessible spaces $ onezone-rest-cli listEffectiveUserSpaces | jq '.' # Printing the curl command without running it $ onezone-rest-cli listEffectiveUserSpaces --dry-run Working with PID / Handle It’s possible to mint a Permanent Identifier (PID) for a space or a subdirectory of a space using a handle service (like Handle.net) that is registered in the Onezone (EGI DataHub).\nOnce done, accessing the PID using its URL will redirect to the Onedata share allowing to retrieve the files.\nPrerequisites: access to a Handle service registered in the Onezone. See the Handle Service API documentation for documentation on registering a new Handle service or ask a Onezone administrator to authorize you to use an existing Handle service already registered in the Onezone.\nThe following variables should be set:\nAPI_ACCESS_TOKEN: Onedata API access token ONEZONE_HOST: name or IP of the Onezone host (to use Onezone API). ONEPROVIDER_HOST: name or IP of the Oneprovider host (to use Oneprovider API) # Getting the IDs of the available Handle Services $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handle_services\" HANDLE_SERVICE=\u003cHANDLE_SERVICE_ID\u003e # Getting details about a specific Handle service $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handle_services/$HANDLE_SERVICE\" # Listing all spaces $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/effective_spaces/\" | jq '.' # Displaying details of a space $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/spaces/$SPACE_ID\" | jq '.' # Listing content of a space $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEPROVIDER_HOST/api/v3/oneprovider/files/EGI%20Foundation/\" | jq '.' # Creating a share of a subdirectory of a space $ DIR_ID_TO_SHARE=\u003cDIR_ID\u003e $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ -X POST -H 'Content-Type: application/json' \\ -d '{\"name\": \"input\"}' \"$ONEPROVIDER_HOST/api/v3/oneprovider/shares-id/$DIR_ID_TO_SHARE\" | jq '.' # Displaying the share $ SHARE_ID=\u003cSHARED_ID\u003e $ curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/shares/$SHARE_ID\" | jq '.' # Registering a handle # Proper Dublin Core metadata is required # It can be created using https://nsteffel.github.io/dublin_core_generator/generator_nq.html $ cat metadata.xml # Escape double quotes and drop line return $ METADATA=$(cat metadata.xml | sed 's/\"/\\\\\"/g' | tr '\\n' ' ') # On handle creation the created handles is provided in the Location header $ curl -D - --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ -H \"Content-type: application/json\" -X POST \\ -d '{\"handleServiceId\": \"'\"$HANDLE_SERVICE_ID\"'\", \"resourceType\": \"Share\", \"resourceId\": \"'\"$SHARE_ID\"'\", \"metadata\": \"'\"$METADATA\"'\"}' \\ \"$ONEZONE_HOST/api/v3/onezone/user/handles\" # Listing handles $ curl --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handles\" # Displaying a handle $ HANDLE_ID=\u003cHANDLE_ID\u003e $ curl --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\ \"$ONEZONE_HOST/api/v3/onezone/user/handles/$HANDLE_ID\" Subscribe to file events Following is an example of how to subscribe to DataHub to receive notification on file events which is described in details in the official documentation Subscribe to file events:\n$ curl -N -H \"X-Auth-Token: $TOKEN\" \\ -X POST \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/changes/metadata/$SPACE_ID\" \\ -H \"Content-Type: application/json\" -d \"@./changes_req.json\" This requires the permission set as following:\nFor groups or single users. For single users, one way to add one is to select Effective members\" -\u003e in the user list search for the required user and “Make an owner”. In this case the user will have admins privileges in addition to the one required. As this might not be the desired configuration it will be enough to remove all the unwanted permissions, e.g.: make it the same as the VO to which the user belongs to, and leave only, as extra, the permission shown in the screenshot.\n","categories":"","description":"The programmatic interface of EGI DataHub\n","excerpt":"The programmatic interface of EGI DataHub\n","ref":"/users/data/management/datahub/api/","tags":"","title":"DataHub API"},{"body":"EGI Secrets Store is based on Hashicorp Vault and is fully integrated with EGI Check-in, allowing users to use it with their community credentials.\nImportant When this service is adopted by an application, it becomes a critical dependency for that application, thus the EGI Secrets Store is implemented using a highly-available (HA) approach. Service design Multiple Vault servers in HA mode, located at different geographical locations (in different resource centres) are connected into a cluster, with one active and multiple standby instances. Data is replicated among servers using the Raft Consensus algorithm supported by the Vault Integrated Storage.\nShould the active server fail, one of the standby servers will become active automatically. Each server does a periodic check, and the active one will update the main service endpoint to point to itself, via Dynamic DNS.\nService endpoints During normal operation users can access any of the nodes of EGI Secrets Store directly:\nIISAS node IFCA node INFN node Note Accessing EGI Secrets Store via the node endpoints is not recommended: if a server is down, its endpoint is not accessible. For convenience and high availability, you should use the generic endpoint to access the service. This generic endpoint will be pointed to one of the service nodes automatically via Dynamic DNS. A simple cron script periodically checks and assigns the generic hostname to a healthy server.\nThe recovery time of the generic endpoint, in the case of an unscheduled downtime of the generic endpoint’s server, is T+1 minutes, where T is the interval between cron checks (usually 1 minute). In the case of scheduled downtime for maintenance, administrators simply assign the generic endpoint to another server instance.\n","categories":"","description":"The service architecture of EGI Secrets Store\n","excerpt":"The service architecture of EGI Secrets Store\n","ref":"/users/security/secrets-store/architecture/","tags":"","title":"Secrets Store Architecture"},{"body":"Moving the Site BDII to another machine Ensure the new Site BDII is working fine and publishing all the necessary site information\nSee MAN01 for general information about how to configure a Site BDII. In particular, remember that the Site BDII configuration must include the BDII node itself. Put the old Site BDII in scheduled downtime in the Configuration Database for a couple of hours\nRegister the new service in the Configuration Database by adding a new Service Endpoint:\nselect Site BDII in Service Type field fill in at least the hostname select Y for production and monitoring, N for beta service field Properly modify the GIIS URL field with the new SITE-BDII ldap URL\nNote that the site name in the GIIS URL is case-sensitive Mark the old Site BDII as not production and turn off its monitoring\nTop BDIIs updates their sites list every hour so the new Site BDII should be published in less than an hour; instead Nagios updates the monitored hosts every 3 hours\nWhen the new Site BDII is appeared on Nagios and the old one is disappeared, turn off the old Site BDII and remove it from the Configuration Database\nTurning off a Site BDII co-hosted with other services Stop and remove the service\nservice bdii stop yum remove glite-BDII glite-yaim-bdii Delete some info-providers and ldif file ( /opt/glite/etc/gip/ldif/stub-site.ldif, /opt/glite/etc/gip/site-urls.conf, /opt/glite/etc/gip/provider/glite-info-provider-site, /opt/glite/etc/gip/provider/glite-info-provider-service-bdii-site-wrapper, /opt/glite/etc/gip/ldif/glite-info-site.ldif)\nReconfigure with yaim without specifying the BDII_site profile\n","categories":"","description":"Changing the Site BDII","excerpt":"Changing the Site BDII","ref":"/providers/high-throughput-compute/changing-site-bdii/","tags":"","title":"Changing the Site BDII"},{"body":"Introduction After discussions in the GGUS-AB and the OMB meetings, members agreed on implementing a work flow for closing tickets waiting for submitter input after a reasonable amount of time. This work flow was also presented at the UCB meeting.\nStatus value “waiting for reply” The status value “waiting for reply” should only be used when input from the ticket submitter is required. Setting the ticket status to “waiting for reply” in GGUS triggers an email notification to the submitter’s email address registered in the ticket. After the submitter has replied either by\nsending an email using the same sender mail address as registered in the ticket or by updating the ticket in web portal using the same credentials/certificate used for submitting the ticket, the ticket status changes to “in progress” automatically. Updates on the ticket done using different credentials/certificates or email addresses than registered in the ticket will not be recognized as reply by the system and hence have no impact on the work flow. The status value “waiting for reply” must not be used in case of waiting for input of any other person involved in the solving process.\nWorkflow First action 5 working days after setting the ticket status to “waiting for reply” the first reminder is sent to the ticket submitter requesting input. The ticket will also be added to the ticket monitoring dashboard. The ticket monitoring team will make sure that the status value “waiting for reply” is used in a correct sense.\nSecond action In case the submitter does not reply, the second reminder is sent to the ticket submitter requesting input after 5 more working days.\nThird action In case the submitter does not reply after another 5 more working days the ticket monitoring team gets notified. The monitoring team will check the ticket for any updates by the submitter not recognized by the system and set the ticket status to “unsolved” if none. The ticket will follow the usual process for “solved”/“unsolved” tickets and be closed after 10 working days without re-opening the ticket again.\nAction Ticket Status Working Days Work Flow 1 waiting for reply 5 first reminder to submitter; adding ticket to monitoring dashboard 2 waiting for reply 5 second reminder to submitter 3 waiting for reply 5 notification to monitoring team; the monitoring team sets status “unsolved” 4 unsolved 10 manuel status change to “closed” by ticket monitoring team Summary In this workflow there is always a human intervention before closing a ticket. The submitter has 15 working days in total for replying to a ticket. Additionally, they have 10 more working days for re-opening the ticket in case they do not agree with setting the ticket to “unsolved”.\nWorkflow graph ","categories":"","description":"Process for closing tickets lacking of submitter's input\n","excerpt":"Process for closing tickets lacking of submitter's input\n","ref":"/internal/helpdesk/workflows/waiting-for-submitter/","tags":"","title":"Closing tickets lacking of submitter input"},{"body":"Working with compute Most scientific work researchers do includes computation. Could be the need to run a small analysis. Or a data reduction job that has to be repeated a million times on every measurement coming in from an ionosphere observation facility. Or a handful of cutting-edge simulations of climate models that each demand a huge number of compute cores.\nEGI provides a variety of compute services for each of these use cases. Researchers can choose the best compute tool for each application, from customisable and elastic Virtual Machine (VM) based cloud to fully managed distributed platforms that will run their jobs using CPUs, GPUs or both.\nThe compute services are summarized below:\nCloud Compute means VM-based computing with associated storage. It delivers customisable resources where users have complete control over the software, the supporting compute type and capacity. Typical use-cases are user gateways or portals, interactive computing platforms and almost any kind of data- and/or compute-intensive workloads. Container Compute supports running container-based applications with either Docker or Kubernetes on top of Cloud Compute. Typical use-cases are multi-tenant, microservices-based applications that must easily scale horizontally. High Throughput Compute provides access to large, shared grid computing systems for running computational jobs at scale. Typical use-cases include analysis of large datasets in an “embarrassingly parallel” fashion (i.e. by splitting the data into small pieces), and executing thousands, or even more independent computing tasks simultaneously, each processing one piece of data. When researchers encounter the need for large amounts of computing power, need hybrid solutions based on the above mentioned services, or simply do not care to manage the underlying compute facilities that will run their workloads, cloud orchestrators can help provision, manage, and monitor compute resources, which then run scientific workloads.\nThe following sections describe each compute service that is available in the EGI infrastructure.\n","categories":"","description":"Compute services in the EGI Cloud\n","excerpt":"Compute services in the EGI Cloud\n","ref":"/users/compute/","tags":"","title":"Compute Services"},{"body":"Working with data One of the main daily activities of researchers is to collect or create data that needs to be processed, analyzed, or shared. The EGI offering in this area is quite vast and includes both low level storage services and data management frameworks, as well as specialized data transfer services.\nExplore the areas below for further details.\n","categories":"","description":"Data services in the EGI Cloud\n","excerpt":"Data services in the EGI Cloud\n","ref":"/users/data/","tags":"","title":"Data Services"},{"body":"Some procedures to deal with security events Sites facing or suspecting a security incident on their resources have to follow Incident Handling Procedure\nNew vulnerability issues of the middleware should be handled as defined in the related procedure with a guide what to do when you find a vulnerability.\nSites having critical vulnerabilities are handled according to SEC03 EGI-CSIRT Critical Vulnerability Handling Procedure, and if they do not respond properly, they may face suspension.\n","categories":"","description":"Information about procedures to deal with security incidents.","excerpt":"Information about procedures to deal with security incidents.","ref":"/providers/rod/security/","tags":"","title":"Dealing with security incidents"},{"body":"Getting help Requests for technical assistance and other issues related to Check-in can be obtained by sending an email to:\ncheckin-support@mailman.egi.eu\nAlternatively, if you already have a Check-in account, the EGI Helpdesk can be used. More information can be found in the Helpdesk documentation. In that case, submit a new ticket and make sure that the support unit is assigned to “Check-in (AAI)”.\nConnect to Check-in an IdP federated in an hub and spoke federations I get an error similar to: “Error - EGI Check-in Service not accessible through your institution” (SURFconext example) In case of a “hub and spoke” federation the federation coordinator may require that the IdP administrators explicitly request to connect to a SP and let their users to authenticate on these SP.\nIn most of the cases this is not a configuration problem neither for the Check-in service nor for the Identity provider. The connection needs to be implemented in the hub and spoke IdP Proxy.\nOne example of such federation is SURFconext, the national IdP federation for research and education in the Netherlands operated by SURFnet. If you are using credentials from a Dutch IdP in eduGAIN, the SURFconext administrator of your institute needs to request the connection.\nAuthentication error with ADFS-based Identity Providers Why do I get the error below after successfully authenticating at my Home IdP? opensaml::FatalProfileException at (https://aai.egi.eu/registry.sso/SAML2/POST) SAML response reported an IdP error. Error from identity provider: Status: urn:oasis:names:tc:SAML:2.0:status:Responder The Responder error status is typically returned from ADFS-based IdP implementations (notably Microsoft ADFS 2.0 and ADFS 3.0) that cannot properly handle Scoping elements. Check-in can be configured to omit the scoping element from the authentication requests sent to such IdPs in order to allow successful logins. Please send an email to the Check-in Support team using checkin-support \u003cAT\u003e mailman.egi.eu and include a screenshot of your error.\nI have linked an IGTF X.509 certificate to my Check-in identity but the information is inaccurate or incomplete What can I do? To update your certificate information, follow these steps to log into your Check-in profile page using your IGTF certificate:\nClick here to access your profile page Warning This may log you out of any service you have accessed with Check-in on this browser! 2. On the Check-in identity provider discovery page, select IGTF\nWarning If prompted to log in with a different identity provider, click CHOOSE ANOTHER ACCOUNT and then select IGTF. Alternatively, you can click here for your convenience ","categories":"","description":"Most frequent questions about EGI Check-in\n","excerpt":"Most frequent questions about EGI Check-in\n","ref":"/users/aai/check-in/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"How do I install library X? You can install new software easily on the notebooks using conda or pip. The %conda and %pip magics can be used in a cell of your notebooks to do so, e.g. installing rdkit:\n%conda install rdkit Once installed you can import the library as usual.\nWarning Any modifications to the libraries/software of your notebooks will be lost when your notebook server is stopped (automatically after 1 hour of inactivity)! Can I request library X to be installed permanently? Yes! Just let us know what are your needs. You can contact us via:\nOpening a ticket in the EGI Helpdesk, or Creating a GitHub Issue We will analyse your request and get back to you.\n","categories":"","description":"Most frequent questions asked about EGI Notebooks\n","excerpt":"Most frequent questions asked about EGI Notebooks\n","ref":"/users/dev-env/notebooks/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"Site endpoints must be registered in EGI Configuration Management Database (GOCDB). Cloud services can coexist within an existing (Grid) site, but we are recommending to register a new site for the cloud services (as per PROC09 Resource Centre Registration and Certification procedure - see Joining as a provider section)\nExpected Services These are the expected services for a working site:\norg.openstack.nova for the Nova endpoint of the site. The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3. Set the Host DN so the cloud-info-provider can be enabled.\neu.egi.cloud.accounting for the host sending the records to the accounting repository (executing SSM send). This host needs a valid IGTF-accredited X.509 certificate. Set the Host DN so the SSM can be enabled.\norg.openstack.horizon for the dashboard endpoint of the site (optional). The endpoint URL field must contain the horizon URL: https://hostname:port/url/.\n(Optional, if offering object storage) org.openstack.swift for the swift endpoint of the site. The endpoint URL field must contain the Keystone v3 URL: https://hostname:port/url/v3.\nDeprecated services Deprecated services for cloud providers:\nSite-BDII. This service collects and publishes site's data for the Information System. There is no need to run a BDII for cloud providers. eu.egi.cloud.information.bdii is also deprecated and no longer in use.\neu.egi.cloud.vm-metadata.vmcatcher for the VMI replication mechanism.\neu.egi.cloud.vm-management.occi for the OCCI endpoint offered by the site. OCCI is no longer in use in FedCloud.\n","categories":"","description":"Registration of service endpoints in GOCDB\n","excerpt":"Registration of service endpoints in GOCDB\n","ref":"/providers/cloud-compute/registration/","tags":"","title":"GOCDB Registration"},{"body":"What is it? Support for EGI services is available through the EGI Helpdesk.\nThe EGI Helpdesk is a distributed tool with central coordination, which provides the information and support needed to troubleshoot product and service problems. Users can report incidents, bugs or request changes.\nThe support activities are grouped into first and second level support.\n","categories":"","description":"EGI Helpdesk\n","excerpt":"EGI Helpdesk\n","ref":"/internal/helpdesk/","tags":"","title":"Helpdesk"},{"body":"Introduction With the term “Central” or “Core” Service we refer to a category of services in the EGI Infrastructure providing capabilities that support and complement the other services of the infrastructure and the related activities. A list of those services is available in the section Services for Federation of our site. The EGI Core Services are co-funded by EGI Foundation and the providers are selected through a bidding process: the technical details of the services that should be delivered are advertised to the EGI Council, and only the providers with the EGI Participant role in the Council can apply to the bid. How to join the EGI Federation as a member of the Council. Current members of the EGI Council Differently from services with a distributed nature such as HTC, Cloud, and Storage, they cannot be ordered through the Marketplace, but they become available as soon as a user joins the infrastructure (e.g., the access to the EGI Helpdesk service). Selection of the providers and registration With the selected providers, EGI Foundation negotiates and signs an OLA defining the terms and conditions for the delivery of the services; at the same time, the process to add the services to the EGI Service portfolio is started, if the service is not already included.\nAt this point, steps, similar to the ones for Resource Centers and Technology Providers, follow in order to guarantee the regular day-to-day operation of the service, such as:\nregistration in the Configuration Database and certification; definition of the Support Unit in the Helpdesk system to handle incidents and service requests; enabling of the monitoring; periodic performance reports as defined in the given OLA to verify that the Service Level Targets are achieved. In addition to this, the providers are also requested to create a capacity plan, an availability and continuity plan, and to interact with Change Management (CHM) and Release and Deployment Management (RDM) processes for managing changes and new releases of their services.\nCapacity plan The capacity plan is important to assess if the capacity of the service is sufficient to respond to the current and to the future demand of the service.\nAny capacity aspect of the service delivery is analysed (human, technical, and financial) with the definition of quantitative parameters to measure the usage and the load of the service. The approach to adjust the capacity of the service in relation to a change in the demand is defined, and recommendations on capacity requirements for the next reporting period are provided as well.\nEGI Foundation defined a template for the Capacity plans and contacts and supports the service providers for the creation of a new Capacity plan or for their regular reviews (the reviews are usually performed at least twice per year).\nAvailability and continuity plan In the Availability and Continuity plan, a number of risks affecting the availability and continuity of the service is identified and assessed: each risk is rated in terms of likelihood and impact with the definition of countermeasures to implement that should avoid the occurrence of the given risk. Any remaining vulnerability is identified as well, and in case the rating of a risk is considered to be too high in relation to the risk acceptance criteria, a plan either to improve the existing countermeasures or to implement new ones is created, with the aim either to reduce the likelihood of a risk or to mitigate the impact in case a risk occurs.\nThe plan is completed by a continuity and recovery test, where the continuity of the service and its recovery capacity are tested against a simulated disruption scenario: the performance of this test is useful to spot any issue in the recovery procedures of the service.\nAlso in this case, the discussion of the Availability and Continuity plan is started and overseen by EGI Foundation who shares with the providers a template that will be filled in with the details of the given service. Availability and Continuity plans are reviewed on a yearly basis.\nManaging changes and new releases All changes to the services should follow the EGI Change Management (CHM) process according to the Change Policy, in order to evaluate the potential impact that a change can have on the service and on the infrastructure as a whole.\nWhen registering a Request for Change, besides a general description of the change, the providers are expected to provide:\nthe risk level as a result of assessing the impact and the likelihood of things going wrong, the type of change, the eventual list of other services potentially affected by the change, if it is possible to revert the change, the proposed date for the implementation of the change, if it is needed to schedule a downtime of the service. Requests for Changes are assessed by the Change Advisory Board (CAB), deciding whether the Change is going to be approved or rejected.\nFor recurrent changes with a relatively low risk level, the providers can request to classify them as Standard Changes, so that invoking the CAB won’t be necessary and they can undergo the release process after the release plan is agreed with the EGI CHM staff.\nThe Emergency Changes are created when there is an urgent need to fix either a newly discovered security vulnerability or a critical issue: in this case the formal approval of the CAB is not required, and it is enough that the release plan is accepted by CHM staff.\nIn all of the other situations the changes are classified as Normal and depending on their risk level they might need to be assessed by the CAB.\nBefore the release in the production environment, the providers may invite the users or other impacted service providers, to test the new changes on a pre-production instance in order to gather feedback and to find out possible issues that might be overlooked during the preparation of the new release. If no problems are found, the release can go live, and a Post Implementation Review is conducted after a few days to close the case.\nSecurity aspects The providers of central services are subject to the same policies, procedures and requirements applying to the federated service providers, as documented at this page.\nNevertheless, they are also subject to the following requirement that is documented in the service OLA that is being agreed with them:\nThey should immediately report suspected security incidents to the EGI Foundation. This is not exempting them to follow the SEC01 EGI CSIRT Security Incident Handling Procedure and inform EGI CSIRT within 4 hours. It’s also important to understand that when processing of personal data is taking place, EGI Foundation holds the role of Data Controller and the provider is a Data Processor, as defined in the Global Data Protection Regulation (GDPR).\nData Processing Agreements, regulating the conditions and constraints of the data processing activities conducted by the Data Processor on behalf of the Data controller, will be put in place on the initiative of the EGI Foundation staff. EGI Foundation will also prepare, together with the service provider, adequate Privacy Notice and Acceptable Use policy that have to be presented and made available to the users of the service. EGI Foundation is also complying with all the principles set out by the REFEDS Data Protection Code of Conduct in its most recent version, implying that the central service provider should also comply with them. The central service provider should also follow requirements relating to the software and covering the usage of a proper licence, the access to and management of the source code, implementation of best practices; as well as requirements relating to the IT Service management and covering the need for having key staff properly trained about IT Service Management, committing to continual improvement and having their Service Management System (SMS) interfacing with the EGI SMS, especially for important processes like the Change Management process.\n","categories":"","description":"Guidelines for joining the EGI Infrastructure to provide a Core Service supporting the other services and activities","excerpt":"Guidelines for joining the EGI Infrastructure to provide a Core …","ref":"/providers/joining/core-service/","tags":"","title":"Joining as a provider of a Core Service"},{"body":"Overview Each instance of the FTS3 service, offers a Web monitoring interface, that can be accessed by end users in order to monitor their submitted transfers and obtain statistics.\nFeatures The Web monitoring can be accessed without user authentication, only access to the transfer log files needs an X.509 user certificate installed on the browser.\nOverview page The Overview page offers a way to access the information about the transfers submitted and executed in the last 6 hours. Users can filter transfers per Virtual Organization, source or destination storage or JobId.\nJob details page By selecting a specific job the information about the job details are displayed. Each transfer part of the job is listed with his own information. From this page it’s also possible to access the transfer logs (upon authentication).\nOptimizer page The Optimizer page shows Optimizer information about a specific link, detailing the throughput evolution and the parallel transfer/stream per link at a given time.\n","categories":"","description":"Monitoring file transfers of EGI Data Transfer\n","excerpt":"Monitoring file transfers of EGI Data Transfer\n","ref":"/users/data/management/data-transfer/monitoring/","tags":"","title":"Monitoring Data Transfer"},{"body":"Here you can find documentation about the internal architecture and operations of the EGI Notebooks service.\n","categories":"","description":"Architecture and Operations of EGI Notebooks","excerpt":"Architecture and Operations of EGI Notebooks","ref":"/providers/notebooks/","tags":"","title":"Notebooks"},{"body":"The oidc-agent is a command-line tool for managing OpenID Connect tokens developed by Karlsruhe Institute of Technology (KIT).\nObtain a Token If you haven’t installed oidc-agent in your system, please read the installation guide.\nNote Please make sure that you have installed the version 4.3.0 or later Make sure that the oidc-agent-service is started by executing the command:\n$ eval `oidc-agent-service start` To obtain a Token you need to execute the following command:\nProduction Demo Development $ oidc-gen --pub --issuer https://aai.egi.eu/auth/realms/egi $ oidc-gen --pub --issuer https://aai-demo.egi.eu/auth/realms/egi $ oidc-gen --pub --issuer https://aai-dev.egi.eu/auth/realms/egi Then enter a short name for the account to configure, and the scopes that the Access Token should contain.\nNote To get a Refresh Token you need to specify the offline_access scope in the requested scopes After that, you need to log in either by visiting the provided URL or by scanning the displayed QR code, and then provide the user code displayed in the oidc-agent.\nLast but not least, you will need to provide an encryption password to protect the obtained Access/Refresh Token.\nFor more information, please read the oidc-agent documentation.\n","categories":"","description":"Usage guide of oidc-agent\n","excerpt":"Usage guide of oidc-agent\n","ref":"/users/aai/check-in/obtaining-tokens/oidc-agent/","tags":"","title":"oidc-agent"},{"body":"Introduction QoS stands for Quality of Support. It describes the level of support provided by Support Units in GGUS system.\nIt has an impact on the ticket priority colour in GGUS and the warnings are sent to SUs if 75% of the maximum response time of the QoS level are over.\nQoS levels There are three different QoS levels, each defining different response times for given Ticket Priority.\nBase Medium Advanced The default QoS level, if not declared differently, is Base.\nBase Level Base QoS level defines a response time of 5 working days regardless of the ticket priority.\nMedium Level Ticket Priority Response time less urgent 5 working days urgent 5 working days very urgent 1 working day top priority 1 working day Advanced service Ticket Priority Response time less urgent 5 working days urgent 1 working day very urgent 1 working day top priority 4 working hours QoS level declaration The QoS level for all of the SUs are available here\nFor the several EGI services, the QoS levels are defined in the specific Operational Level Agreements linked in the EGI OLA SLA framework page\n","categories":"","description":"Quality of Support (QoS) levels\n","excerpt":"Quality of Support (QoS) levels\n","ref":"/internal/helpdesk/features/quality-of-support-levels/","tags":"","title":"Quality of Support (QoS) levels"},{"body":"Definitions and prerequisites The GGUS Report Generator is available from the support section.\nThe implementation of the report generator started in October 2011. Hence, the report generator does not provide data for tickets submitted before December 2011!\nTimestamps and metrics submit timestamp: timestamp when the ticket is submitted assign timestamp: timestamp when a ticket gets assigned to a support unit response timestamp: timestamp when the ticket status changes from “assigned” to any other status value or the ticket gets re-assigned to another support unit expected response timestamp: timestamp when a ticket should have changed the status to any other status value than “assigned” or be re-assigned to another support unit at latest solution timestamp: timestamp when the status changes to either “solved” or “unsolved” Response time The response time is a performance figure calculated from the support unit’s point of view. It describes how quick a support unit is reacting on tickets. Response time is the time from\neither assigning a ticket to a support unit and the support unit is kept until the first status changes to any other value than “assigned”, or assigning a ticket to a support unit and the status value “assigned” is kept until the support unit changes to any other support unit (re-assign). The response time is calculated as difference between the timestamp changing the status or re-assigning the ticket and the assign timestamp. While assigning a ticket to a support unit the expected response timestamp is calculated by adding an amount of time to the assign timestamp. The amount of time added depends on the ticket priority and the kind of support unit. For support units that have declared a quality of service level the response times are defined by the QoS level. For all the other support units a medium QoS is assumed for calculating the expected response timestamp. In case the actual response timestamp is greater than the “expected response” timestamp for middleware support units the “violate” flag is set.\nResponse times are based on office hours. Hence the results unit is working days.\nSolution time The solution time is a performance figure also calculated from the support unit’s point of view. It describes how long it took the support unit for providing a solution. Solution time is the time from assigning a ticket to a support unit until it provides a solution for the problem described. “Providing a solution” means setting the ticket status to “solved” or “unsolved”. The solution time is calculated as the difference between the solution timestamp and the assign timestamp.\nSolution times are based on office hours. Hence the results unit is working days.\nWaiting time Waiting time is the sum of all time slots the ticket was set to “waiting for reply”. Calculating the waiting time has started in July 2012. For tickets submitted before July 2012 no waiting time calculation was done. The waiting time for these tickets may be zero. The waiting time can be excluded when calculation solution times by ticking the checkbox “exclude waiting time”.\nTicket lifetime The ticket lifetime is calculated from the user’s point of view. It describes how long it takes to provide a meaningful solution for a problem reported by the user. Ticket lifetime is the time from ticket submission to ticket solution (status “solved/unsolved”).\nThe ticket lifetime is based on calendar days.\nTime zones GGUS support units are spread over a wide range of time zones. Some of the support units themselves are spread over several time zones. However most support units are located in European time zones. Support units and their time zones are listed on a dedicated page. For middleware product teams GGUS assumes time zone “UTC +1” for all support units. For all the other support units the system uses the relevant time zone for calculating timestamps as far as possible.\nOffice hours The systems assumes usual office hours from 09:00 to 17:00 Monday to Friday for all support units. National holidays are not taken into account. For middleware product teams the timezone UTC+1 is used as default timezone.\nTicket priorities For the calculation of performance figures the original priority set during ticket submission is used. This priority value is kept as long as the support unit is in charge of the ticket. Updating the priority value during ticket lifetime doesn’t affect the calculation of performance figures.\nUnit abbreviations [wd] means working days [d] means calendar days Reports description Although some of the drop-down lists request the selection of values a query can be started anyway. In this case the query is equivalent to a query over all tickets!\nThe “Reset” button resets all fields to their default settings besides the time frame.\nTickets submitted This metric gives the number of tickets submitted within the specified time frame. Major criteria is the submit timestamp. The result lists shows the current status of the tickets.\nOpen tickets time The open tickets time report calculates the time from ticket submission until now in calendar days. The submit timestamp must match the specified time frame for the report.\nTickets closed This metric is focused on the ticket life time. It gives the number of tickets that reached the status “solved” or “unsolved” within the specified time frame. Major criteria is the solution timestamp which is the timestamp setting a ticket to “solved” or “unsolved”. Tickets in other terminal status like “verified” or “closed” appear in the result list as long as they have been set to “solved/unsolved” in the given time frame. Besides date, status and the number of closed tickets the results list displays the average ticket lifetime and the median ticket lifetime. The result lists shows the current status of the tickets.\nResponse time This metric focuses on the responsiveness of support units. Major criteria for this report is the submit timestamp which must match the selected time frame. The result list shows:\nthe number of tickets responded the number of responses average response time and median response time for the specified time frame. The result list shows all support units that have ever been in charge of a ticket. Hence the same ticket ID may appear several times. The number of responses may be greater than the number of tickets.\nResponse times are based on office hours. Hence an average response time of 1d 3h 23min means 13 hours and 23 minutes in total.\nViolated response time Specific privileges are required for this report. Only people belonging to a dedicated technology provider (TP) are able doing reports for this TP. Major criteria for this report are the expected response time defined in SLAs and the TP.\nSolution time Major criteria for this report is the solution timestamp. The result list shows:\nthe number of solutions average response time and median response time for the specified time frame. The result lists shows the current status of the tickets.\nSolution times are based on office hours. Hence, an average solution time of 12d 3h 5min means 99 hours and 5 minutes in total.\nThe waiting time can be excluded by ticking the checkbox “exclude waiting time”. In case a ticket gets re-opened the metrics calculation starts again from scratch. Hence, the same ticket can appear several times in the solution times calculation.\nInput parameters and results Input parameters The input parameters vary depending on the report type chosen. Possible input parameters are:\nTime frame Responsible Unit Status Priority Concerned VO Ticket type Ticket category Ticket Scope Notified site Technology provider Date aggregation Time frame The time frame defines begin and end date of the report. The begin date starts at 00:00:00. The end date ends at 00:00:00. As the implementation of the report generator started in October 2011 the report generator does not provide data for tickets submitted before December 2011!\nResponsible Units The drop-down list offers all responsible units integrated in GGUS system. They can be filtered by keywords. Responsible Units can be either selected all, one by one or by checking the boxes in front of the responsible unit groups. In case no responsible units are selected all responsible units are considered in the reports.\nStatus The drop-down list offers all status values available in GGUS system. Multiple selections are possible.\nPriority The drop-down list offers all priority values available in GGUS system. Multiple selections are possible.\nConcerned VO “Concerned VO” provides a drop-down list of all VOs supported by GGUS. Multiple selections are possible.\nTicket type This drop-down list lists all ticket types in GGUS. Multiple selections are possible.\nTicket category Selectable categories are “Incident”, “Change request”, “Documentation”, “Test”. Multiple selections are possible. The category “Test” should not be considered for all reports and therefore, if necessary, be excluded from the selection.\nTicket scope To distinguish between tickets under the responsibility of EGI or WLCG. See ticket scope.\nNotified site “Notified site” lists all sites integrated in GGUS. They are derived from GOC DB and OIM DB. In case of reporting exclusively on tickets without any site value specified please ticket the “blank” value in the drop-down list.\nTechnology Provider This input parameter is only visible if choosing the “violated response time” report. For accessing this report specific privileges are required. The drop-down list offers all technology providers currently integrated in GGUS and the “DMSU”. Multiple selections are possible. In case no technology provider is selected the reports will be done for tickets without any technology provider specified.\nDate aggregation The results aggregation level can be chosen from the drop down list “Choose date aggregation”.\nGroup by Results can be grouped by one or more of the input parameters.\nResults The results are displayed below the input parameter area. They can be sorted in different ways by clicking on the column labels. A drill-down is possible by clicking on any row of the results list. The detail results open in a new window. They can be sorted by clicking the column labels too. Clicking on the ticket ID opens the ticket in GGUS system. At the bottom of the result panel there are various icons offering features like:\nSearch Refresh Export Export what you see Doing a simple “Export” saves the data in a csv file stripping all column headers. For exporting the data including the column headers please use “Export what you see”. The “Search” feature allows searching in the result list. Possible search parameters are the columns of the result list.\n","categories":"","description":"Features and usage of the Report generator","excerpt":"Features and usage of the Report generator","ref":"/internal/helpdesk/features/report-generator/","tags":"","title":"Report generator"},{"body":" Target Audience: Data scientists Domain scientists and research (in Environmental, Climate research) e-Infrastructure platform/ technology \u0026 service providers \"Harnessing Advanced Research Infrastructures for Earth and Environmental Data Management and Analysis: A MATLAB Perspective\" (May, 2024) Agenda, slides and recording: https://www.egi.eu/event/harnessing-advanced-research-infrastructures-for-earth-and-environmental-data-management-and-analysis-a-matlab-perspective/ About: The increasing volume and complexity of Earth and environmental data requires an efficient interdisciplinary collaboration and management. The use of advanced research infrastructures enables data integration, interoperability, and seamless data exchange, leveraging analysis and visualization tools and cloud facilities. In the first part of the presentation there is an introduction to MATLAB and a review of existing and new features relevant to the EGI users. In the second part, a case study is presented on the ENES Data Space, a cloud-based data science environment for climate data analysis, part of the European Open Science Cloud (EOSC) Compute Platform. Users can access and work with climate datasets, including past recordings and future projections from the CMIP project. The case study demonstrates geodata import, analysis, and visualization using MATLAB Online and Jupyter notebooks, emphasizing ease of use, data manipulation, and sharing capabilities within the ENES workspace without requiring local software installations or data downloads. Target Audience: oceanographers climate researchers ocean modellers ocean data managers \"The EGI-ACE webODV – Online extraction, analysis and visualisation of SeaDataNet and Argo data\" (Nov. 2022) Agenda, slides and recording: https://indico.egi.eu/event/5980/ About: ODV (Ocean Data View) is a widely used software package for the analysis, exploration and visualization of oceanographic and other environmental data with almost 100,000 registrations since the 1990’s and more than 10,000 active users. ODV has been further developed as part of the EU supported activities of SeaDataNet, the pan-European infrastructure for marine and ocean data management. To make working with web-based large community datasets easier, an online version of the ODV software called webODV has been developed, which provides typical ODV functionality in the form of web services. webODV has been deployed in the cloud in the framework of the EU EGI-ACE project, which seeks to promote scientific analytical cloud services in support of the European Open Science Cloud (EOSC) initiative. In this webinar, we will be able to learn about the webODV Explorer and Extractor components in combination with the large validated Temperature \u0026 Salinity data collections as provided by SeaDataNet and EuroArgo – Argo. Instructions on how to register as a user to perform analytics, extractions, and visualisations will also be presented. Target Audience: Scientific communities and end users. \"Can deep learning models help accelerate electrostatics-driven protein pKa predictions?\" (Nov. 2022) Agenda, slides and recording: https://indico.egi.eu/event/5981/ About: pH is a crucial physicochemical property that affects proteins molecular structure, folding, stability, and function. Many computational methods have been developed to calculate pKa values. In the highly accurate, but slow, Poisson–Boltzmann (PB)-based methods, proteins are represented by point charges in a low dielectric medium surrounded by an implicit solvent (high dielectric). Empirical methods rely on statistically fitting parameters over large datasets of experimental pKa values. These are much faster than the physics-based methods, although at the cost of less microscopic insights and unknown predictive power on mutations and proteins dissimilar to those in the training set. In this webinar we will present a novel strategy to combine the best features of PB models – accuracy and interpretability – with the speed of classical empirical methods. The deep learning pKa predictors obtained were trained on a database of 3M theoretical pKa values estimated from 50k structures using a PB method. With this approach, we can retrieve the physics-based predictions with an average error below 0.4 pK units while being up to 1000x faster. Target Audience: Scientific communities and end users. \"The Virtual Imaging Platform: Scientific Applications as a Service and Beyond\" (March, 2022) Agenda, slides and recording: https://indico.egi.eu/event/5824/ About: The Virtual Imaging Platform (VIP) is a web portal for medical simulation and image data analysis. It leverages resources available in the EGI biomed Virtual Organisation to offer an open service to academic researchers worldwide. In the last few years, VIP has addressed interoperability and reproducibility concerns, in the larger scope of a FAIR (Findable, Accessible, Interoperable, Reusable) approach to scientific data analysis. The presentation will give an overview of VIP and its main interests for the audience: Use existing applications as a service on VIP. Import your own applications to make them available to the community benefit from EGI biomed resources in a transparent way and Foster open and reproducible science Target Audience: Scientific communities and end users. \"ENES Data Space Service\" (March, 2022) Agenda, slides and recording: https://indico.egi.eu/event/5743/ About: The ENES Data Space delivers an open, scalable and cloud-enabled data science environment for climate data analysis on top of the EOSC Compute Platform. It provides access to a set of specific Coupled Model Intercomparison Project (CMIP) variable-centric collections to support meteorological and industrial researchers in realistic climate model analysis experiments. Data is downloaded and kept in sync with the Earth System Grid Federation (ESGF) federated data archive. This webinar will provide a general overview of the ENES Data Space service and its main features. Moreover, the webinar will include a tutorial on how to join the ENES Data Space service and a short demo for the participants to get started with the data analysis and visualisation features. Target Audience: Scientific communities, end users and service providers who are interested in discovering how we have been making efficient use of EGI High Throughput Compute (HTC) resources over more than 10 years to provide services to worldwide researchers in structural biology and life sciences. \"WeNMR - Structural biology in the cloud - 10 years of experience of using EGI services\" (April 2020) Agenda, slides and recording: https://indico.egi.eu/event/5084/ About: Structural biology deals with the characterization of the structural (atomic coordinates) and dynamic (fluctuation of atomic coordinates over time) properties of biological macromolecules and adducts thereof. Gaining insight into 3D structures of biomolecules is highly relevant with numerous applications in health and food sciences, with as current example unravelling the structural details of Sars-Cov2 in the COVID-19 pandemic. Since 2010, the WeNMR project has implemented numerous web-based services to facilitate the use of advanced computational tools by researchers in the field, using the grid computational infrastructure provided by EGI. These services have been further developed in subsequent initiatives under the H2020 EGI-ENGAGE West-Life project and the BioExcel Center of Excellence for Biomolecular Computational Research. The WeNMR services are currently operating under the European Open Science Cloud with the H2020 EOSC-Hub project, with the HADDOCK portal sending \u003e10 millions jobs and using ~2700 CPU years per year. In my talk, I will summarise 10 years of successful use of e-infrastructure solutions to serve a large worldwide community of users (\u003e16’000 to date), providing them with user-friendly, web-based solutions that allow to run complex workflows in structural biology. I will illustrate this with details of the HADDOCK service and how we could increase our capacity to serve COVID-related projects. Suggested tutorials Submitting HTC Jobs: Submitting High Throughput Compute jobs Target Audience: Scientific communities working in the domain of Earth Observation. \"C-SCALE Notebooks for Earth Observation\" (June 2023) About: The C-SCALE project has been federating compute and data resource providers around centralized EGI services, aiming at providing users with seamless access to processing capacities as well as source data for their analyses. Alongside the traditional IaaS and PaaS services, Jupyter Notebooks have been identified as an environment suitable not only for interactive analysis within C-SCALE, but also for documenting the different steps one needs to take in discovering and accessing geospatial data across Europe. The demonstration of C-SCALE's example notebooks and procedures will focus on those essential features: simple steps to get started using the federated resources for interactive resources of Earth Observation data. Slides and code https://github.com/c-scale-community/c-scale-notebooks Target Audience: Scientific communities working in the domain of Earth Observation. \"C-SCALE Earth Observation - Metadata Query Service\" (March 2023) About: The C-SCALE Earth Observation Metadata Query Service (EO-MQS) makes Copernicus data distributed across providers within the C-SCALE Data federation discoverable and searchable. Slides and code https://github.com/c-scale-community/c-scale-tutorial-eo-mqs Target Audience: Scientific communities working in the domain of Earth Observation. \"Introduction to openEO Platform\" (December 2022) About: openEO platform provides intuitive programming libraries to process a wide variety of earth observation datasets. This large-scale data access and processing is performed on multiple infrastructures, which all support the openEO API. This allows use cases from explorative research to large-scale production of EO-derived maps and information. Slides and code https://github.com/c-scale-community/c-scale-tutorial-openeo Target Audience: Scientific communities and end users. \"I-NERGY, European AI-on demand platform\" (July 2023) About: I-NERGY aims to support and develop novel AI-based energy services as part of the enrichment of European AI-on demand platform. This webinar will present the objectives and scope of the project, its requirements in terms of resources and the successful utilisation of EGI infrastructure. The webinar will conclude with a demo of I-NERGY services. Target Audience: Scientific communities and end users. \"Access to the EISCAT tools with help of EGI Check-in\" (June 2023) About: The present era of rapid technological advances creates a challenge for data providers and scientists to create and maintain FAIR data and services not just for future operations but also for historical data gathered and analysed with technologies that are slowly phasing out of their usage. GUISDAP is an open-source software package, written in MATLAB, C and Fortran and provided and maintained by EISCAT, for analysis and visualisation of its incoherent scatter radar data as well as for some other radars in the world. One way how to preserve GUISDAP operability and accessibility by the user community is to make it accessible through a Jupyter notebook docker deployment through EISCAT resources and in the frame of an EOSC project. This will help to ensure the FAIRness of EISCAT data by providing tools for reanalysis and visualisation that will be accessible by any potential EISCAT user with the help of EGI check-in technology. ","categories":"","description":"Examples of domain-specific Thematic Services benefiting from the solutions offered by the EGI Infrastructure.\n","excerpt":"Examples of domain-specific Thematic Services benefiting from the …","ref":"/users/tutorials/scientific/","tags":"","title":"Scientific level"},{"body":"In addition to the formatting support provided by Markdown, Hugo adds support for shortcodes, which are Go templates for easily including or displaying content (images, notes, tips, advanced display blocks, etc.).\nFor reference, the following shortcodes are available:\nHugo’s shortcodes Docsy theme shortcodes Highlighted paragraphs This is achieved using Docsy shortcodes.\nPlaceholders The following code:\n{{% pageinfo %}} This is a placeholder. {{% /pageinfo %}} Will render as:\nThis is a placeholder. Information messages The following code:\n{{% alert title=\"Note\" color=\"info\" %}} This is a Note. {{% /alert %}} Will render as:\nNote This is a Note. Warning messages The following code:\n{{% alert title=\"Important\" color=\"warning\" %}} This is a warning. {{% /alert %}} Will render as:\nImportant This is a warning. Code or shell snippets The code or instructions should be surrounded with three backticks, followed by an optional highlighting type parameter.\nThe supported languages are dependent on the syntax highlighter, which depends itself on the Markdown parser.\nNote Hugo uses the goldmark parser, which relies on Prism syntax highlighting. The following Markdown creates a shell excerpt:\n```shell $ ssh-keygen -f fedcloud $ echo $HOME ``` Will render as:\n$ ssh-keygen -f fedcloud $ echo $HOME Tip If you click the Copy button in the top-right corner of a shell example, all commands in that block are copied to the clipboard. The prompt in front of each command, and any command output is not copied. Note In case the command(s) in your shell example cause the introduction of a horizontal scroll bar, consider breaking the command(s) into multiple lines with trailing backslashes (\\). However, you should never break command output to multiple lines, as that makes understanding the output, and recognizing it in real life, very difficult. Code in multiple languages This is also achieved using Docsy shortcodes.\nWhen you need to include code snippets, and you want to provide the same code in multiple programming languages, you can use a tabbed pane for code snippets:\n{{\u003c tabpane \u003e}} {{\u003c tab header=\"C++\" lang=\"C++\" \u003e}} #include \u003ciostream\u003e int main() { std::cout \u003c\u003c \"Hello World!\" \u003c\u003c std::endl; } {{\u003c /tab \u003e}} {{\u003c tab header=\"Java\" lang=\"Java\" \u003e}} class HelloWorld { static public void main( String args[] ) { System.out.println( \"Hello World!\" ); } } {{\u003c /tab \u003e}} {{\u003c tab header=\"Kotlin\" lang=\"Kotlin\" \u003e}} fun main(args : Array\u003cString\u003e) { println(\"Hello, world!\") } {{\u003c /tab \u003e}} {{\u003c tab header=\"Go\" lang=\"Go\" \u003e}} import \"fmt\" func main() { fmt.Printf(\"Hello World!\\n\") } {{\u003c /tab \u003e}} {{\u003c /tabpane \u003e}} Will render as:\nC++ Java Kotlin Go #include \u003ciostream\u003e int main() { std::cout \u003c\u003c \"Hello World!\" \u003c\u003c std::endl; } class HelloWorld { static public void main( String args[] ) { System.out.println( \"Hello World!\" ); } } fun main(args : Array\u003cString\u003e) { println(\"Hello, world!\") } import \"fmt\" func main() { fmt.Printf(\"Hello World!\\n\") } Content with multiple variants When you need to include multiple variants of the same content, other than code snippets in multiple programming languages, you can use the following shortcode:\n{{\u003c tabpanex \u003e}} {{\u003c tabx header=\"Linux\" \u003e}} You can list all files in a folder using the command: ```shell $ ls -a -l ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Mac\" \u003e}} To get a list of all files in a folder, press **Cmd** + **Space** to open a spotlight search, type terminal, then press Enter. In the terminal window then run the command: ```shell $ ls -a -l ``` {{\u003c /tabx \u003e}} {{\u003c tabx header=\"Windows\" \u003e}} You can list all files in the current folder using the command: ```shell \u003e dir ``` or you can use PowerShell: ```powershell \u003e Get-ChildItem -Path .\\ ``` {{\u003c /tabx \u003e}} {{\u003c /tabpanex \u003e}} Will render as:\nLinux Mac Windows You can list all files in a folder using the command:\n$ ls -a -l To get a list of all files in a folder, press Cmd + Space to open a spotlight search, type terminal, then press Enter. In the terminal window then run the command:\n$ ls -a -l You can list all files in the current folder using the command:\n\u003e dir or you can use PowerShell:\n\u003e Get-ChildItem -Path .\\ Tip You can include any valid markdown content in each tab, including code or shell snippets. ","categories":"","description":"Helpers for writing EGI documentation","excerpt":"Helpers for writing EGI documentation","ref":"/about/contributing/shortcodes/","tags":"","title":"Shortcodes"},{"body":"Introduction: purpose and conditions The purpose of TEAM tickets is to allow all team members to modify all tickets of their team, although they may not have support access, and they do not have submitted the ticket themselves. Only members of a few VOs are allowed to submit team tickets: Alice Atlas CMS LHCb Biomed Belle The VO members need to have the appropriate permissions in GGUS user database Other VOs can request this functionality by opening a GGUS ticket Team tickets are routed to the NGI/ROC the site belongs to automatically. They do not need a routing by the TPM. The NGI/ROC is notified about the ticket in the usual way. In parallel, the site receives a notification email. Becoming a Team-ticket member People who want to become a team member have to:\nregister in GGUS first be added to the appropriate group in a VOMS server. GGUS system synchronizes its user database once per night with the VOMS servers. The synchronization is based on the DN string. Please make sure the DN of your GGUS account is the same to the one registered in VOMS.\nTechnical description This section describes the workflows of team tickets from a technical point of view.\nTeam ticket submission Team tickets can either be submitted using the GGUS web portal. On top of the ticket submit form in GGUS web portal there is a link to the submit form for team tickets.\nAs team ticket submitters are expected to be experts who will hopefully provide all necessary information, the number of fields on the team ticket submit form is reduced to a minimum, compared to the number of fields on the user ticket submit form\nThree fields on this form are mandatory:\nSubject MoU Area Notified Site All of the other fields are optional.\nTeam ticket processing The processing of a team ticket consists of two main parts, the notification of the notified site and the routing of the ticket to the NGI/ROC the site belongs to.\nSite notification In parallel to the creation of a team ticket the GGUS system sends an email notification directly to the site specified in field “Notify Site”. This email is sent to a specific site contact mail address.\nTicket routing Team tickets are bypassing the TPMs and routed to the appropriate NGI/ROCs automatically as long as field “Routing type” is not changed. The decision to which NGI/ROC a ticket has to be routed is done automatically, based on the value of the “Notify Site” field.\nWorking on team tickets For working on team tickets and resolving please use the GGUS portal. A reference link to the team ticket is given in the team notification mail. Team tickets can be upgraded to alarm tickets clicking on the button “Click to convert to ALARM”.\nTeam tickets process schema See the schema of the Team Tickets process.\n","categories":"","description":"Definition and usage of the Team tickets\n","excerpt":"Definition and usage of the Team tickets\n","ref":"/internal/helpdesk/features/team-tickets/","tags":"","title":"Team tickets"},{"body":"Priority definition If you plan to adjust the ticket priority of a ticket we kindly ask you to provide also justification for this change in the reply text box of the ticket.\nThe following table will help you to find an appropriate priority value:\nPriority Comment very urgent service interrupted; needs to be addressed as soon as possible urgent service degraded; a workaround might or might not be available less urgent wishes and enhancements that are “nice to have” In particular, be very economical when choosing ‘very urgent’. This value, when reaching the supporters via another ticketing system interface, might become a beep or phone alert even in the middle of the night. This level of support is ONLY committed by WLCG Tier0 and Tier1s and ONLY for ALARM tickets.\nFinally, please be aware that the supporter who will try to solve your problem may change the value you have chosen to a more realistic one, putting their justification in the ticket.\n","categories":"","description":"Definition and computation of the ticket priority in relation to the QoS levels\n","excerpt":"Definition and computation of the ticket priority in relation to the …","ref":"/internal/helpdesk/features/ticket-priority/","tags":"","title":"Ticket Priority"},{"body":"Priority definition The GGUS ticket scope filter is available in GGUS ticket search. It offers filtering for tickets in EGI or WLCG scope. The ticket scope field is automatically set by the system based on the qualifications listed below.\nWLCG scope tickets are either:\nof type TEAM or ALARM related to VOs alice, atlas, cms, lhcb, belle assigned to VOSupport related to VOs alice, atlas, cms, lhcb, belle assigned to OSG Software Support, USCMS, USATLAS, USBELLE, CRIC with issue type like CMS_, ATLAS_ All other tickets are EGI scope.\n","categories":"","description":"Description of ticket scopes","excerpt":"Description of ticket scopes","ref":"/internal/helpdesk/features/ticket-scope/","tags":"","title":"Ticket Scope"},{"body":"The EGI Check-in Token Portal allows users to create Access and Refresh Tokens.\nObtain an Access Token In order to obtain an Access Token from EGI Check-in Token Portal, please follow the steps below:\nGo to https://aai.egi.eu/token and click on “Authorise” to authenticate yourself. After logging in you will obtain an Access Token as it is shown below: The value of the Access Token The command to get user’s information from userinfo endpoint. Running this curl command will also print out your OIDC entitlements. Obtain a Refresh Token (more info in Obtain a Refresh Token section) View and manage the applications you have given permissions Obtain a Refresh Token In order to obtain an Refresh Token from EGI Check-in Token Portal, please follow the steps below:\nGo to https://aai.egi.eu/token and click on “Authorise” to authenticate yourself. After logging in click on “Create Refresh Token”: Then you will be redirected back to EGI Check-in Token Portal and you will have obtained a Refresh Token: The value of the Refresh Token The command to generate new Access Token using the Refresh Token ","categories":"","description":"Usage guide of EGI Check-in Token Portal\n","excerpt":"Usage guide of EGI Check-in Token Portal\n","ref":"/users/aai/check-in/obtaining-tokens/token-portal/","tags":"","title":"EGI Check-in Token Portal"},{"body":"Overview The following guides and tutorials show you how to perform common tasks in the EGI infrastructure. Each of the following sections addresses a specific use case: it describes how to set up the EGI services needed for that use case, how to connect or operate them together, and will walk you step-by-step towards achieving a task or setup that can be immediately used by researchers and scientists.\n","categories":"","description":"Step-by-step tutorials for common tasks in the EGI Infrastructure.\n","excerpt":"Step-by-step tutorials for common tasks in the EGI Infrastructure.\n","ref":"/users/tutorials/adhoc/","tags":"","title":"Ad-hoc tutorials"},{"body":"Setting up GPU flavours Support for GPU can be added to flavours using the PCI passthrough feature in OpenStack. This allows to plug any kind of PCI device to the Virtual Machines.\nAs a summary of the OpenStack documentation, these are the steps needed to add a GPU enabled flavour (be aware this may need tuning to your specific hardware/configuration!):\nOn computing node, get vendor/product ID of your hardware: lspci | grep NVIDIA to get pci slot of GPU, then virsh nodedev-dumpxml pci_xxxx_xx_xx_x On computing node, unbind device from host kernel driver. Unbinding is system dependent, and can be done in many ways, e.g.: if the kernel does not uses the devices (no GPU drivers included in kernel, or drivers disable in GRUB), nothing to unbind via pci-stub grubby --args=\"pci-stub.ids=10de:11fa\" --update-kernel DEFAULT (see RedHat manual, section 12.1, step 1-2; where the pci-stub.ids value is vendor_ID: product_id from lspci. via echo command: echo $dev \u003e /sys/bus/pci/devices/$dev/driver/unbind where $dev is the PCI device ID xx:xx.x or xxxx:xx:xx.x from lspci On computing node, add pci_passthrough_whitelist = {\"vendor_id\":\"xxxx\",\"product_id\":\"xxxx\"} to nova.conf (see nova-compute) On controller node, add pci_alias = {\"vendor_id\":\"xxxx\",\"product_id\":\"xxxx\", \"name\":\"GPU\"} to nova.conf (see nova-api) On controller node, enable PciPassthroughFilter in the scheduler (see nova-scheduler) Create new flavours with pci_passthrough:alias (or add key to existing flavour), e.g. openstack flavor set m1.large --property \"pci_passthrough:alias\"=\"GPU:2\" GPU description in flavour metadata Users should be able to easily discover the flavours that provide GPUs (or accelerators in general). The following table describes the agreed metadata for EGI providers to add to those flavours:\nMetadata Definition Comments Accelerator:Type Type of accelerator (e.g. GPU) Possible values: GPU, MIC, FPGA, TPU, NPU Accelerator:Number Number of accelerators available in the flavour (e.g. 1.0) Non integers allowed for the case of sharing GPU between VMs Accelerator:Vendor Name of accelerator Vendor (e.g. NVIDIA) Accelerator:Model Model of accelerator (e.g. Tesla V100) Need to make consensus and enforce. A100 is usually marketed without “Tesla” class name. Similarly, RTX A6000 usually marketed without “GeForce”. For clarity, full names should be used: “Tesla A100” and “GeForce RTX A6000” Accelerator:Version Version of the accelerator Some cards have different versions, e.g. A100 PCIe and NVLink. Openstack does not allow empty value, so we should give 0 if no version is specified Accelerator:Memory RAM in GB of the accelerator Accelerator:VirtualizationType Type of virtualisation used (e.g. PCI passthrough) Not relevant for accounting, but may be still useful in some cases There are some extra fields that are defined in the GLUE2.1 schema but not so relevant for GPUs and therefore not considered at the moment. These are listed below for completeness:\nMetadata Definition Comments Accelerator:ComputeCapability Compute capabilities Defined by GLUE2.1, e.g. floating point type, NVLink, … may be used informally so far Accelerator:ClockSpeed Clockspeed of accelerator Defined by GLUE2.1, not so relevant, as ClockSpeed no longer related to performance. May be reserved for other types of accelerators Accelerator:Cores Number of cores of the accelerator Not so useful as there are several types of cores now (CUDA, tensor). May be reserved for other types of accelerators Adding metadata to flavours has no effects on site operations. End users can see the metadata easily via openstack flavor list --long or openstack flavor show \u003cflavor id\u003e commands without any additional tools, e.g.:\n$ fedcloud openstack flavor show gpu1cpu2 --site IISAS-GPUCloud --vo eosc-synergy.eu -f json Site: IISAS-GPUCloud, VO: eosc-synergy.eu { \"OS-FLV-DISABLED:disabled\": false, \"OS-FLV-EXT-DATA:ephemeral\": 0, \"access_project_ids\": null, \"disk\": 40, \"id\": \"a8082202-f647-4d1f-9b97-4f5ddb38ae8e\", \"name\": \"gpu1cpu2\", \"os-flavor-access:is_public\": false, \"properties\": \"Accelerator:Version='0', Accelerator:Memory='5', Accelerator:Model='Tesla K20m', Accelerator:Number='1.0', Accelerator:Type='GPU', Accelerator:Vendor='NVIDIA', Accelerator:VirtualizationType='PCI passthrough', pci_passthrough:alias='GPU:1'\", \"ram\": 8192, \"rxtx_factor\": 1.0, \"swap\": \"\", \"vcpus\": 2 } ","categories":"","description":"Configuring GPU flavours\n","excerpt":"Configuring GPU flavours\n","ref":"/providers/cloud-compute/openstack/gpu/","tags":"","title":"GPU flavours"},{"body":"With High-Throughput Compute you can run computational jobs at scale on the EGI infrastructure. It allows you to analyse large datasets and execute thousands of parallel computing tasks.\nHigh-Throughput Compute is provided by a distributed network of computing centres, accessible via a standard interface and membership of a virtual organisation. EGI offers more than 1,000,000 cores of installed capacity, supporting over 1.6 million computing jobs per day.\nThis service supports research and innovation at all scales: from individuals to large collaborations.\nMain characteristics of the service:\nAccess to high-quality computing resources Integrated monitoring and accounting tools to provide information about the availability and resource consumption Workload and data management tools to manage all computational tasks Large amounts of processing capacity over long periods of time Faster results for your research Shared resources among users, enabling collaborative research Service management Miscellaneous collection of documentation related to High-Throughput Compute.\nChanging the Site BDII How to configure Site and Top BDII (external page) HTCondor-CE Accounting Storage Accounting ","categories":"","description":"Execute thousands of computational tasks to analyse large datasets","excerpt":"Execute thousands of computational tasks to analyse large datasets","ref":"/providers/high-throughput-compute/","tags":"","title":"High-Throughput Compute"},{"body":"The EGI services are contributed by the EGI Council members, who can propose new services through the EGI Service Strategy, that defines the overall direction to EGI services for a few years time window. The services are designed, validated with the EGI Community, and with the help of the EGI Services and Solutions Board (SSB).\nThe evolution of the services is driven by 5 Strategic Objectives (SO) which are based on the knowledge of the user community needs that are being gathered through research and innovation projects, customer interviews and analysis of trends. Each defines an area for possible future work, and if approved, can be developed into more concrete goals and intended results.\nSO1: Federated compute continuum SO2: Federated Data Lakes and repositories SO3: Data analytics and scientific tools including AI SO4: Professional support and consultancy SO5: Investigation of a trusted compute platform for sensitive data processing If you want to propose a new service, you should approach your national EGI Council member, or fallback to the EGI SSB.\n","categories":"","description":"How new services are added to the EGI service catalogue","excerpt":"How new services are added to the EGI service catalogue","ref":"/providers/joining/new-service/","tags":"","title":"Introducing a new EGI service"},{"body":" EGI DataHub service Overview slides Community Forum EGI Webinar and YouTube video System requirements Official Onedata documentation Onedata homepage Getting started Source code ","categories":"","description":"Links to additional DataHub resources\n","excerpt":"Links to additional DataHub resources\n","ref":"/users/data/management/datahub/links/","tags":"","title":"DataHub Links"},{"body":"Document control Property Value Title MAN05 top and site BDII High Availability Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Deploying top or site BDII service in High Availability Owner SDIS team Top BDII and Site BDII with High Availability This document objective is to provide guidelines to improve the availability of the information system, addressing three main areas:\nRequirements to deploy a top or site BDII service High Availability from a client perspective Configuration of a High Availability top or site BDII service Requirements to deploy a top or site BDII service Hardware dual core CPU 10GB of hard disk space 2-3 GB RAM. If you decide to set BDII_RAM_DISK=yes in your YAIM configuration, it’s advisable to have 4GB of RAM. Co-hosting Due to the critical nature of the information system with respect to the operation of the grid, the top or site BDII should be installed as a stand-alone service to ensure that problems with other services do not affect the BDII. In no circumstances should the BDII be co-hosted with a service which has the potential to generate a high load. Physical vs Virtual Machines There is no clear vision on this topic. Some managers complain that there are performance issues related to deploying a top or site BDII service under a virtual machine. Others argue that such performance issues are related to the configuration of the service itself. The only agreed feature is that the management and disaster recovery of any service deployed under a virtual machine is more flexible and easier. This could be an important point to take into account considering the critical importance of the top or site BDII service. Best practices from a client perspective for top BDII In gLite 3.2 and EMI you can set up redundancy of top BDIIs for the clients (WNs and UIs) setting up a list of top BDII instances to support the automatic failover in the GFAL clients. If the first Top level BDII fails to be contacted, the second will be used in its place, and so on. This mechanism is implemented defining the BDII_LIST YAIM variable according to the following syntax: BDII_LIST=topbdii.domain.one:2170[,topbdii.domain.two:2170[...]]. After running YAIM, the client enviroment should contain the following definition: LCG_GFAL_INFOSYS=topbdii.domain.one:2170,topbdii.domain.two:2170 The data management tools (lcg_utils) contact the information system for every operation (lcg-cr, lcg-cp, …). So, if you have your client properly configured with redundancy for the information system, the lcg_utils tools will use that mechanism in a transparent way. Be aware that lcg-infosites doesn’t work with multiple BDIIs. Only gfal, lcg_utils, lcg-info and glite-sd-query.\nSite administrators should configure their services with this failover mechanism where the first top BDII of the list should be the default top BDII provided by their NGI.\nBest practices for a top or site BDII High Availability service The best practice proposal to provide a high availability site or top BDII service is based on two mechanisms working as main building blocks: DNS round robin load balancing A fault tolerance DNS Updater We will provide a short introduction to some of these DNS mechanisms but for further information on specific implementations, please contact your DNS administrator.\nDNS round robin load balancing Load balancing is a technique to distribute workload evenly across two or more resources. A load balancing method, which does not necessarily require a dedicated software or hardware node, is called round robin DNS.\nWe can assume that all transactions (queries to top or site BDII generate the same resource load. For an effective load balancing, all top or site BDII instances should have the same hardware configurations. In other case, a load balancing arbiter is needed.\nSimple round robin DNS load balancing is easy to deploy. Assuming that there is a primary DNS server (dns.domain.tld) where the DNS load balancing will be implemented, one simply has to add multiple A records mapping the same hostname to multiple IP addresses under the core.top.domain DNS zone. It is equally applicable to site BDII.\n# In dns.domain.tld: Add multiple A records mapping the same hostname to multiple IP addresses Zone core.domain.tld topbdii.core.domain.tld IN A x.x.x.x topbdii.core.domain.tld IN A y.y.y.y topbdii.core.domain.tld IN A z.z.z.z The 3 records are always served as answer but the order of the records will rotate in each DNS query\nThis does NOT provide fault tolerance against problems in the top or site BDIIs themselves\nif one top or site BDII fails its DNS “A” record will still be served one in each three DNS queries will provide the failed top or site BDII first answer Fault tolerance DNS Updater The DNS Updater is a mechanism (to be implemented by you) which tests the different top or site BDIIs and decides to remove or add DNS entries through DNS dynamic updates. The fault tolerance is implemented by dynamically nsupdate introduced in bind V8 offers the possibility of changing DNS records dynamically: The nsupdate tool connects to a bind server on port 53 (TCP or UDP) and can update zone records Updates are authorized based on keys Updates can only be performed on the DNS primary server In the DNS bind implementation, the entire zone is rewritten by the DNS server upon “stop” to reflect the changes. Therefore, the zone should not be managed manually; and the changes are kept in a zone journal file until a “stop” happens. Implementation There are several alternatives to implement the DNS Updater: NAGIOS based tests a demonized service scripts running as crons What to test: BDII metrics Status information about the BDII is available by querying the o=infosys root for the UpdateStats object. This entry contains a number of metrics relating to the latest update such as the time to update the database and the total number of entries. And example of such entry is shown below. $ ldapsearch -x -h \u003cTopBDII/siteBDII\u003e -p 2170 -b \"o=infosys\" (...) dn: Hostname=localhost,o=infosys objectClass: UpdateStats Hostname: lxbra2510.cern.ch FailedDeletes: 0 ModifiedEntries: 4950 DeletedEntries: 1318 UpdateTime: 150 FailedAdds: 603 FailedModifies: 0 TotalEntries: 52702 QueryTime: 8 NewEntries: 603 DBUpdateTime: 11 ReadTime: 0 PluginsTime: 4 ProvidersTime: 113 More extensive information can be obtained (modifyTimestamp,createTimestamp) adding the +: $ ldapsearch -x -h \u003cTopBDII/siteBDII\u003e -p 2170 -b \"o=infosys\" + (...) # localhost, infosys dn: Hostname=localhost,o=infosys structuralObjectClass: UpdateStats entryUUID: 09bf40e0-7b23-4992-af55-fd74f036a454 creatorsName: o=infosys createTimestamp: 20110612223435Z entryCSN: 20110615120723.216201Z#000000#000#000000 modifiersName: o=infosys modifyTimestamp: 20110615120723Z entryDN: Hostname=localhost,o=infosys subschemaSubentry: cn=Subschema hasSubordinates: FALSE The following table shows the meaning of the most relevant metrics: Metric Description ModifiedEntries The number of objects to modify DeletedEntries The number of objects to delete UpdateTime To total update time in seconds FailedAdds The number of add statements which failed FailedModifies The number of modify statements which failed TotalEntries The total number of entries in the database QueryTime The time taken to query the database NewEntries The number of new objects DBUpdateTime The time taken to update the database in seconds ReadTime The time taken to read the LDIF sources in seconds PluginsTime The time taken to run the plugins in seconds ProvidersTime The time taken to run the information providers in seconds Previous BDII metrics can be checked to take a decision regarding the reliability and availability of a top or site BDII instance.\nMore information is available in gLite-BDII_top Monitoring.\nDNS caching DNS records obtained in queries are cached by the DNS servers (usually during 24 hours). Therefore to propagate DNS changes fast enough it is important to have very short TTL lifetimes. DNS has not been built to have very short TTL values and these may increase highly the number of queries and as result increase the load of the DNS server The TTL lifetime to be used will have to be tested. If the top BDII are only used by sites in the region and if queries are only from the DNS servers of these few sites then the number of queries may be low enough to allow for a very small TTL. This value should not be lower than 30s - 60s. Example 1: The IGI Nagios based mechanism In IGI, the DNS update of the number of instances participating in the DNS round robin mechanism depends on the results provided by a Nagios instance.\nWhen Nagios needs to check the status of a service it will execute a plugin and pass information about what needs to be checked. The plugin verifies the operational state of the service and reports the results back to the Nagios daemon.\nNagios will process the results of the service check and take appropriate action as necessary (e.g. send notifications, run event handlers, etc).\nEach instance is checked every 5 minutes. If a failure occurs, Nagios runs the event handler to restart the BDII service AND remove the instance from the DNS round robin set using dnsupdate:\nan email is sent as notification; If 4 (out of 5) instances are failing, a SMS message is sent as notification; If a failed instance appears to be restored, Nagios will re-add it to the DNS round robin mechanism. This approach has some single points of failures: The Nagios instance can fail The master DNS where the DNS entries are updated can fail Example 2: The IBERGRID scripting based mechanism In IBERGRID, an application (developed by LIP) verifies the health of each top BDII. The application can connect to the DNS servers and remove the “A” records of top BDIIs that become unavailable (non responsive to tests).\nThe monitoring application (nsupdater) is a simple program that performs tests, and based on their result acts upon DNS entries\nWritten in perl Can be run as daemon or at the command prompt The tests are programs that are forked Tests are added in a “module” fashion way Can be used to manage several DNS round robin scenarios Can manage multiple DNS servers To remove the DNS single point of failure as in previous example, one could configure all DNS servers serving the core.ibergrid.eu domain as primary Three primary servers would then exist for core.ibergrid.eu All three DNS servers could be dynamically updated independently The monitoring application should also have three instances, one running at each site The downside is that DNS information can become incoherent. It would be up to the monitoring application to manage the three DNS servers content and their coherence ","categories":"","description":"Deploying the BDII service in High Availability","excerpt":"Deploying the BDII service in High Availability","ref":"/providers/operations-manuals/man05_top_and_site_bdii_high_availability/","tags":"","title":"MAN05 top and site BDII High Availability"},{"body":"Downtimes To properly manage downtimes it is important to highlight the differences between the possible types of downtimes. Downtimes can’t be added retroactively, and the date is always defined in UTC. If the error messages received when adding a downtime is cryptic, usually it’s due to a parsing error on the time/date.\nDowntimes classification scheduled: e.g. for software/hardware upgrades, planned and agreed in advance. This needs to be announced at least 24h in advance. unscheduled: e.g. power outages, unplanned, usually triggered by an unexpected failure. Downtimes severities WARNING: Resource will probably be working as normal, but may experience problems. OUTAGE: Resource will be completely unavailable. WARNING, formerly known as AT_RISK, implies a type of severity that does not have operational consequences. It is only an information for users that some small temporary failures can appear. All failures during that time will be taken into account in the reliability calculations. Examples include:\nAdmins not present on site (conference, vacations). Reduced redundancy in network, power or cooling. Failed disk in RAID sets. OUTAGE implies that the site/node is completely unavailable and no tickets should be created. This does not affect site metrics.\nMore information about downtimes can be found in the Configuration Database user documentation.\nSites in downtime When a ticket has been raised against a site that subsequently enters in a downtime, the expiry date on the ticket can be extended.\nWhen a ticket is opened against a site that continues to add or increase the downtime the ticket must be closed, and the NGI requested to take action either by suspending or un-certifying the site until such time that the problem is resolved. This usually happens when a middleware upgrade is due or a bug in the middleware is causing a site to fail. Sites then may choose to wait for the next middleware release rather than spend effort trying to resolve the issue locally.\nSites that are in downtime will still have monitoring switched on and therefore may appear to be failing tests. ROD must take care that when opening tickets to ensure that they don’t open tickets against sites in downtime.\nIf a site is in downtime for more than a month, then it is advised that the site should go to the uncertified state.\nNodes in downtime When a node of a site is in downtime, alarms are generated but the Operations Portal distinguishes these alarms, and marks the downtime accordingly in the dashboard.\nROD should not open tickets against nodes that are in downtime.\nInstructions for accounting monitoring failures The Accounting monitoring tests are not run against the site but query the central accounting repository.\nIf there is more than one failure for a given site, create a ticket for one of the alarms and mask all others by this one. Edit the description of the ticket to state clearly that even though the failure is reported for a given CE, this is not a CE failure but a failure on the Accounting service for the whole site. Proceed with all sites in the same way. Please beware: Accounting tests are not helped by scheduling downtime, the site admins need to get Accounting publishing working again. Nodes not in production When a node of a production site is declared as non-production in the Configuration Database or the node appears in BDII but is not declared in the Configuration Database, then the ROD should do the following:\nRecommend to the sites to take these nodes out of their site BDII If this is not a possibility then the site should set those nodes in downtime in the EGI Configuration Database If the node is a test node and is in BDII but not in the Configuration Database, then the sites should register it and turn monitoring off. ","categories":"","description":"Description on how to deal with scheduled and unscheduled interventions and downtimes.","excerpt":"Description on how to deal with scheduled and unscheduled …","ref":"/providers/rod/downtimes/","tags":"","title":"Managing downtimes"},{"body":"What is it? The EGI Messaging Service is powered by ARGO Messaging Service (AMS), a real-time messaging service that allows the user to send and receive messages between independent applications.\nIt’s a Publish/Subscribe Service implementing the Google PubSub protocol and providing an HTTP API that enables Users/Systems to implement message oriented service using the Publish/Subscribe Model over plain HTTP.\nThis central service is used by other EGI central services in order to exchange messages, like for sending information about accounting or resources available at a cloud site. More specifically, the services that use the Messaging service are:\nAAI Federation Registry: uses the service to exchange information among the different components (examples: SimpleSamlPHP, MITREid, Keycloak). Operations Portal: reads the alarms from predefined topics, stores them in a database and displays them in the operations portal. Accounting: uses the service as a transport layer for collecting accounting data from the sites. The accounting information is gathered from different collectors into a central accounting repository where it is processed to generate statistical summaries that are available through the EGI Accounting Portal. FedCloud: used the service as a transport layer for the cloud information system. It makes use of the ams-authN. The entry point for users, topics and subscriptions is the Configuration Database. ARGO Availability and Reliability Monitoring Service: It uses the service to send the messages from the monitoring engine to other components. Features Ease of use: it supports an HTTP API and a python library so as to easily integrate with the service. Push Delivery: the service instantly pushes asynchronous event notifications when messages are published to the message topic. Subscribers are notified when a message is available. Replay messages: replay messages that have been acknowledged by seeking a timestamp. Schema Support: on demand mechanism that enables a) the definition of the expected payload schema, b) the definition of the expected set of attributes and values and c) the validation for each message if the requirements are met and immediately notify the client. Replicate messages on multiple topics: Republisher script that consumes and publishes messages for specific topics (e.g. sites). It supports both push and pull message delivery. In push delivery, the Messaging Service initiates requests to the subscriber application to deliver messages. In pull delivery, the subscription application initiates requests to the server to retrieve messages.\nApart from the main service a number of valuable components are also supported. These components are extensively used by the connected services.\nArgo-ams-library: a simple library written in python to interact with the ARGO Messaging Service. Argo-AuthN: Argo-authN is a new Authentication Service. This service provides the ability to different services to use alternative authentication mechanisms without having to store additional user info or implement new functionalities. The authentication service holds various information about a service’s users, hosts, API URLs, etc, and leverages them to provide its functionality. AMS Metrics: Metrics about the service Architecture Instead of focusing on a single Messaging service specification for handling the logic of publishing/subscribing to the broker network, the service focuses on creating nodes of Publishers and Subscribers as a Service. In the Publish/Subscribe paradigm, Publishers are users/systems that can send messages to named-channels called Topics. Subscribers are users/systems that create Subscriptions to specific topics and receive messages.\nAs shown in Figure below, the current deployment of messaging service comprises a haproxy server, which acts as a load balancer for the 3 AMS servers running in the backend.\nUsing the Messaging Service Note Documentation for the ARGO Messaging Service is available on the ARGO documentation site. ","categories":"","description":"Messaging service supporting other central services","excerpt":"Messaging service supporting other central services","ref":"/internal/messaging/","tags":"","title":"Messaging service"},{"body":" How is this documentation organized This documentation is specific for the EGI users and use cases. Each section briefly describes the concepts in Perun and available options. How-To guides for related common actions are available at the end of each section. Overview Perun is attribute management system, and within the EGI it provides an alternative solution for VO management. Perun provides more advanced features and supports complex use-cases compared to COManage and VOMS.\nYou should consider using Perun for VO management if you want or require the following features:\nCustomizable registration forms, flows, and notifications for both VOs and Groups Advanced group management with group hierarchies and relations VO/Group management roles delegation Membership expiration/renewals rules Membership sponsorships as exceptions to membership rules Fine-grained access management for service owners User/group synchronizations with external systems and services VO management Administrative GUI for VO managers is available at https://perun.egi.eu\nIt is assumed that VO managers and members have already registered their EGI Check-in account (A step-by-step guide is provided in this link).\nRegistering your VO Standard EGI procedure for registering VO takes place. See related Check-In documentation about Registering your VO.\nSteps that are specific to Perun are:\nYou will provide us with the necessary information through GGUS ticket (as described in the procedure). You will register to your own VO using provided link, and we will set you as the VO manager. You will register to your own VO using provided link, and we will set you as the VO manager. Any further VO management and configuration is the responsibility of VO manager.\nManaging members All basic features regarding members management are available in the administrative GUI under the VO manager section. All features are described in the following sections in more detail.\nAccepting new members Users can register in your VO using the same registration link provided during the VO registration. It has the following format:\nhttps://perun.egi.eu/egi/registrar/?vo=[voName] where [voName] is to be replaced with your actual VO name (as displayed on VO ID card in OperationsPortal).\nAlternatively, you can configure an invitation notification template and send the registration link to the users directly from Perun.\nOnce the user submits the registration, it is up to you to review and approve or reject it. Perun can be configured to automatically approve valid registrations or reject them in case of user inactivity (for example when email is not verified by the user in timely manner).\nThere is a possibility to use custom registration modules that are programmable and can perform any specific and complex actions with the users and the VO. Please consult Perun user support using GGUS in case you need this feature.\nMembership lifecycle, expiration rules and renewal Almost identical features are available for each Group within the VO. See the same section under the Groups management for the differences. By default, Perun expects that the membership lifecycle is managed by the VO manager directly - manually. This can be done by switching each members’ status between VALID, EXPIRED and DISABLED states or by removing the member from VO.\nMembers’ status affects which VO/group membership information is released to the service, hence affecting users’ ability to access the service or the state of users’ account in the service.\nMember status Meaning VALID User is active member of VO. VO/group membership information is provisioned to services (e.g. through Check-In proxy). EXPIRED User is inactive member of VO, but allowed to renew membership. VO/group membership information is not provisioned to services through Check-In proxy but can be provisioned to services directly connected to Perun. DISABLED User is inactive member of VO and can’t renew membership by himself. VO/group membership information is not provisioned outside of Perun. Switching between the states can be automated based on membership expiration rules of VO or simply by setting membership expiration date manually to the member.\nNotifications can be set to notify members about upcoming membership expiration. They can then ask for membership renewal using the same registration link. The Modified version of the form is displayed to them in such a case.\nMembership expiration rules can be set either to a fixed date within the year (in such case member is expected to renew membership every year before that date) or be dynamically calculated based on the date of registration. In the second case renewal period can be shorter or longer than one year (e. g. +2y, +6m, meaning 2 years or 6 months respectively). In both cases, you also need to define the timespan, how long before the membership expiration are members allowed to ask for the renewal (e.g. 1m, meaning one month).\nManaging groups Groups are used to further categorize VO members within the VO and to assign them roles or access rights for the services. They can also be used to delegate members’ management to others as you can assign a group manager, which is then responsible for that particular group and its child groups.\nAll VOs have one system group named members, which automatically holds all VO members and can be used in groups relations.\nGroups can be organized in a flat or hierarchical structure, or you can combine them as you need. Groups’ names must be unique on each level of hierarchy, and group full name consist of all groups names on the path from the root divided by colon :, e.g. projectX:students.\nTop-level groups can be created only by the VO managers.\nHOW-TO Search and display groups Create/delete groups and subgroups Create/remove relations between the groups Rename group Group memberships Perun recognizes two kinds of group memberships, direct and indirect. If you use only flat group structure, all users are direct members of the groups. Once you start using hierarchical group structures or group relations, you will also get indirect members in some groups.\nBoth types of memberships are equal regarding the VO/group membership information released for the services (e.g. Check-In proxy).\nMembership in the hierarchical group structures is inherited from the most bottom groups to the root, and the user can be both a direct and indirect member of the group at the same time.\nIndirect members can’t be removed from the group and are displayed in the GUI using the italic font in the group members list. In order to remove them from the group you have to remove them from the sourcing subgroup first or remove relation between the groups.\nHOW-TO Search and display group members Add/remove group members Accepting new group members Group members can be added to the group directly by the VO/group manager chosen from the available VO members as described in the above How-To.\nThey can also register themselves using similar link like for VO registrations: https://perun.egi.eu/egi/registrar/?vo=[voName]\u0026group= [groupName] where [voName] is to be replaced with your actual VO name (as displayed on VO ID card in OperationsPortal) and [groupName] is to be replaced with the groups full name (including hierarchy), e.g.: projectX:students.\nAll settings related to the group registration are basically the same as for the VO registrations, they are just located under the Group manager section in the GUI.\nThere is just one big difference regarding the registration workflow for the groups. The user must become a member of VO before the group registration can be accepted by the group manager (even automatically). The user is able to submit VO and group registration at once using a two steps form, but VO registration must be approved first.\nGroup membership lifecycle, expiration rules and renewal Members’ lifecycle within the group is roughly the same as within the VO, but there are some differences.\nBoth VO and Group member status affect what vo/group membership information is provided to the services about the user. Group membership statuses are only VALID and EXPIRED. In hierarchical group structures, resulting in members’ status in a parent group is dependent on his/hers status in all child groups and VALID status takes preference. Membership expiration in VO and group are independent of each other. Member can be expired just in some groups within the VO or be expired within the whole VO a still be valid within its groups. In such a case member is not considered as a member of VO by the services. Once membership is renewed, membership information is restored, including all groups. Access management While Perun generally uses the concept of Resources to represent the access rights and roles (as a service provider is giving them to the VO) and Groups are assigned to it (by the VO managers), it is not usually used like this within the EGI infrastructure.\nHaving Groups and Resources separated allows you to represent your community or organization structure without the conflict with the roles and access rights for each service. You can then more easily use one single group and give its members access to the various services with specific roles/settings. Any change to the group membership can then be reflected to all associated services, which is useful especially for the de-provisioning.\nEGI Check-In proxy For the primary and most basic use-case, when services are behind the EGI Check-In proxy, access rights are based solely on VO and group membership information.\nSuch information is then released by the EGI Check-In proxy as eduPersonEntitlement attribute conforming AARC-G002 specification.\nBoth VOs’ short name and groups’ full name are used to construct the entitlement value, so using correct naming for the group is required.\nAssociation of specific VO/group membership value with the role or access rights within the service are the responsibility of the service provider.\nExample Let’s assume, that the user is a member of myvo.egi.eu VO and the top-level group vm_operator in it. Resulting eduPersonEntitlement will look like this: urn:mace:egi.eu:group:myvo.egi.eu:vm_operator:role=member#aai.egi.eu\nPerun doesn’t support other roles within the groups than member.\nVOMS Note Please note, that because of the nature of the VOMS, users must have personal certificate associated with their user entry in Perun in order to be provisioned to the VOMS at all. Perun supports direct provisioning of the VOMS. In this use-case, users and VOs are primarily managed within the Perun. The user data, including group memberships and roles, is then provisioned to the VOMS, which is used solely as a backend for the services requiring it.\nFrom the VO manager’s perspective, you are given a Resource by the service provider (VOMS operator). It represents your association with the VOMS in Perun. You can have multiple resources and they can represent either a different set of VOMS roles within the same VOMS or represent different VOMS servers.\nYou can assign any of your groups to it, and all users from all the assigned groups will be provisioned to the VOMS as members of your VO.\nOn the Service settings page of the Resource you can set VOMS roles, which will be given to all users associated with it through all the groups. Similarly, you can set VOMS roles on the Group-Resource level. Once you assign the group to the Resource, you can check Group settings page of the Resource, select the proper Group and set VOMS roles from there. The same group can have different roles for different VOMS servers like this. Also, you can associate group from Perun with the group in VOMS by settings Voms group name on the same settings page.\nFAQ I need all VO members to get “vm_operator” role in OpenStack You must create a top-level group within the VO and name it vm_operator. All members of this group will get this role, and Check-In proxy will release this information as an entitlement for the OpenStack like this:\nurn:mace:egi.eu:group:[VO_NAME]:vm_operator:role=member#aai.egi.eu\nYou can make sure all VO members are in this group by creating a relation between this group and the system group members (basically you include members as a subgroup through this relation).\nAccess with personal certificates Due to historical reasons (wide adoption of VOMS), users might have their user entry in Perun already directly associated with their personal certificate. For such cases, direct authentication using the certificates is still available. Please consult Perun support through GGUS if you wish to merge your accounts and start using EGI Check-In as identity linking needs to be done on Perun side and not through EGI Check-In proxy.\n","categories":"","description":"VO management through the EGI Attribute Management service Perun\n","excerpt":"VO management through the EGI Attribute Management service Perun\n","ref":"/users/aai/check-in/vos/perun/","tags":"","title":"Perun"},{"body":"Information to provide when registering a new service (the fields marked with an asterisk are mandatory):\nHosting Site * (select the appropriate RC under which you are registering the service) Service Type * (select the appropriate service type) Service URL (Alphanumeric and $-_.+!*’(),:) Hostname * (valid FQDN format) Host IP a.b.c.d Host IPv6 (0000:0000:0000:0000:0000:0000:0000:0000[/int]) (optional [/int] range) Host DN (/C=…/OU=…/…) Description * (Alphanumeric and basic punctuation) Host Operating System (Alphanumeric and basic punctuation) Host Architecture (Alphanumeric and basic punctuation) Is it a beta service (formerly PPS service)? Y/N Is this service in production? Y/N Is this service monitored? Y/N Contact email * (valid email format) Scope Tags ✓ Optional Tags (At least 1 optional tags must be selected): EGI Local FedCloud ✓ Reserved Tags Inheritable from Parent Site: none ✓ Reserved Tags Directly Assigned (WARNING - If deselected you will not be able to reselect the tag - it will be moved to the ‘Protected Reserved Tags’ list): none ✗ Protected Reserved Tags (Can only be assigned on request): alice atlas cms elixir lhcb tier1 tier2 wlcg ","categories":"","description":"Information required for registering a service into the Configuration Database","excerpt":"Information required for registering a service into the Configuration …","ref":"/internal/configuration-database/service-registration-requirements/","tags":"","title":"Service registration requirements"},{"body":"Introduction GGUS is the Helpdesk service of the EGI Infrastructure. Incident and Service request tickets can be recorded, and their progress is tracked until the solution. The users of the service should not need to know any of the details of what happens to the ticket in order to get it from creation to solution. However, an understanding of the operation of the system may be helpful to explaining what happens when you request help.\nTickets can be created through the GGUS web interface, whose access is described in the page Access and roles\nOnce the ticket has been created in GGUS, it is processed by supporters and assigned to the appropriate group to deal with the issue. The groups are generally addressed via mailing lists, so when a ticket is assigned to a support group, an email message is sent to people included to the associated list.\nSubmitting a ticket using the web interface To submit a ticket through GGUS you first need to login through EGI Check-in, as explained in the page Access and roles.\nAfter login, you are prompted to the dashboard page:\nIn the lower left corner, you can find a “+” button:\nClick on it and select “New ticket”: you will be prompted to the ticket submit form.\nThe ticket submit form on web interface The tickets submit form offers a set of fields which should help you to describe the issue you want to record as detailed as possible. Most of the fields on the submit form are optional.\n“TITLE” is a mandatory field. It should give a short description of the issue. “CUSTOMER”: name and email address of the user creating the ticket “TEXT”: here you can add a detailed description of the issue. “CATEGORY” (mandatory) provides a drop-down list with possible values, like Incidents and Service Request. Choose the appropriate value according to the issue you are going to record. “GROUP”: provides a drop-down list of all the support groups enabled in GGUS, which are grouped by category: click on the green arrow besides the name of each category to see eventual sub-categories and finally the support groups. If you already know the name of the support group, you can start to type it in the field to display it and then to select it. If you don’t know whom to address your ticket, you can leave this field unselected, and the ticket will be automatically assigned to TPM, the EGI First Level support who will process your ticket and assign it to the most appropriate support group. “SITE” provides a drop-down list with all the EGI sites registered in GOCDB and OSG sites registered in OIM DB. When selecting a site from the list, the appropriate NGI/ROC is set automatically, and they will be both notified about this ticket by email once created. “TICKET AREA” provides a drop-down list with possible values. This field is for categorizing the issue. It defaults to “Other”. Check “Issue type values” page . “PRIORITY” (mandatory) provides a drop-down list with possible priority values. They are “less urgent” (which is the default), “urgent”, and “very urgent”. See the page Ticket priority “AFFECTED VO” provides a drop-down list of all VOs supported by GGUS. “TIME OF ISSUE” defaults to the submitting time. “NOTIFIED GROUPS”: with this field you can select other support groups that might be interested in following the progress of your ticket. “NOTIFIED USERS”: you can involve other people in the ticket by adding their email address into this field. “NOTIFICATION MODE” defaults to “on every change”. The Notification mode manages the update notifications the user receives. If “On solution” is selected, then you only get notified when the ticket status is set to “solved”. After clicking the “Create” button you get a confirmation page showing the information submitted and the ticket ID.\nTicket categories The ticket category is for differentiating between incidents and service requests. This distinction is helpful for supporters as well as for the GGUS reporting, e.g. for excluding test tickets. Other categories were added over the time. Currently the following values are available for selection:\nIncident (see the FitSM definition). Service request (see the FitSM definition). See the list of service requests for the internal and for the external services of the EGI portfolio. Documentation (used to request creation and update on documentation). Release (used when a new version of the EGI Core Services is ready to be tested; see the Change, Release, and Deployment Management procedures) CMS Internal (for cms VO specific tickets) EGI Coordination/Planning (to track activities not falling into the service request definition) WLCG Coordination/Planning (to track activities not falling into the service request definition) Test Special tickets Users with special roles can submit special kind of tickets, such as:\nMultisite tickets ALARM tickets TEAM tickets Tickets overview After the login, you land to the dashboard page as explained above. If you click on “Overviews” in the menu on the left you will have an overview of the tickets created in the helpdesk. Several views are available: in each view you can further filter the list of tickets by GROUP, STATUS, and PRIORITY; besides, depending on the roles you own, in the displayed lists you will see only the tickets you are allowed to access.\n“My Tickets”: here you have the list of tickets that you created. “My Subscribed Tickets”: the list of tickets you subscribed to. “My Assigned Tickets Open”: the list of tickets specifically assigned to you in any of the open statuses. “My Assigned Tickets All”: the list of tickets specifically assigned to you in any status. “Tickets Open”: all the tickets in any open status “Tickets All”: all the tickets, also the ones that were closed. “Alarm Tickets”: the list of Alarm tickets. “Site Tickets Open”: the list of tickets in any of the open statuses where a site is involved. “EGI Services Tickets Open”: the list of tickets in any of the open statuses assigned to the GROUPS within the ticket category “EGI Services and Service Components” “VO Support Tickets Open”: the list of tickets in any of the open statuses assigned to the GROUPS within the ticket category “VO Support” “Second Level Tickets Open”: the list of tickets in any of the open statuses assigned to the GROUPS within the ticket categories “LifeScience Services”, “Networking”, “Other Middleware”, “PITHIA Community”, and “WLCG”. “Other Infrastructures Tickets Open”: the list of tickets in any of the open statuses assigned to the OSG related groups. “Other Projects Tickets Open”: the list of tickets in any of the open statuses assigned to the GROUPS representing the several middleware products (3rd level support). Modifying tickets The easiest way to modify a ticket is to add a response by replying to the email notification you get when a ticket is modified by someone else: to do this in the correct way, please pay attention on not to change the subject line of the email message you received.\nThe other way is to use the web interface, which allows you to modify also other aspects of a ticket. When you display a ticket in your web browser, if you scroll down at the bottom of the page you can find a text box to add your reply to the ticket.\nYou also have the opportunity to attach files along with your text reply, in a similar way as explained in the previous section. When you have completed your reply, click on the “Update” button in the bottom right corner of the screen to add your response to the ticket.\nIf you have a supporter role and you want to share your reply only with the other supporters following up the ticket, you can click on the lock icon on the left side of the text box to keep your comment as “internal”: in this way, the ticket submitter will not receive any notification related to your reply.\nIn addition to the reply to the ticket, you can also modify the other fields associated to it (if you the required permissions associated to the role owned in the system). Besides the fields that were mentioned in the previous sections, you can now see also:\n“Owner”: (Only if you have a at least supporter role) The given support group can appoint one of their members to directly follow-up the ticket. “TAGS”: you can tag your ticket with a word (useful for example if you want to associate tickets per topic without directly linking them, but that topic is not included in the “TICKET AREA” field). “LINKS”: you can link your ticket to another one, using in case the PARENT/CHILD relation. “RELATED ANSWERS”: you can link a page from the Knowledge page if related to the topic discussed in the ticket. Subscribe button: if you have the rights to see a ticket and you are interested in follow its progress, you can subscribe to it: at any update of the ticket you will get an email notification. Ticket state Setting the right ticket state is important to say the users if for example the ticket has been acknowledged by someone or if some feedback from the submitter is required to progress with the processing of a given ticket. Here is a description of the different state values and how they should be used.\nassigned: this status is set automatically and can’t be selected in the drop-down list menu. After a ticket is assigned to a support group, this group is notified via email about the ticket assignment. in progress: support staff who work on the ticket should change status to “in progress” when they acknowledge it. This is necessary to announce that someone is taking care of the ticket and is working on it. waiting for submitter’s reply: this status value should be set ONLY by the supporters and ONLY when asking the SUBMITTER for further information. on hold: this status should be used in those situations where a solution cannot be provided in short times but is planned for a certain point in the future because depending for example on the release of a software fix or on other related activities that require more time to be completed. solved: this state is used to close a ticket when a solution is provided. Adding a full explanation of the solution is recommended and will be helpful when the given ticket is used as a reference and to create documentation from it. unsolved: This status is for tickets that can not be solved due to any reason (ticket duplicated, topic not valid any longer, won’t fix, etc.). Please add a comment in the text field explaining why it can’t be solved. Ticket history If you click on the word “Ticket” displayed in top-right corner of the page, you can access the ticket history. It shows all relevant changes of the ticket in chronological order. Changes of these fields lead to a new entry in ticket history:\nCategory Group Owner Ticket Area State Priority Affected VO Time of Issue Notified Groups Notified Users Notification Mode TAGS LINKS Related Answers Notifications (subscriptions) Ticket Participation As described in the previous sections, GGUS system offers various possibilities for involving additional people in the tickets. In summary, there are three ways:\ninvolve other support groups in the progress of a ticket (“Notified groups” field). Involve individual people in the progress of a ticket by specifically adding their email address in the “NOTIFIED USERS” field. Subscribe yourself to a ticket. ","categories":"","description":"Guide for users","excerpt":"Guide for users","ref":"/internal/helpdesk/user-guide/","tags":"","title":"User guide"},{"body":"In this page you can find a summary of the needed steps for supporting a new VO in your OpenStack infrastructure.\nLocal project creation The usual method of supporting a VO is by creating a local project for it. You should assign quotas to this project as agreed in the OLA defining the support for the given VO.\nCreate a group where users belonging to the VO will be mapped to:\ngroup_id=$(openstack group create -f value -c id \u003cnew_group\u003e) Add that group to the desired local project:\n$ openstack role add member --group $group_id --project \u003cyour project\u003e Set the egi.VO property to the name of the VO that you are supporting:\n$ openstack project set --property egi.VO=\u003cname of the VO\u003e \u003cyour project\u003e Keystone Mapping Expand your mapping.json with the VO membership to the created group (substitute group_id and entitlement as appropriate). The expected mappings for the VOs are listed in vo-mappings.yaml of fedcloud-catchall-operations repository:\n[ \u003cexisting mappings\u003e, { \"local\": [ { \"user\": { \"name\": \"{0}\", \"email\": \"{1}\" }, \"group\": { \"id\": \"\u003cgroup_id\u003e\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_EMAIL\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai.egi.eu/auth/realms/egi\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^\u003centitlement\u003e$\" ] } ] } ] And update the mapping in your Keystone IdP:\n$ openstack mapping set --rules mapping.json egi-mapping You can include as many mappings as needed in the json file. Users will be members of all the matching groups.\nAccounting Add the project supporting the VO to cASO:\nIn the projects field of /etc/caso/caso.conf :\nprojects = vo_project1, vo_project2, \u003cyour_new_vo_project\u003e and as a new mapping in /etc/caso/voms.json :\n{ \"\u003cyour new vo\u003e\": { \"projects\": [\"\u003cyour new vo project\u003e\"] } } Be sure to include the user running cASO at least as reader of the project if it does not have admin privileges:\nopenstack role add --user \u003cyour caso user\u003e --project \u003cyour new vo project\u003e reader Information system / VM Image management If you are correctly setting the egi.VO property to your projects, the configuration will be automatically retrieved by the catch-all components.\n","categories":"","description":"Summary of steps for configuring new VOs in OpenStack\n","excerpt":"Summary of steps for configuring new VOs in OpenStack\n","ref":"/providers/cloud-compute/openstack/vo-config/","tags":"","title":"VO Configuration guide"},{"body":"What is it? EGI Workload Manager is a service provided to the EGI community to efficiently manage and distribute computing workloads on the EGI infrastructure.\nWorkload Manager is configured to support a number of HTC and cloud resource pools from the EGI Federation. This pool of computing resources can be easily extended and customized to support the needs of new scientific communities.\nNote Workload Manager is based on DIRAC technology. The delivery of the service is coordinated by the EGI Foundation and IN2P3 provides the resources and operates the service. Main Features The EGI Workload Manager:\nMaximizes usage efficiency by choosing appropriately computing and storage resources on real-time.\nProvides a large-scale distributed environment to manage and handle data storage, data movement, accessing and processing.\nHandles job submission and workload distribution in a transparent way.\nImproves the general job throughput compared with native management of EGI Grid or Cloud computing resources.\nOffers pilot-based task scheduling method, that submits pilot jobs to resources to check the execution environment before to start the user’s jobs. From a technical standpoint, the user’s job description is delivered to the pilot, which prepares its execution environment and executes the user application. The pilot-based scheduling feature solves many problems of using heterogeneity and unstable distributed computing resources.\nIncludes easy extensions to customize the environment checks to address the needs of a particular community. Users can choose appropriately computing and storage resources maximising their usage efficiency for particular user requirements.\nHandles different storage supporting both cloud and grid capacity.\nProvides a user-friendly interface that allows users to choose among different DIRAC services.\nTarget User Groups The service suits for the established Virtual Organization communities, long tail of users, SMEs and Industry\nEGI and EGI Federation participants Research communities Architecture The EGI Workload Manager service is a cluster of DIRAC services running on EGI resources (HTC, Cloud, HPC) supporting multi-VO. All the DIRAC services are at or above TRL8. The main service components include:\nWorkload Management System (WMS) architecture is composed of multiple loosely coupled components working together in a collaborative manner with the help of a common Configuration Services ensuring reliable service discovery functionality. The modular architecture allows to easily incorporate new types of computing resources as well as new task scheduling algorithms in response to evolving user requirements. DIRAC services can run on multiple geographically distributed servers which increases the overall reliability and excellent scalability properties.\nREST server providing language neutral interface to DIRAC service.\nWeb portal provides simple and intuitive access to most of the DIRAC functionalities including management of computing tasks and distributed data. It also has a modular architecture designed specifically to allow easy extension for the needs of particular applications.\nThe DIRAC Web portal\nHow to access the EGI Workload Manager service There are several options to access the service:\nMembers of a scientific community whose resources pool is already configured in the EGI Workload Manager instance can use the EGI Workload Manager web portal to access the service, or use DIRAC Client. Individual researchers who want to do some number crunching for a limited period of time, with a reasonable (not too high) number of CPUs can use the catch-all VO resource pool (vo.access.egi.eu). Contact us. Representatives of a community who want to try DIRAC and EGI \u003e Same as #2. Representative of a community who wants to request DIRAC for the community’s own resource pool \u003e Same as #2. Getting Started Before starting Apply for your user credentials DIRAC uses X.509 certificates to identify and authenticate users. These certificates are delivered to each individual by trusted certification authorities. If you have a personal certificate issued by a EUGridPMA-certified authority you can use it for this tutorial. Otherwise refer to the information available in this section, to obtain a certificate. Your certificate may take a few days to be delivered, so please ask for your certificate well in advance and in any case, before the tutorial starts.\nInstall your credentials Your personal certificate is usually delivered to you via a site and is automatically loaded in your browser. You need to export it from the browser and put it in the appropriate format for DIRAC to use. This is a one-time operation. Please follow the instructions in detailed in VOMS documentation page to export and in install your certificate.\nSend your certificate’s subject to the DIRAC team In order to configure the DIRAC server so that you gets registered as a user, the team needs to know your certificate’s subject.\nPlease use the command below on any Unix machine and send its output to\ndirac-support \u003cAT\u003e mailman.egi.eu\n$ openssl x509 -in $HOME/.globus/usercert.pem -subject -noout The EGI Workload Manager Web Portal To access the EGI Workload Manager open a web browser to: https://dirac.egi.eu/DIRAC/\nThe EGI Workload Manager service Web portal\nIf you are a new user, you can see the welcome page where you can find links to user documentations.\nVO options: you can switch to different VOs that you have membership.\nLog In options: the service supports both X.509, Certificate and Check-in log-in.\nView options: allow to choose either desktop or tabs layout.\nMenu: a list of tools that enable the selected VO.\nUpload Proxy Before submitting your job, you need to upload your Proxy. Login to the portal. Go to:\nMenu \u003e Tools \u003e Proxy Upload, enter your certificates .p12 file and the passphrase, click Upload.\nThe wizard to upload the .p12 proxy certificate\nJob Submission Go to:\nMenu \u003e Tools \u003e Job Launchpad. First check the Proxy Status, click it until it shows Valid in green color.\nIn the Job Launchpad, you can select your jobs from the list; add parameters, indicating the output Sandbox location.\nNow, select Helloworld from the job list, and click Submit, you just launch your very first job to the EGI HTC cluster.\nSubmit a job with the Job Launchpad\nMonitor Job status Go to:\nMenu \u003e Applications \u003e Job Monitor.\nThe left panel gives all kinds of search options for your jobs. Set your search criteria, and click Submit, the jobs will list on the right panel.\nTry the various options to view different information about the jobs.\nMonitor the job execution with the Job Monitor panel\nGet Results from Sandbox Once the job has been successfully processed, the Status of the job will change to green. Right click the job, select:\nSandbox \u003e Get Output file(s), you can get the result file(s).\nFull User Guide for DIRAC Web Portal For further instructions, please refer to DIRAC Web Portal Guide\nThe DIRAC client tool The easiest way to install the client is via Docker Container. If you have a Docker client installed in your machine, install the DIRAC CLI as follows:\n$ docker run -it -v $HOME:$HOME -e HOME=$HOME diracgrid/client:egi Once the client software is installed, it should be configured in order to access the EGI Workload Manager service:\n$ source /opt/dirac/bashrc To proceed further a temporary proxy of the user certificate should be created. This is necessary to get information from the central Configuration Service:\n$ dirac-proxy-init -x Generating proxy... Enter Certificate password: ... Now the client can be configured to work in conjunction with the EGI Workload Manager service:\n$ dirac-configure defaults-egi.cfg Executing: /home/jdoe/DIRAC/DIRAC/Core/scripts/dirac-configure.py defaults-egi.cfg Checking DIRAC installation at \"/home/jdoe/DIRAC\" Created vomsdir file /home/jdoe/DIRAC/etc/grid-security/vomsdir/vo.formation.idgrilles.fr/cclcgvomsli01.in2p3.fr.lsc [..] Created vomsdir file /home/jdoe/DIRAC/etc/grid-security/vomsdir/fedcloud.egi.eu/voms2.grid.cesnet.cz.lsc Created vomses file `/home/jdoe/DIRAC/etc/grid-security/vomses/fedcloud.egi.eu` Generate the proxy containing the credentials of your VO. Specify the VO in the --group option:\nIn this example, we are going to use the resources allocated for the WeNMR project.\n$ dirac-proxy-init --debug --group wenmr_user -U --rfc $ dirac-proxy-init --debug --group wenmr_user -U --rfc Generating proxy... Enter Certificate password: Contacting CS... Checking DN /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu Username is jdoe Creating proxy for jdoe@wenmr_user (/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu) Requested adding a VOMS extension but no VOMS attribute defined for group wenmr_user Uploading proxy for wenmr_user... Uploading wenmr_user proxy to ProxyManager... Loading user proxy Uploading proxy on-the-fly Cert file /home/jdoe/.globus/usercert.pem Key file /home/jdoe/.globus/userkey.pem Loading cert and key User credentials loaded Uploading... Proxy uploaded Proxy generated: subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu/CN=0123456789 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu timeleft : 23:59:58 DIRAC group : wenmr_user rfc : True path : /tmp/x509up_u0 username : jdoe properties : LimitedDelegation, GenericPilot, Pilot, NormalUser Proxies uploaded: DN | Group | Until (GMT) /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | access.egi.eu_user | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | fedcloud_user | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | access.egi.eu_admin | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | wenmr_user | 2021/09/14 23:54 As a result of this command a user proxy with the same validity period of the certificate is uploaded to the DIRAC ProxyManager service.\nFor checking the details of you proxy, run the following command:\n$ dirac-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu/CN=0123456789 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu timeleft : 23:59:26 DIRAC group : wenmr_user rfc : True path : /tmp/x509up_u0 username : jdoe properties : LimitedDelegation, GenericPilot, Pilot, NormalUser Access the client via CVMFS The DIRAC client may be accessed via CVMFS with the following two command lines:\n$ source /cvmfs/dirac.egi.eu/dirac/bashrc_egi_dev $ dirac-login --issuer=https://dirac.egi.eu/auth Managing simple jobs DIRAC commands Note dirac-wms-job-status To check the status of a job dirac-wms-job-delete To delete a job dirac-wms-job-logging-info To retrieve history of transitions for a DIRAC job dirac-wms-job-get-output To retrieve the job output dirac-wms-job-submit To submit a job DIRAC commands\nHave a look at the official command reference documentation for the complete list of the Workload Management commands.\nIn general, you can submit jobs, check their status, and retrieve the output. For example:\nCreate a simple JDL file (test.jdl) to submit the job:\n[ JobName = \"Simple_Job\"; Executable = \"/bin/ls\"; Arguments = \"-ltr\"; StdOutput = \"StdOut\"; StdError = \"StdErr\"; OutputSandbox = {\"StdOut\",\"StdErr\"}; ] Submit the job:\n$ dirac-wms-job-submit test.jdl JobID = 53755998 Check the job status:\n$ dirac-wms-job-status 53755998 JobID=23844073 Status=Waiting; MinorStatus=Pilot Agent Submission; Site=ANY; $ dirac-wms-job-status 53755998 JobID=53755998 Status=Done; MinorStatus=Execution Completed; Site=EGI.NIKHEF.nl; Site=EGI.HG-08-Okeanos.gr; Retrieve the outputs of the job (when the status is Done):\n$ dirac-wms-job-get-output --Dir joboutput/ 53755998 Job output sandbox retrieved in joboutput/53755998/ Jobs with Input Sandbox and Output Sandbox In most cases the job input data or executable files are available locally and should be transferred to the grid to run the job. In this case the InputSandbox attribute can be used to move the files together with the job.\nCreate the InputAndOuputSandbox.jdl\nJobName = \"InputAndOuputSandbox\"; Executable = \"testJob.sh\"; StdOutput = \"StdOut\"; StdError = \"StdErr\"; InputSandbox = {\"testJob.sh\"}; OutputSandbox = {\"StdOut\",\"StdErr\"}; Create a simple shell script (testJob.sh)\n#!/bin/bash /bin/hostname /bin/date /bin/ls -la After creation of JDL file the next step is to submit the job, using the command:\n$ dirac-wms-job-submit InputAndOuputSandbox.jdl JobID = XXXXXXXX List of supported VOs acc-comp.egi.eu, beapps, compchem, eiscat.se, eli-laser.eu, eli-beams.eu, eng.vo.ibergrid.eu, enmr.eu, fedcloud.egi.eu, hungrid, km3net.org, lofar, opencoast.eosc-hub.eu, see, training.egi.eu, virgo, vlemed, vo.formation.idgrilles.fr, vo.plgrid.pl, vo.access.egi.eu, auger, biomed, bitp, eng_cloud and breakseq_cloud More details JDL language and simple jobs submission: JDLs and Job Management Basic Submitting Parametric jobs, using DIRAC API: Advanced Job Management Past tutorials Technical Support DIRAC User Guide: https://dirac.readthedocs.io/en/latest/UserGuide/\nFor technical issues and bug reports, please submit a ticket in GGUS, in Assign to support unit, indicate:\nEGI Services and Service Components \u003e Workload Manager (DIRAC).\n","categories":"","description":"Distribute and manage workloads in the EGI infrastructure\n","excerpt":"Distribute and manage workloads in the EGI infrastructure\n","ref":"/users/compute/orchestration/workload-manager/","tags":"","title":"EGI Workload Manager"},{"body":"GPU resources on EGI Cloud GPUs resources are available on selected providers of the EGI Cloud. These are available as specific flavours that when used to instantiate a Virtual Machine will make the hardware available to the user.\nThe table below summarises the available options:\nSite VM configuration options Flavors Supported VOs with GPUs Access conditions More information IISAS-FedCloud up to 7 NVIDIA A100 40GB, up to 8 NVIDIA Tesla K20m g1.c08r30-K20m, g1.c16r60-2xK20m, A100 GPUs accessible using private flavors vo.access.egi.eu, training.egi.eu, eosc-synergy.eu, vo.ai4eosc.eu, icecube, vo.beamide.com Sponsored, conditions to be negotiated IFCA-LCG2 up to 80 NVIDIA T4, up to 20 NVIDIA V100 Pay-per-use IFCA-LCG2 Documentation CESNET-MCC up to 2 NVIDIA Tesla T4, up to 4 NVIDIA A40, up to 2 GeForce RTX 1080/2080 (experimental use only) hpc.8core-64ram-nvidia-1080-glados, hpc.19core-176ram-nvidia-1080-glados, hpc.19core-176ram-nvidia-2080-glados, hpc.32core-256ram-nvidia-t4-single-gpu, hpc.64core-238ram-nvidia-t4, hpc.64core-512ram-nvidia-t4, meta-hdm.16core-120ram-nvidia-a40, meta-hdm.64core-485ram-nvidia-a40-quad vo.clarin.eu, biomed, eosc-synergy.eu, peachnote.com, cryoem.instruct-eric.eu, fusion, icecube, vo.carouseldancing.org, vo.pangeo.eu, vo.neanias.eu, umsa.cerit-sc.cz, vip, vo.notebooks.egi.eu, vo.emphasisproject.eu, vo.inactive-sarscov2.eu Sponsored, conditions to be negotiated CESNET-MCC Documentation IN2P3-IRES up to 1 NVIDIA Tesla T4 per VM, up to 1 NVIDIA A40 per VM, up to 4 GeForce RTX 2080 per VM (experimental use only) g1.xlarge-4xmem, g2.xlarge-4xmem, g4.xlarge-4xmem biomed, vo.emphasisproject.eu, vo.france-grilles.fr Sponsored, conditions to be negotiated Access to GPU resources on EGI Cloud Check whether you belong to one of the supported Virtual Organisations (VOs). If you are not sure what VO to join, request access to the pilot VO vo.access.egi.eu by visiting the enrollment URL with your Check-In account. More information is available in the Check-in section.\nGPUs sites can be accessed in different ways: via site-specific dashboards and endpoints or via common federated-cloud services like the OpenStack Horizon dashboards or a cloud orchestrator.\nIt is also possible to use the FedCloud Client for command-line access. Below is an example on how to use the fedcloud command to show the GPU properties of the available GPU flavors on all sites for the specific VO in the command:\nfedcloud openstack flavor list --long --site ALL_SITES --vo vo.access.egi.eu --json-output | \\ jq -r 'map(select(.\"Error code\" == 0)) | map(.Result = (.Result| map(select(.Properties.\"Accelerator:Type\" == \"GPU\")))) | map(select(.Result | length \u003e 0))' Site-specific dashboards and endpoints are described in the following table:\nSite Openstack Horizon dashboard Keystone endpoint IISAS-FedCloud https://cloud.ui.savba.sk https://cloud.ui.savba.sk:5000/v3/ IFCA-LCG2 https://portal.cloud.ifca.es https://api.cloud.ifca.es:5000/ CESNET-MCC https://dashboard.cloud.muni.cz https://identity.cloud.muni.cz/ A VM image with pre-installed NVIDIA driver and Docker is available at AppDB. Some VOs (vo.access.egi.eu, eosc-synergy.eu) have the image included in the VO image list.\n","categories":"","description":"GPU resources in the EGI Cloud\n","excerpt":"GPU resources in the EGI Cloud\n","ref":"/users/compute/cloud-compute/gpgpu/","tags":"","title":"GPUs"},{"body":"Overview One of the challenges for researchers in recent years is to manage an ever-increasing amount of compute and storage services, which then form ever more complex end user applications or platforms.\nTo address this need, EGI provides several orchestrators that facilitate the management of your workload using resources on the federation. These tools have different levels of abstractions and features. See below to understand which to choose (or gets used automatically) in specific scenarios:\nService Name Workload Type Use-Case Infrastructure Manager VMs, containers, storage Used to run workloads on a single IaaS Cloud provider. Workload Manager Jobs Used to efficiently distribute, manage, and monitor computing workloads. The following sections offer more details about each of these orchestrators.\n","categories":"","description":"Compute resource orchestration in the EGI infrastructure\n","excerpt":"Compute resource orchestration in the EGI infrastructure\n","ref":"/users/compute/orchestration/","tags":"","title":"Compute Orchestration"},{"body":"Here you can find documentation about the EGI DataHub for service providers\n","categories":"","description":"Documentation for EGI DataHub Service Providers","excerpt":"Documentation for EGI DataHub Service Providers","ref":"/providers/datahub/","tags":"","title":"DataHub"},{"body":"The Dynamic DNS service provides a unified, federation-wide Dynamic DNS support for VMs in EGI infrastructure. Users can register their chosen meaningful and memorable DNS hostnames in given domains (e.g. my-server.vo.fedcloud.eu) and assign to public IPs of their servers.\nBy using Dynamic DNS, users can host services in EGI Cloud with their meaningful service names, can freely move VMs from sites to sites without modifying server/client configurations (federated approach), can request valid server certificates in advance (critical for security) and many other advantages.\nA short demonstration video is available at fedcloud.eu YouTube channel.\nDynamic DNS GUI portal The Dynamic DNS offers a web GUI portal where users can login using their Check-in credentials. For doing so, click on the Login link (top left) and then click on the egi button.\nOnce logged in, you will be presented with the following page:\nTo register a new DNS host name:\nClick on Overview and then on Add Host.\nType in the hostname you’d like to register and select the domain to use.\nThe portal will then show you a secret than can be used for updating the host ip whenever needed. Note it down so you can use it later.\nFrom the VM you’d like to assign the name to, run a command like follows:\ncurl \"https://\u003chostname\u003e:\u003csecret\u003e@nsupdate.fedcloud.eu/nic/update\" where \u003chostname\u003e is the full hostname generated before, e.g. myserver.fedcloud-tf.fedcloud.eu and \u003csecret\u003e is the secret generated in the previous step. You can add that as a boot command in your cloud-init configuration:\n#cloud-config runcmd: - [ curl, \"https://\u003chostname\u003e:\u003csecret\u003e@nsupdate.fedcloud.eu/nic/update\" ] You can also manually edit your registered hostnames in the Overview menu by clicking on the hostname you’d like to manage\nNote Hostnames/IP addresses are not expired so no need to refresh IP addresses if no changes, it is enough to run once. You can add the following command curl https://HOSTNAME:SECRET@nsupdate.fedcloud.eu/nic/update to cloud-init as described above to assign hostname automatically at VM start.\nDNS server set Time-to-Live (max time for caching DNS records) to 1 min for dynamic DNS, but MS Windows seems to not respect that. You can clear DNS cache in Windows with ipconfig /flushdns command with Administrator account.\nAPI Dynamic DNS update server uses dyndns2 protocol, compatible with commercial providers like dyn.com, and noip.com. The API is specified as follows:\nGET /nic/update?hostname=yourhostname\u0026myip=ipaddress Host: nsupdate.fedcloud.eu Authorization: Basic base64-encoded-auth-string User-Agent: where:\nbase64-encoded-auth-string: base64 encoding of username:password username: your hostname password: your host secret hostname in the parameter string can be omitted or must be the same as username myip in the parameter string if omitted, the IP address of the client performing the GET request will be used Security For updating IP address, only hostname and its secret are needed. No user information is stored on VM in any form for updating IP.\nNS-update server uses HTTPS protocol, hostname/secret are encrypted as data and not visible during transfer so it is secure to use the update URL\nNS-update portal does not store host secret in recoverable form. If you forget the secret of your hostname, simply generate new one via “Show configuration” button in the host edit page. The old secret will be invalid.\nSupport Support for Dynamic DNS service is provided via EGI Helpdesk in “Dynamic DNS” support unit, where users can ask questions, report issues or make requests for additional domains for specific projects or user communities.\n","categories":"","description":"Dynamic domain names for VMs in the EGI Cloud\n","excerpt":"Dynamic domain names for VMs in the EGI Cloud\n","ref":"/users/compute/cloud-compute/dynamic-dns/","tags":"","title":"Dynamic DNS"},{"body":"Alarms Alarms are automatically generated notifications created by the Service Monitoring and are handled from within the Operations Portal dashboard.\nHandling alarms When an alarm is generated, the site administrators have 24 hours to start acting on the issue. If ROD spots an alarm, he can notify the site’s administrators about the problem.\nIf the problem is fixed within 24 hours and the solution is tested by the Service Monitoring (the alarm’s color turns green in the Dashboard), then ROD has to make sure that the results are not flapping and can close alarm without any other action. If the problem cannot be fixed within 24 hours, and the site administrators put the service into an unscheduled downtime, ROD should just wait until the problem is fixed or until the downtime is over. No other action is necessary. If the problem cannot be fixed within 24 hours and the administrators don’t put the service into a downtime, then a ticket must be issued. The procedure is described below in the Tickets section. If the service is in downtime and the problem is fixed (as verified by the Service Monitoring), then ROD can close the alarm. If the downtime is over and the problem is still present (e.g. if the administrators forgot to extend the downtime), then a ticket must be issued. If an alarm is raised for a service that has its monitoring status set to OFF in EGI Configuration Database (also visible in the Dashboard in the Nodes box or in the alarm row as Node status) then ROD should not open a ticket. The alarm can be cleared even if it is marked red by pressing the lightning icon and giving an explanation. For handling tickets during public holidays, see below. There is also a video tutorial on handling alarms available.\nTickets In contrast with alarms, which are mere notifications, tickets are created manually. They are used to report problems to the responsible support units. Additionally, they allow to track the actions taken in order to resolve the issue.\nCreating tickets Ticket creation occurs when the age of an alarm in an error state has passed 24 hours, whether or not a site has already made some action on the alarm. A ticket has to be created from the Operations Portal. In order to actually create a ticket, click on the double arrow in the upper left corner next to the NGI name. That opens the drop down box with information on the site. Then, open the New NAGIOS alarms drop down box. Click the “T+” icon to create the ticket.\nRefer to the Dashboard How-to if you need a more detailed guide.\nIf more than one alarm should be handled by the same ticket, proceed as follows:\nCreate a ticket for one of the alarms. Open the Assigned Alarms drop-down box. Click on the mask icon next to the alarm identifier. A window will open in which you can select the alarms to be masked. If an alarm, which is masked by another alarm, remains in “critical” condition because of another (unrelated) problem, you can unmask it by clicking on the mask icon again and close the ticket for the solved alarms.\nFill in the relevant information in the ticket section. If there was information in the site notepad, ensure that the ticket information reflects that information. Also ensure that the TO: select boxes, and FROM: and SUBJECT: fields are all correct. Generally, a ticket should go to all of site, NGI and ROD. Press the Submit button and a pop up window will appear confirming that the ticket was correctly submitted. Your ticket has now been assigned a Helpdesk ID, but also an internal (hidden) Dashboard ID, which means that if you create a ticket through Dashboard, you have to close it through Dashboard as well. If you close a ticket opened through Dashboard in the Helpdesk, it will remain open in Dashboard! Creating tickets without an alarm It is also possible to create a ticket for a site without an alarm. This can happen if there is an issue with one of the tools that does not create an alarm in Dashboard. In this case, click on the “T+” icon in the upper right corner of the site box - the one with “Create a ticket (without an alarm)” tooltip, and fill in the appropriate fields as when creating a ticket for an alarm.\nTicket content templates The email is addressed to the corresponding NGI, together with the site and ROD. To view the list of NGI email addresses, click the Regional List link in the Dashboard menu.\nGenerally, you should not remove any content from the template, but you are free to add any information you think the site might find helpful in any of the three editing fields (Header content, Main content, and Footer content).\nChanging the state of and closing a ticket When the state of an alarm for a site with an open ticket changes to OK, then the ticket associated with that alarm can be updated in the Dashboard. Do this by clicking Update for the ticket in the Tickets drop down. Now change Escalate to Problem solved and fill in any information about how the problem has been solved. Clicking Update will then close the ticket in both the Helpdesk and the Dashboard. If the Nagios alarm is in an unstable state, and the site has not responded to the problem in 3 days then a 2nd email can be sent to the site by updating the Escalate field to 2nd step. If a new failure is detected for the site, the existing ticket should not be modified (though the deadline can be extended) but a new ticket should be submitted for this new problem. If the site’s problem can not be fixed in 3 days from the 2nd step of the escalation procedure then escalate the ticket to Political procedure. This means that the NGI manager will contact both EGI Operations and the site to negotiate about suspending the site. Sites with multiple tickets open When opening a ticket against a site with existing tickets ROD should consider that these problems may be linked or dependant on pending solutions. If the problem is different but maybe linked the expiry dates for each ticket should be synchronized to the latest date.\nAlso consider masking new problems with an old ticket.\nHandling alarms and tickets during weekends and public holidays Due to the fact that weekends are not considered working days, it is noted that ROD teams do not have any responsibilities during weekends and that RODs should ensure that tickets do not expire during weekends. The alarm age does not increase during the weekend.\nCurrently there is no automatic mechanism for handling ticket expiration over public holiday periods, because they differ among countries. If some of the sites the ROD team is in charge of are located in another country, the ROD is encouraged to get them to announce their public holidays, so that ticket expiration can be set accordingly. (Correspondingly, ROD operators also have no duties when they are on public holidays.) The ROD can edit the ticket’s expiration day by clicking the “T+” (Edit Ticket) icon. The value is set to 3 days by default.\nPlease note that ROD is not requested to announce their national holidays to the EGI Operations team. However, the last day before a public holiday, ROD is requested to check\nif there are any tickets that are to be expired during the holiday and change their expiration date; if there are any alarms that will pass the 72 hour period during the holidays and handle them properly in advance. Workflow and escalation procedure The workflow and escalation procedures are documented in more detail at PROC01 Infrastructure Oversight escalation.\n","categories":"","description":"How the ROD should deal with alarms and tickets using the dashboard.","excerpt":"How the ROD should deal with alarms and tickets using the dashboard.","ref":"/providers/rod/alarms-tickets/","tags":"","title":"Handling alarms and tickets"},{"body":"Once the site services are registered in GOCDB (and flagged as \"monitored\") they will appear in the EGI service monitoring tools. EGI will check the status of the services. Check if your services are present in the EGI service monitoring tools and passing the tests; if you experience any issues (services not shown, services are not OK...) please contact back EGI Operations or your reference Resource Infrastructure.\nExtra checks for your installation:\nCheck in ARGO-Mon2 that your services are listed and are passing the tests. If all the tests are OK, your installation is already in good shape.\nCheck that you are publishing cloud information in your site BDII: :\nldapsearch -x -h \u003csite bdii host\u003e -p 2170 -b Glue2GroupID=cloud,Glue2DomainID=\u003cyour site name\u003e,o=glue Check that all the images listed in the AppDB for the VOs you support (e.g. AppDB page for fedlcoud.egi.eu VO) are listed in your BDII. This sample query will return all the template IDs registered in your BDII: :\nldapsearch -x -h \u003csite bdii host\u003e -p 2170 -b Glue2GroupID=cloud,Glue2DomainID=\u003cyour site name\u003e,o=glue objectClass=GLUE2ApplicationEnvironment GLUE2ApplicationEnvironmentRepository Try to start one of those images in your cloud. You can do it with [onetemplate instantiate]{.title-ref} or OCCI commands, the result should be the same.\nExecute the site certification manual tests against your endpoints.\nCheck in the accounting portal that your site is listed and the values reported look consistent with the usage of your site.\n","categories":"","description":"Validate your installation\n","excerpt":"Validate your installation\n","ref":"/providers/cloud-compute/validation/","tags":"","title":"Installation Validation"},{"body":"Document control Property Value Title MAN06 Failover for MySQL grid-based services Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement Implementing failover of MySQL grid-based services. Owner SDIS team Introduction Several critical grid services such as the VO Management Service (VOMS) server represent single points of failure in a grid infrastructure. When unavailable, a user can no longer access to the infrastructure since it is prevented from issuing new proxies, or is no longer able to access to the physical location of his data.\nHowever, those services rely on MySQL backends which opens a window to replicate the relevant databases to different / backup services which could be used when the primary instances are unavailable. The MySQL DB replication process is one of the ways to get scalability and higher availability.\nArchitecture In this document we propose to follow a Master-Slave architecture for the MySQL replication, consisting in keeping DB copies of a main host (MASTER) in a secondary host (SLAVE). The slave host will only have read access to the Database entries.\nSecurity For better availability, it is preferable to deploy the Master and the Slave services in different geographical locations, which normally means exposing the generated traffic to the internet. In that case, you will have to find a mechanism to encrypt the communication between the two hosts.\nIn this document, we propose to use Stunnel:\nStunnel is a free multi-platform computer program, used to provide universal TLS/SSL tunneling service. Stunnel can be used to provide secure encrypted connections for clients or servers that do not speak TLS or SSL natively. It runs on a variety of operating systems, including most Unix-like operating systems and Windows. Stunnel relies on a separate library such as OpenSSL or SSLeay to implement the underlying TLS or SSL protocol. Stunnel uses Public-key cryptography with X.509 digital certificates to secure the SSL connection. Clients can optionally be authenticated via a certificate too For more references, please check www.stunnel.org There are other possibilities for encryption, like enabling SSL Support directly in MySQL, but these approach was not tested. Details can be obtained here. MySQL replication Assumptions We are assuming that both grid services instances were previously installed and configured (manually or via YAIM) so that they support the same VOs.\nGeneric information Description Value MASTER hostname server1.domain.one MASTER username root MASTER mysql superuser root MASTER mysql replication user db_rep SLAVE hostname server2.domain.two SLAVE username root SLAVE mysql superuser root DB to replicate DB_1 DB_2 … Setup the MySQL MASTER for replication Step 1: Install stunnel. It is available in the SL5 repositories. $ yum install stunnel (...) $ date; rpm -qa | grep stunnel Wed Jun 15 16:00:23 WEST 2011 stunnel-4.15-2.el5.1 Step 2: Configure stunnel (via /etc/stunnel/stunnel.conf) to: Accept incoming connections on port 3307, and allow to connect to port 3306 Use the server X509 certificates to encrypt data $ cat /etc/stunnel/stunnel.conf # Authentication stuff verify = 2 CApath = /etc/grid-security/certificates cert = /etc/grid-security/hostcert.pem key = /etc/grid-security/hostkey.pem # Auth fails in chroot because CApath is not accessible #chroot = /var/run/stunnel #debug = 7 output = /var/log/stunnel.log pid = /var/run/stunnel/master.pid setuid = nobody setgid = nobody # Service-level configuration [mysql] accept = 3307 connect = 127.0.0.1:3306 Step 3: Start the stunnel service and add it to rc.local $ mkdir /var/run/stunnel $ chown nobody:nobody /var/run/stunnel $ stunnel /etc/stunnel.conf $ echo 'stunnel /etc/stunnel.conf' \u003e\u003e /etc/rc.local Step 4: Setup the firewall to allow connections from the SLAVE instance (server2.domain.two with IP XXX.XXX.XXX.XXX) on port 3307 TCP # MySQL replication -A INPUT -s XXX.XXX.XXX.XXX -m state --state NEW -m tcp -p tcp \\ --dport 3307 -j ACCEPT Step 5: Configure MySQL (via /etc/my.cnf) setting up the Master server-id (usually 1), and declaring the path for the MySQL binary log and the DBs to be replicated. Please make sure that the path declared under log-bin has the mysql:mysql ownerships, and that the binary log exists. $ cat /etc/my.cnf (...) [mysqld] server-id=1 log-bin = /path_to_log_file/log_file.index binlog-do-db=DB_2 binlog-do-db=DB_1 Step 6: Restart MySQL $ /etc/init.d/mysqld restart Step 7: Add a specific user for the MySQL replication mysql \u003e GRANT REPLICATION SLAVE ON *.* TO 'db_rep'@'127.0.0.1' \\ IDENTIFIED BY '16_char_password'; Step 8: Backup the databases using the following command. Please note tha the option --master-data=2 writes a comment on dump.sql that shows the log file and the ID to be used on the Slave setup. This option also locks all the tables while they are being copied, avoiding problems. $ mysqldump -u root -p --default-character-set=latin1 \\ --master-data=2 --databases DB_1 DB_2 \u003e dump.sql Setup the MySQL SLAVE for replication Step 1: Configure stunnel (via /etc/stunnel/stunnel.conf) to:\nAccept incoming SSL connections on port 3307, and allow to connect to server1.domain.one on port 3307 Use the server X509 certificates to encrypt data $ cat /etc/stunnel/stunnel.conf # Authentication stuff verify = 2 CApath = /etc/grid-security/certificates cert = /etc/grid-security/hostcert.pem key = /etc/grid-security/hostkey.pem # Auth fails in chroot because CApath is not accessible #chroot = /var/run/stunnel #debug = 7 output = /var/log/stunnel.log pid = /var/run/stunnel/master.pid setuid = nobody setgid = nobody # Use it for client mode client = yes # Service-level configuration [mysql] accept = 127.0.0.1:3307 connect = server1.domain.one:3307 Step 2: Start stunnel and add it to rc.local $ mkdir /var/run/stunnel $ chown nobody:nobody /var/run/stunnel $ stunnel /etc/stunnel.conf $ echo 'stunnel /etc/stunnel.conf' \u003e\u003e /etc/rc.local Step 3: Configure MySQL to include the SLAVE server-id (typically 2) and the replicated databases $ cat /etc/my.cnf (...) [mysqld] server-id=2 replicate-do-db=DB_1 replicate-do-db=DB_2 Step 4: Restart MySQL and insert the dump.sql created on the Master $ /etc/init.d/mysql restart $ mysql -u root -p \u003c dump.sql Step 5: Start the Slave logging in the mysql of slave, and running the following queries, changing the values xxxxx and yyyyy by the values on top of dump.sql file: mysql \u003e CHANGE MASTER TO MASTER_HOST='127.0.0.1', MASTER_PORT=3307, \\ MASTER_USER='db_rep', MASTER_PASSWORD='16_char_password', \\ MASTER_LOG_FILE='xxxxx', MASTER_LOG_POS=yyyyy; mysql \u003e SLAVE START; Step 6: Your replication should be up and running. In case of troubles, check the Troubleshooting section. Troubleshooting On the Slave Check if you can connect to the Master on port 3307 from the Slave #root@server2.domain.two]$ telnet server1.domain.one 3307 Check the /var/log/stunnel.log on the Slave to see if stunnel is working. For established connections, you should find a message like: 2011.05.30 11:57:45 LOG5[9000:1076156736]: mysql connected from XXX.XXX.XXX.XXX:PORTY 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=1, /DC=COUNTRY/DC=CA/CN=NAME 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=0, /DC=COUNTRY/DC=CA/O=INSTITUTE/CN=host/HOSTNAME Check the MySQL process list. You should get an answer like the one bellow: mysql\u003e SHOW PROCESSLIST; +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ | Id | User | Host | db | Command | Time | State | Info | +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ | 1 | system user | | NULL | Connect | 1236539 | Waiting for master to send event | NULL | | 2 | system user | | NULL | Connect | 90804 | Slave has read all relay log; waiting for the slave I/O thread to update it | NULL | +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ Check the status of the slave mysql\u003e SHOW SLAVE STATUS; +----------------------------------+-------------+------------------+---------------------+ | Slave_IO_State | Master_Port | Master_Log_File | Read_Master_Log_Pos | +----------------------------------+-------------+------------------+---------------------+ | Waiting for master to send event | 3307 | mysql-bin.000001 | 126167682 | +----------------------------------+-------------+------------------+---------------------+ On the Master Check the /var/log/stunnel.log on the Slave to see if stunnel is working. For established connections, you should find a message like: 2011.05.30 11:57:45 LOG5[9000:1076156736]: mysql connected from XXX.XXX.XXX.XXX:PORTY 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=1, /DC=COUNTRY/DC=CA/CN=NAME 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=0, /DC=COUNTRY/DC=CA/O=INSTITUTE/CN=host/HOSTNAME Check if you have established connections from the Slave on port 3307 $ netstat -tapn | grep 3307 tcp 0 0 0.0.0.0:3307 0.0.0.0:* LISTEN 9000/stunnel tcp 0 0 XXX.XXX.XXX.XXX:3307 YYY.YYY.YYY.YYY:34378 ESTABLISHED 9000/stunnel Check the Master status on MySQL. You should get an answer like: mysql\u003e SHOW MASTER STATUS; +------------------+-----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+-----------+--------------+------------------+ | mysql-bin.000001 | 126167682 | DB_1 | | +------------------+-----------+--------------+------------------+ 1 row in set (0.00 sec) Check the MySQL process list. You should get an answer like the one bellow: mysql\u003e SHOW PROCESSLIST; +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ | Id | User | Host | db | Command | Time | State | Info | +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ | 2778 | janedoe | localhost:42281 | NULL | Binlog Dump | 1400477 | Has sent all binlog to slave; waiting for binlog to be updated | NULL | +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ The VOMS case A working example It is also possible to deploy a backup VOMS server with a MySQL replica of the main VOMS server. This will enable users to still start proxies even if the main VOMS server is down.\nThe VOMS Admin interface of the backup VOMS server should be switched off so that new users can only request registration via the main VOMS Admin Web interface.\nUser interfaces should be configured to use both VOMS servers\n$ voms-proxy-init --voms ict.vo.ibergrid.eu Enter GRID pass phrase: Your identity: /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=Goncalo Borges Creating temporary proxy .............................................................. Done Contacting voms01.ncg.ingrid.pt:40008 [/C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=voms01.ncg.ingrid.pt] \"ict.vo.ibergrid.eu\" Done Creating proxy .......................................................................................................................................................... Done Your proxy is valid until Thu Jun 16 07:27:31 2011 $ voms-proxy-init --voms ict.vo.ibergrid.eu Enter GRID pass phrase: Your identity: /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=Goncalo Borges Creating temporary proxy ............................................................. Done Contacting ibergrid-voms.ifca.es:40008 [/DC=es/DC=irisgrid/O=ifca/CN=host/ibergrid-voms.ifca.es] \"ict.vo.ibergrid.eu\" Done Creating proxy ............................................................................ Done Your proxy is valid until Thu Jun 16 07:27:37 2011 ","categories":"","description":"Implementing failover of MySQL grid-based services.","excerpt":"Implementing failover of MySQL grid-based services.","ref":"/providers/operations-manuals/man06_failover_for_mysql_grid_based_services/","tags":"","title":"MAN06 Failover for MySQL grid-based services"},{"body":"Document control Property Value Title MAN07 VOMS Replication Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement How to implement a MySQL VOMS server replication Owner SDIS team Introduction In this manual we will show you how to implement a MySQL VOMS server replication: you need one master server, on which you can perform writing operations, and you can have from 1 to “n” replica servers that will work in read-only mode. In such a scenario you can do a whatever intervention on one of the servers without breaking the service, i.e. proxies creation and grid-mapfile downloads: just the users registration and the usual VOs management operations might be forbidden during an intervention on the master server (because it is the only server in writing mode).\nThis failover procedure is simply based on MySQL replication therefore every MySQL setting is referred to the current MySQL version (5.0.77 in this moment)\nSettings on the MASTER SERVER In order to allow the replica server to read the master database, you have to create an user with which the slave will connect to the master. Suppose the replica hostname is vomsrep.cnaf.infn.it, the user is janedoe and the password is always. What you have to launch on the master server is:\n$ mysql -p -e \"grant super, reload, replication slave, replication client \\ on *.* to janedoe@'vomsrep.cnaf.infn.it' identified by 'always'\" ; Then for each DB (VO) you want to replicate, you have to assign the right permissions, by launching:\nmysql -p -e \"grant select, lock tables on voms_myvo.* to \\ janedoe@'vomsrep.cnaf.infn.it'\" Eventually you have to modify the file /etc/my.cnf by adding the following lines into the section [mysqld]:\nlog-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 It is important that on the master server it is set server-id=1: it is the identification number that distinguish a master from its several slaves (each slave will have a unique number starting from 2)\nFor example, the content of my.cnf file may appear like this:\n# less /etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock user=mysql # Default to using old password format for compatibility with mysql 3.x # clients (those using the mysqlclient10 compatibility package). old_passwords=1 max_connections = 800 log-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 # Disabling symbolic-links is recommended to prevent assorted security risks; # to do so, uncomment this line: # symbolic-links=0 [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid At this point, you have to restart MySQL, by launching:\n$ service mysqld restart In order to check that on the master side the mechanism is working, you can launch for example:\nmysql\u003e show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 | 24844 | | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) Eventually, through the web interface, in the ACL section of each VO you want to replicate, add an entry granting all the permissions to the slave server:\nselect “a non VO member” from the menu fill in the replica server DN and a reference email address select “all” for the permissions and tick the “Propagate entry to children contexts” option In this way, when the slave server copies the DB, it will have the proper permissions on acting on the DB. Moreover, in order to avoid the sending of notification to the email address you filled in before, connect to the MySQL database and do the following:\nmysql\u003e use voms_myvo; mysql\u003e update admins set email_address=NULL where \\ email_address=\"what you filled before\"; Settings on the SLAVE SERVER Install a VOMS server as usual, configuring the VOs you want to replicate: keep in mind that every modification done on the slave DB breaks the replica mechanism, so that on this server disable the users registration, by setting the yaim variable:\nVOMS_ADMIN_WEB_REGISTRATION_DISABLE=true Then ask the VO managers to not perform any action on the slave server web interface.\nThen launch the following scripts:\nfirst_replica.sh for the first database you want to replicate or in the case it is the only one next_replicas.sh for the next databases (one database for each launch) For both the scripts, set the following variables:\nmaster_host, master_mysql_user, master_mysql_pwd that refers to the master server and to the user created on it mysql_username_admin and mysql_password_admin that refers to the slave Example:\nvoms_database=\"\" # VOMS database (leave unset) master_host=\"voms.cnaf.infn.it\" # Master hostname master_mysql_user=\"janedoe\" # Master MySQL admin user for replication master_mysql_pwd=\"always\" # Master MySQL admin pass for replication user master_log_file=\"\" # Master LOG file (leave unset) master_log_pos=\"\" # Master LOG file (leave unset) mysql_username_admin=\"root\" # Slave MySQL admin username mysql_password_admin=\"secret\" # Slave MySQL admin pass With the launch of first-replica.sh, the file /etc/my.cnf will be properly written; if you need to replicate further databases, modify /etc/my.cnf adding the following lines related to the db you are replicating (similar to the first db you’ve replicated):\nreplicate-do-db=\u003cmaster_vo_db_name\u003e replicate-ignore-table=\u003cmaster_vo_db_name\u003e.seqnumber replicate-ignore-table=\u003cmaster_vo_db_name\u003e.realtime replicate-ignore-table=\u003cmaster_vo_db_name\u003e.transactions replicate-ignore-table=\u003cslave_vo_db_name\u003e.seqnumber replicate-ignore-table=\u003cslave_vo_db_name\u003e.realtime replicate-ignore-table=\u003cslave_vo_db_name\u003e.transactions Having set the variables in the way shown above, for replicating the first database the scripts launch syntax is the following:\n$ ./first_replica.sh --master-db=voms_myvo --db=voms_myvo In your /etc/my.cnf file you will find lines like the following:\n# Connection with master server-id=2 master-host=voms.cnaf.infn.it master-user=janedoe master-password=always # Replicas settings replicate-do-db=voms_myvo replicate-ignore-table=voms_myvo.seqnumber replicate-ignore-table=voms_myvo.realtime replicate-ignore-table=voms_myvo.transactions replicate-ignore-table=voms_myvo.seqnumber replicate-ignore-table=voms_myvo.realtime replicate-ignore-table=voms_myvo.transactions Now you may want to replicate a second database, let’s say voms_hervo: therefore in my.cnf file add the following lines:\nreplicate-do-db=voms_hervo replicate-ignore-table=voms_hervo.seqnumber replicate-ignore-table=voms_hervo.realtime replicate-ignore-table=voms_hervo.transactions replicate-ignore-table=voms_hervo.seqnumber replicate-ignore-table=voms_hervo.realtime replicate-ignore-table=voms_hervo.transactions Modify the script next_replicas.sh in according to the VO parameters and launch it:\n$ ./next_replicas.sh --master-db=voms_hervo --db=voms_hervo When you finished to replicate all the desired VOs, in order to make active the database modifications, restart voms and voms-admin:\n$ /etc/init.d/voms-admin stop $ /etc/init.d/voms stop $ /etc/init.d/voms start $ /etc/init.d/voms-admin start Keep in mind that every modification done on the slave DB breaks the replica mechanism, so that on this server disable the users registration, by setting the yaim variable:\nVOMS_ADMIN_WEB_REGISTRATION_DISABLE=true And ask the VO managers to not perform any action on the slave server web interface.\n","categories":"","description":"How to implement a MySQL VOMS server replication","excerpt":"How to implement a MySQL VOMS server replication","ref":"/providers/operations-manuals/man07_voms_replication/","tags":"","title":"MAN07 VOMS Replication"},{"body":"What is it? The EGI Operations Portal is a central portal for supporting the operations and coordination of the EGI Infrastructure. It offers a bundle of different capabilities, such as:\nThe Broadcast tool: used to contact and inform the different actors of the project for specific problems or global announcements. The operations and security dashboards: to detect, track and follow-up problems and incident on the resource centers. The VO Management tools: to register, update, consult information about virtual communities. The Metrics module: to provide metrics and indicators about the different activities of the project linked to VOs, users. The SLA module: to provide information about performances for the cloud sites and SLA Violations for different group of services. Note Documentation for the Operations Portal is available in the IN2P3 Wiki. ","categories":"","description":"Central portal supporting EGI infrastructure operations","excerpt":"Central portal supporting EGI infrastructure operations","ref":"/internal/operations-portal/","tags":"","title":"Operations Portal"},{"body":"What is it? Your applications and services may need different secrets (credentials, tokens, passwords, etc.) during deployment and operation. These secrets are often stored as clear texts in code repositories or configuration files, which poses security risks. Furthermore, the secrets stored in files are static and difficult to change (rotate).\nThe EGI Secrets Store helps you to securely store, retrieve, and rotate credentials for your services.\nThe main features of EGI Secrets Store:\nSimplifies, standardizes, and secures the way users store secrets. Offers secure storage of secrets, encrypted both in transit and at rest. Allows easy rotation of secrets, while administrators can track usage and manage access. Integrates with EGI Check-in to allow access with home organisation credentials, no additional registration needed. High-availability ensures the service is always accessible to users, applications and other services that rely of it. Easy to maintain, upgrade, and extend, as it is based on well-known open-source software, with many client tools and libraries with strong community support. Concepts Secret objects (or secrets, for short) in EGI Secrets Store are identified by their paths, like files on disk. Each user has a private secret space for storing their secret objects, and cannot see secrets of other users. Each secret object may contain several secret values, and each value is identified by its key (name).\nExample A secret object for your service may contain several different passwords for different components: database, message queue, cache, storage system, etc. Each of these passwords will be a distinct secret value in your secret object. Secret objects are always created, retrieved, updated, and deleted as a whole, users cannot manipulate individual secret values of an existing secret.\n","categories":"","description":"Managing secrets in the EGI infrastructure\n","excerpt":"Managing secrets in the EGI infrastructure\n","ref":"/users/security/secrets-store/","tags":"","title":"EGI Secrets Store"},{"body":"What is it? Security Coordination improves the capabilities of local security activities for a safer federated infrastructure environment.\nThe EGI Computer Security Incident Response Team (EGI CSIRT) has the tools and the knowledge to run Security Coordination on behalf of the federation. The EGI CSIRT is a certified Trusted Introducer since 2015.\nSecurity Coordination is especially important in a federated environment, where incidents are often not isolated and can affect several service providers. A coordinated response is essential to minimize the impact of incidents and vulnerabilities.\nThis service provides:\nSecurity Operations Coordination - Central coordination of the security activities ensures that policies, operational security, and maintenance are compatible amongst all partners, improving availability and lowering access barriers for use of the infrastructure. Security Policy Coordination - The Security Policy Group (SPG) develops policies covering diverse aspects, including operational policies (agreements on vulnerability management, intrusion detection and prevention, regulation of access, and enforcement), incident response policies (governing the exchange of information and expected actions), participant responsibilities (including acceptable use policies, identifying users and managing user communities), traceability, legal aspects, and the protection of personal data. Software Vulnerability Group Coordination - The Software Vulnerability Group SVG aims to eliminate existing software vulnerabilities from the deployed infrastructure and prevent the introduction of new ones, and runs a process for handling software vulnerabilities reported. International Grid Trust Federation (IGTF) and EUGridPMA - Representation of EGI in IGTF and EUGridPMA. A common authentication trust domain is required to persistently identify all EGI participants. EGI Computer Security Incident Response Team (EGI CSIRT) expertise Security Incident Response Coordination - Coordination of incident response activities in collaboration with the Incident Response Task Force (EGI-CSIRT IRTF). Security monitoring - Monitoring services to check for security vulnerabilities and other security-related problems in the EGI production infrastructure. Tools for Security Service Challenge support - Security challenges are a mechanism to check the compliance of sites/NGIs/EGI with security requirements. Runs of Security Service Challenges need a set of tools that are used during various stages of the runs. ","categories":"","description":"Enhance local security for a safer global infrastructure\n","excerpt":"Enhance local security for a safer global infrastructure\n","ref":"/internal/security-coordination/","tags":"","title":"Security Coordination"},{"body":"In this page we gather a list of useful resources for OpenStack administrators.\nOpenStack IDLE VM Detector (osidle) Our partners from GRyCAP UPV have created a tool to analyze the usage of Virtual Machines running in OpenStack. For more information please visit: https://github.com/grycap/osidle/.\nShare yours Would you like to share your favourite tool to make OpenStack management easier? Then contact us!\n","categories":"","description":"Collection of useful resources for OpenStack administrators\n","excerpt":"Collection of useful resources for OpenStack administrators\n","ref":"/providers/cloud-compute/openstack/useful-resources/","tags":"","title":"Useful Resources"},{"body":"Description of some of the Helpdesk workflows.\n","categories":"","description":"Workflows","excerpt":"Workflows","ref":"/internal/helpdesk/workflows/","tags":"","title":"Workflows"},{"body":"The OpenStack sites in the EGI Cloud that provide compute resources to run virtual machines (VMs) allow nearly everything to be done via an Application Programming Interface (API) or a command-line interface (CLI). This means that repetitive tasks or complex architectures can be turned into shell scripts.\nBut creating VMs happens so often in the EGI Cloud that tools were developed to capture the provisioning of these VMs, and allow users to recreate them in a flash, in a deterministic and repeatable way, using an Infrastructure-as-Code (IaC) approach.\nAutomating this activity will help researchers to:\nNot forget important configuration (e.g. the size and type of the hardware resources needed). Ensure the same steps are performed, in the same order (e.g. making sure the correct datasets are attached to each VM). Easily share scientific pipelines with collaborators. Make scientific applications cloud agnostic. To automate VM deployment, you can use any of the cloud orchestrators available in the EGI Cloud.\n","categories":"","description":"Use Infrastrucure-as-Code in the EGI Cloud\n","excerpt":"Use Infrastrucure-as-Code in the EGI Cloud\n","ref":"/users/compute/cloud-compute/automate/","tags":"","title":"Automating Deployments"},{"body":"Why joining the EGI Cloud? To support international communities supported by EGI (e.g. these research communities and applications or these research infrastructures in EOSC-hub or these business pilots in the EOSC Digital Innovation Hub. To participate in e-Infrastructure projects (H2020, EOSC) as an EGI compliant IaaS cloud provider. To participate in resource allocation and in pay-for-use campaigns run by EGI. To align access policies and operational model of your cloud with international good practices. To adopt best practices of multi-cloud federation for the benefit of your local users. Do I lose control on who can access my resources if I join federated cloud? No. EGI uses the concept of Virtual Organisation (VO) to group users. The resource provider has complete control on which VOs he wants to allow on its resources and which quotas or restrictions to assign to each VO. In the case of OpenStack, each VO is mapped to a regular OpenStack project that can be managed as any other and are isolated to other projects you may have configured in your deployment. Although not recommended, you can even restrict the automatic access of users within a VO and manually enable individual members.\nHow many components do I have to install? Depending on your cloud management framework and the kind of integration this will vary.\nIn general, the federation requires your cloud management framework to be configured to support Federated AAI with EGI Check-in. This may require changes in your current setup.\nOther components are designed to access your cloud management framework public APIs and do not require modification of your deployment. For OpenStack, these components can be run on a single VM that encapsulates them for convenience.\nWhich components of my cloud will interact with the federated cloud components? For OpenStack they are:\nKeystone Nova Glance Swift (optional) Users will also interact with:\nNeutron Cinder to perform their regular activities.\nHow will my daily operational activities change? For the most part daily operations will not change.\nA resource centre part of the EGI Federation, and supporting international communities, needs to provide support through the EGI channels. This means following up GGUS tickets. This includes requests from user communities and tickets triggered by failures detected by the monitoring infrastructure.\nA resource centre needs to maintain the services federated in EGI properly configured with the EGI AAI.\nThe resource centre will have to comply with the operational and security requirements. All the EGI policies aim at implementing service provisioning best practices and common requirements. EGI operations may conduct campaigns targeted to mitigate security vulnerabilities and to update unsupported operating system and software. These activities are part of the regular activities of a resource centre anyways (also for the non-federated ones). EGI and the Operations Centres coordinate these actions in order to have them implemented in a timely manner.\nIn summary, most of the site activities that are coordinated by EGI and the NGIs are already part of the work plan of a well-maintained resource centre, the additional task for a site manager is to acknowledge to EGI that the task has been performed.\n","categories":"","description":"Frequently Asked Questions\n","excerpt":"Frequently Asked Questions\n","ref":"/providers/cloud-compute/faq/","tags":"","title":"FAQ"},{"body":"How to handle issues during weekends and public holidays? Due to the fact that weekends and public holidays are not considered working days it is noted that ROD teams do not have any responsibilities during these days. RODs should ensure that in these days tickets do not expire and alarms will not age above 72h.\nWhat to do with alarms when node is not in production and is part of production site? It often happens that testing nodes on production sites are set as non-production. In such case Nagios monitoring system will send information about all nodes. As a result ROD will see on their dashboard alarms for non-production node. If it necessary to monitor such testing node it is recommended to put such non-production node in downtime.\nWhat to do when a sites have multiple alarms/ticket? When opening a ticket against a site with existing tickets ROD should consider that these problems may be linked or dependant on pending solutions. In such case ROD should use grouping mechanism to gather and assign alarms to one ticket rather than open a ticket for each alarm.\nIf the problem is different but maybe linked the expiry dates for each ticket should be synchronized to the latest date.\nHow to handle issues for site/node in downtime? Handling tickets for site/node in downtime When a ticket has been raised against a site that subsequently enters in downtime, the expiry date on the ticket can be extended.\nSites that are in downtime will still have monitoring switched on and therefore may appear to be failing tests but no alarms on Operations Portal will be raised against them. ROD must take care that when opening tickets to ensure that they don’t open tickets against sites in downtime.\nHandling alarms for site/node in downtime It often happens that a failure occurred generating a lot of alarms and then site manager decided to put site in Downtime. Getting these alarms OK may take more than 72h when the issue is escalated to Operations.\nROD should not create a ticket for sites/nodes in Downtime and is not obligated to deal with such alarms but it is recommended to close these alarms to avoid being escalated to Operations. In such case as a reason of closing NON-OK alarm ROD should put link to the downtime in the EGI Configuration Database.\nSite in downtime for more than a month If a site is in DOWNTIME for more than a month then it is advised that the site should go to the suspended status.\nWhat to do in case of accounting issue? In case of problems with accounting it is not recommended to suggest downtime at the second step of the escalation process for this test. Accounting service is not a functionality which is critical for users but it still need to be follow up.\nWatch out for flapping states You may want to wait for a second test to be run before closing an alarm which is in an OK status. This ensures that the OK result for that tests is stable. The waiting period is, of course, dependent on how long the test takes and how frequently it is checked.\nHow to handle the eu.egi.lowAvailability alarm? Go to procedure PROC04 Quality verification of monthly availability and reliability statistics.\n","categories":"","description":"FAQ concerning the ROD activity.","excerpt":"FAQ concerning the ROD activity.","ref":"/providers/rod/faq/","tags":"","title":"FAQ"},{"body":"Document control Property Value Title Accounting data publishing Policy Group Operations Management Board (OMB) Document status Approved Procedure Statement How to publish accounting information for different middleware Owner SDIS team Introduction In this manual we will show you how to publish accounting information from different middleware.\nGeneral information Publishing with the APEL Client/SSM To start sending accounting records:\nregister each endpoint sending the accounting records to the central repository as ‘gLite-APEL’ endpoint in GOCDB with host DN information either HTCondorCE or ARC-CE endpoints it is needed to authorise the endpoint to use the ARGO Message Service (AMS) Changes in GOCDB can take up to 4 hours to make it to AMS. install the APEL client and APEL SSM on your publisher host and edit client.cfg and sender.cfg install the APEL parser relevant to your batch system on each CE and edit parser.cfg documentation for APEL here GitHub apel/apel GitHub apel/ssm The old ActiveMQ network was dismissed: have a look at the new settings to properly publish the accounting records via AMS configure the client to publish data from the start of the current month. Run the parser(s) and client. If there are no errors messages and the client says it has sent messages then wait a day and you should see summaries here If it doesn’t open a GGUS ticket for the APEL Team. Monitoring the accounting data publication In order to monitor the regular publication of accounting data, it is enough registering only one CE with the APEL service type.\nSee here the details about the related Nagios probes.\nPublishing summarized records or single ones Sites can send either but the preferred option is summaries. Larger sites are recommended to send summaries.\nThe frequency for sending aggregated/summary records to APEL database? We recommend sending data daily for all sites, whether sending summaries or individual records. I think this is in the interests of people who are using the portal so that what they see is accurate. The summaries are for a complete month or the current month so far.\nAuthorization and authentication Authorization and authentication is made by the host certificate (which is signed by a trusted CA from the ca_policy_core package). Certificate should be registered in GOCDB. The certificate must not have the X509 extension: Netscape Cert Type: SSL Server because the message brokers will reject it.\nARC ARC uses its own system to publish the accounting data through AMS, so please refer to the NorduGrid ARC 6 documentation:\nInformation relevant only for 6.4 ARC releases and beyond: Accounting-NG The old ActiveMQ network was dismissed. ARC 6.12 introduces new settings for publishing the accounting records via AMS. HTCondor-CE To collect and publish the accounting data you need to install the APEL software as explained in the general information section. In addition, HTCondor-CE must be configured to create accounting records:\nInformation on configuring HTCondor-CE for APEL accounting: APEL Accounting for HTCondor-CE ","categories":"","description":"Publishing accounting information for different middleware.","excerpt":"Publishing accounting information for different middleware.","ref":"/providers/operations-manuals/man09_accounting_data_publishing/","tags":"","title":"MAN09 Accounting data publishing"},{"body":"The EGI Application Database (AppDB) includes a web GUI for management of Virtual Machines (VMs) on the federated infrastructure.\nThis GUI is available for a set of selected VOs. If your VO is not listed and you are interested in getting support, please open a ticket or contact us at support _at_ egi.eu.\nMain user features User identification with Check-in, with customised view of the VAs and resource providers based on the VO membership of the user. Management of VMs in topologies, containing one or more instances of a given VA. Attachment of additional block storage to the VM instances. Start/Stop VMs without destroying the VM (for all VMs of a topology or for individual instances within a topology) Single control of topologies across the whole federation. Quick start Log into the VMOps dashboard using EGI Check-in.\nClick on \"Create a new VM Topology\" to start the topology builder, this will guide you through a set of steps:\nSelect the Virtual Appliance you want to start, these are the same shown in the AppDB Cloud Marketplace, you can use the search field to find your VA;\nselect the VO to use when instantiating the VA;\nselect the provider where to instantiate the VA; and finally\nselect the template (VM instance type) of the instance that will determine the number of cores, memory and disk space used in your VM.\nNow you will be presented with a summary page where you can further customise your VM by:\nAdding more VMs to the topology\nAdding block storage devices to the VMs\nDefine contextualisation parameters (e.g. add new users, execute some script)\nClick on \"Launch\" and your deployment will be submitted to the infrastructure.\nThe topology you just created will appear on your \"Topologies\" with all the details about it, clicking on a VM of a topology will give you details about its status and IP. VMOps will create a default cloudadm user for you and create ssh-key pair for login (you can create as many users as needed with the contextualisation options of the wizard described above).\nVMOps was presented in one of the EGI Webinars in 2020. The indico page contains more details and there is also a video recording available on YouTube.\n","categories":"","description":"Use VMOPs Dashboard to monitor and manage VMs in the EGI Cloud\n","excerpt":"Use VMOPs Dashboard to monitor and manage VMs in the EGI Cloud\n","ref":"/users/compute/cloud-compute/monitor/","tags":"","title":"Monitoring and Management"},{"body":"Here you can find documentation about the EGI Online Storage for service providers.\n","categories":"","description":"Documentation for EGI Online Storage Service Providers","excerpt":"Documentation for EGI Online Storage Service Providers","ref":"/providers/online-storage/","tags":"","title":"Online Storage"},{"body":"Using cloud services securely Security in the EGI Federated infrastructure is the highest priority. The following services can help you to configure and consume EGI services while meeting your security and compliance objectives:\nThe Secrets Store allows users to securely store credentials or other sensitive information, instead of putting it as plaintext into code, configuration files, or environment variables. Explore the areas below for further details.\n","categories":"","description":"Security services in the EGI infrastructure\n","excerpt":"Security services in the EGI infrastructure\n","ref":"/users/security/","tags":"","title":"Security Services"},{"body":"What is it? The EGI Service Monitoring keeps an eye on the performance of the EGI services to quickly detect and resolve issues.\nThe service monitors the infrastructure by collecting data generated by functional probes. The raw data is merged into statistics and available through the user interface in a user-friendly way. It provides automated reporting tools with minimal development or operational effort for setting up monitoring.\nNote Documentation for Service Monitoring is available on the ARGO site. Accessing the monitoring information In order to access the information it’s required to have an X509 client certificate provided by an IGTF-accredited Certificate Authority.\nFor people not having access to an IGTF client certificate it’s possible to access Availability and Reliability information on the EGI ARGO page.\nService Monitoring endpoints Different service instances are available for different purposes:\nCertified sites Uncertified sites The endpoint for uncertified sites is using a certificate from a Certificate Authority (CA) that is part of the IGTF distribution but that is not in the default Operating System and browser stores.\nYour browser may be presenting you a security warning about an unknown CA, it’s a known issue with certificate having IGTF trust but not public trust (ie. not by default in the Operating Systems and browsers’ trust stores).\nIf you want to address this you can try to manually download the certificate for the ROOT CA and add it to your trust store and mark it as trusted. The exact process is dependant on the Operating System and browser that you use.\nAccess rules For an individual site, people having specific roles for the site can access using the X509 certificate linked to their account in Configuration Database:\nSite Administrator Site Operations Manager For all sites, the members of the dteam VO can access.\n","categories":"","description":"Monitor performance of EGI services\n","excerpt":"Monitor performance of EGI services\n","ref":"/internal/monitoring/","tags":"","title":"Service Monitoring"},{"body":"This page documents usage the CernVM-FS (CVMFS) service operated for EGI by UKRI-STFC. For information on how to install a client, follow the instruction in the CVMFS official documentation\nOverview The CernVM-File System (CVMFS) provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics collaborations to deploy software on the worldwide distributed computing infrastructure used to run data processing applications. CVMFS is implemented as a POSIX read-only file system in user space. Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs. CernVM-FS uses outgoing HTTP connections only, thereby it avoids most of the firewall issues of other network file systems. It transfers data and metadata on demand and verifies data integrity by cryptographic hashes. CVMFS is actively used by small and large collaborations. In many cases, it replaces package managers and shared software areas on cluster file systems as means to distribute the software used to process experiment data.\nThe current list of EGI repositories is as follows (disclaimer, some of them are inactive, but we keep them for archival purposes):\nRepository Project’s URL auger.egi.eu biomed.egi.eu web page cernatschool.egi.eu chipster.egi.eu comet.egi.eu config-egi.egi.eu dirac.egi.eu web page eiscat.egi.eu web page eosc.egi.eu extras-fp7.egi.eu galdyn.egi.eu ghost.egi.eu glast.egi.eu gridpp.egi.eu hyperk.egi.eu intertwin.egi.eu km3net.egi.eu web page ligo.egi.eu lucid.egi.eu mice.egi.eu na62.egi.eu neugrid.egi.eu notebooks.egi.eu web page omnibenchmark.egi.eu web page pheno.egi.eu phys-ibergrid.egi.eu pravda.egi.eu researchinschools.egi.eu seadatanet.egi.eu snoplus.egi.eu web page solidexperiment.egi.eu supernemo.egi.eu t2k.egi.eu unpacked.egi.eu wenmr.egi.eu west-life.egi.eu The list of EGI repositories can also be found online via the CVMFS monitor.\nThis documentation is for the VO content managers.\nOfficial CVMFS pages CVMFS Documentation Q\u0026As and Discussion Forum Requesting the creation of a new repository In the case of a new repository for EGI, steps are described in PROC22.\nOnboarding new Content Managers Steps for a new VO Content Manager to be granted access to the Stratum-0.\nRequesting access Request access to the service sending an email to cvmfs-support@gridpp.rl.ac.uk In the email, include the following information:\nName of the VO or CVMFS repository. Your Check-in ID from EGI Check-in. Mailing list All VO content managers should join the CVMFS-UPLOADER-USERS mailing list in JISCMAIL.\nDistributing new content To log into the service, just use ssh. The hostname is cvmfs-uploader-egi.gridpp.rl.ac.uk. Also, to maintain backwards compatibility, the alias cvmfs-upload01.gridpp.rl.ac.uk can be used. You need to specify explicitly which username you want to use to log in. The username is composed as reponame+\"sgm\". For example, for the repository dirac.egi.eu, the username is diracsgm.\n# Replace with the proper username $ ssh diracsgm@cvmfs-upload01.gridpp.rl.ac.uk To copy data:\n# Replace with the proper username $ scp source_file.txt diracsgm@cvmfs-upload01.gridpp.rl.ac.uk:destination_folder/ When running the ssh or scp commands, a message like this is displayed:\n# Replace with the proper username $ ssh diracsgm@cvmfs-upload01.gridpp.rl.ac.uk Authenticate at ----------------- https://aai.egi.eu/device?user_code=AAAAA-BBBBB ----------------- Hit enter when you have finished authenticating Copy and paste the URL into a browser, and follow the instructions to authenticate yourself using your home institution Identity Management Service.\nAfter login, you will find a single directory in the home directory:\n$ ls cvmfs_repo Add the new content you want to distribute into that directory.\nBuilding your software CVMFS is an infrastructure to distribute software world-wide. However, the uploader host should not be used for the purposes of building and compiling it prior to distribution.\nThe right approach is for you to have your own local building environment, and use the uploader host only to upload the new content for distribution.\nIf you have non-relocatable software, then you will need a /cvmfs/\u003cmyrepo\u003e/ directory on your building host. One option is to use an actual CVMFS client, so you have ready all the existing content being already distributed by CVMFS. By default, the /cvmfs/ directory on a CVMFS client host is read-only, but that can be solved using an ephemeral writable container.\n","categories":"","description":"Software distribution in the EGI infrastructure\n","excerpt":"Software distribution in the EGI infrastructure\n","ref":"/users/compute/software-distribution/","tags":"","title":"Software Distribution"},{"body":"Overview Achieving the core objective of the EGI Federation to advance research and innovation, embracing Open Science, is possible by empowering scientists and researchers to perform domain-specific research tasks, without having to deal with the complexities of the underlying infrastructure.\nAn essential tool that contributes to this objective is a flexible development environment, where users can create and share documents containing live code, equations, narrative text, and rich media output.\nThe following development environments are available:\nNotebooks is a browser-based, scalable tool for interactive data analysis. Replay allows re-creation of custom computing environments for reproducible execution of notebooks. Their dedicated sections offer a detailed description of each development environment.\n","categories":"","description":"Interactive environments for scientific development in the EGI Cloud\n","excerpt":"Interactive environments for scientific development in the EGI Cloud\n","ref":"/users/dev-env/","tags":"","title":"Development Environments"},{"body":"Introduction GOCDB, the software powering the EGI Configuration Database, is multi-tenanted; it can host multiple projects in the same instance. There are a number of different deployment scenarios that can be used to support new projects detailed below. Please contact the EGI Configuration Database admins and EGI Operations to discuss the options available.\n1) Add resources (sites/services) to an existing project Resources (NGIs, Sites, Services) would be hosted under an existing project, e.g. the ‘EGI’ project. The new resources would be subject to the rules of the existing project, such as site certification status changes and project controlled user memberships. The resources could not be filtered using a custom scope tag. 2) Add resources (sites/services) to an existing project and add a new Scope tag to represent a sub-grouping Resources would be hosted under an existing project, and a new scope tag would be added for the purposes of resource filtering. Since the resources are still hosted under an existing project, the new resources would still be subject to the rules of the existing project, such as site certification status changes and project controlled user memberships. The resources could be filtered using the new scope tag, but this scope tag would not strictly represent a project, rather a sub-grouping under the existing project, e.g. get_services\u0026scope=SubGroupX Note, resources can be tagged multiple times to declare support for multiple projects and sub-groups:\nget_services\u0026scope=SubGroupX,EGI\u0026scope_match=all 3) Add resources (sites/services) to a new Project and add a new Scope tag to filter by project Resources would be hosted under a new project, and a new scope tag would be added named after the project for the purposes of resource filtering. The resources would not be subject to the rules of other projects, for example, allowing the project to control its site certification status changes and project controlled user memberships. The resources could be filtered using the scope tag named after the new project, e.g. get_services\u0026scope=ProjectX Note, resources can be tagged multiple times to declare support for multiple projects:\nget_services\u0026scope=ProjectX,EGI\u0026scope_match=all ","categories":"","description":"How to create a new project in EGI Configuration Database.","excerpt":"How to create a new project in EGI Configuration Database.","ref":"/internal/configuration-database/adding-new-projects/","tags":"","title":"Adding a new project"},{"body":" Note Exhaustive documentation of the API for the GOCDB service, powering the EGI Configuration Database, is available on the dedicated GOCDB API documentation site. The GOCDB Programmatic Interface (PI) is available under /gocdbpi.\nAPI components The GOCDB PI has two main components:\nThe Read API The Write API The Read API provides programmatic access to the data. Access to some information (security/critical, personal details, otherwise sensitive information) is restricted, more details are available in the section about data protection levels.\nThe Write API provides limited functionality to add, update, and delete entities. Access is restricted, more details can be found in the section about authentication and authorisation.\nUsing the Read API Querying API calls can be tested in a browser or done from the command-line interface, using curl.\nBelow are some examples, including methods with different data protection levels.\nAPI calls starting with https://goc.egi.eu/gocdbpi/private require the client to present a valid credential.\nPublic calls, no authentication required: Retrieving all service endpoints Collecting information about endpoints under the EGI and FedCloud scopes Private calls, with authentication required: Retrieving all COD Staff and EGI CSIRT Officer to be allowed in secmon Querying for certification history for a site Querying for CSIRT emails of FedCloud sites Querying for Security officer at FedCloud sites Extracting content It is possible to filter content using xpath. Download information about endpoints under the EGI and FedCloud scopes as egi_fedcloud_service_endpoints.xml.\n# Extracting endpoints in production $ xpath -q -e \"//SERVICE_ENDPOINT[IN_PRODUCTION='Y']/HOSTNAME/text()\" \\ egi_fedcloud_service_endpoints.xml | sort | uniq Using an X.509 client certificate to authenticate from the CLI Querying information about a specific site using CURL, and authenticating with an X.509 client certificate.\n$ curl -v --cert ~/.globus/usercert.pem --key ~/.globus/userkey.pem \\ 'https://goc.egi.eu/gocdbpi/private/?method=get_site\u0026sitename=CESGA' Querying using python Looking for FedCloud endpoints See python script from cloud-info-provider repository.\nUsing the Write API Examples of using the Write API can be found on the GOCDB PI site.\n","categories":"","description":"Accessing the Configuration Database API","excerpt":"Accessing the Configuration Database API","ref":"/internal/configuration-database/api/","tags":"","title":"API"},{"body":"This document is intended for developers who want to write applications that interact with the AppDB API over the web using HTTP commands following the REST paradigm. The API endpoint is located at https://appdb-pi.egi.eu and it allows information retrieval and modification from third party applications without having to reside on the rich user interface of the AppDB portal. Thus one is given the opportunity to design one’s own frontends.\nGetting started Operations Starting with version 1.0, the AppDB API features write access as well, by supporting HTTP verbs such as PUT, POST, and DELETE. Verb mappings to data operations follow a CRUD convention, as depicted in the following table:\nOperation HTTP Verb Create PUT Read GET Update POST Delete DELETE The API also supports the Listing operation (CRUDL extension), by passing the parameter listmode=listing in the querystring when performing a GET request. Please note that in order to simplify the access model, Update operations are always partial, meaning that properties of the resource that is being updated which are entirely missing from the representation, are ignored (i.e. their state in the backend does not change). Therefore, in order to unset/remove a property, one has to explicitly specify it as NULL, provided that this is permitted. This is the reason why Create and Update CRUD mappings are inverted with regards to what is usually accustomed. Finally, the API also supports the OPTIONS HTTP verb, which returns a list of the operations that are permitted, in principle, for the resource in question. The base URI for this version of the RESTful API is\nhttps://appdb-pi.egi.eu/rest/1.0/\nand requests must be followed by at least one resource name, which may be followed by one or more optional sub-resource names, separated by slashes, as in the examples given bellow:\nhttps://appdb-pi.egi.eu/rest/1.0/applications/ https://appdb-pi.egi.eu/rest/1.0/applications/50/ Response types Because the API conforms to the REST paradigm, responses to all CRUD operations are always XML document representations of the resource in question. These documents are described by schema files which reside publicly in the web server. All the XML documents are enveloped in a common root element named appdb with attributes that describe request status, such as paging, or errors. These attributes are:\ncount (number) : the count of the entries found in the applications database. In case of paging, where only a subset of the results gets returned, the attribute is left unaffected. pagelength, pageoffset (numbers) : paging data in case the response is a list of resources. More information follows in the next section. datatype (string) : an identifier for the resource data that is enclosed in the response as defined in the XML schema. type (string) : the type of the response. Possible values are: list: A collection of references to entries in the applications’ database. In order to obtain the referenced entry, a request should be made using the entry’s ID. entry: Detailed information about an entry in the applications’ database. Paging is never used for this type of response. version (number) : The version of the API that was accessed. error (string) : If an error occurs, this attribute will contain the error message. errornum (number) : If an error occurs, this attribute will contain the error number. errordesc (string) : If an error occurs, this attribute might contain a more detailed description about the error. host, apihost (strings) : the URIs of the host that provided the data and API access, respectively. Paging and Filtering The paging mechanism that the API provides, as far as Read/Listing operations are concerned, can be used through two query parameters (if allowed) and can be displayed in the attributes of the root element of the response XML document. The query parameters are:\npagelength: defines the count of entries to be displayed in each page. pageoffset: defines the position in the list of entries from which the page will start. So if the client wants a list of 50 entries and wants to display the first 10, then the parameters should be set as page length=10 and page offset=0. To view the next 10 entries it should change the page offset to 10, for the next 10, it should set page length to 20, etc. One must remember that paging is not valid for all resources, as documented in the query parameters section in the API reference. A final note about paging is that if the client hasn’t explicitly enable paging, then the server will default to a preset paging value, in order to reduce load; this value can be retrieved from the relevant attributes of the response’s root node. If the paging parameters were set by the client but the page length exceeds that of the server’s default maximum value, then the later is used. Otherwise the page length set by the client will be used. Moreover, some of the API resources support filtering when doing Read/Listing operations; filter expressions may be passed in the querystring by assigning a value to the flt parameter, in order to retrieve just the subset of data that match certain criteria. These filter expressions are strings that may range from simple keywords to complex queries. They are search arguments which will be matched to results that are deemed relevant in the scope of the search target. These expressions can range from the simple and obvious space separated keyword list, to complex operator and field specific queries. In particular, filter expressions are composed of one or more keywords, optionally prefixed by operators and/or specifiers. The syntax in BNF is:\n\u003cexpression\u003e ::= \u003ckeyword\u003e* \u003ckeyword\u003e ::= [\u003coperator\u003e][\u003cspecifier\u003e:]some_string \u003coperator\u003e ::= [\u003ccontext_operator\u003e]\u003ccomparison_operator\u003e \u003ccontext_operator\u003e ::= \u0026 \u003ccomparison_operator\u003e ::= = | * | ~ | $ | \u003c[=] | \u003e[=] | +[=|*|~|$|\u003c[=]|\u003e[=]] | -[=|*|~|$|\u003c[=]|\u003e[=]] \u003cspecifier\u003e ::= \u003crelated_entity\u003e[.property_name] | property_name \u003crelated_entity\u003e ::= application | person | vo | middleware | country | [sub]discipline Rules are as follow:\nEach keyword without an operator may partially match any property related to the filtered entity.\nIf a keyword is prefixed with the = operator, then the keyword may exactly match any related property.\nIf a keyword is prefixed with the * operator, then any part of a comma-separated set of keywords may exactly match any related property.\nPlease note that in this case, the keyword NULL takes on the special meaning of a nil value\nIf a keyword is prefixed with the \u003c or \u003e comparison operator, then the keyword may be greater than or lesser than any related property, respectively.\nOptionally prefixing the = operator as well, will make the comparison operator non-strict.\nIf a keyword is prefixed with the ~ operator, then the keyword will be treated as a regular expression which may match any related property.\nIf a keyword is prefixed with the $ operator, then the keyword will be treated as a soundex phoneme which may match any related property that sounds alike in the English language.\nIf a keyword is prefixed with the + operator, then the keyword must match any related property.\nIf a keyword is prefixed with the - operator, then the keyword must not match any related property.\nIf a specifier is present, then the keyword applies only to the specified property.\nSpecifiers referring to related entities may omit the property name, in which case the special property any will be implicit.\nIn keywords without specifiers, the special specifier any.any will be implicit.\nThe \u0026 context operator makes an entity’s context private when present; this reduces results to the immediate neighbourhood of the target.\nConsider the following examples when searching for software:\ncms atlas will return a software item named CMSSW, another named ATLAS, another belonging to the atlas VO, etc.\ncms atlas +greece will return only the subset of the previous example’s results that actually mention Greece (e.g. in the description, or by having a researcher from Greece in their contact list, etc.)\ncms atlas -greece will return only the subset of the first example’s results that do not mention Greece\nvo:biomed will return software that belong to the VO’s that contain biomed in their name or description\n=vo.name:biomed will return software that belong to the VO named biomed only\n\u003cdateadded:2011 will return software that have been registered before the year 2011\n\u003e=name:x will return software whose names begin with x,y, or z\nperson:\"john doe\" will return software that has someone whose name contains john doe listed as a contact\ndiscipline:physics +tag:portal will return software that are filed under disciplines that are related to physics, and that are tagged as portals\nFor a list of possible specifiers, you can look up the /filter/reflect subresource of any searchable resource (e.g. /applications/filter/reflect), or you can also try using any search box in the portal, and check out the autocompletion list.\nAuthenticated Access Some or all operations on certain resources may require authentication, as indicated in the API Reference section below. In order to perform authenticated API calls, users need to create an access token and provide the following parameters in the query string, or POST fields accordingly:\naccesstoken: a valid access token. Registered users can generate access tokens from within the AppDB portal, under their profile preferences tab Access tokens may be assigned netfilters, which will only allow authenticated access from specific sources. Defining netfilters for an access token is not required, but it is strongly suggested, since they can help safeguard its use.\nAPI Reference Below you may find an exhaustive list of the resources v1.0 of the AppDB RESTful API offers. Details and documentation about a resource’s representation may be found as XSD annotations inside the appropriate schema file, under the schemata base resource. Note that when performing POST operations, the representation must be passed as a URL-encoded string in the query-string under the parameter data, whereas when performing PUT operations, the representation must be passed as a normal text stream. Representations passed to PUT/POST operations must be enclosed within an appdb:appdb root element, with the appropriate XML namespaces declared, the same way that responses are; nevertheless, this will be omitted in all following examples, in order to reduce clutter. Also note that since all Update (POST) operations are partial, XML elements that represent properties with a cardinality 0..* must be either\nomitted, in which case the present state in the backend is left untouched, fully disclosed, in which case entities present in the backend but absent from the POSTed representation are deleted, and vice versa explicitly declared NULL by passing a single instance of the element, with no node value (is of mixed type) and the single attribute xsi:nil with value true, in which case all instances of the entity are erased from the backend. This way, it is possible to perform modifications to a resource’s properties without having to re-declare all instances of other properties with 0..* cardinality, lest they were to be erased. Properties of 0..1 cardinality may also be declared NULL in a similar fashion, in which case their value in the backed becomes undefined. Moreover, one should also note that the response of all PUT/POST/DELETE operations equivalent to that of a GET operation, with the state of the resource\nafter the operation, in case of PUT/POST before the operation, in case of DELETE This way, it is possible - as well as advised - to differentiate the response with the input to such operations, in order to verify that the result is what was intended, since properties that are malformed or invalid will not break the operation, but rather be ignored, as long as the representation is well-formed (i.e. passes the XSD compilation).\nApplication List Resource: applications/ Type: list Datatype: application Schema: application Filtering: yes Public Operations: GET Authenticated Operations: PUT POST List of all application entries registered in the database\nExamples:\nGET https://appdb-pi.egi.eu/rest/1.0/applications?flt=metatype:0 will return all applications that are listed under the Software Marketplace (i.e. Software)\nGET https://appdb-pi.egi.eu/rest/1.0/applications?flt=metatype:1 will return all applications that are listed under the Cloud Marketplace (i.e. Virtual Appliances)\nGET https://appdb-pi.egi.eu/rest/1.0/applications?flt=country:Greece will return all applications that are related to Greece\nPOST https://appdb-pi.egi.eu/rest/1.0/applications?data={data}\u0026username={username}\u0026passwd={passwd}\u0026apikey={apikey} where {data} is\n\u003capplication:application id=\"123\"\u003e \u003capplication:description\u003esome new description\u003c/application:description\u003e \u003c/application:application\u003e will update application with ID 123 setting it description to \"some new description\" and leaving all other properties as-is.\nSimilarly, providing\n\u003capplication:application id=\"123\"\u003e \u003cdiscipline:subdiscipline xsi:nil=\"true\" /\u003e \u003c/application:application\u003e as {data} will remove all subdiscipline associations from the application, and providing\n\u003capplication:application id=\"123\"\u003e \u003cdiscipline:discipline id=\"1\"/\u003e \u003cdiscipline:discipline id=\"5\"/\u003e \u003c/application:application\u003e will replace the list of associated disciplines of the application with ID 123, with the disciplines with IDs 1 and 5 (“Life Sciences” and “Earth Sciences” respectively)\nNote that providing {data} as\n\u003capplication:application id=\"123\"\u003e \u003cdiscipline:discipline xsi:nil=\"true\" /\u003e \u003c/application:application\u003e to a POST operation will result in an error, since discipline elements are not defined as nillable in the schemata, meaning that all application entries must have at least on discipline in their complete representation.\necho {data} | PUT https://appdb-pi.egi.eu/rest/1.0/applications?username={username}\u0026passwd={passwd}\u0026apikey={apikey} with {data} defined as\n\u003capplication:application tagPolicy=\"0\"\u003e \u003capplication:name\u003eMyGridApp\u003c/application:name\u003e \u003capplication:description\u003emy grid application\u003c/application:description\u003e \u003capplication:abstract/\u003ethis is a grid application which performs task X\u003c/application:abstract\u003e \u003capplication:category id=\"1\" primary=\"true\"/\u003e \u003capplication:category id=\"2\" primary=\"false\"/\u003e \u003cdiscipline:discipline id=\"1\"/\u003e \u003capplication:status id=\"6\"/\u003e \u003cvo:vo id=\"951\"/\u003e \u003capplication:contact id=\"189\"/\u003e \u003capplication:contact id=\"190\"/\u003e \u003cpublication:publication \u003e \u003cpublication:title\u003eEvidence of Y using novel method X\u003c/publication:title\u003e \u003cpublication:url\u003ehttp://linkto.my.pub\u003c/publication:url\u003e \u003cpublication:conference\u003eX developments 2012\u003c/publication:conference\u003e \u003cpublication:proceedings/\u003e \u003cpublication:isbn\u003eISSN 0000-0000\u003c/publication:isbn\u003e \u003cpublication:startPage\u003e592\u003c/publication:startPage\u003e \u003cpublication:endPage\u003e597\u003c/publication:endPage\u003e \u003cpublication:volume\u003e18\u003c/publication:volume\u003e \u003cpublication:publisher/\u003e \u003cpublication:journal/\u003e \u003cpublication:year\u003e2012\u003c/publication:year\u003e \u003cpublication:type id=\"1\"\u003eFull Paper\u003c/publication:type\u003e \u003cpublication:author main=\"true\" type=\"internal\"\u003e \u003cperson:person id=\"189\"/\u003e \u003c/publication:author\u003e \u003cpublication:author type=\"external\"\u003e \u003cpublication:extAuthor\u003eJohn Doe\u003c/publication:extAuthor\u003e \u003c/publication:author\u003e \u003c/publication:publication\u003e \u003cmiddleware:middleware id=\"1\"\u003egLite\u003c/middleware:middleware\u003e \u003cmiddleware:middleware id=\"5\"\u003eMyGrid\u003c/middleware:middleware\u003e \u003capplication:tag\u003emethodX\u003c/application:tag\u003e \u003capplication:tag\u003ecountryZ\u003c/application:tag\u003e \u003c/application:application\u003e will add a new application to the database, with the following properties:\nbelonging to two categories, of which the one with ID equal to 1 will be the primary related to one discipline with an application status with ID 6 listed as supported by the VO with ID 951 having two people in its scientific contact list, those with IDs 189 and 190 having one publication with two authors, one internal (i.e. registered with the AppDB) and one external (not registered, name-only entry) listed as supported by two middleware, and with two tags applied Application Entry Resource: applications/{id} Type: entry Datatype: application Schema: application Filtering: N/A Public Operations: GET Authenticated Operations: DELETE Detailed description of a specific application entry\nModerated Application List Resource: applications/moderated Type: list Datatype: application Schema: application Filtering: no Public Operations: none Authenticated Operations: GET PUT List of applications that have been moderated for some particular reason, and thus hidden from public view until the issue gets resolved. Administrative access only.\nExamples:\necho {data} | PUT https://appdb-pi.egi.eu/rest/1.0/applications/moderated?username={username}\u0026passwd={passwd}\u0026apikey={apikey} where {data} is\n\u003capplication:application id=\"123\"\u003e \u003capplication:moderationReason\u003ecopyright violation\u003c/application:moderationReason\u003e \u003c/application:application\u003e will moderate the application with ID 123, providing \"copyright violation\" as the reason\nApplication Publication List Resource: applications/{id}/publications Type: list Datatype: publication Schema: publication Filtering: no Public Operations: GET Authenticated Operations: PUT POST List of scientific publications related to an application entry. Publications can be individually managed from this resource (and its sub-resources), or collectively defined directly from application/{id} resources\nApplication Publication Entry Resource: applications/{id}/publications/{id} Type: entry Datatype: publication Schema: publication Filtering: N/A Public Operations: GET Authenticated Operations: DELETE Detailed description of a publication related to an application entry\nApplication Tag List Resource: applications/{id}/tags Type: list Datatype: tag Schema: application Filtering: no Public Operations: GET Authenticated Operations: PUT List of tags applied to an application entry. Tags can be individually managed from this resource (and its sub-resources), or collectively defined directly from application/{id} resources\nExamples:\necho {data} | PUT https://appdb-pi.egi.eu/rest/1.0/applications/123/tags?username={username}\u0026passwd={passwd}\u0026apikey={apikey} where {data} is\n\u003capplication:tag\u003eC++\u003c/application:tag\u003e will apply the tag C++ to the application with ID 123\nApplication Tag Entry Resource: applications/{id}/tags/{id} Type: entry Datatype: tag Schema: application Filtering: N/A Public Operations: GET Authenticated Operations: DELETE Detailed description of a tag applied to an application\nRelated Application List Resource: applications/{id}/relatedapps Type: list Datatype: relatedapp Schema: application Filtering: no Public Operations: GET Authenticated Operations: none List of applications related to an application entry\nApplication Rating Report Resource: applications/{id}/ratingsreport Type: entry Datatype: ratingreport Schema: appratingreport Filtering: N/A Public Operations: GET Authenticated Operations: none Report on all user ratings of an application entry\nExternal Application Rating Report Resource: applications/{id}/ratingsreport/internal Type: entry Datatype: ratingreport Schema: appratingreport Filtering: N/A Public Operations: GET Authenticated Operations: none Report on user ratings of an application entry made by registered users\nInternal Application Rating Report Resource: applications/{id}/ratingsreport/external Type: entry Datatype: ratingreport Schema: appratingreport Filtering: N/A Public Operations: GET Authenticated Operations: none Report on user ratings of an application entry made by anonymous users\nApplication Rating List Resource: applications/{id}/ratings Type: list Datatype: rating Schema: apprating Filtering: no Public Operations: GET Authenticated Operations: none List of user rating entries applied to an application entry\nApplication Rating Entry Resource: applications/{id}/ratings/{id} Type: entry Datatype: rating Schema: application Filtering: N/A Public Operations: GET Authenticated Operations: none Detailed description of a rating entry applied to an application entry\nApplication State History List Resource: applications/{id}/history Type: list Datatype: history Schema: history Filtering: no Public Operations: none Authenticated Operations: GET List of previous states of an application entry. Administrative access only.\nApplication State History Entry Resource: applications/{id}/history/{id} Type: entry Datatype: history Schema: history Filtering: no Public Operations: none Authenticated Operations: GET Detailed description of a previous state of an application entry. Administrative access only.\nBookmarked Application List Resource: people/{id}/applications/bookmarked Type: list Datatype: application Schema: application Filtering: yes Public Operations: none Authenticated Operations: GET PUT List of application entries bookmarked by a user\nExamples:\nGET https://appdb-pi.egi.eu/rest/1.0/people/123/applications/bookmarked?flt=discipline:chemistry will return all application entries with a discipline related to chemistry and which have been bookmarked by the user with ID 123\nBookmarked Application Entry Resource: people/{id}/applications/bookmarked/{id} Type: entry Datatype: application Schema: application Filtering: N/A Public Operations: none Authenticated Operations: DELETE Detailed description of an application entry bookmarked by a user\nExamples:\nDELETE https://appdb-pi.egi.eu/rest/1.0/people/123/applications/bookmarked/111?username={username}\u0026passwd={passwd}\u0026apikey={apikey} will delete the application with ID 111 from the list of bookmarked applications of the user with ID 123\nEditable Application List Resource: people/{id}/applications/editable Type: list Datatype: application Schema: application Filtering: yes Public Operations: none Authenticated Operations: GET List of application entries that a user has permission to edit\nOwned Application List Resource: people/{id}/applications/owned Type: list Datatype: application Schema: application Filtering: no Public Operations: none Authenticated Operations: GET List of application entries that a user holds ownership of\nAssociated Application List Resource: people/{id}/applications/associated Type: list Datatype: application Schema: application Filtering: no Public Operations: none Authenticated Operations: GET List of application entries that a user is associated to, as a member of the application’s contact list\nPeople List Resource: people/ Type: list Datatype: person Schema: person Filtering: yes Public Operations: GET Authenticated Operations: PUT POST List of users (people) that have registered with the database\nGET https://appdb-pi.egi.eu/rest/1.0/people/?flt={flt} where {flt} is the URL-encoded representation of \u003e=registeredon:2012-02 +=vo.name:SEE -country:Greece\nwill return a list of all people who have registered after or in Feb 2012, have a relation to the VO named SEE and that are not related with Greece\nPerson Entry Resource: people/{id} Type: entry Datatype: person Schema: person Filtering: N/A Public Operations: GET Authenticated Operations: DELETE Detailed description of a user entry\nRegional Information List Resource: regional/ Type: list Datatype: regional Schema: regional Filtering: no Public Operations: GET Authenticated Operations: none List of countries, regions, and service providers participating in EGI and available to application and user entries\nApplication Category List Resource: applications/categories/ Type: list Datatype: category Schema: application Filtering: no Public Operations: GET Authenticated Operations: none List of category types available for application entries\nDiscipline/Subdiscipline List Resource: disciplines/ Type: list Datatype: discipline Schema: discipline Filtering: no Public Operations: GET Authenticated Operations: none List of discipline and subdiscipline types available for application entries\nMiddleware List Resource: middleware/ Type: list Datatype: middleware Schema: middleware Filtering: no Public Operations: GET Authenticated Operations: none List of grid middleware supported by application entries and VOs\nApplication Status List Resource: applications/statuses/ Type: list Datatype: status Schema: application Filtering: no Public Operations: GET Authenticated Operations: none List of status states available to application entries\nVO List Resource: vos/ Type: list Datatype: vo Schema: vo Filtering: yes Public Operations: GET Authenticated Operations: none List of VOs registered with the EGI Operations Portal and available to application entries\nGET https://appdb-pi.egi.eu/rest/1.0/vos/?flt={flt} where {flt} is the URL-encoded representation of application:CMS +description:CERN\nwill return the list of VOs that contain the word CERN in their description, and that are related to any application that mentions CMS in any of its properties\nVO Entry Resource: vos/{id} Type: entry Datatype: vo Schema: vo Filtering: N/A Public Operations: GET Authenticated Operations: none Detailed description of a VO registered with the EGI Operations Portal and available to application entries\nUser Role List Resource: people/roles/ Type: list Datatype: role Schema: person Filtering: no Public Operations: GET Authenticated Operations: none List of user roles available, which define base user privileges\nAvailable Tag List Resource: applications/tags/ Type: list Datatype: tag Schema: application Filtering: no Public Operations: GET Authenticated Operations: none List of all tag entries that have been attached at least once on any application entry\nContact Type List Resource: people/contacttypes/ Type: list Datatype: tag Schema: person Filtering: no Public Operations: GET Authenticated Operations: none List of contact types available to user contact information entries\nApplication Filter Normalization Resource: applications/filter/normalize Type: entry Datatype: filter Schema: filter Filtering: N/A Public Operations: GET Authenticated Operations: none Validates and normalizes a filter expression for application searches, defined by the querystring parameter flt\nApplication Filter Reflection Resource: applications/filter/reflect Type: entry Datatype: filter Schema: filter Filtering: N/A Public Operations: GET Authenticated Operations: none Returns a nested representation of the filtering expression specifiers available to application searches\nPerson Filter Normalization Resource: people/filter/normalize Type: entry Datatype: filter Schema: filter Filtering: N/A Public Operations: GET Authenticated Operations: none Validates and normalizes a filter expression for user searches, defined by the querystring parameter flt\nPerson Filter Reflection Resource: people/filter/reflect Type: entry Datatype: filter Schema: filter Filtering: N/A Public Operations: GET Authenticated Operations: none Returns a nested representation of the filtering expression specifiers available to user searches\nVO Filter Normalization Resource: vos/filter/normalize Type: entry Datatype: filter Schema: filter Filtering: N/A Public Operations: GET Authenticated Operations: none Validates and normalizes a filter expression for VO searches, defined by the querystring parameter flt\nVO Filter Reflection Resource: vos/filter/reflect Type: entry Datatype: filter Schema: filter Filtering: N/A Public Operations: GET Authenticated Operations: none Returns a nested representation of the filtering expression specifiers available to VO searches\nDissemination Log List Resource: dissemination/ Type: list Datatype: dissemination Schema: dissemination Filtering: no Public Operations: none Authenticated Operations: GET List of dissemination messages that have been dispatched to registered users. Administrative access only.\nDissemination Log Entry Resource: dissemination/{id} Type: entry Datatype: dissemination Schema: dissemination Filtering: N/A Public Operations: none Authenticated Operations: GET Detailed description of a dissemination message that has been dispatched to registered users. Administrative access only.\n","categories":"","description":"AppDB Rest API documentation\n","excerpt":"AppDB Rest API documentation\n","ref":"/users/compute/cloud-compute/appdb/","tags":"","title":"AppDB REST API v1.0"},{"body":"Overview This tutorial describes how to start a Virtual Machine in the EGI Federation that runs a browser-accessible Jupyter server with DataHub spaces mounted. This setup can be used in the EGI Federation or in any other provider which synchronise images from AppDB but is not part of the federation.\nRequirements This tutorial assumes you have:\nA valid EGI account: learn to can create one in Check-in. Access to a cloud provider where the Jupyter DataHub VM is available. Alternatively, this VM can be run on your computer using a virtualisation tool like VirtualBox. Create a VM with Jupyter and DataHub Step 1: Start your VM Start your VM on your cloud provider or virtualisation tool. You can check the tutorial on how to start a VM to learn how to start a VM at EGI’s Federated Cloud infrastructure.\nThis VM does not contain any default credentials, in order to access it you need a ssh key. Check this FAQ entry for more information. If you are starting this VM on VirtualBox, you will need to pass some valid context for cloud-init, see here how to prepare it.\nThe VM image is ready to listen on port 22 for ssh access and port 80 for accessing the notebooks interface. Make sure your have those ports open on your security groups, otherwise you will not be able to reach the Jupyter notebooks.\nOnce your instance is ready, assign it a public IP so you can reach it from your computer.\nStep 2: Get a hostname and certificate for your VM Your VM is ready to be accessible, but runs a plain HTTP server, which is not secure enough. If you try to connect with your browser to your VM, you will get a message as shown in the screenshot below:\nYou must enable HTTPS to encrypt requests and responses, thus making your VM safer and more secure.\nFirstly, you need a valid name for your VM. You can use the FedCloud Dynamic DNS to create a name. See Dynamic DNS docs for more information on the service. Once you have your name ready, assign it your VM’s IP.\nSecondly, you need to get a certificate to enable HTTPS. The VM has certbot already installed, you just need to run it with the hostname you have allocated and your email address as shown here:\n# log into your VM $ ssh ubuntu@\u003cyour VM's IP\u003e # now request the certificate $ sudo certbot --nginx -d \u003cyour registered name\u003e -m \u003cyour email\u003e Finally, open your browser and go to https://\u003cyour registered name\u003e/ to see Jupyter started. Follow next steps for getting the credentials to access the service.\nEnabling insecure access If you really need to use HTTP (e.g. your VM is not accessible publicly and you cannot create a certificate for it), you can disable the error shown by default in the nginx configuration.\nOpen /etc/nginx/sites-enabled/default and comment out lines 14-16:\n# if ( $https != 'on' ) { # return 406; # } And restart nginx:\n$ sudo systemctl restart nginx Step 3: Get your token for the Jupyter server Your VM will spawn a Jupyter notebooks server upon starting. This server runs as an unprivileged user named jovyan with the software installed using micromamba. The server uses a randomly generated token for authentication that you can obtain by logging into the VM and becoming jovyan:\n$ ssh ubuntu@\u003cyour VM's IP\u003e # become jovyan user and activate the default environment $ sudo su - jovyan $ micromamba activate $ jupyter server list --jsonlist | jq -r .[].token \u003cyour token\u003e Step 4: Start your notebooks Now point your browser to http://\u003cyour VM's IP\u003e and you will be able to enter the token to get started with Jupyter.\nYou can install additional packages with mamba from a terminal started from Jupyter or via ssh. For example for installing tensorflow:\n$ micromamba activate $ micromamba install -c conda-forge tensorflow Step 5: Mount DataHub spaces Log into EGI’s DataHub and create a token for mounting your data in the VM.\nYou will also need the IP or address of your closest Oneprovider for the spaces you are interested in accessing. This information is easily obtainable via DataHub’s web interface.\nGo to your Jupyter session in your browser and edit the mount.sh file in your home directory. Set the ONECLIENT_ACCESS_TOKEN and ONECLIENT_PROVIDER_HOST values to get access to DataHub:\nOpen a terminal from the launcher screen and execute the mount.sh script:\nYou should now see a datahub folder with all your spaces available directly from your Jupyter interface\n","categories":"","description":"Step by step guide to get a Virtual Machine for Jupyter and DataHub in your cloud provider\n","excerpt":"Step by step guide to get a Virtual Machine for Jupyter and DataHub in …","ref":"/users/tutorials/adhoc/jupyter-datahub-virtual-machine/","tags":"","title":"Create a VM with Jupyter and DataHub"},{"body":"I have lost access to my account, what should I do? If you were registered but are not recognised anymore (e.g. because your certificate DN changed), do not register again!\nInstead, follow the steps Lost access to your Configuration Database account section.\nI am responsible for a site that has recently entered the EGI infrastructure. How do I register it? Only registered users with an approved role on an NGI can add a new site. If you are the site administrator, the first thing to do is to contact your NGI staff and ask them to add the site for you. Then, register to EGI Configuration Database (see the user account section) and ask for a site admin role for your site (see the requesting a role section). Once your role approved, you will be able to edit and change your site information.\nHow do I extend a declared schedule downtime? Because of EGI policies it is not possible to extend a downtime. Recommended good practice for any downtime extension is to declare a new unscheduled downtime, starting just when the first one finishes. Please refer to the downtimes section of this documentation for more information, especially the “downtime extension” paragraph.\nI have declared a downtime “at risk”, and it turns out to be an outage. How can I declare this properly? If you have declared the downtime as being at risk and an outage actually happens half way through, you need to update the Configuration Database to reflect the fact that your site is now down. There is currently no way of doing this by updating the downtime on the fly without having the system considering the whole downtime as being an outage. The best way to proceed is:\nModify end date of your “at risk” downtime, so that it ends in a few minutes Enter a new “outage” downtime, starting when the other ends How do I switch monitoring on/off for my nodes? Monitoring status in Configuration Database cannot always be switched off. If a node is declared as delivering a production service, rules apply and the node has to be monitored. If you are running a test node and want to switch monitoring off, set both “monitoring” and “production” to “N”.\nWhy nobody has approved my role request yet? Someone has to approve any request you make, in order to ensure nobody is trying to get inappropriate roles. If yours is not getting approved, this can either be because your request was not legitimate, or most likely because the people that are supposed to do it forgot about it. Please refer to the Roles permissions definitions section of this documentation to determine who should validate your role, and try to get in touch with them. If you are requesting a site admin role, they are likely to be your fellow site admins or your NGI operators.\nI am not an EGI user but need access to the backend to retrieve information for my project. What can I do? Accessing the backend through another way than the Configuration Database web interface is out of the scope of this documentation. Please refer to the technical documentation instead, which is available from GOCDB Documentation.\n","categories":"","description":"Frequently Asked Questions","excerpt":"Frequently Asked Questions","ref":"/internal/configuration-database/faq/","tags":"","title":"FAQ"},{"body":"To ensure interoperability within and outside of EGI, the Policy on Acceptable Authentication Assurance defined a common set of trust anchors (in a PKIX implementation “Certification Authorities”) that all sites in EGI should install. In short, all CAs accredited to the Interoperable Global Trust Federation under the classic, MICS or SLCS Authentication Profiles are approved for use in EGI. When installing the ‘combined-assurance’ bundle, also IOTA issuers complying with assurance level DOGWOOD are included. Of course, sites may add additional CAs as long as the integrity of the infrastructure as a whole is not compromised. Also, if there are site or national policies/regulations that prevent you from installing a CA, these regulations take precedence – but you then must inform the EGI Security Officer (see EGI CSIRT) about this exception.\nRelease notes Review the release notes containing important notices about the current release, as well as a list of changes to the trust fabric.\nInstallation To install the EGI trust anchors on a system that uses the RedHat Package Manager (RPM) based package management system, we provide a convenience package to manage the installation. To install the currently valid distribution, all RPM packages are provided at\nhttps://repository.egi.eu/sw/production/cas/1/current/ The current version is based on the IGTF release with the same version number. Install the meta-package ca-policy-egi-core (or ca-policy-egi-cam) and its dependencies to implement the core EGI policy on trusted CAs.\nUsing YUM package management Add the following repo-file to the /etc/yum.repos.d/ directory:\n[EGI-trustanchors] name=EGI-trustanchors baseurl=https://repository.egi.eu/sw/production/cas/1/current/ gpgkey=https://repository.egi.eu/sw/production/cas/1/GPG-KEY-EUGridPMA-RPM-3 gpgcheck=1 enabled=1 and then update your installation. How to update depends on your previous activity:\nif you have previously ever installed the lcg-CA package, remove any references to https://linuxsoft.cern.ch/LCG-CAs/current from your YUM setup, and run $ yum clean cache metadata $ yum update lcg-CA and you are done. This will update the packages installed to the latest version, and also install the new ca-policy-egi-core package as well as a ca-policy-lcg package. All packages encode the same set of dependencies\nif you are upgrading from a previous EGI version only, just run\n$ yum update ca-policy-egi-core although at times you may need to clean the yum cache using $ yum clean cache metadata if you are installing the EGI trust anchors for the first time, run $ yum install ca-policy-egi-core Using the distribution on a Debian or Debian-derived platform The 1.39+ releases experimentally add the option to install the trust anchors from Debian packages using the APT dependency management system. Although care has been taken to ensure that this distribution is installable and complete, no guarantees are given, but you are invited to report your issues through GGUS. You may have to wait for a subsequent release of the Trust Anchor release to solve your issue, or may be asked to use a temporary repository. To use it:\nInstall the EUGridPMA PGP key for apt: $ wget -q -O - \\ https://dist.eugridpma.info/distribution/igtf/current/GPG-KEY-EUGridPMA-RPM-3 \\ | apt-key add - Add the following line to your sources.list file for APT: #### EGI Trust Anchor Distribution #### deb https://repository.egi.eu/sw/production/cas/1/current egi-igtf core Populate the cache and install the meta-package $ apt-get update $ apt-get install ca-policy-egi-core Using the distribution on other (non-RPM) platforms The trust anchors are provided also as simple ’tar-balls’ for installation on other platforms. Since there is no dependency management in this case, please review the release notes carefully for any security issues or withdrawn CAs. The tar files can be found in the EGI repository at\nhttps://repository.egi.eu/sw/production/cas/1/current/tgz/ Once you have downloaded the directory, you can unpack all the CA tar,gz as follows to your certificate directory:\n$ for tgz in $(ls \u003cca download dir\u003e); do tar xzf \u003cca download dir\u003e/$tgz --strip-components=1 \\ -C /etc/grid-security/certificates done Installing the distribution using Quattor Quattor templates are provided as drop-in replacements for both QWG and CDB installations. Update your software repository (re-generating the repository templates as needed) and obtain the new CA templates from:\nhttps://repository.egi.eu/sw/production/cas/1/current/meta/ca-policy-egi-core.tpl for QWG https://repository.egi.eu/sw/production/cas/1/current/meta/pro_software_meta_ca_policy_egi_core.tpl for CDB Make sure to mirror (or refer to) the new repository at https://repository.egi.eu/sw/production/cas/1/current/ and create the appropriate repository definition file.\nFor WLCG sites that are migrating from the lcg-CA package: the WLCG policy companion of the EGI templates can be found at QWG and CDB and can be included in the profile in parallel with the EGI core template. All packages needed are also included in the EGI repository, so only a single repository reference is necessary.\nCombined Assurance/Adequacy Model The release contains a “cam” (combined assurance/adequacy) package based on the approved policy on differentiated assurance. Technically, this means that you must ONLY install the new ca-policy-egi-cam packages if you ALSO at the same time implement VO-specific authorization controls in your software stack. This may require reconfiguration or a software update. Otherwise, just only install or update the regular ca-policy-egi-core package. There are no changes in this case. The ca-policy-egi-core package is approved for all VOs membership and assurance models. No configuration change is needed.\nAcceptable Authentication Assurance If a VO registration service or e-infrastructure registration service is accredited by EGI to meet the approved authentication assurance, an IGTF “DOGWOOD” accredited Authority - used solely in combination with said registration service - is also adequate for user authentication. See the policy for details.\nThis additional restriction policy must be implemented by each service in the authorization software. The “combined assurance” model package MUST NOT be installed unless the additional authorization is in place. You will need to reconfigure and may need to install upgrades. Not installing the new “cam” package does not have any detrimental effect on current users - only a new class of users (that can only obtain an opaque identifiers and do not do full vetting at their electronic identity provider) could be affected, and then only those users that are member of one of the communities that has part of the combined-assurance programme: LCG-Atlas, LCG-Alice, LCG-LHCb, and LCG-CMS.\nPatches and workarounds Reminder notice for VOMS AA operators Several updates to this trust anchor distribution incorporate changes to the name of the issuing authority, but the name of the end-entities and the users remains exactly the same. To make the change transparent, all operators of VOMS and VOMS-Admin services are requested to enable the subject-only name resolution mechanisms in VOMS and VOMS Admin, see additional documentation in VOMS services configuration reference:\non the VOMS core Attribute Authority service, configure the -skipcacheck flag on start-up. Set voms.skip_ca_check=True in the service properties. Concerns, issues and verification If you experience problems with the installation or upgrade of your trust anchors, or with the repository, please report such an issue through the GGUS system. For issues with the contents of the distribution, concerns about the trust fabric, or with questions about the integrity of the distribution, please contact the EGI IGTF liaison at egi-igtf-liaison@nikhef.nl.\nYou can verify the contents of the EGI Trust Anchor (CA) release with those of the International Grid Trust Federation, or its mirror. See the IGTF and EUGridPMA web pages for additional information.\nMake sure to verify your trust anchors with TACAR, the TERENA Academic CA Repository, where applicable.\n","categories":"","description":"Using the IGTF CA distribution","excerpt":"Using the IGTF CA distribution","ref":"/providers/operations-manuals/howto01_using_igtf_ca_distribution/","tags":"","title":"HOWTO01 Using IGTF CA distribution"},{"body":"Generic documentation The Operations start guide will help you start with EGI Operations duties.\nThe Resource Centre integration check list provides an overview of what are the required steps to get a new Resource Centre integrated into the EGI Federation.\nManuals The EGI Operations Manuals are approved technical documents that provide prescriptive guidelines on how to complete a given task. These documents are periodically reviewed, and need to be followed by all partners (as opposed to a best practice documents that provide optional guidelines).\nMAN01 How to publish Site Information MAN02 Service intervention management MAN04 Tool intervention management MAN05 Top and site BDII High Availability MAN06 failover for MySQL grid-based service MAN07 VOMS replication MAN09 Accounting data publishing How-Tos Miscellaneous collection of short How-Tos which are relevant to various other documentation and procedures.\nHOWTO01 Using IGTF CA distribution HOWTO02 Site Certification Required Documentation HOWTO03 Site Certification GIIS Check HOWTO04 Site Certification Manual tests ","categories":"","description":"Operations manuals for Service Providers","excerpt":"Operations manuals for Service Providers","ref":"/providers/operations-manuals/","tags":"","title":"Operations manuals and How-Tos"},{"body":"Authentication Some EGI services authentication is based on X.509 certificates. The certificates are issued by Certification Authorities (CAs) part of the EUGridPMA federation which is also part of IGTF (International Global Trust Federation).\nThe role of a Certification Authority (CA) is to guarantee that users are who they claim to be and are entitled to own their certificate. It is up to the users to discover which CA they should contact. In general, CAs are organised geographically and by research institutes. Each CA has its own procedure to release certificates.\nEGI sites, endpoints and tools accept certificates part of the EUGridPMA distribution. If your community VO is enabled on that site, your certificate will be accepted by that site since all certificates are recognized at site level.\nUsually, a certificate can be installed by command-line tools, but they can also be stored in the web browser to access EGI web tools and services.\nGet a Certificate The list of EGI recognised CAs provides a clickable map to find your nearby CA. Several of these offer the option to get an ’eScience Personal’ certificate online from the Terena Certificate Service CA. Check the countries where this is available.\nIf eScience Personal certificate is not available in your country, then request a certificate from a regular IGTF CA. The request is normally generated using either a web-based interface or console commands. Details of which type of request a particular CA accepts can be found on each CA’s site.\nFor a web-based certificate request, a form must usually be filled in with information such as the name of the user, home institute, etc. After submission, a pair of private and public keys are generated, together with a request for the certificate containing the public key and the user data. The request is then sent to the CA, while the private key stays in the browser, hence the same browser must be used to retrieve the certificate once it is issued.\nUsers must usually install the CA root certificate in their browser first. This is because the CA has to sign the user certificate using its private key, and the user’s browser must be able to validate the signature.\nFor some CAs, the certificate requests are generated using a command line interface. The details of the exact command and the requirements of each CA will vary and can be found on the CA’s site.\nOnce received the request, the CA will have to confirm your authenticity through your certificate. This usually involves a physical meeting or a phone call with a Registration Authority (RA). A RA is delegated by the CA to verify the legitimacy of a request, and approve it if it is valid. The RA is usually someone at your home institute, and will generally need some kind of ID to prove your identity.\nInstall a Certificate After approval, the certificate is generated and delivered to you. This can be done via email, or by giving instructions to you to download it from a web page.\nBrowser installation Install the certificate in your browser. If you don’t know how to upload your certificate in your browser have a look at the examples.\nHost installation To use EGI services with your certificate, you must first save your certificate to disk.\nThe received certificate will usually be in one of two formats:\nPrivacy Enhanced Mail Security Certificate (PEM) with extension .pem or Personal Information Exchange File (PKCS12) with extensions .p12 or .pfx. The latter is the most common for certificates exported from a browser (e.g. Internet Explorer, Mozilla and Firefox), but the PEM format is currently needed on EGI user interface. The certificates can be converted from one format to the other using the openssl command.\nIf the certificate is in PKCS12 format, then it can be converted to PEM using pkcs12:\nFirst you will need to create the private key, use -nocerts. Open your terminal, enter the following command:\nopenssl pkcs12 -nocerts -in my_cert.p12 -out userkey.pem where:\nFilename Description my_cert.p12 is the input PKCS12 format file; userkey.pem is the output private key file; usercert.pem is the output PEM certificate file. When prompted to “Enter Import Password”, simply press enter since no password should have been given when exporting from keychain. When prompted to “Enter PEM pass phrase”, enter the pass phrase of your choice, e.g. 1234.\nNow you can create the certificate, use -clcerts, (use -nokeys here will not output private key), and the command is:\nopenssl pkcs12 -clcerts -nokeys -in my_cert.p12 -out usercert.pem When prompted to “Enter Import Password”, simply press enter since no password should have been given when exporting from keychain.\nFor further information on the options of the pkcs12 command, consult man pkcs12\nIt is strongly recommended that the names of all these files are kept as shown. Once in PEM format, the two files, userkey.pem and usercert.pem, should be copied to a User Interface (UI). For example, the ‘standard’ location for Mac would be .globus directory in your $HOME. I.e. $HOME/.globus/\nRenewing the Certificate CAs issue certificates with a limited duration (usually one year); this implies the need to renew them periodically. The renewal procedure usually requires that the certificate holder sends a request for renewal signed with the old certificate and/or that the request is confirmed by a phone call; the details depend on the policy of the CA. The certificate usually needs to be renewed before the old certificate expires; CAs may send an email to remind users that renewal is necessary, but users should try to be aware of the renewal date, and take appropriate action if they are away for extended periods of time.\nTaking Care of Private Keys A private key is the essence of your identity. Anyone who steals it can impersonate the owner and if it is lost, it is no longer possible to do anything. Certificates are issued personally to individuals, and must never be shared with other users. To user EGI services, users must agree to an Acceptable Use Policy, which among other things requires them to keep their private key secure.\nOn a UNIX UI, the certificate and private key are stored in two files. Typically they are in a directory called $HOME/.globus and are named usercert.pem and userkey.pem, and it is strongly recommended that they are not changed. The certificate is public and world-readable, but the key must only be readable by the owner. The key should be stored on a disk local to the user’s UI rather than, for example, an NFS-mounted disk. If a certificate has been exported from a browser, a PKCS12-format file (.p12 or .pfx), which contains the private key, will have been locally stored and this file must be either encrypted, hidden or have its access rights restricted to only the owner.\nIf a private key is stored under the Andrew File System (AFS), access is controlled by the AFS Access Control Lists (ACL) rather than the normal file permissions, so users must ensure that the key is not in a publicly-readable area.\nWeb browsers also store private keys internally, and these also need to be protected. The details vary depending on the browser, but password protection should be used if available; this may not be the default (it is not with Internet Explorer). The most secure mode is one in which every use of the private key needs the password to be entered, but this can cause problems as some sites ask for the certificate many times. Reaching a compromise between security and convenience is vital here, so that neither come too short.\nIt is important not to lose the private key, as this implies loss of all access to the services, and registration will have to be started again from scratch. Having several securely protected copies in different places is strongly advised, so the certificate can be used from a web browser and several UI machines.\nA private key stored on a UI must be encrypted, meaning that a passphrase must be typed whenever it is used. A key must never be stored without a passphrase. The passphrase should follow similar rules to any computer password. Users should be aware of the usual risks, like people watching them type or transmitting the passphrase over an insecure link.\nAuthorisation The sites authorise the access to their resources to a VO according to their own access policies, resource location, how many resources is the VO allowed to use. There are finer authorization policies, including groups, roles, in this way, the users can be structured in a VO. So, it is not a 0/1 authorization policy.\nThe community has full control of the access to the VO according to community authorization policies. The VO membership, groups and roles are managed by VO managers (Privileged VO members) independently by using the Virtual Organization Membership Service (VOMS).\nVOMS The Virtual Organization Membership Service (VOMS) is an attribute authority which serves as central repository for VO user authorization information, providing support for sorting users into group hierarchies, keeping track of their roles and other attributes in order to issue trusted attribute certificates and SAML assertions used in the Grid environment for authorization purposes. VOMS is composed of two main components:\nthe VOMS core service, which issues attribute certificates to authenticated clients the VOMS Admin service, which is used by VO manager to administer VOs and manage user membership details. How does it work? Usually, users submit tasks/jobs to the infrastructure that are attached with their own credential, and the credential is attached with a proxy certificate that is a short-term credential signed with the user certificate and is extended with the VO attributes. In general speaking, a user credential is just an ID, and a proxy contains the VO details, so a resource site by receiving the proxy can recognize that the user is part of such a VO with such a role from such a group. A user can be part of multiple VO, thus can generate multiple proxies.\nRegister to a VO Visit Operation Portal to search for existing VOs\nIf there are any community VOs matching your requirements (with Registry System is VOMS), then click Action-\u003e Details to look at the VO information. In the VO ID Card page, click the link for Enrollment URL, it will take you to the VO VOMS page. You should have already discussed with the EGI support team, they would help you to contact the VO managers and get approval for your access. If there are no relevant VOs, you can send a request to register a new VO. (Note, for EGI services, you should request for VOMS configuration, once VO is configured, you will be notified about your VO VOMS link). More information can be found at Guideline for VO registration. Again, this is usually guided by the EGI support team. You should already have a meeting with them to discuss your requirements. They will help you to get resources from EGI providers, and sign SLA with you.\nRequest your VO membership at VO VOMS page. You will have to enter required information and then wait for approval.\nCreating a proxy VOMS configuration Every VO needs two different pieces of information:\nthe vomses configuration files, where the details of the VO are stored (e.g. name, server, ports). These are stored by default at /etc/vomses and are normally named following this convention: \u003cvo name\u003e.\u003cserver name\u003e (e.g. for fedcloud.egi.eu VO, you would have fedcloud.egi.eu.voms1.grid.cesnet.cz and fedcloud.egi.eu.voms2.grid.cesnet.cz. the .lsc files that describe the trust chain of the VOMS server. These are stored at /etc/grid-security/vomsdir/\u003cvo name\u003e and there should be one file for each of the VOMS server of the VO. You can check specific configuration for your VO at the Operations portal. Normally each VOMS server has a Configuration Info link where the exact information to include in the vomses and .lsc files is shown.\nProxy creation Once you have the VO information configured (vomses and .lsc) and your certificate available in your $HOME/.globus directory you can create a VOMS proxy to be used with clients with:\nvoms-proxy-init --voms \u003cname of the vo\u003e --rfc See for example, using fedcloud.egi.eu VO:\nvoms-proxy-init --voms fedcloud.egi.eu --rfc Enter GRID pass phrase: Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=EGI/OU=UCST/CN=Enol Fernandez Creating temporary proxy ......................................................... Done Contacting voms1.grid.cesnet.cz:15002 [/DC=cz/DC=cesnet-ca/O=CESNET/CN=voms1.grid.cesnet.cz] \"fedcloud.egi.eu\" Done Creating proxy ................................................................... Done Your proxy is valid until Mon Feb 4 23:37:21 2019 ","categories":"","description":"X.509 / VOMS based authentication and authorisation\n","excerpt":"X.509 / VOMS based authentication and authorisation\n","ref":"/users/aai/check-in/vos/voms/","tags":"","title":"VOMS"},{"body":"Documents required for Resource Centre registration and certification.\nThe Resource Centre administrators should read and understand the first five documents. It is a further requirement that the OLA between the NGI and RC, and the Service Operations Policy are accepted by the Site Operations Manager. Further, documents referred within the Service Operations Policy should also be understood.\nDocumentation PROC09 Resource Centre Registration and Certification: Procedure document describing the steps required to register new Resource Centres and certify new and suspended ones. Service Operations Policy: This security policy presents the conditions that apply to anyone running a Service on the Infrastructure, or to anyone providing a Service that is part of the Infrastructure. Resource Centre Operational Level Agreement (OLA) PROC11 Resource Centre Decommissioning Procedure: Procedure document describing the steps required to decommission a Resource Centre. PROC12 Production Service Decommissioning Procedure: Procedure document describing the steps required to decommission a production service. Helpdesk: A collection of documentation relevant to the EGI Helpdesk (GGUS). Tools Configuration Database (GOCDB) Operations Portal ARGO Monitoring: RCs status, A/R reports: The ARGO Monitoring service periodically checks the functionality of the production services. EGI Operations Architecture: Document describing the roles and responsibilities of the various layers in EGI and also the architectural nomenclature. ","categories":"","description":"Documents required for Resource Center registration and certification.","excerpt":"Documents required for Resource Center registration and certification.","ref":"/providers/operations-manuals/howto02_site_certification_required_documentation/","tags":"","title":"HOWTO02 Site Certification Required Documentation"},{"body":"Introduction ROD (Regional Operator on Duty) is a role which oversees the smooth operation of EGI infrastructure in the respective NGI. ROD team is responsible for solving problems on the infrastructure within own Operations Centre according to agreed procedures. They ensure that problems are properly recorded and progress according to specified timelines. They ensure that necessary information is available to all parties. The team is provided by each Operations Centre and requires procedural knowledge on the process. The role is usually covered by a team of people and is provided by each NGI. Depending on how an NGI is organised there might be a number of members in the ROD team who work on duty roster (shifts on a daily or weekly basis), or there may be one person working as ROD on a daily basis and a few deputies who take over the responsibilities when necessary. This latter model is generally more suitable for small NGIs.\nIn this text, the acronym ROD will be used both for the whole team, or for the person who is actually working on shift.\nIn order to become a ROD member, one first needs to go through the steps described in the Overview for ROD.\nThe following text describes the duties that ROD (teams) are responsible for.\nContact: all-operator-on-duty AT mailman.egi.eu\nDuties A list describing duties.\nAlarms and tickets Information on how to deal with alarms raised in the Operations Portal Dashboard and how to generate and deal with tickets.\nDowntimes How downtimes are managed.\nCommunication Communication channels for ROD to Sites and to management.\nSecurity How ROD should deal with security issues.\nManuals and procedures In this section are linked manuals and procedures which RODs should be familiar with:\nPROC01: Infrastructure Oversight escalation Dashboard How-Tos and Training Guides Webinar shortcuts. Introduction. ROD duties ROD procedures Becoming ROD team Member Obtaining X.509 certificate Registration in GOCDB Registration in GGUS Registration in dteam VO ROD shift Dashboard overview Issues aka alarms Tickets Notepads hand over Webinar Presentation Slides ROD FAQ Resources Operational tools Procedures ","categories":"","description":"Description of the NGI oversight activity.","excerpt":"Description of the NGI oversight activity.","ref":"/providers/rod/","tags":"","title":"Regional Operator on Duty (ROD)"},{"body":"Basics How can I get access to the cloud compute service? There is a VO available for 6 months piloting activities that any researcher in Europe can join. Just request access to the pilot Virtual Organisation.\nHow can I get an OAuth2.0 token? Authentication via CLI or API requires a valid Access Token from Check-in. The EGI Check-in Token Portal allows you to get one as needed. Check the Authentication and Authorisation guide for more information.\nIs OCCI still supported? OCCI is now deprecated as API for the EGI Cloud providers using OpenStack. Some providers still support OCCI (a list of active endpoints can be queried at GOCDB) but it should note be used for any new developments.\nMigration from rOCCI CLI to OpenStack CLI is quite straightforward, we summarize the main commands in rOCCI and OpenStack equivalent in the table below:\nAction rOCCI OpenStack List images occi -a list -r os_tpl openstack image list Describe images occi -a describe -r \u003cimage_id\u003e openstack image show \u003cimage_id\u003e List flavors occi -a list -r resource_tpl openstack flavor list Describe flavors occi -a describe -r \u003ctemplate_id\u003e openstack flavor show \u003cimage_id\u003e Create VM occi -a create -r compute -t occi.core.title=\"MyFirstVM\" -M \u003cflavor id\u003e -M \u003cimage id\u003e -T user_data=\"file://\u003cfile\u003e\" openstack server create --flavor \u003cflavor\u003e --image \u003cimage\u003e --user-data \u003cfile\u003e MyFirstVM Describe VM occi -a describe -r \u003cvm id\u003e openstack server show \u003cvm id\u003e Delete VM occi -a delete -r \u003cvm id\u003e openstack server delete \u003cvm id\u003e Create volume occi -a create -r storage -t occi.storage.size='num(\u003csite in GB\u003e)' -t occi.core.title=\u003cstorage_resource_name\u003e openstack volume create --size \u003csize in GB\u003e \u003cstorage resource name\u003e List volume occi -a list -r storage openstack volume list Attach volume occi -a link -r \u003cvm_id\u003e -j \u003cstorage_resource_id\u003e openstack server add volume \u003cvm id\u003e \u003cvolume id\u003e Detach volume occi -a unlink -r \u003cstorage_link_id\u003e openstack server remove volume \u003cvm id\u003e \u003cvolume id\u003e Delete volume occi -a delete -r \u003cvolume id\u003e openstack volume delete \u003cvolume id\u003e Attach public IP occi -a link -r \u003cvm id\u003e --link /network/public openstack server add floating ip \u003cvm id\u003e \u003cip\u003e If you still rely on OCCI for your access, please contact us at support _at_ egi.eu for support on the migration.\nDiscovery How can I get the list of the EGI Cloud providers? The list of certified providers is available in GOCDB. The following command with the fedcloud client can help you to get that list:\n$ fedcloud site list 100IT BIFI CESGA CESNET-MCC CETA-GRID CLOUDIFIN CYFRONET-CLOUD DESY-HH GSI-LCG2 IFCA-LCG2 IISAS-FedCloud IISAS-GPUCloud IN2P3-IRES INFN-CATANIA-STACK INFN-CLOUD-BARI INFN-PADOVA-STACK Kharkov-KIPT-LCG2 NCG-INGRID-PT SCAI TR-FC1-ULAKBIM UA-BITP UNIV-LILLE fedcloud.srce.hr The providers also generate dynamic information about their characteristics via the Argo Messaging System which is easily browsable from AppDB.\nHow can I choose which site to use? Sites offer their resources to users through Virtual Organisations (VO). First, you need to join a Virtual Organisation that matches your research interests, see authorisation section on how VOs work. AppDB shows the supported VOs and for each VO you can browse the resource providers that support it.\nHow can I get information about the available VM images? The Application Database contains information about the VM images available in the EGI Cloud. Within the AppDB Cloud Marketplace, you can look for a VM and get all the information about which VO the VM is associated, the sites where the VM is available and the endpoints and identifiers to use it in practice.\nManaging VMs The disk on my VM is full, how can I get more space? There are several ways to increase the disk space available at the VM. The fastest and easiest one is to use block storage, creating a new storage disk device and attaching it to the VM. Check the storage guide for more information.\nHow can I keep my data after the VM is stopped? After a VM has been stopped and unless backed up in a block storage volume, all data in the VM is destroyed and cannot be recovered. To ensure your data will be available after the VM is deleted, you need to use some form of persistent storage.\nHow can I assign a public IP to my VM? Some providers do not automatically assign a public IP address to a VM during the creation phase. In this case, you can attach a public IP by first allocating a new public IP and then assigning it to the VM.\nHow can I assign a DNS name to my VM? If you need a domain name for your VMs, we offer a Dynamic DNS service that allows any EGI user to create names for VMs under the fedcloud.eu domain.\nJust go to EGI Cloud nsupdate and login with your Check-in account. Once in, you can click on \"Add host\" to register a new hostname in an available domain.\nWhat is contextualisation? Contextualisation is the process of installing, configuring and preparing software upon boot time on a predefined virtual machine image. This way, the predefined images can be stored as generic and small as possible, since customisations will take place on boot time.\nContextualisation is particularly useful for:\nConfiguration not known until instantiation (e.g. data location). Private Information (e.g. host certs) Software that changes frequently or under development. Contextualisation requires passing some data to the VMs on instantiation (the context) and handling that context in the VM.\nHow can I inject my public SSH key into the machine? The best way to login into the virtual server is to use SSH keys. If you don't have one, you need to generate it with the ssh-keygen command:\nssh-keygen -f fedcloud This will generate two files:\nfedcloud, the private key. This file should never be shared fedcloud.pub, the public key. That will be sent to your VM. To inject the public SSH key into the VM you can use the key-name option when creating the VM in OpenStack. Check keypair management option in OpenStack documentation. This key will be available for the default configured user of the VM (e.g. ubuntu for Ubuntu, centos for CentOS).\nYou can also create users with keys with a contextualisation file:\n#cloud-config users: - name: cloudadm sudo: ALL=(ALL) NOPASSWD:ALL lock-passwd: true ssh-import-id: cloudadm ssh-authorized-keys: - \u003cpaste here the contents of your SSH key pub file\u003e Warning YAML format requires that the spaces at the beginning of each line is respected in order to be correctly parsed by cloud-init. How can I use a contextualisation file? If you have a contextualisation file, you can use it with the --user-data option to server create in OpenStack.\nopenstack server create --flavor \u003cyour-flavor\u003e --image \u003cyour image\u003e \\ --user-data \u003cyour contextualisation file\u003e \\ \u003cserver name\u003e Note We recommend using cloud-init for contextualisation. EGI images in AppDB do support cloud-init. Check the documentation for more information. How can I pass secrets to my VMs? EGI Cloud endpoints use HTTPS so information passed to contextualize the VMs can be assumed to be safe and only readable within your VM. However, take into account that anyone with access to the VM may be able to access also the contextualisation information.\nWarning Take into account that anyone with access to the VM may be able to access also the contextualisation information, so ensure that no sensitive data like clear text passwords is used during contextualisation. How can I use ansible? Ansible relies on ssh for accessing the servers it will configure. VMs at EGI Cloud can be also accessed via ssh, just make sure you inject the correct public keys in the VMs to be able to access.\nIf you don't have public IPs for all the VMs to be managed, you can also use one as a gateway as described in the Ansible FAQ.\nHow can I release resources without destroying my data? Whenever you delete a VM, the ephemeral disks associated with it will be also deleted. If you don’t plan to use your VM for some time, there are several ways to release resources consumed by the VM (e.g. CPU, RAM) and recover the data or boot your VM in a previous state when you need it back. We list below the main strategies you can use:\nUse a volume to store the data to be kept: Check the Storage section of the documentation to learn how to use volumes. If you start your VM from a volume, the VM can be destroyed and recreated easily. OpenStack documentation cover how to start a VM from a volume with CLI or using the Horizon dashboard\nSuspend or shelve instance: Suspending a VM will pause a VM, releasing CPU and memory, and allowing to resume later in time at the exact same state. Shelving shuts down the VM, thus RAM contents will be lost but disk will be kept. This releases more resources from the provider while still allows to easily boot the VM back without losing disk contents.\nCreate snapshot of instance: a snapshot will create a new VM image at your provider that can be used to boot a new instance of the VM with the same disk content. You can use this technique for creating a base template image that can be later re-used to start similar VMs easily.\nHow can I find all the VMs that I own in the EGI Federated Cloud? We suggest using the fedcloudclient:\n# list the Virtual Organisations that you belong to fedcloud token list-vos # then, for each VO, run: list-all-my-own-vms.sh --vo \u003cvirtual-organisation\u003e See the fedcloudclient documentation for more information.\n","categories":"","description":"Most frequent questions about EGI Cloud Compute\n","excerpt":"Most frequent questions about EGI Cloud Compute\n","ref":"/users/compute/cloud-compute/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"Be sure that Resource Centre GIIS URL is contained in the BDII you use for certification.\nCheck the consistency of the published information These are the main branches of the LDAP tree:\nGlueSiteUniqueID GlueSubClusterUniqueID GlueCEUniqueID GlueCESEBind GlueSEUniqueID GlueServiceUniqueID It is recommended to use the Apache Studio LDAP browser, although in this page ldapsearch queries are shown.\nContact information Under the branch GlueSiteUniqueID check the values of the following fields:\nGlueSiteName GlueSiteUserSupportContact GlueSiteSysAdminContact GlueSiteSecurityContact GlueSiteOtherInfo Example:\n$ ldapsearch -x -LLL -H ldap://sibilla.cnaf.infn.it:2170 \\ -b mds-vo-name=INFN-CNAF,o=grid 'objectClass=GlueSite' \\ GlueSiteName GlueSiteUserSupportContact GlueSiteSysAdminContact \\ GlueSiteSecurityContact GlueSiteOtherInfo dn: GlueSiteUniqueID=INFN-CNAF,Mds-Vo-name=INFN-CNAF,o=grid GlueSiteSecurityContact: mailto:grid-sec@cnaf.infn.it GlueSiteSysAdminContact: mailto:grid-operations@lists.cnaf.infn.it GlueSiteName: INFN-CNAF GlueSiteOtherInfo: CONFIG=yaim GlueSiteOtherInfo: EGEE_SERVICE=prod GlueSiteOtherInfo: EGI_NGI=NGI_IT GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: WLCG_TIER=3 GlueSiteUserSupportContact: mailto:grid-operations@lists.cnaf.infn.it Information related to software environment Under the branch GlueSubClusterUniqueID check the values of the following fields:\nCheck that the GlueHostApplicationSoftwareRunTimeEnvironment contains a list of software tags supported by the site. The list can include VO-specific software tags. In order to ensure backwards compatibility it should include the entry ‘LCG-2’, the current middleware version and the list of previous middleware tags (i.e. LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 LCG-2_3_0 LCG-2_3_1 LCG-2_4_0 LCG-2_5_0 LCG-2_6_0 LCG-2_7_0 GLITE-3_0_0 GLITE-3_1_0 GLITE-3_2_0 R-GMA). GlueHostProcessorOtherDescription (see FAQ HEP SPEC06) GlueHostOperatingSystemName, GlueHostOperatingSystemVersion and GlueHostOperatingSystemRelease (see publishing the OS name). Example:\n$ ldapsearch -x -LLL -H `\u003cldap://virgo-ce.roma1.infn.it:2170\u003e` \\ -b mds-vo-name=resource,o=grid 'objectClass=GlueSubCluster' \\ GlueHostProcessorOtherDescription dn: GlueSubClusterUniqueID=virgo-ce.roma1.infn.it,GlueClusterUniqueID=virgo-ce.roma1.infn.it,Mds-Vo-name=resource,o=grid GlueHostProcessorOtherDescription: Cores=4, Benchmark=7.83-HEP-SPEC06 Publishing the OS name It has been decided that the 3 fields\nGlueHostOperatingSystemName GlueHostOperatingSystemRelease GlueHostOperatingSystemVersion should be parsed from the output of /usr/bin/lsb_release, like this:\nGlueHostOperatingSystemName: lsb_release -i | cut -f2 GlueHostOperatingSystemRelease: lsb_release -r | cut -f2 GlueHostOperatingSystemVersion: lsb_release -c | cut -f2 yielding values like\nGlueHostOperatingSystemName: CentOS GlueHostOperatingSystemRelease: 7.9.2009 GlueHostOperatingSystemVersion: Core This has been tested on various Linux flavours and should work on every serious GNU/Linux distribution.\nInformation about the batch system Under the branch GlueCEUniqueID check the values of the following fields:\nGlueCEInfoTotalCPUs: Check that the value is higher than 0. GlueCEStateWaitingJobs: If there is a “44444”, the information providers are not working properly. GlueCEInfoLRMSType: any supported batch system (sge, pbs, lsf…) GlueCEStateStatus: Production, Draining, Queuing or Closed are accepted values. GlueCEAccessControlBaseRule: VOs enabled on the queue GlueCECapability Example:\n$ ldapsearch -x -LLL -H ldap://virgo-ce.roma1.infn.it:2170 \\ -b mds-vo-name=INFN-ROMA1-VIRGO,o=grid 'objectClass=GlueCE' \\ GlueCEInfoTotalCPUs GlueCEInfoJobManager GlueCEImplementationName dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-theophys,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-cert,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-virgoglong,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-argo,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-virgogshort,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 $ ldapsearch -x -LLL -H ldap://cmsrm-bdii.roma1.infn.it:2170 \\ -b mds-vo-name=INFN-ROMA1-CMS,o=grid 'objectclass=GlueCE' GlueCECapability dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsgcert,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsglong,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsgshort,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsglong,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsgcert,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsgshort,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 Information on Computing Element about Storage Elements For each SE, on the CEs the following values must be present:\nGueCESEBindSEUniqueID. GlueCESEBindCEAccesspoint and GlueCESEBindMountInfo. Example:\n$ ldapsearch -x -LLL -H ldap://cremino.cnaf.infn.it:2170 \\ -b mds-vo-name=resource,o=grid 'objectClass=GlueCESEBind' \\ GlueCESEBindSEUniqueID GlueCESEBindCEUniqueID GlueCESEBindMountInfo dn: GlueCESEBindSEUniqueID=sunstorm.cnaf.infn.it,GlueCESEBindGroupCEUniqueID=cremino.cnaf.infn.it:8443/cream-pbs-cert,Mds-Vo-name=resource,o=grid GlueCESEBindSEUniqueID: sunstorm.cnaf.infn.it GlueCESEBindMountInfo: n.a GlueCESEBindCEUniqueID: cremino.cnaf.infn.it:8443/cream-pbs-cert dn: GlueCESEBindSEUniqueID=sunstorm.cnaf.infn.it,GlueCESEBindGroupCEUniqueID=cremino.cnaf.infn.it:8443/cream-pbs-prod,Mds-Vo-name=resource,o=grid GlueCESEBindSEUniqueID: sunstorm.cnaf.infn.it GlueCESEBindMountInfo: n.a GlueCESEBindCEUniqueID: cremino.cnaf.infn.it:8443/cream-pbs-prod Information on Storage Elements Under the branch GlueSEUniqueID check the values of the following fields:\nGlueSALocalID: VO information GlueSEAccessProtocolLocalID : rfio, srm_v2, gsiftp, gsidcap GlueSEImplementationName (deprecated) GlueSEArchitecture GlueSAStateUsedSpace GlueSAStateAvailableSpace GlueSACapability Example:\n$ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\ -b mds-vo-name=resource,o=grid 'objectclass=GlueSE' dn: GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSEImplementationVersion: 1.7.4 GlueSETotalOnlineSize: 8795 GlueSEStatus: Production objectClass: GlueTop objectClass: GlueSE objectClass: GlueKey objectClass: GlueSchemaVersion GlueSETotalNearlineSize: 0 GlueSEArchitecture: multidisk GlueSESizeTotal: 8795 GlueSESizeFree: 5458 GlueSEName: INFN-PAVIA DPM server GlueSchemaVersionMinor: 3 GlueSEUsedNearlineSize: 0 GlueForeignKey: GlueSiteUniqueID=INFN-PAVIA GlueSEUsedOnlineSize: 3336 GlueSchemaVersionMajor: 1 GlueSEImplementationName: DPM GlueSEUniqueID: grid-se.pv.infn.it $ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\ -b mds-vo-name=resource,o=grid 'objectclass=GlueSA' \\ GlueSAAccessControlBaseRule GlueSACapability dn: GlueSALocalID=storage:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSAAccessControlBaseRule: VO:atlas GlueSAAccessControlBaseRule: VO:dteam GlueSAAccessControlBaseRule: VO:infngrid GlueSAAccessControlBaseRule: VO:ops GlueSACapability: InstalledOnlineCapacity=8258 GlueSACapability: InstalledNearlineCapacity=0 dn: GlueSALocalID=ATLASHOTDISK:SR:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSAAccessControlBaseRule: VOMS:/atlas/Role=production GlueSACapability: InstalledOnlineCapacity=536 GlueSACapability: InstalledNearlineCapacity=0 $ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\ -b mds-vo-name=resource,o=grid '(\u0026(objectclass=GlueSA)(GlueSALocalID=storage:replica:online))' \\ GlueSAReservedNearlineSize GlueSAFreeNearlineSize \\ GlueSATotalNearlineSize GlueSAUsedNearlineSize GlueSACapability \\ GlueSATotalOnlineSize GlueSAFreeOnlineSize \\ GlueSAReservedOnlineSize GlueSAStateAvailableSpace \\ GlueSAUsedOnlineSize GlueSAStateUsedSpace dn: GlueSALocalID=storage:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSATotalNearlineSize: 0 GlueSAFreeOnlineSize: 4921 GlueSAUsedNearlineSize: 0 GlueSAFreeNearlineSize: 0 GlueSAReservedNearlineSize: 0 GlueSAStateAvailableSpace: 4921376492 GlueSAReservedOnlineSize: 0 GlueSAUsedOnlineSize: 3336 GlueSAStateUsedSpace: 3336991982 GlueSATotalOnlineSize: 8258 GlueSACapability: InstalledOnlineCapacity=8258 GlueSACapability: InstalledNearlineCapacity=0 Information about other services There is a branch GlueServiceUniqueID for each service published by the site (WMS, LFC, DPM, GRIDICE, LB, MYPROXY, BDII, etc): what discriminates the services are the values of GlueServiceType, example:\nlcg-file-catalog org.glite.wms.WMProxy org.glite.lb.Server srm_v1, SRM Example:\n$ ldapsearch -x -LLL -H ldap://sibilla.cnaf.infn.it:2170 \\ -b mds-vo-name=INFN-CNAF,o=grid 'objectClass=GlueService' \\ GlueServiceType GlueServiceEndpoint GlueServiceName dn: GlueServiceUniqueID=lfcserver.cnaf.infn.it,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: lfcserver.cnaf.infn.it GlueServiceName: INFN-CNAF-lfc GlueServiceType: lcg-file-catalog dn: GlueServiceUniqueID=local-lfcserver.cnaf.infn.it,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: lfcserver.cnaf.infn.it GlueServiceName: INFN-CNAF-lfc GlueServiceType: lcg-local-file-catalog dn: GlueServiceUniqueID=\u003chttp://lfcserver.cnaf.infn.it:8085/,Mds-Vo-name=INFN-CNAF,o=grid\u003e GlueServiceEndpoint: \u003chttp://lfcserver.cnaf.infn.it:8085/\u003e GlueServiceName: INFN-CNAF-lfc-dli GlueServiceType: data-location-interface dn: GlueServiceUniqueID=myproxy.cnaf.infn.it_MyProxy_4027652676,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: myproxy://myproxy.cnaf.infn.it:7512/ GlueServiceName: INFN-CNAF-MyProxy GlueServiceType: MyProxy dn: GlueServiceUniqueID=sibilla.cnaf.infn.it_bdii_site_3877936872,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003cldap://sibilla.cnaf.infn.it:2170/mds-vo-name=INFN-CNAF,o=grid\u003e GlueServiceName: INFN-CNAF-bdii_site GlueServiceType: bdii_site dn: GlueServiceUniqueID=local-http://lfcserver.cnaf.infn.it:8085/,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttp://lfcserver.cnaf.infn.it:8085/\u003e GlueServiceName: INFN-CNAF-lfc-dli GlueServiceType: local-data-location-interface dn: GlueServiceUniqueID=mon-it.cnaf.infn.it_Regional-NAGIOS_2937827985,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://mon-it.cnaf.infn.it:443/nagios\u003e GlueServiceName: INFN-CNAF-Regional-NAGIOS GlueServiceType: Regional-NAGIOS dn: GlueServiceUniqueID=httpg://sunstorm.cnaf.infn.it:8444/srm/managerv2,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: httpg://sunstorm.cnaf.infn.it:8444/srm/managerv2 GlueServiceName: INFN-CNAF-SRM GlueServiceType: SRM dn: GlueServiceUniqueID=albalonga.cnaf.infn.it_org.glite.lb.server_889826742,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://albalonga.cnaf.infn.it:9003/\u003e GlueServiceName: INFN-CNAF-server GlueServiceType: org.glite.lb.server dn: GlueServiceUniqueID=gridit-ce-001.cnaf.infn.it_org.edg.gatekeeper_715226072,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: gram://gridit-ce-001.cnaf.infn.it:2119/ GlueServiceName: INFN-CNAF-gatekeeper GlueServiceType: org.edg.gatekeeper dn: GlueServiceUniqueID=egee-wms-01.cnaf.infn.it_org.glite.wms.WMProxy_2200630265,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://egee-wms-01.cnaf.infn.it:7443/glite_wms_wmproxy_server\u003e GlueServiceName: INFN-CNAF-WMProxy GlueServiceType: org.glite.wms.WMProxy dn: GlueServiceUniqueID=cremino.cnaf.infn.it_org.glite.ce.CREAM_860197007,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://cremino.cnaf.infn.it:8443/ce-cream/services\u003e GlueServiceName: INFN-CNAF-CREAM GlueServiceType: org.glite.ce.CREAM dn: GlueServiceUniqueID=cremino.cnaf.infn.it_org.glite.ce.Monitor_2670664997,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://cremino.cnaf.infn.it:8443/ce-monitor/services/CEMonitor\u003e GlueServiceName: INFN-CNAF-Monitor GlueServiceType: org.glite.ce.Monitor dn: GlueServiceUniqueID=top-bdii01.cnaf.infn.it_bdii_top_1813027130,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003cldap://egee-bdii.cnaf.infn.it:2170/mds-vo-name=local,o=grid\u003e GlueServiceName: INFN-CNAF-bdii_top GlueServiceType: bdii_top [...] See the Site Certification Manual tests HOWTO04\nSee to Resource Centre registration and certification procedure PROC09.\n","categories":"","description":"How to test a Resource Centre during certification","excerpt":"How to test a Resource Centre during certification","ref":"/providers/operations-manuals/howto03_site_certification_giis_check/","tags":"","title":"HOWTO03 Site Certification GIIS Check"},{"body":"This page provides instructions on how to test manually the functionality of the grid and cloud services offered by a site. These checks are executed by the EGI Operations Team for sites that want to be formally included into EGI Production infrastructure. The site must successfully pass either the grid or the cloud certification tests to become part of the EGI Production infrastructure.\nCheck the functionality of the grid elements Be sure that the site’s GIIS URL is contained in the Top level BDII/Information System your NGI will use for your certification.\nNote that the examples here use the Italian NGI and sites. Please substitute with YOUR OWN NGI and site credentials when running the test.\nARC CE checks A first test can be done using ARC’s ngstat command:\n$ /usr/bin/ngstat -q -l -c \u003cCE hostname\u003e -t 20 ... ... plenty of output ... If a monitoring host of your NGI is available, then the probes can easily be executed from there:\nCheck the status of the CE with:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-status \\ -H \u003cCE hostname\u003e -x /etc/nagios/globus/userproxy.pem-ops Status is active Test gsiftp:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-auth -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops gsiftp OK Test the versions of the CA’s:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-caver -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops version = 1.38 - All CAs present Check the versions of ARC and Globus:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-softver \\ -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops nordugrid-arc-0.8.3.1, globus-5.0.3 Copy a file:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-gridftp -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Submit a test job:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-jobsubmit \\ -H \u003cCE hostname\u003e \\ --vo ops -x /etc/nagios/globus/userproxy.pem-ops Job submission successful Check the LFC:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-lfc -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Check the SRM:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-srm -H \u003cCE hostname\u003e \\ -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Before continuing, you may want to make sure that the probes for all services which the CE intends to offer, do actually succeed.\nStorage Element (SE) checks Check if gridftp server on SE works:\n$ uberftp inaf-se-01.ct.pi2s2.it For STORM SE: check if SRM client works (on the published information you can find the right port to use)\n$ /opt/storm/srm-clients/bin/clientSRM \\ ping -e httpg://sunstorm.cnaf.infn.it:8444 ============================================================ Sending Ping request to: httpg://sunstorm.cnaf.infn.it:8444 ============================================================ Request status: statusCode=\"SRM_SUCCESS\"(0) explanation=\"SRM server successfully contacted\" ============================================================ SRM Response: versionInfo=\"v2.2\" otherInfo (size=2) [0] key=\"backend_type\" [0] value=\"StoRM\" [1] key=\"backend_version\" [1] value=\"\u003cFE:1.5.0-1.sl4\u003e\u003cBE:1.5.3-4.sl4\u003e\" ============================================================ Try to write on SE. Be sure your UI is pointing to an IS the SE is contained in (you may use your certification BDII)\n1) Setting a top-bdii that is publishing the SE you have to test\n$ export LCG_GFAL_INFOSYS=\u003cTopBDII hostname\u003e:2170 2) Copy a file from the local filesystem to the SE, registering it in the LFC. This command output will return a SURL that you can use latter for other tests.\nA SURL is a path of the type: srm://srm01.ncg.ingrid.pt/ibergrid/iber/generated/2011-02-01/file4034a935-8d7a-48f4-914f-16f2634d4802\n$ lcg-cr -v --vo \u003cVO\u003e-d\u003cYour SE\u003e \\ -l lfn:/grid/\u003cVO\u003e/test.txt file:\u003c/path/to/your/local/file\u003e 3) Create a new replica in other SE (to check the third-party transfer between 2 SEs)\n$ lcg-rep -v --vo \u003cVO\u003e-d\u003cOther SE\u003e \u003cSURL\u003e 4) List Replicas\n$ lcg-lr -v --vo \u003cVO\u003e lfn:/grid/\u003cVO\u003e/test.txt 5) Delete all replicas\n$ lcg-del -v --vo \u003cVO\u003e-a\u003cguid\u003e Job submission Submit a test job to Cream-CE through the WMS, i.e. using the glite-wms-job-submit command. In case, submit a mpi test job. The NGI_IT certification WMS is gridit-cert-wms.cnaf.infn.it.\nRegistration into 1st level HLR NOTE: this step is needed if your infrastructure uses DGAS as accounting system\nAfter the site entered in production, it needs to register the site resources in the HLR. Ask the site admins to open a ticket towards the HLR administrators, passing them the following information:\ngrid queues names, in the form: gridit-ce-001.cnaf.infn.it:2119/jobmanager-lcgpbs-cert not-grid queues names, in the form: hostname:queue Name, surname ad certificate subject of each site admin Certificate subject of Computing Element Eventually, the site admins have to open a ticket to DGAS support unit asking to enable the forwarding of accounting data from the 2° level HLR to APEL.\nCertification Job The test job checks several things, like the environment on WN and installed RPMs. Moreover it performs some replica management tests. With a grep TEST you may get a summary of the results: in case of errors, you have to see in detail what is gone wrong!\nGlobus checks These checks should be executed depending on the services registered in GOCDB under a Resource Centre. Not all services are compulsory for a RC, but upon registration of new ones, the corresponding tests should be executed.\nGSISSH Initialize grid proxy and check if GSISSH works:\n$ grid-proxy-init $ gsissh USER@HOST -p 2222 /bin/date (Debug with: USER@HOST -vvv -p 2222 /bin/date) GridFTP Check if upload works:\n$ globus-url-copy file:/tmp/test.txt gsiftp://HOST:2811/tmp/test.txt (Debug with: globus-url-copy -dbg -v -vb file:/tmp/test.txt gsiftp://HOST:2811/tmp/test.txt) Check if download works:\n$ globus-url-copy gsiftp://HOST:2811/tmp/test.txt file:/tmp/test.txt (Debug with: globus-url-copy -dbg -v -vb gsiftp://HOST:2811/tmp/test.txt file:/tmp/test.txt) Delete the remote file:\n$ uberftp HOST 'rm /tmp/test.txt' (Debug with: uberftp HOST 'rm /tmp/test.txt' -debug 3) GRAM Check authentication:\n$ globusrun -a -r HOST:2119 Check job submission:\n$ globusrun -s -r HOST:2119 \"\u0026(executable=\"/bin/date\")\" QCG checks QCG Computing checks The presented tests of QCG-Computing service use the qcg-comp, the client program for QCG-Computing, that may be installed from provided RPMS. In order to connect to QCG-Computing the grid proxy must be created.\nGenerate user’s proxy:\n$ grid-proxy-init Your identity: /C=PL/O=GRID/O=PSNC/CN=Jane Doe Enter GRID pass phrase for this identity: Creating proxy ............................ Done Your proxy is valid until: Fri Jun 10 06:23:32 2011 Query the QCG-Computing service:\n# the xmllint is used only to present the result in more pleasant way $ qcg-comp -G | xmllint --format - \u003cbes-factory:FactoryResourceAttributesDocument xmlns:bes-factory=\"http://schemas.ggf.org/bes/2006/08/bes-factory\"\u003e … a lot of information … \u003c/bes-factory:FactoryResourceAttributesDocument\u003e Submit a sample job:\n$ qcg-comp -c -J /opt/plgrid/qcg/share/qcg-comp/doc/examples/date.xml Activity Id: ccb6b04a-887b-4027-633f-412375559d73 Query its status:\n$ qcg-comp -s -a ccb6b04a-887b-4027-633f-412375559d73 status = Executing $ qcg-comp -s -a ccb6b04a-887b-4027-633f-412375559d73 status = Finished exit status = 0 QCG Notification checks The tests of QCG-Notification require qcg-ntf-client program to be installed in a system. The program is provided in RPM package.\nCreate a sample subscription:\n$ qcg-ntf-client -d \\ -S \"cons=http://127.0.0.1:2212 top=http://schemas.qoscosgrid.org/comp/2011/04/notification/topic;//*;Full\" ... INF May 17 14:15:51 1128 0xa0262720 [qcg-client-gsoa] Subscribed, subRef: '810917963' ... Remove the created subscription:\n$ qcg-ntf-client -d -U \"id=810917963\" ... INF May 17 14:41:48 3318 0xa0262720 [qcg-client-gsoa] Unsubscribed: '810917963' … Checking the connection with QCG-Computing: In one shell run ‘tail -f’ on the QCG-Computing log file and in the other try to submit a sample job using the qcg-comp program (as described above). Check the tail output if there are no error messages on sending notifications. E.g. the following lines means that the connection problems occurred:\n$ tail -f /opt/qcg/var/log/qcg-comp/qcg-compd.log INF Oct 04 10:55:33 18929 0x2adadc2abe30 [notification_ws] Sending notify: 320f014c-3181-4daf-bbd9-1824b7d8216a -\u003e Queued NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultCode: 'SOAP-ENV:Client' NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultString: 'smcm:ActivityState' NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultDetail: '\u003cSOAP-ENV:Detail xmlns:SOAP-ENV=\"http://schemas.xmlsoap.org/soap/envelope/\"\u003econnect failed in tcp_connect()\u003c/SOAP-ENV:Detail\u003e' ERR Oct 04 10:55:33 18929 0x2adadc2abe30 [notification_ws] Failed to send notification to http://grass1.man.poznan.pl:19011/ QCG Broker checks The basic tests of QCG-Broker service may be proceeded with help of qcg-simple-client, the software that provides a set of commands for interaction with QCG-Broker. qcg-simple-client may be installed from RPMs.\nCreate a sample job description:\n$ cat \u003e sleep.qcg \u003c\u003c EOF #!/bin/bash #QCG queue=plgrid #QCG host=nova.wcss.wroc.pl #QCG persistent sleep 30 EOF Submit a job:\n$ qcg-sub sleep.qcg https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl Your identity: C=PL,O=GRID,O=PSNC,CN=Jane Doe Enter GRID pass phrase for this identity: Creating proxy, please wait... Proxy verify OK Your proxy is valid until Tue Mar 12 14:50:27 CET 2013 UserDN = /C=PL/O=GRID/O=PSNC/CN=Jane Doe ProxyLifetime = 24 Days 23 Hours 59 Minutes 58 Seconds jobId = J1360936230540__0152 Check the job statuses:\n$ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Jane Doe ProxyLifetime = 24 Days 23 Hours 59 Minutes 49 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Jane Doe TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: ProxyLifetime: PT0S Status: PREPROCESSING StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: PREPROCESSING StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: LocalFinishTime: $ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Jane Doe ProxyLifetime = 24 Days 23 Hours 59 Minutes 23 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Jane Doe TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: ProxyLifetime: PT0S Status: RUNNING StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: RUNNING StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: Fri Feb 15 14:50:47 CET 2013 LocalFinishTime: $ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Jane Doe ProxyLifetime = 24 Days 23 Hours 56 Minutes 10 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Jane Doe TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: Fri Feb 15 14:52:17 CET 2013 ProxyLifetime: PT0S Status: FINISHED StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: FINISHED StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: Fri Feb 15 14:52:12 CET 2013 LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: Fri Feb 15 14:50:47 CET 2013 LocalFinishTime: Fri Feb 15 14:52:09 CET 2013 Check the functionality of the cloud elements Sites can provide any (not necessarily all) of the interfaces listed below:\nOpenStack Compute for VM Management CDMI for Object Storage Cloud Compute checks prerequisites AppDB integration Go to AppDB and look for a OS image member of the fedcloud.egi.eu VO (all sites should support), e.g. EGI CentOS 7 image\nCheck that the site is visible into the AppDB “Availability and Usage” panel for the image. If not, probably the site has not registered the FedCloud VO into their middleware (vmcatcher) or it did not properly configured the BDII provider script.\nFrom that “Availability and Usage” panel, click on the Site name, then on the latest VM Image version, select a resource template (preferably with the smallest quantity of resources (RAM \u0026 CPU)) and click on the “get IDs” button on the right of the resource template. You will get the “Site Endpoint”, “Template ID” and “OCCI ID”. Save these values since they will be needed in the next steps.\nCredentials Generate a set of keys for your user (it is not required to set a passphrase for the keys, since these are just temporary keys for the test), make sure to set key permissions to 400:\n$ ssh-keygen -t rsa -b 2048 -f tempkey Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in tempkey. Your public key has been saved in tempkey.pub. The key fingerprint is: (...) $ chmod 400 tempkey Create a simple contextualization script, to setup access keys on the machine and test contextualization\n$ cat \u003c\u003c EOF \u003e ctx.txt Content-Type: multipart/mixed; boundary=\"===============4393449873403893838==\" MIME-Version: 1.0 --===============4393449873403893838== Content-Type: text/x-shellscript; charset=\"us-ascii\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\"deploy.sh\" #!/bin/bash echo \"OK\" \u003e /tmp/deployment.log --===============4393449873403893838== Content-Type: text/cloud-config; charset=\"us-ascii\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\"userdata.txt\" #cloud-config users: - name: testadm sudo: ALL=(ALL) NOPASSWD:ALL lock-passwd: true ssh-import-id: testadm ssh-authorized-keys: - `cat tempkey.pub` --===============4393449873403893838==-- EOF Create a proxy in RFC format for your tests:\n$ voms-proxy-init -rfc -voms fedcloud.egi.eu Your identity: /DC=es/DC=irisgrid/O=ifca/CN=Enol-Fernandez-delCastillo Creating temporary proxy .......................................................................... Done Contacting voms2.grid.cesnet.cz:15002 [/DC=org/DC=terena/DC=tcs/OU=Domain Control Validated/CN=voms2.grid.cesnet.cz] \"fedcloud.egi.eu\" Done Creating proxy .............................................................................. Done Your proxy is valid until Fri Nov 14 04:59:26 2014 OpenStack Compute checks (org.openstack.nova service type) Export the following variables on your shell (keystone URL can be obtained from GOCDB URL of the endpoint)\n$ export OS_AUTH_URL= \u003ckeystone URL\u003e $ export OS_AUTH_TYPE=v2voms $ export OS_X509_USER_PROXY=$X509_USER_PROXY Get the list of tenants supporting your proxy\n$ keystone_tenants Tenant id: 999f045cb1ff4684a15ebb338af69460 Tenant name: VO:fedcloud.egi.eu Enabled: True Description: VO fedcloud.egi.eu Export the tenant name as shown from the command in the following variable:\n$ export OS_PROJECT_NAME=\"VO:fedcloud.egi.eu\" Describe the available flavors, check also that the template ID provided by AppDB is listed:\n$ openstack flavor list +----+-----------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +----+-----------+-------+------+-----------+-------+-----------+ | 1 | m1.tiny | 512 | 0 | 0 | 1 | True | | 2 | m1.small | 2000 | 10 | 20 | 1 | True | | 3 | m1.medium | 4000 | 10 | 40 | 2 | True | | 4 | m1.large | 7000 | 20 | 80 | 4 | True | | 5 | m1.xlarge | 14000 | 30 | 160 | 8 | True | +----+-----------+-------+------+-----------+-------+-----------+ Describe available images, again check that the ID provided by AppDb is listed\n$ openstack image list +--------------------------------------+----------------------------------------------+ | ID | Name | +--------------------------------------+----------------------------------------------+ | 1414f242-d6d1-4a8c-8b26-8c0ada32f343 | IFCA Fedora Cloud 23 | | 8a700834-04b4-4e91-a3d5-9246ef95167e | LW Jupyter-R Ubuntu 14.04 | | 7f361fba-21d6-40ca-892d-17aa60b63a66 | IFCA CentOS 7 | | f3544cc8-421f-4d93-ac35-eba7fdc75329 | IFCA CentOS 6 | ... +--------------------------------------+----------------------------------------------+ Start a VM (using the flavor and images checked above and the context file created previously). The returned ID will be used in the following commands:\n$ openstack server create \\ --flavor \u003cflavor\u003e --image \u003cimage id\u003e \\ --user-data ctx.txt test +--------------------------------------+------------------------------------------------------+ | Field | Value | +--------------------------------------+------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | config_drive | | | created | 2016-03-01T13:07:10Z | | flavor | m1.tiny (1) | | hostId | | | id | 5d3ed7d6-d5ac-4f09-a353-0c2bd0fbd0ea | | image | IFCA CentOS 7 (7f361fba-21d6-40ca-892d-17aa60b63a66) | | key_name | None | | name | test | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | project_id | 999f045cb1ff4684a15ebb338af69460 | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2016-03-01T13:07:11Z | | user_id | a31b8c452b594369a49a8329103e241a | +--------------------------------------+------------------------------------------------------+ Check that the VM is active by describing it (it may take a few minutes):\n$ openstack server show \u003cvm id\u003e +---------------------+--------+ | Field | Value | +---------------------+--------+ (...) | OS-EXT-STS:vm_state | active | (...) +---------------------+--------+ If the VM does not have a public IP, you will need to get an IP for it\n$ openstack ip floating pool list +-------------+ | Name | +-------------+ | nova | +-------------+ $ openstack ip floating pool create \u003cpool name\u003e +-------------+----------------+ | Field | Value | +-------------+----------------+ | fixed_ip | None | | id | 1265 | | instance_id | None | | ip | 193.146.75.245 | | pool | nova | +-------------+----------------+ $ openstack ip floating add \u003cip\u003e \u003cvm id\u003e The VM should have now a public IP available when shown:\n$ openstack server show \u003cvm id\u003e +-----------+-------------------------------------+ | Field | Value | +-----------+-------------------------------------+ (...) | addresses | private=172.16.8.14, 193.146.75.245 | (...) +-----------+-------------------------------------+ ssh to the machine to the provided IP (the options avoid problems when different VMs have the same IP, don’t use them in production) and check that contextualization script was executed:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\ testadm@ \u003cip\u003e \"cat /tmp/deployment.log\" Warning: Permanently added '193.146.75.245' (ECDSA) to the list of known hosts. OK Create a storage volume:\n$ openstack volume create --size 1 test +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | created_at | 2016-03-01T13:29:44.392423 | | display_description | None | | display_name | test | | id | 8f46046d-cd9b-4219-9ce7-f0abe30ad992 | | properties | | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | None | +---------------------+--------------------------------------+ And show its status:\n$ openstack volume show \u003cvol id\u003e Attach it to the running VM:\n$ openstack server add volume \u003cvm id\u003e \u003cvol id\u003e And check it’s attached:\n$ openstack volume show 8f46046d-cd9b-4219-9ce7-f0abe30ad992 +---------------------+---------------------------------------------------------+ | Field | Value | +---------------------+---------------------------------------------------------+ | attachments | [{u'device': u'/dev/vdb', | | | u'server_id': u'21f6123f-926c-4816-b4fb-53df91907a63', | | | u'id': u'8f46046d-cd9b-4219-9ce7-f0abe30ad992', | | | u'volume_id': u'8f46046d-cd9b-4219-9ce7-f0abe30ad992'}] | | availability_zone | nova | | bootable | false | | created_at | 2016-03-01T13:29:44.000000 | | display_description | None | | display_name | test | | id | 8f46046d-cd9b-4219-9ce7-f0abe30ad992 | | properties | | | size | 1 | | snapshot_id | None | | source_volid | None | | status | in-use | | type | None | +---------------------+---------------------------------------------------------+ And login into the machine to create a filesystem, mount it, create a file, and umount:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\ testadm@ \u003cip_addr\u003e testadm $ sudo mke2fs \u003cdevice_id\u003e (...) testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ touch /mnt/test testadm $ ls -l /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Delete the VM:\n$ openstack server delete \u003cvm id\u003e And create a new one with the volume attached directly (substitute vdb for the same device name shown above):\n$ openstack server create \\ --flavor \u003cflavor\u003e --image \u003cimage id\u003e \\ --block-device-mapping vdb= \u003cvolume id\u003e \\ --user-data ctx.txt test +-----------------------------+---------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | veryverysecret | | config_drive | | | created | 2016-03-01T13:45:21Z | | flavor | m1.tiny (1) | | hostId | | | id | 10000d50-239b-4e86-bbd8-224143d6d346 | | image | Image for EGI Centos 6 [CentOS/6/KVM]_egi | | key_name | None | | name | test | | progress | 0 | | project_id | fffd98393bae4bf0acf66237c8f292ad | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2016-03-01T13:45:23Z | | user_id | 6c254b295af64644904a813db0d3d88a | +-----------------------------+---------------------------------------------+ Assign the public IP (if it does not have one already):\n$ openstack ip floating add \u003cip\u003e \u003cvm id\u003e And check that the volume is attached and usable:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\ testadm@ \u003cnew_vm_ip_addr\u003e testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ ls -ltr /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Finally delete VM, volume and IP address (NOTE: use the id of the IP as returned by openstack ip floating list\\!): $ openstack server delete \u003cvm id\u003e $ openstack volume delete \u003cvolume id\u003e $ openstack ip floating delete \u003cip id\u003e Cloud Storage (CDMI) checks (eu.egi.cloud.storage-management.cdmi service type) List the content of the repository:\n$ bcdmi -e \u003ccdmi_endpoint\u003e list / Create a test directory:\n$ bcdmi -e \u003ccdmi_endpoint\u003e mkdir test { \"completionStatus\": \"Complete\", \"objectName\": \"test/\", \"capabilitiesURI\": \"/cdmi/AUTH_df37f5b1ebc94604964c2854b9c0551f/cdmi_capabilities/container/\", \"parentURI\": \"/cdmi/AUTH_df37f5b1ebc94604964c2854b9c0551f/\", \"objectType\": \"application/cdmi-container\", \"metadata\": {} } Create a test file and upload it to the created directory:\n$ echo \"TEST OK\" \u003e testfile $ bcdmi -e \u003ccdmi_endpoint\u003e put -T testfile test/test.txt** Try to download back the file and compare to previous:\n$ bcdmi -e \u003ccdmi_endpoint\u003e get test/test.txt -o testfile.downloaded $ diff testfile testfile.downloaded \u0026\u0026 echo \"Files are equal\" \\ || echo \"Files differ\" Files are equal Delete the file:\n$ bcdmi -e \u003ccdmi_endpoint\u003e delete test/test.txt Check that the file is not present anymore\n$ bcdmi -e \u003ccdmi_endpoint\u003e list test/ Upload the file again and recursively delete the directory:\n$ bcdmi -e \u003ccdmi_endpoint\u003e put -T testfile test/test.txt $ bcdmi -e \u003ccdmi_endpoint\u003e delete -r test/ Check that the folder does not exist anymore\n$ bcdmi -e \u003ccdmi_endpoint\u003e list / See Site Certification GIIS Check HOWTO03.\nSee to Resource Centre registration and certification procedure PROC09.\n","categories":"","description":"Manual testing of services offered by a site","excerpt":"Manual testing of services offered by a site","ref":"/providers/operations-manuals/howto04_site_certification_manual_tests/","tags":"","title":"HOWTO04 Site Certification Manual tests"},{"body":"Transition to HEP SPEC, a new CPU benchmark Q1: Why adopting HEP SPEC 06? The traditional si2k CPU benchmark is now obsolete and it is time to move to HEP SPEC, a new CPU benchmark that will replace si2k and will become the reference benchmark for accounting purposes.\nDetailed description of the reasons are provided on the transition to a new CPU benchmarking unit for the WLCG.\nQ2: What is HEP SPEC 06? The HEP-SPEC06 benchmark is designed to scale with the performances of the high-energy physics codes on similar machines. The goal was to have an accuracy of ± 5% but for the moment the agreement is significantly higher.\nThe measurement of HEP-SPEC repeated on identical machines varies less than 1%. If the computing machines are similar, i.e. same processors and at least 2 GB per core, the results obtained are very close, within some percent, so that it is unnecessary to perform measures on all computing hosts. It is enough to do the measurement for one type of processor and consider it valid for all the machines with the same processor.\nIf you are using different OS and specially different compilers, the data will change.\nQ3: Where can I find information about HEP SPEC 06 measurements? Some example results are available on the HEPIX group-page, where one can see the differences between gcc3.4.x and gcc4.1.x.\nAdditional results tables are available from various EGI partners:\nGRIDPP If you don’t find your computing machine in that table, then it is better to try to do the measurement because extrapolating the results increases further the error.\nQ4: How can I run the HEP SPEC 06 benchmark? If you want to make HEP-SPEC06 on your own own, detailed instructions are available at CERN wiki.\nIn Short you need the following:\nA machine with any version of Linux compatible with Scientific Linux (RHEL, SL, SLC, CentOS) The gcc compiler should be installed Configuration files and run script (available as a gzipped tar archive from the CERN Wiki). The archive’s md5sum is 9fed92b8d515b88904705f76809c4028 A tar ball of the SPECcpu2006 DVD called SPEC2006_v11.tar.bz2 that should be in the same directory as the run script Q5: My site already adopted HEP SPEC 06. Do I still need to publish SpecInt2000? The transition to HEP-SPEC does not eliminate the need to publish the computing power in SpecInt2000 (due to backward compatibility with sites not publishing yet HEP-SPEC). In this case you may calculate the value SpecInt2000 starting from HEP-SPEC through the following relation:\nvalue_kSI2K = value_HEP-SPEC / 4 (or value_HEP-SPEC = 4 * value_kSI2K) For the GlueHostBenchmarkSI00 attribute in the GLUE v1.3 schema the following relation is easier to use:\nvalue_SI00 = value_HEP-SPEC * 250 rounded to the nearest integer Q6: How are HEP SPEC 06 results set in YAIM? The YAIM variable CE_OTHERDESCR is used to set the GlueHostProcessorOtherDescription attribute. The value of this variable MUST be defined in your site-info.def file as:\nCores=\u003cCE_LOGCPU/CE_PHYSICALCPU\u003e [, Benchmark=\u003cvalue\u003e-HEP-SPEC06] where the ratio CE_LOGCPU / CE_PHYSICALCPU means the average number of cores per physical CPU in a sub-cluster; in the case of (slightly) heterogeneous sub-clusters it could be non-integer. The second value of this attribute MUST be published only in the case the CPU power of the sub-cluster has been computed using the HEP-SPEC06 benchmark. The Benchmark value must be the average HEP_SPEC06 result per core, in the sub-cluster.\nThese variables are set in your site-info.def file. After this, the variables need to be published by the CE’s resource BDII, configured e.g. by standard YAIM commands.\nThe total CPU capacity of the cluster is computed as Benchmark * CE_LOGCPU.\n","categories":"","description":"Questions about Transition to HEP SPEC, a new CPU benchmark.","excerpt":"Questions about Transition to HEP SPEC, a new CPU benchmark.","ref":"/providers/operations-manuals/faq-hepspec06/","tags":"","title":"FAQ HEP SPEC 06"},{"body":"Overview This tutorial describes how to create a Virtual Machine in the EGI Federation, leveraging oidc-agent to retrieve ODIC tokens from EGI Check-in, fedcloudclient to simplify interacting with the EGI Cloud Compute service, terraform and Ansible to simplify deploying an infrastructure. EGI Dynamic DNS is also used to assign a domain name to the virtual machine, which can then be used to get a valid TLS certificate from Let’s Encrypt.\nStep 1: Signing up for an EGI Check-in account Create an EGI account with Check-in.\nStep 2: Enrolling to a Virtual Organisation Once your EGI account is ready you need to join a Virtual Organisation (VO). Here are the steps to join a VO. Explore the list of available VOs in the Operations Portal. We have a dedicated VO called vo.access.egi.eu for piloting purposes. If you are not sure about which VO to enrol to, please request access to the vo.access.egi.eu VO with your EGI account by visiting the enrolment URL. Check AppDB to see the list of Virtual Appliances and Resource Providers participating in the vo.access.egi.eu VO. AppDB is one of the service in the EGI Architecture.\nThis tutorial will assume you are using vo.access.egi.eu, adapt as required for your specific environment.\nStep 3: Creating a VM Once your membership to a VO has been approved you are ready to create your first Virtual Machine.\nThe OpenID Connect (OIDC) protocol is used to authenticate users and authorise access to Cloud Compute resources that are integrated with EGI Check-in.\nWhile it’s not mandatory, a convenient way to manage the OIDC token is to use oidc-agent.\nSetting up oidc-agent oidc-agent is a set of tools to manage OpenID Connect tokens and make them easily usable from the command line.\nInstall oidc-agent according to official documentation, once oidc-agent is installed it can be used to retrieve an OIDC access token from EGI Check-in.\n# Generating configuration for EGI Check-in $ oidc-gen --pub --issuer https://aai.egi.eu/auth/realms/egi \\ --scope \"email \\ eduperson_entitlement \\ eduperson_scoped_affiliation \\ eduperson_unique_id\" egi # Listing existing configuration $ oidc-add -l # Requesting an OIDC access token $ oidc-token egi # Exporting a variable with a Check-in OIDC access token to be used with OpenStack # XXX access tokens are short lived, relaunch command to obtain a new token # This is *not* required for following this tutorial, it's an example $ export OS_ACCESS_TOKEN=$(oidc-token egi) It’s possible to automatically start oidc-agent in your shell initialisation, example that can be added to ~/.bash_profile or ~/.zshrc:\nif command -v oidc-agent-service \u0026\u003e /dev/null eval $(oidc-agent-service use) # for fedcloudclient, selecting egi configuration generated with oidc-gen export OIDC_AGENT_ACCOUNT=egi fi When using oidc-agent-service, fedcloudclient will be able to automatically request a new access token from oidc-agent.\nSee full documentation.\nInstalling fedcloudclient and ansible fedcloudclient is an high-level Python package for a command-line client designed for interaction with the OpenStack services in the EGI infrastructure. The client can access various EGI services and can perform many tasks for users including managing access tokens, listing services, and mainly execute commands on OpenStack sites in EGI infrastructure.\nfedcloudclient can leverage oidc-agent if it’s installed and properly configured.\nfedcloudclient and openstackclient, the official OpenStack python client, will be used to interact with the EGI Cloud Compute service.\nRequired python dependencies are documented in a requirements.txt file (Ansible will be used at a later stage, but is installed at the same time):\nopenstackclient fedcloudclient ansible For keeping the main system tidy and isolating the environment, the python packages will be installed in a dedicated python virtualenv:\n# Creating an arbitrary directory where to store python virtual environments $ mkdir -p ~/.virtualenvs # Creating a python 3 virtual environment $ python3 -m venv ~/.virtualenvs/fedcloud # Activating the virtual environment $ source ~/.virtualenvs/fedcloud # Installing required python packages in the virtual environment $ pip install -r requirements.txt Identifying a suitable cloud site It’s possible to deploy an OpenStack Virtual Machine (VM) on any of the sites supporting the Virtual Organisations (VO) you are a member of.\nOnce fedcloudclient is installed it’s possible to get information about the OIDC token accessed via oidc-agent.\n# Listing the VO membership related to the OIDC access token $ fedcloud token list-vos In order to look for sites supporting a particular VO, you can use the EGI Application Database.\nYou can retrieve information from the AppDB about the sites supporting the vo.access.egi.eu VO.\nIn the following example, the IN2P3-IRES site supporting the vo.access.egi.eu VO will be used, see Step 2: Enrolling to a Virtual Organisation to request access.\nDeploying the Virtual Machine with terraform Instead of creating the server manually, it is possible to use terraform with EGI Cloud Compute.\nThe Terraform OpenStack provider provides official documentation.\nTerraform provides installation instructions for all usual platforms.\nOnce terraform is installed locally, we will create a deployment as documented in the following sections.\nSetting up the environment The OS_* variables that will be used by terraform can be generated using fedcloudclient.\n# Activating the virtual environment $ source ~/.virtualenvs/fedcloudclient/bin/activate # Exporting variable for VO and SITE to avoid having to repeat them $ export EGI_VO='vo.access.egi.eu' $ export EGI_SITE='IN2P3-IRES' eval $(fedcloud site env) # Obtaining an OS_TOKEN for terraform # XXX this breaks using openstackclient: use fedcloudclient # or unset OS_TOKEN before using openstackclient $ export OS_TOKEN=$(fedcloud openstack token issue --site \"$EGI_SITE\" \\ --vo \"$EGI_VO\" -j | jq -r '.[0].Result.id') Describing the terraform variables The main terraform configuration file, main.tf is using variables that have to be described in a vars.tf file:\n# Terraform variables definition # Values to be provided in a *.tfvars file passed on the command line variable \"internal_net_id\" { type = string description = \"The id of the internal network\" } variable \"public_ip_pool\" { type = string description = \"The name of the public IP address pool\" } variable \"image_id\" { type = string description = \"VM image id\" } variable \"flavor_id\" { type = string description = \"VM flavor id\" } variable \"security_groups\" { type = list(string) description = \"List of security groups\" } The SITE and VO specific values for those variables will be identified and documented in a $EGI_SITE.tfvars file.\nIdentifying the cloud resources Once the environment is properly configure, fedcloudclient is used to gather information and identify flavor, image, network and security groups for the site you want to use.\nfedcloud openstack currently requires an explicit --site parameter, this will be addressed in a future fedcloud release. In the meantime the $EGI_SITE environment variable can be reused using --site \"$EGI_SITE\".\n# Selecting an image $ fedcloud select image --image-specs \"Name =~ 'EGI.*22'\" # Selecting a flavor $ fedcloud select flavor --flavor-specs \"RAM\u003e=2096\" \\ --flavor-specs \"Disk \u003e 10\" --vcpus 2 # Identifying available networks $ fedcloud openstack --site \"$EGI_SITE\" network list $ fedcloud select network --network-specs default # Identifying security groups $ fedcloud openstack --site \"$EGI_SITE\" security group list # Listing rules of a specific security group $ fedcloud openstack --site \"$EGI_SITE\" security group rule list default Documenting the cloud resources for the selected site The chosen flavor, image, network and security group should be documented in a $EGI_SITE.tfvars file that will be passed as an argument to terraform commands.\nThe network configuration can be tricky, and is usually dependant on the site. For IN2P3-IRES, one has to request a floating IP from the public network IP pool ext-net, and assign this floating IP to the created instance. For another site it may not be needed, in that case the main.tf will have to be adjusted accordingly.\nSee the example IN2P3-IRES.tfvars below, to be adjusted according to the requirements and to the selected site and VO:\n# Internal network internal_net_id = \"7ae7b0ca-f122-4445-836a-5fb7af524dcb\" # Public IP pool for floating IPs public_ip_pool = \"ext-net\" # Flavor: m1.medium flavor_id = \"ab1fbd4c-324d-4155-bd0f-72f077f0ebce\" # Image for EGI CentOS 7 # https://appdb.egi.eu/store/vappliance/egi.centos.7 image_id = \"09093c70-f2bb-46b8-a87f-00e2cc0c8542\" # Image: EGI CentOS 8 # https://appdb.egi.eu/store/vappliance/egi.centos.8 # image_id = \"38ced5bf-bbfd-434b-ae41-3ab35d929aba\" # Image: EGI Ubuntu 22.04 # https://appdb.egi.eu/store/vappliance/egi.ubuntu.22.04 # image_id = \"fc6c83a3-845f-4f29-b44d-2584f0ca4177\" # Security groups security_groups = [\"default\"] Creating the main terraform deployment file To be more reusable, the main.tf configuration file is referencing variables described in the vars.tf file created previously, and will take the values from the $EGI_SITE.tfvars file passed as an argument to the terraform command.\n# Terraform versions and providers terraform { required_version = \"\u003e= 0.14.0\" required_providers { openstack = { source = \"terraform-provider-openstack/openstack\" version = \"~\u003e 1.35.0\" } } } # Allocate a floating IP from the public IP pool resource \"openstack_networking_floatingip_v2\" \"egi_vm_floatip_1\" { pool = var.public_ip_pool } # Creating the VM resource \"openstack_compute_instance_v2\" \"egi_vm\" { name = \"egi_test_vm\" image_id = var.image_id flavor_id = var.flavor_id security_groups = var.security_groups user_data = file(\"cloud-init.yaml\") network { uuid = var.internal_net_id } } # Attach the floating public IP to the created instance resource \"openstack_compute_floatingip_associate_v2\" \"egi_vm_fip_1\" { instance_id = \"${openstack_compute_instance_v2.egi_vm.id}\" floating_ip = \"${openstack_networking_floatingip_v2.egi_vm_floatip_1.address}\" } # Create inventory file for Ansible resource \"local_file\" \"hosts_cfg\" { content = templatefile(\"${path.module}/hosts.cfg.tpl\", { ui = \"${openstack_networking_floatingip_v2.egi_vm_floatip_1.address}\" } ) filename = \"./inventory/hosts.cfg\" } The last resource is relying on templatefile to populate the inventory file that will later be used by ansible.\nInitial configuration of the VM using cloud-init cloud-init is the industry standard multi-distribution method for cross-platform cloud instance initialization.\nThe initial configuration of the VM is done using a cloud-init.yaml file.\nThe curl call from the runcmd block in the cloud-init.yaml configuration below, will register the IP of the virtual machine in the DNS zone managed using the EGI Dynamic DNS service, allowing to access the virtual machine using a fully qualified hostname and allowing to retrieve a Let’s Encrypt certificate.\nPlease look at the EGI Dynamic DNS documentation for instructions on creating the configuration for a new host.\nThe users block in the cloud-init.yaml configuration below, will create a new user with password-less sudo access.\nWhile this egi user can only be accessed via the specified SSH key(s), setting a user password and requesting password verification for using sudo should be considered, as a compromise of this user account would mean a compromise of the complete virtual machine.\nReplace \u003cNSUPDATE_HOSTNAME\u003e, \u003cNSUPDATE_SECRET\u003e, \u003cSSH_AUTHORIZED_KEY\u003e (the content of your SSH public key) by the proper values.\n--- # cloud-config runcmd: - [ curl, \"https://\u003cNSUPDATE_HOSTNAME\u003e:\u003cNSUPDATE_SECRET\u003e@nsupdate.fedcloud.eu/nic/update\", ] users: - name: egi gecos: EGI primary_group: egi groups: users shell: /bin/bash sudo: ALL=(ALL) NOPASSWD:ALL ssh_authorized_keys: - \u003cSSH_AUTHORIZED_KEY\u003e packages: - vim package_update: true package_upgrade: true package_reboot_if_required: true Launching the terraform deployment Now that all the files have been created, it’s possible to deploy the infrastructure, currently only a single VM, but it can easily be extended to a more complex setup, using terraform:\n# Initialising working directory, install dependencies $ terraform init # Reviewing plan of actions for creating the infrastructure # Use relevant site-specific config file $ terraform plan --var-file=\"${EGI_SITE}.tfvars\" # Creating the infrastructure # Manual approval can be skipped using -auto-approve # The SERVER_ID will be printed (openstack_compute_instance_v2.scoreboard) $ terraform apply --var-file=\"${EGI_SITE}.tfvars\" # Wait a few minutes for the setup to be finalised # Connecting to the server using ssh $ ssh egi@$NSUPDATE_HOSTNAME From here you can extend the cloud-init.yaml and/or use Ansible to configure the remote machine, as well as doing manual work via SSH.\nDebugging terraform The token used by Terraform for accessing OpenStack is short lived, it will have to be renewed from time to time.\n# Creating a new token to access the OpenStack endpoint $ export OS_TOKEN=$(fedcloud openstack token issue --site \"$EGI_SITE\" \\ --vo \"$EGI_VO\" -j | jq -r '.[0].Result.id') It is possible to print a verbose/debug output to get details on interactions with the OpenStack endpoint.\n# Debugging $ OS_DEBUG=1 TF_LOG=DEBUG terraform apply --var-file=\"${EGI_SITE}.tfvars\" Destroying the resources created by terraform # Destroying the created infrastructure $ terraform destroy --var-file=\"${EGI_SITE}.tfvars\" Step 4: Using Ansible Ansible can be used to manage the configuration of the crated virtual machine.\nThe terraform deployment generated an Ansible inventory, inventory/hosts.cfg, that can directly be used by Ansible.\nConfigure a basic Ansible environment in the ansible.cfg file:\n[defaults] # Use user created using cloud-init.yml remote_user = egi # Use inventory file generated by terraform inventory = ./inventory/hosts.cfg [privilege_escalation] # Escalate privileges using password-less sudo become = yes Then you can verify that the Virtual Machine is accessible by Ansible:\n# Confirming ansible can reach the VM $ ansible all -m ping Once this works, you can create advanced playbooks to configure your deployed host(s).\nVarious Ansible roles are available in the egi-qc/ansible-playbooks repository and in the EGI Federation GitHub organisation.\nA style guide for writing Ansible roles is providing a skeleton that you can use fore creating new roles.\nAdditional resources Additional resources are available, and can help with addressing different use cases, or be used as a source of inspiration:\negi-qc/deployment-howtos: Deployment recipes extracted from Jenkins builds for the UMD and CMD products EGI-ILM/fedcloud-terraform: providing an advanced helper script allowing to interact with EGI Cloud Compute. EGI-ILM/automated-containers: providing documentation for automated on-demand execution of Docker containers Asking for help If you find issues please do not hesitate to contact us.\n","categories":"","description":"Step by step guide to automating the deployment using Ansible with Terraform, oidc-agent and fedcloudclient\n","excerpt":"Step by step guide to automating the deployment using Ansible with …","ref":"/users/tutorials/adhoc/oidc-agent-fedcloudclient-terraform/","tags":"","title":"Automate with oidc-agent, fedcloudclient, terraform and Ansible"},{"body":"Overview The training infrastructure is a resource pool within the EGI Federated Cloud infrastructure providing IaaS as well as access services (login, application catalogue and application management portal) for face-to-face events, online training courses or self-paced learning modules.\nThe training infrastructure is integrated with Check-in allowing trainers to generate short-lived user accounts for training participants. Such accounts can identify students individually, and for a limited lifetime - typically few hours or days, depending on the length of the training event - allow them to interact with the services.\nThe infrastructure currently includes enough capacity to scale up to class-room size audiences, approximately up to 100 participants.\nUsage models The training infrastructure is suitable for two types of courses:\nCloud computing courses: Such courses teach students about IaaS clouds and on how Virtual Appliances, Virtual Machines, block storage and other types of ’low level’ resources are managed. For such courses, the trainer does not need to deploy applications or online services in advance of the course. The applications/services will be deployed by the students themselves as training exercises. Such courses typically target developers or other rather technical members of scientific communities or projects. Scientific courses: Such courses teach scientists or developers about a specific software suite relevant for their work. For example a specific gene sequence analysis application, an earthquake visualisation tool, or a data processing pipeline. In this operational mode, the trainer deploys the domain specific application/tool on the training infrastructure before the training and the students interact directly with those applications/tools without even knowing where those are deployed and running. Depending on how computationally or data intensive the exercises are, multiple students may share a single software deployment instance, or each student can have their own. The configuration can be controlled by the trainer when the setup is deployed. In both cases the deployment of applications/tools/services can happen in the form of ‘Virtual Appliances’ (VAs), and block storage - the latter basically behaving like a virtual USB drive that can be attached/detached to VMs to provide data and storage space for applications.\nThe AppDB has a growing catalogue of Virtual Appliances that includes both basic applications (e.g. latest version of clean Linux distribution) and more specialised applications (e.g. Jupyter Notebook). The list of VAs available on the training infrastructure is configurable and listed in the training.egi.eu VO entry of AppDB.\nInfrastructure Manager Dashboard can be used as web interface for both trainers and students to deploy and manage VMs.\nAvailable resources The available resources are offered by a set of providers included in the training.egi.eu VO Operation Level Agreement (OLA). Check the document for the exact amount of resources and conditions of access for each provider.\nJoin the training infrastructure! Do you want to join as a resource provider? Please email at support \u003cat\u003e egi.eu. The list of providers and VAs is also discoverable in the training.egi.eu VO entry of AppDB. The VO is also described at the EGI Operations Portal training.egi.eu VO id card.\nBooking the infrastructure The infrastructure currently includes enough capacity to scale up to class-room size audiences, approximately up to 100 participants.\nDo you want to book the infrastructure for a course? Please send a request through our site.\n","categories":"","description":"The training infrastructure on EGI Cloud\n","excerpt":"The training infrastructure on EGI Cloud\n","ref":"/users/training/","tags":"","title":"Training Infrastructure"},{"body":"In the following sections you can find documentation for communities that are using EGI services. These will typically cover an introduction of the scientific or research community, the procedure to gain access to the resources of the community, and guidelines about how to participate in the community.\n","categories":"","description":"User documentation for EGI Communities\n","excerpt":"User documentation for EGI Communities\n","ref":"/users/getting-started/communities/","tags":"","title":"Community Specific Documentation"},{"body":"Overview This tutorial describes the EGI Data Transfer using FTS transfers services and WebFTS. In the following paragraphs you will learn how to:\nuse the FTS command-line client use the WebFTS web interface to perform data transfers between two Grid storage.\nPrerequisites As first step please make sure that you have installed the FTS client as described in Data Transfer, and in particular Clients for the command-line FTS and to have your certificate installed in your browser to use WebFTS browser based client.\nTo access services and resources in the EGI Federated Cloud, you will need:\nAn EGI Check-in account, you can sign up here Enrollment into a Virtual Organisation (VO) that has access to the services and resources you need FTS client usage Step 1 Configuration check To verify that everything is configured properly you can check with the following command and pointing to the certificates directly:\n$ fts-rest-whoami --key ~/.globus/userkey.pem --cert ~/.globus/usercert.pem \\ -s https://fts3-public.cern.ch:8446/ User DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu VO: JaneDoejd@egi.eu@tcs.terena.org VO id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Delegation id: XXXXXXXXXXXXXXXX Base id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX In general the commands can be used by specifying the user public and private key like shown in the example or by creating a proxy certificate as described in the following section.\nStep 2 Proxy creation As you have seen in the previous section it is possible to use the FTS commands by specifying the location of the user public and private key. With the use of voms-proxy-init it is possible to create a proxy certificate for the user. With this you don’t need to specify the location of the public and private key for each FTS command. When running voms-proxy-init it’s possible to specify the location of the public and private key. If this are not included as options, the tool expect to find them in:\n~/.globus/usercert.pem for the public key ~/.globus/userkey.pem for the private key with read access only for the owner Following is an example of running this command with the public and private key already setup as described:\n$ voms-proxy-init Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu Creating proxy ........................................... Done Your proxy is valid until Wed Aug 25 04:18:14 2021 The output of the command shows, a proxy certificate valid for 12 hours has been generated This is the default behaviour and can be usually increased, for example to 48 hours, with the following option:\n$ voms-proxy-init -valid 48:00 Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu Creating proxy ................................... Done Your proxy is valid until Thu Aug 26 16:23:01 2021 To verify for how long the proxy is still valid you can use the following command: command:\n$ voms-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu/CN=1451339003 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu type : RFC compliant proxy strength : 1024 bits path : /tmp/x509up_u1000 timeleft : 19:59:57 When the timeleft reaches zero the same command will produce the following message:\n$ fts-rest-whoami -s https://fts3-public.cern.ch:8446/ Error: Proxy expired! The last option that you need to use is specify the VO that you want to use for the proxy being created. In the following example the dteam VO has been used:\n$ voms-proxy-init --voms dteam Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jd@egi.eu Creating temporary proxy ................................................................... Done Contacting voms2.hellasgrid.gr:15004 [/C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr] \"dteam\" Done Creating proxy .............................................................................. Done Your proxy is valid until Wed Sep 8 04:37:07 2021 With a proxy now available for the user it is now possible to execute the FTS commands without specifying the public and private keys as it will be shown in the following sections.\nStep 3 Find the storage In general, the source and destination storage for a specific project should be already known. However, to discover the available source or destination endpoints to be used for a transfer, you can use the VAPOR service.\nOnce the page is loaded on the left column it is possible to filter by VO or scroll the list and click the desired VO as show in the following picture:\nOnce selected, you can see all the resources associated with the specific VO. In particular in this case you are interested in the information on the status, capacity, type of storage, etc. Following is a screenshot of the visualisation of the list of storage available to dteam.\nStep 4 Starting a transfer Once you have identified the source and destination storage needed for the transfer you can proceed with the transfer between the two endpoints. To do that you can use a command of this type, returning the job ID corresponding to the transfer that you started:\n$ fts-transfer-submit -s https://fts3-public.cern.ch:8446/ \\ --source https://dc2-grid-64.brunel.ac.uk/dpm/brunel.ac.uk/home/dteam/1M \\ --destination https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/dteam/1M \\ -o cfc884f8-1181-11ec-b9c7-fa163e5dcbe0 To check the status of the transfer you can use the returned job ID and use the following command specifying the server controlling the transfer, the source and the transfer itself:\n$ fts-transfer-status -s https://fts3-public.cern.ch:8446/ \\ cfc884f8-1181-11ec-b9c7-fa163e5dcbe0 FINISHED The last option -o specify that the file should be overwritten if present on the destination. If this option is not present and a file with the same name exists on the destination, the transfer itself will fail. If you use this option you should make sure that it is safe to do so.\nUsing the WebFTS Data Transfer interface Step 1 Access the WebFTS interface The WebFTS is accessible at this CERN FTS URL. Similarly to what has been done from the command-line interface you need to provide our private key for delegation of the credential. To do that you use the following command:\n$ openssl pkcs12 -in yourCert.p12 -nocerts -nodes | openssl rsa Enter Import Password: writing RSA key (...) Which extract the private key in RSA format and you can paste it in the windows that opens:\nAnd select the desired VO. Once the delegation is set it’s possible to move to the following steps.\nWarning please be careful and avoid sharing this information with any third party or saving this information in plain text. WebFTS uses the key to acquire a proxy certificate on your behalf as described previously and does not store it. Step 2 Submitting a transfer The tab Submit a transfer is divided in two parts in which is possible to add two endpoints that can be used both as source or destination. After adding the URL for the two endpoints, it is possible to browse and select the files and directories to be transferred. In the destination select the destination directory. In the following example the file 1MB has been selected and by simply clicking the arrow in the middle directing to the right you are instructing the system to copy the file from the storage and path on the left to the one on the right:\nSimilarly to what can be done with the command-line interface, there is the option to overwrite the destination if it already exists. To enable this option tick the Overwrite Files below the arrow for the transfer. On the top of the page is also shown a confirmation that the transfer has been submitted successfully. This same web page shows the status of the current transfers in the My jobs tab shown in the following screenshot:\nEach line on the list shows a different job. When clicked it will expand to show additional details. The reason for the failure of a job which can be seen by moving the mouse pointer over the File ID on the detailed view.\n","categories":"","description":"Use EGI Data transfer to handle data in grid storage\n","excerpt":"Use EGI Data transfer to handle data in grid storage\n","ref":"/users/tutorials/adhoc/data-transfer-grid-storage/","tags":"","title":"Data transfer with grid storage"},{"body":"Overview This tutorial describes how to submit High Throughput Compute (HTC) jobs using command-line.\nThis tutorial is meant for somewhat advanced users or the ones willing or needing to interact with the resources at a low level.\nPrerequisites To submit an EGI HTC job, you will have to:\nObtain an X.509 user certificate. The supported certificates are issued by Certification Authorities (CAs) part of the European Policy Management Authority for Grid Authentication (EUGridPMA), which is also part of the International Global Trust Federation (IGTF). Enrol into a VO having access to HTC resources. This tutorial will be using dteam a test Virtual Organisation that can be used by resource providers, commands should be adjusted to the appropriate VO.\nStep 1: getting access to a User Interface (UI) In order to interact with HTC resources, you should have access to a User Interface, often referred to as a UI. This software environment will provide all the tools required to interact with the different middleware, as different sites can be using different Computing Element (CE), such as HTCondorCE and ARC-CE (CREAM is a legacy software stack that is not officially supported).\nDifferent possibilities are available to access an UI:\nHaving access to an UI provided by/for your community, please get in touch with them about this. Deploying a UI, as documented below. Deploying an UI The UI is available as a package in the UMD software distribution, but it will also require additional software and configuration.\nIn order to help with deploying an UI, different solutions are possible:\nDeploying an UI manually, using the packages available from UMD repositories. You will need to install at least the ui meta-package, the IGTF distribution, and configure the system to use voms-client. Some Ansible roles are available in the EGI Federation GitHub organisation, mainly ansible-role-ui that should be used together with ansible-role-VOMS-client, providing software and material required for the authentication and authorisation, and ansible-role-umd configuring the software repositories from where all the software will be installed. The repository ui-deployment provides a terraform based deployment allowing to deploy a User Interface (UI) in a Cloud Compute virtual machine. This integrated deployment is based on the Ansible modules, and should be adjusted to your environment and needs. This tutorial is based on using a VM deployed using the ui-deployment repository, refer to the repository for detailed instructions on deploying the UI.\nStep 2: creating a VOMS proxy The Virtual Organization Membership Service (VOMS) enables Virtual Organisation (VO) access control in distributed services. A proxy allows limited delegation of rights, allowing remote services to securely interact with other resources and services on behalf of the user.\nConfiguring the system to use voms-client When using ansible-role-VOMS-client, the full environment has been setup for you, and there is no need for manual configuration.\nBefore being able to use voms-client, it is required to configure access to the VOMS server of the chosen VO, using the proper .vomses and .lsc files, based on the information available on the VOMS server of the specific VO.\nas an example with dteam, you can find the VOMS server address in the Operations Portal: https://operations-portal.egi.eu/vo/view/voname/dteam. Then looking at dteam VOMS configuration, you can create: /etc/vomses/dteam-voms2.hellasgrid.gr with the content of the VOMSES string. /etc/grid-security/vomsdir/dteam/voms2.hellasgrid.gr.lsc with the content for the LSC configuration. If you cannot edit content in /etc/vomses and /etc/grid-security/vomsdir, you can respectively use ~/.glite/vomses and ~/.glite/vomsdir. You may have to export X509_VOMSES and X509_VOMS_DIR in your shell, as documented on CERN’s twiki:\n$ export X509_VOMSES=~/.glite/vomses $ export X509_VOMS_DIR=~/.glite/vomsdir Preparing the X.509 credentials Once you have obtained an X.509 user certificate issued by a Certification Authority (CA) part of the International Global Trust Federation (IGTF), you should extract the certificate and private key, and add them to a ~/.globus directory.\nIf the X.509 certificate is in your browser’s keyring, you should export it to a passphrase protected .p12 file, then using openssl pkcs12 you can extract the required PEM files:\n# Creating and protecting ~/.globus directory $ mkdir -p ~/.globus $ chmod 750 ~/.globus # Extracting the certificate from the p12 file \"exported_cert.p12\" openssl pkcs12 -in exported_cert.p12 -out ~/.globus/usercert.pem -clcerts -nokeys # Adjusting rights on the user certificate $ chmod 640 ~/.globus/usercert.pem # Extracting the certificate key from the p12, protecting it with a passphrase $ openssl pkcs12 -in exported_cert.p12 -out ~/.globus/userkey.pem -nocerts # Adjusting rights on the certificate key $ chmod 400 ~/.globus/userkey.pem If you are using a certificate provided by the GÉANT Trusted Certificate Service (TCS), in addition to the official documentation provided by your organisation, you may be interested by looking at the following documentation:\nGeneration 4 GEANT Trusted Certificate Service TCS, covering how to get and install your credentials, addressing potential issues with an improper .p12. Highly recommended. SUNET TCS 2020- Information for administrators, an exhaustive documentation mainly for administrators but also covering client-related aspects. Using voms-client Once the configuration for the VOMS client has been completed, and when the X.509 credentials have been prepared, you can create a VOMS proxy for dteam VO:\n# Creating the proxy $ voms-proxy-init -voms dteam Enter GRID pass phrase for this identity: Contacting voms2.hellasgrid.gr:15004 [/C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr] \"dteam\"... Remote VOMS server contacted successfully. Created proxy in /tmp/x509up_u1001. Your proxy is valid until Wed Oct 26 23:27:30 CEST 2022 # Checking the proxy $ voms-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe/CN=123456319 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe type : RFC3820 compliant impersonation proxy strength : 2048 path : /tmp/x509up_u1001 timeleft : 11:58:48 key usage : Digital Signature, Key Encipherment References VOMS Documentation USG Proxy Certificates Step 3: identifying available resources It is possible to identify available resources by querying the information system.\nTwo Computing Element (CE) “flavours” are used in production:\nHTCondorCE, a Compute Entrypoint (CE) based on HTCondor. ARC-CE, the ARC Compute Element (CE). In this section we will document querying the EGI Information System to retrieve information about the available resources.\nTip It’s also possible to use VAPOR to query resources using a graphical interface. Use case: identifying all the Computing Elements supporting the dteam VO As documented in the pages covering the querying of the Information System, in GLUE 2.0, the access granted to a given VO to a compute or storage resource, is published using the GLUE2Share and GLUE2Policy objects. The GLUE2ComputingShare object specifically documents sharing of compute resources.\n# Querying GLUE2ComputingShare for all the computing resources available to dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*))' It is possible to filter for the different types of Computing Element, and select only specific attributes.\nOnce you will have selected a site, using the ldapsearch queries from the next subsections, you will be able to send jobs to them, as documented in the Step4: submitting and managing jobs.\nInformation The following Computing Elements have been arbitrarily chosen, like due to the site location, available resources, prior experience, or any other reason, and will be used in this tutorial:\nHTCondorCE: condorce1.ciemat.es ARC-CE: alex4.nipne.ro Looking for a HTCondorCE for dteam # Information about the HTCondorCE supporting dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*)(GLUE2ComputingShareComputingEndpointForeignKey=*HTCondorCE*))' \\ GLUE2ShareEndpointForeignKey \\ GLUE2ShareID \\ GLUE2ComputingShareTotalJobs \\ GLUE2ComputingShareRunningJobs \\ GLUE2ComputingShareWaitingJobs # XXX Most HTCondorCE have the Endpoint ending in `HTCondorCE`, but some have # it ending with `htcondorce`, like in this tutorial for `condorce1.ciemat.es` # XXX The attribute `GLUE2ComputingShareComputingEndpointForeignKey` is matched $ in a case sensitive way, and the filter should be updated to match them $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*)(|(GLUE2ComputingShareComputingEndpointForeignKey=*HTCondorCE*)(GLUE2ComputingShareComputingEndpointForeignKey=*htcondorce*)))' \\ GLUE2ShareEndpointForeignKey \\ GLUE2ShareID \\ GLUE2ComputingShareTotalJobs \\ GLUE2ComputingShareRunningJobs \\ GLUE2ComputingShareWaitingJobs As it was decided to go for condorce1.ciemat.es, the information about the CE can be requested using the following request, filtering on the GLUE2ShareID from the previous query: grid_dteam_condorce1.ciemat.es_ComputingElement.\n# condor_submit needs CE (condorce1.ciemat.es) and pool (condorce1.ciemat.es:9619) $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*grid_dteam_condorce1.ciemat.es_ComputingElement*))' \\ GLUE2ShareID \\ GLUE2ShareDescription \\ GLUE2ComputingShareExecutionEnvironmentForeignKey \\ GLUE2EntityOtherInfo HTCondorCE are usually running on port 9619, this is confirmed by the results. Based on those results, it’s possible to guess the following parameters that will have to be used when submitting the job:\nCE Name: condorce1.ciemat.es (reported in GLUE2ComputingShareExecutionEnvironmentForeignKey: condorce1.ciemat.es) CE Pool: condorce1.ciemat.es:9619 (reported in GLUE2EntityOtherInfo: HTCondorCEId=condorce1.ciemat.es:9619/htcondorce-condor-group_dteam) Looking for an ARC-CE for dteam // jscpd:ignore-start\n# Information about the ARC-CE supporting dteam VO $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*dteam*)(GLUE2ComputingShareComputingEndpointForeignKey=*urn:ogf*))' \\ GLUE2ComputingShareComputingEndpointForeignKey \\ GLUE2ShareEndpointForeignKey \\ GLUE2ComputingShareTotalJobs \\ GLUE2ComputingShareRunningJobs \\ GLUE2ComputingShareWaitingJobs As it was decided to go for alex4.nipne.ro, the information about the CE can be requested using the following request, filtering on the GLUE2ShareID from the previous query: urn:ogf:ComputingShare:alex4.nipne.ro:dteam_dteam.\n# arcsub needs CE name (alex4.nipne.ro) $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 \\ -b \"GLUE2GroupID=grid,o=glue\" \\ '(\u0026(objectClass=GLUE2ComputingShare)(GLUE2ShareID=*urn:ogf:ComputingShare:alex4.nipne.ro:dteam_dteam*))' \\ GLUE2ShareID \\ GLUE2ShareDescription \\ GLUE2ComputingShareComputingServiceForeignKey \\ GLUE2ComputingShareExecutionEnvironmentForeignKey // jscpd:ignore-end\nCE Name: alex4.nipne.ro (exported from the GLUE2ComputingShareComputingServiceForeignKey: urn:ogf:ComputingService:alex4.nipne.ro:arex) Step 4: submitting and managing jobs To an HTCondorCE Computing Element The HTCondor-CE software is a Compute Entrypoint (CE) based on HTCondor for sites that are part of a larger computing grid (e.g. EGI, Open Science Grid (OSG)).\nThe condor package will install all the required dependencies.\nyum install condor Condor will use the VOMS proxy created earlier.\nWhile HTCondor provides an official HTCodnor Quick Start Guide, the main steps for managing a job will be highlighted below.\nCreate env.sub, the compute job to be executed on the remote Computing Element:\nexecutable = /usr/bin/env log = env.log output = outfile.txt error = errors.txt should_transfer_files = Yes when_to_transfer_output = ON_EXIT queue The format of the submit description file, is documented in HTCondor manual and condor_submit man page.\nSubmission of a job with the -spool option causes HTCondor to spool all input files, the job event log, and any proxy across a connection to the machine where the condor_schedd daemon is running. After spooling these files, the machine from which the job is submitted may disconnect from the network or modify its local copies of the spooled files.\nSubmit job using condor_submit:\n# Submitting a job, spooling input and output files to $ condor_submit --spool --name condorce1.ciemat.es \\ --pool condorce1.ciemat.es:9619 env.sub Submitting job(s). 1 job(s) submitted to cluster 97412. Monitor the status of the job using condor_q:\n# Checking the status of a specific job $ condor_q --name condorce1.ciemat.es --pool condorce1.ciemat.es:9619 97412 -- Schedd: condorce1.ciemat.es : \u003c192.101.161.188:9619?... @ 10/26/22 16:31:00 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS dteam050 ID: 97412 10/26 16:21 _ _ _ 1 97412.0 Total for query: 1 jobs; 1 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 884 jobs; 412 completed, 0 removed, 200 idle, 259 running, 13 held, 0 suspended # Checking the status of all jobs running on that $ condor_q --name condorce1.ciemat.es --pool condorce1.ciemat.es:9619 -- Schedd: condorce1.ciemat.es : \u003c192.101.161.188:9619?... @ 10/26/22 16:25:03 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS dteam050 ID: 97400 10/26 15:46 _ _ _ 1 97400.0 dteam050 ID: 97412 10/26 16:21 _ _ _ 1 97412.0 Total for query: 2 jobs; 2 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for dteam050: 2 jobs; 2 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 883 jobs; 411 completed, 0 removed, 200 idle, 259 running, 13 held, 0 suspended It is also possible to view the output of a running job using condor_tail.\nDownload the job output to the UI using condor_transfer_data:\n# Retrieving the output of a specific job $ condor_transfer_data -name condorce1.ciemat.es -pool condorce1.ciemat.es:9619 97412.0 Fetching data files... References HTCondor Quick Start Guide HTCondor Application Programming Interfaces (APIs) OSG Site Documentation: HTCondor-CE Overview To an ARC-CE Computing Element ARC Compute Element (CE) is a Grid front-end on top of a conventional computing resource (e.g. a Linux cluster or a standalone workstation). ARC CE is sometimes also called ARC server.\nWhile there is an official documentation on How to submit a job, the main steps will be documented below.\nIf you haven’t already generated a credential with voms-proxy-init, you can generate a proxy certificate using ARC’s own tool: arcproxy, which is using the same credentials as voms-proxy-init, and should produce an equivalent proxy. To do this you first need to prepare your X.509 credentials.\n# Generating a proxy for ARC $ arcproxy --voms dteam Enter pass phrase for private key: Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe Contacting VOMS server (named dteam): voms2.hellasgrid.gr on port: 15004 Proxy generation succeeded Your proxy is valid until: 2022-10-27 02:23:52 Create testjob.xrsl, a test job expressed in xRSL, showing environment where it will run:\n\u0026( executable = \"/usr/bin/env\" ) ( jobname = \"arctest\" ) ( stdout = \"stdout\" ) ( join = \"yes\" ) ( gmlog = \"gmlog\" ) Then review the ARC CE information and send the job using arcsub:\n# Getting info about the selected CE # Example CE: alex4.nipne.ro:2811/nordugrid-SLURM-dteam $ ldapsearch -x -LLL -H ldap://lcg-bdii.egi.eu:2170 -b \"Mds-Vo-Name=local,o=grid\" \\ '(\u0026(objectClass=GlueCE)(GlueCEUniqueID=alex4.nipne.ro:2811/nordugrid-SLURM-dteam))' # Submitting the job, the JOB_ID will be written on the output $ arcsub --jobdescrfile testjob.xrsl --computing-element alex4.nipne.ro Job submitted with jobid: gsiftp://alex4.nipne.ro:2811/jobs/.... # Export JOB_ID to be used for other commands $ JOB_ID=\"gsiftp://alex4.nipne.ro:2811/jobs/....\" Then you can use arcstat to monitor the job:\n# Monitoring the status of the job $ arcstat \"$JOB_ID\" # Use -l parameter with arcstat to get more information on the status of the Job $ arcstat -l \"$JOB_ID\" The jobs will be in state Finished once completed.\nYou can finally retrieve the output of a finished job using arcget:\n# Retrieve output files of the finished job, removing them from the server $ arcget \"$JOB_ID\" Results stored at: Ow7KmRKv71nuvw3Vp3UrRNqABFKDmABFKDmfJKDmABFKDmoeT5zn Jobs processed: 1, successfully retrieved: 1, successfully cleaned: 1 Instead of manually selecting a site, it’s possible to do some automatic selection from CE registered in a central registry, such as nordugrid.org.\n# Automatic selection on the CE in the nordugrid.org registry $ arcsub --jobdescrfile testjob.xrsl --registry nordugrid.org (...) gsiftp://vm3.tier2.hep.manchester.ac.uk:2811/jobs/... JOB_ID=\"gsiftp://vm3.tier2.hep.manchester.ac.uk:2811/jobs/..\" # Use -l parameter to get more information on the status of the Job $ arcstat -l \"$JOB_ID\" References ARC: submit a job ARC client tools To a CREAM Computing Element The CREAM (Computing Resource Execution And Management) Service is a simple, lightweight service that implements all the operations at the Computing Element (CE) level.\nThe first step is to prepare a JDL as testjob.jdl. The CREAM JDL Guide, documents the creation of the JDL:\n[ Type = \"Job\"; JobType = \"Normal\"; Executable = \"/usr/bin/env\"; StdOutput = \"output.txt\"; StdError = \"error.txt\"; OutputSandbox = {\"output.txt\", \"error.txt\"}; ] Then you can submit, monitor and retrieve the output of the job.\nThose commands are broken on the UI installed from UMD via our Ansible module, but are provided here as a reference, and for users having access to UI maintained by or for their community and providing the required commands.\n# Submitting a job, job ID would be printed to the output $ glite-ce-job-submit 'lpsc-cream-ce.in2p3.fr:8443/cream-pbs-dteam' testjob.jdl # Use a variable with the job ID to be reused later JOB_ID='...' # Monitoring the job $ glite-ce-job-status \"$JOB_ID\" # Retrieving the output of the job $ glite-ce-job-output \"$JOB_ID\" References CREAM User’s guide Via the EGI Workload Manager The EGI Workload Manager is a service provided to the EGI community to efficiently manage and distribute computing workloads on the EGI infrastructure.\nUsing the Workload Manger web interface or the DIRAC command-line interface (CLI) is documented in the EGI Workload Manager.\nTroubleshooting In case you receive errors when submitting jobs to Computing Elements, it may be possible that the service is in Downtime for an intervention/upgrade or there is an issue already reported by the EGI Monitoring System ARGO.\nTo check the information about downtimes or issues you can browse the ARGO Issues Page and, as shown in the figure below, check if there are active Downtimes for the service you are trying to use (By clicking on the Downtime button) or issues (By clicking on the CRITICAL button).\nAsking for help If you find issues please do not hesitate to contact us.\n","categories":"","description":"Submitting High Throughput Compute jobs\n","excerpt":"Submitting High Throughput Compute jobs\n","ref":"/users/tutorials/adhoc/htc-job-submission/","tags":"","title":"Submitting HTC Jobs"},{"body":"Overview This tutorial describes the EGI Data Transfer using FTS transfers services and WebFTS. In the following paragraphs you will learn how to:\nuse the FTS command-line client use the WebFTS web interface to perform data transfers between a Grid storage and object storage or between two object storage.\nWarning This procedure has been tested with the FTS client 3.11. Older version do not support all the options necessary. To install the latest version please add the FTS3 Production repository to your configuration and update the client Prerequisites As first step please make sure that you have installed the FTS client as described in Data Transfer, and in particular Clients for the command-line FTS and to have your certificate installed in your browser to use WebFTS browser based client.\nTo access services and resources in the EGI Federated Cloud, you will need:\nAn EGI Check-in account, you can sign up here Enrollment into a Virtual Organisation (VO) that has access to the services and resources you need An Object Storage for which you need to have all the credentials available (any S3 compatible storage should work) Permission to add the Object Storage credential to the FTS server or alternatively for this operation you may contact support at egi.eu. FTS client usage Step 1 Configuration check and Proxy creation For this two steps please refer to the “Data transfer with grid storage” tutorial.\nStep 2 Find the storage As for the “Data transfer with grid storage” tutorial you can look for the available storage on VAPOR service while the Object Store can be one created as described in the Object Storage section or trough a provider such as Amazon, Azure, etc\nStep 3 Add the Object Storage credential to the FTS server Following is an example of the command that can be used to add the Object Store credential to the FTS server. The fist step is to register the Object Storage. The name of the storage, is S3: + the domain part of the URL (for example https://s3.cl2.du.cesnet.cz -\u003e S3:s3.cl2.du.cesnet.cz)\n$ curl -E \"${X509_USER_PROXY}\" \\ --cacert \"${X509_USER_PROXY}\" \\ --capath \"/etc/grid-security/certificates\" \\ https://fts3devel01.cern.ch:8446/config/cloud_storage \\ -H \"Content-Type: application/json\" \\ -X POST \\ -d '{\"storage_name\":\"S3:s3.cl2.du.cesnet.cz\"}' And then, add the keys, so the requests can be signed.\n$ curl -E \"${X509_USER_PROXY}\" \\ --cacert \"${X509_USER_PROXY}\" \\ --capath \"/etc/grid-security/certificates\" \\ \"https://fts3devel01.cern.ch:8446/config/cloud_storage/S3:s3.cl2.du.cesnet.cz\" \\ -H \"Content-Type: application/json\" \\ -X POST \\ --data @config.json Where config.json is a JSON file with the following content:\n{ \"vo_name\": \"/dteam/Role=NULL/Capability=NULL\", \"access_key\": \"ACCESS_KEY\", \"secret_key\": \"SECRET_KEY\" } Where ACCESS_KEY and SECRET KEY are the corresponding key necessary to access the object storage. See also the S3 Support pages on the FTS docs pages.\nStep 4 Transfer between a grid storage and an object storage For the grid storage to use please follow the details described in the section “Find the storage” of the “Data transfer with grid storage” tutorial. In the following examples, an object storage available in s3://s3.cl2.du.cesnet.cz/ with an already available bucket is used. To manage the object storage is possible to use any compatible tool. Following will be only shown an example of transfer.\n$ fts-transfer-submit --s3alternate \\ -s https://fts3-public.cern.ch:8446 \\ https://dc2-grid-64.brunel.ac.uk/dpm/brunel.ac.uk/home/dteam/1M \\ s3://s3.cl2.du.cesnet.cz/bucket-name/1M.3 247b7ca2-4c4d-11ec-84d0-fa163e5dcbe0 The command returns a job ID that we can use to check the status of the transfer itself:\n$ fts-transfer-status -d \\ -s https://fts3-public.cern.ch:8446 \\ 247b7ca2-4c4d-11ec-84d0-fa163e5dcbe0 FINISHED Step 5 Transfer between two object storage We can also use the data transfer service to perform transfers between two object storage. In this case the transfer will be controlled by the FTS service you can use a command like:\n$ fts-transfer-submit --s3alternate \\ -s https://fts3-public.cern.ch:8446 \\ s3://s3.cl2.du.cesnet.cz/bucket-name/1M.3 \\ s3://s3.cl2.du.cesnet.cz/bucket-name/A c1d4a8e6-4c81-11ec-8926-fa163e5dcbe0 In this case too we can verify the status of the transfer with the same command as before using the new job ID.\n$ fts-transfer-status -d \\ -s https://fts3-public.cern.ch:8446 \\ c1d4a8e6-4c81-11ec-8926-fa163e5dcbe0 FINISHED Step 6 Web interface An alternative way to check the status of the job is to use the FTS web interface to see using one the FTS servers and appending the job ID returned from one of the previous examples. For instance the link below will show the status of the specified job:\nhttps://fts-public-003.cern.ch:8449/fts3/ftsmon/#/job/247b7ca2-4c4d-11ec-84d0-fa163e5dcbe0 ","categories":"","description":"Use EGI Data transfer to handle data in object storage\n","excerpt":"Use EGI Data transfer to handle data in object storage\n","ref":"/users/tutorials/adhoc/data-transfer-object-storage/","tags":"","title":"Data transfer with object storage"},{"body":"Overview This tutorial describes how to configure access to MinIO console using EGI Check-in as external OpenID Connect Identity Provider.\nPrerequisites Be familiar with the steps to configure an OpenID Connect Service Provider with EGI Check-in.\nThis tutorial assumes that:\nYou have a valid EGI ID (account), you can sign up here. You are a member of a Virtual Organisation. You have deployed MinIO and have access to the console as admin. Step 1: Get the OIDC entitlement for the Virtual Organisation See the page about getting tokens from Check-in. You will get a curl command to get your OIDC entitlements.\nSelect the entitlement for the Virtual Organisation that you want to enable access to. For example, here is the entitlement for the vo.access.egi.eu Virtual Organisation:\nurn:mace:egi.eu:group:vo.access.egi.eu:role=member#aai.egi.eu Step 2: Configure a new policy in MinIO console Go to https://\u003cminio-console-endpoint\u003e/identity/policies and create a new policy, where:\nThe name of the policy is the OIDC entitlement obtained in Step 1. The policy is configured with the value below: { \"Version\": \"2023-02-20\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::${jwt:preferred_username}-*\" ] } ] } Note Pay attention to the use of:\n${jwt:preferred_username}*\nin the Amazon Resource Name string:\narn:aws:s3:::${jwt:preferred_username}*\nThis will allow every user full control of their own buckets under s3://preferred_username- prefix, and restrict access to other users’ buckets.\nSee the official MinIO documentation for more details.\nStep 3: Configure EGI Check-in as external OpenID Connect Identity Provider Step 3.1: Add service to EGI Federation Registry Follow the steps to configure an OpenID Connect Service Provider with EGI Check-in.\nApart from selecting OIDC Service as the protocol when adding this service to the EGI Federation Registry, you should use the below as the Redirect URI:\nhttps://\u003cminio-console-endpoint\u003e/oauth_callback Step 3.2: Configure MinIO console Now configure EGI Check-in as external OpenID Connect Identity Provider for MinIO.\nGo to https://\u003cminio-console-endpoint\u003e/settings/configurations/identity_openid and set the following values\nConfig URL: get Provider configuration value in the docs. Client ID: get value from Step 3.1 above. Secret ID: get value from Step 3.1 above. Claim Name: eduperson_entitlement Claim UserInfo: ON Redirect URI: https://\u003cminio-console-endpoint\u003e/oauth_callback Scopes: eduperson_entitlement,profile Next you need to restart MinIO for the changes to take effect.\nStep 4: Access MinIO console with Check-in All going well, when you restart MinIO and go back to the console endpoint you should see Login with SSO login button.\nPlease note that you will only be able to create buckets with the \u003cpreferred_username\u003e- prefix. preferred_username is an OIDC claim whose value can be obtained using the same curl command as the one in Step 1 above. For example, user John Doe will have a preferred_username similar to jdoe. With the configuration detailed on this tutorial, he will be able to create buckets with the following names:\ns3://johndoe-private-bucket s3://johndoe-public-bucket etc. Step 5: Command-line interface Although the MinIO web interface allows you to manage buckets, advanced users may want to use the command-line interface. MinIO comes with its own client, but it also works with S3-compatible tools. Go to the web interface and create access and secret keys that you can use from the CLI.\nhttps://\u003cminio-console-endpoint\u003e/access-keys Known issues Below is the list known issues when working with this setup.\nWarning When using the web console via Check-in, often you get this error message:\nThe Access Key Id you provided does not exist in our records.\nand suddenly you are not able to see your buckets or access keys.\nSimply log out and log back in and the issue disappears.\nWarning MinIO admin is not able to login via the web console after configuring the OpenID Connect Identity Provider. Check this issue on GitHub for more details. ","categories":"","description":"Configure access to MinIO console with EGI Check-in Virtual Organisations.\n","excerpt":"Configure access to MinIO console with EGI Check-in Virtual …","ref":"/providers/online-storage/minio-oidc/","tags":"","title":"Access MinIO with EGI Check-in Virtual Organisations"},{"body":"This section contains guidelines for software development to be considered when developing a product for the EGI Federation.\nNote Those guidelines are providing a set of aspects to consider together with some reference documentation, and are not meant to be exhaustive, further research is welcome. Licensing Adopt an OSI-approved license; we recommend a business compatible license such as MIT or Apache 2.0 The license should provide unlimited access rights to the EGI Community Source code access Maintain the source code in a publicly-accessible software repository like GitHub You can request to use the EGI Federation GitHub organisation If using another repository, a copy can be kept synchronized under the EGI Federation GitHub organisation All releases must be tagged appropriately Code style Style guidelines must be defined and documented. A general style guide may be made available by EGI as a default. If you are extended an existing software component, then you must use the code style defined by the related product team If you are developing a new component, then you must use the code style practices for the programming language of your choice Code style compliance should be checked by automated means for every change Best practices The industry best practices should be adopted As far as possible adopt 12 factor application pattern Configuration Management modules to deploy and configure the products should be provided and distributed through the corresponding distribution channels Ansible is recommended, roles can be hosted under the EGI Federation organization in Ansible Galaxy Security best practices Security best practices must be taken into account Security-related aspects must be considered from the beginning Security issues must be addressed in priority and following the EGI SVG recommendations and must take into account the points mentioned in the SVG Secure Coding and Software Security Checklist The Open Web Application Security Project (OWASP) provides extensive documentation, standards (such as ASVS) and tools to ensure that your software has capabilities to defend against common attacks. Suggested material and references Microsoft® Open Source Software (OSS) Secure Supply Chain (SSC) Framework Simplified Requirements Secure Software Development Framework | CSRC NIST: Cybersecurity Cybersecurity \u0026 Infrastructure Security Agency Microsoft Security Development Lifecycle (SDL) Introduction to Software Security (video course by Trusted CI, WISC, UAB) Tooling and telemetry If the project is an application or an infrastructure component, it should follow as close as possible the monitoring guidelines set by the Site Reliability Engineer book Testing Unit tests should be provided Unit testing should be automated Code coverage should be computed as part of the continuous integration When possible functional and integration tests should be automated If it’s not possible for some components the product team should provide report about those tests for the new releases Code Review A team of code reviewers shall be specified for each project Changes must be reviewed by the code review team prior to be merged using a Pull Request-like workflow Documentation Documentation must be treated like code Written in a plain text-based format Management in a repository and versioning Markdown and reStructuredText formats are recommended The documentation for an EGI Service should be submitted for publishing on the EGI Documentation site using the related GitHub repository A “Community First” approach should be followed Contributing, onboarding and community guidelines should be available from the start of the project Documentation should be available for Developers Administrators (Deployment and administration) End users Artefacts Release and delivery Artefacts should be tagged according to Semantic Versioning or to Calendar Versioning that can be more appropriate for OS images Artefacts should have a DOI and associated short writeup (see Documentation above) Artefacts should be published in publicly available repositories EGI Application Database can be used for this UMD can be used to distribute middleware components It should be possible to automatically build production-grade distribution artefacts from the repository using provided build scripts and files Where appropriate, native packages for EGI Federation-supported Operating System should be provided: CentOS 7 (rpm) Ubuntu 20.04 LTS (deb) Containers are accepted Containers must be compatible with EGI Cloud services Request for information You can ask for more information about them by contacting us via our site.\n","categories":"","description":"Guidelines for software development","excerpt":"Guidelines for software development","ref":"/internal/guidelines-software-development/","tags":"","title":"Guidelines for software development"},{"body":"","categories":"","description":"","excerpt":"","ref":"/_footer/","tags":"","title":""},{"body":"The EGI documentation is written in Markdown, uses the Docsy theme, and is built using Hugo.\nNote The documentation covering the EGI Services is maintained by the EGI Community and coordinated by the EGI Foundation. Everyone is invited to participate by following the Contributing Guide. ","categories":"","description":"About EGI Documentation","excerpt":"About EGI Documentation","ref":"/about/","tags":"","title":"About"},{"body":" Welcome to the EGI Documentation! EGI is an international e-Infrastructure providing advanced computing and data analytics services for research and innovation. The EGI Federation offers a wide range of services for compute, storage, data management, training, and support. This is the start page for the user and provider documentation of the EGI Services. For Users How to access and use EGI services to benefit from advanced computing. For Providers How to join the EGI Federation and operate services for advanced computing. Learn about EGI The EGI Federation is supporting many different user communities and open science projects. Contribute! We use a GitHub contributions workflow, new authors are always welcome! Follow on Twitter Find out about new EGI features and see how our users are using EGI services. ","categories":"","description":"Documentation related to EGI activities","excerpt":"Documentation related to EGI activities","ref":"/","tags":"","title":"EGI documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"Support for EGI services is available through the EGI Helpdesk. To get started with the EGI Helpdesk check out the documentation.\nEvery ticket in the Helpdesk is assigned to a Support Unit. If you are unsure about what Support Unit to choose, the ticket will be assigned to the default, first-level TPM Support Unit (Ticket Processing Manager). Then TPM will reassign the ticket to the relevant, second-level Support Unit. To accelerate response time and obtain a quick assistance, we recommend selecting the User Community Support Unit when you submit your ticket. For that, please search for User Community Support in the Group box as show in the image below:\nNote Support is also available by contacting us at support \u003cat\u003e egi.eu. ","categories":"","description":"Support for EGI services","excerpt":"Support for EGI services","ref":"/support/","tags":"","title":"Support"}]