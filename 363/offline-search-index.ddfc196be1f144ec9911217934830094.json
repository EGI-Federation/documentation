



























































































































[{"body":"Jupyter is an extensible environment that supports different programming languages. For the EGI service we have enabled two different environments:\n the default environment including commonly used languages like Python, R, and Julia. the MATLAB environment, for running MATLAB.  After logging into the service, you will be shown a form for selecting the environment, pick the desired one and click start\n","categories":"","description":"Available Environments in EGI Notebooks\n","excerpt":"Available Environments in EGI Notebooks\n","ref":"/users/notebooks/kernels/","tags":"","title":"Notebooks Environments"},{"body":"The EGI Notebooks service relies on the following technologies to provide its functionality:\n JupyterHub with custom EGI Check-in oauthentication configured to spawn pods on Kubernetes. Kubernetes as container orchestration platform running on top of EGI Cloud resources. Within the service it is in charge of managing the allocated resources and providing the right abstraction to deploy the containers that build the service. Resources are provided by EGI Federated Cloud providers, including persistent storage for users notebooks. CA authority to allocate recognised certificates for the HTTPS server Prometheus for monitoring resource consumption. Specific EGI hooks for monitoring, accounting and backup. VO-Specific storage/Big data facilities or any pluggable tools into the notebooks environment can be added to community specific instances.  Kubernetes A Kubernetes (k8s) cluster deployed into a resource provider is in charge of managing the containers that will provide the service. On this cluster there are:\n 1 master node that manages the whole cluster Support for load balancer or alternatively 1 or more edge nodes with a public IP and corresponding public DNS name (e.g. notebooks.egi.eu) where a k8s ingress HTTP reverse proxy redirects requests from user to other components of the service. The HTTP server has a valid certificate from one CA recognised at most browsers (e.g. Let's Encrypt). 1 or more nodes that host the JupyterHub server, the notebooks servers where the users will run their notebooks. Hub is deployed using the JupyterHub helm charts. These nodes should have enough capacity to run as many concurrent user notebooks as needed. Main constraint is usually memory. Support for Kubernetes PersistentVolumeClaims for storing the persistent folders. Default EGI-Notebooks installation uses NFS, but any other volume type with ReadWriteOnce capabilities can be used. Prometheus installation to monitor the usage of resources so accounting records are generated.  All communication with the user goes via HTTPS and the service only needs a publicly accessible entry point (public IP with resolvable name)\nMonitoring and accounting are provided by hooking into the respective monitoring and accounting EGI services.\nThere are no specific hardware requirements and the whole environment can run on commodity virtual machines.\nEGI Customisations EGI Notebooks is deployed as a set of customisations of the JupyterHub helm charts.\nAuthentication EGI Check-in can be easily configured as a OAuth2.0 provider for JupyterHub's oauthenticator. See below a sample configuration for the helm chart using Check-in production environment:\nhub:extraEnv:OAUTH2_AUTHORIZE_URL:https://aai.egi.eu/oidc/authorizeOAUTH2_TOKEN_URL:https://aai.egi.eu/oidc/tokenOAUTH_CALLBACK_URL:https://\u003cyour host\u003e/hub/oauth_callbackauth:type:customcustom:className:oauthenticator.generic.GenericOAuthenticatorconfig:login_service:\"EGI Check-in\"client_id:\"\u003cyour client id\u003e\"client_secret:\"\u003cyour client secret\u003e\"oauth_callback_url:\"https://\u003cyour host\u003e/hub/oauth_callback\"username_key:\"sub\"token_url:\"https://aai.egi.eu/oidc/token\"userdata_url:\"https://aai.egi.eu/oidc/userinfo\"scope:[\"openid\",\"profile\",\"email\",\"eduperson_scoped_affiliation\",\"eduperson_entitlement\"]To simplify the configuration and to add refresh capabilities to the credentials, we have created a new EGI Check-in authenticator that can be configued as follows:\nauth:state:enabled:truecryptoKey:\u003csome unique crypto key\u003etype:customcustom:className:oauthenticator.egicheckin.EGICheckinAuthenticatorconfig:client_id:\"\u003cyour client id\u003e\"client_secret:\"\u003cyour client secret\u003e\"oauth_callback_url:\"https://\u003cyour host\u003e/hub/oauth_callback\"scope:- openid- profile- email- offline_access- eduperson_scoped_affiliation- eduperson_entitlementThe auth.state configuration allows to store refresh tokens for the users that will allow to get up-to-date valid credentials as needed.\nAccounting Warning This is Work in progress, expect changes!  Accounting module generates VM-like accounting records for each of the notebooks started at the service. It's available as a helm chart that can be deployed in the same namespace as the JupyterHub chart. The only needed configuration for the chart is an IGTF-recognised certificate for the host registered in GOCDB as accounting.\nssm:hostcert:|-\u003chostcert\u003ehostkey:|-\u003chostkey\u003eMonitoring Monitoring is performed by trying to execute a user notebook every hour. This is accomplished by registering a new service in the hub that has admin permissions. Monitoring is then deployed as a helm chart that must be deployed in the same namespace as the JupyterHub chart. Configuration of JupyterHub must include this section:\nhub:services:status:url:\"http://status-web/\"admin:trueapiToken:\"\u003ca unique API token\u003e\"Likewise the monitoring chart is configured as follows:\nservice:api_token:\"\u003csame API token as above\u003e\"Docker images Our service relies on custom images for the hub and the single-user notebooks. Dockerfiles are available at EGI Notebooks images git repository and automatically build for every commit pushed to the repository to eginotebooks @ dockerhub.\nHub image Builds from the JupyterHub k8s-hub image and adds:\n EGI and D4Science authenticators EGISpawner EGI look and feel for the login page  Single-user image Builds from Jupyter datasicence-notebook and adds a wide range of libraries as requested by users of the services. We are currently looking into alternatives for better managing this image with CVMFS as a possible solution.\nSample helm configuration If you want to build your own EGI Notebooks instance, you can start from the following sample configuration and adapt to your needs by setting:\n secret tokens (for proxy.secretToken, hub.services.status.api_token, auth.state.cryptoKey). They can be generated with openssl rand -hex 32. A valid hostname (\u003cyour notebooks host\u003e below) that resolves to your Kubernetes Ingress Valid EGI Check-in client credentials, these can be obtained by creating a new client at EGI AAI OpenID Connect Provider. When moving to EGI Check-in production environment, make sure to remove the hub.extraEnv.EGICHECKIN_HOST variable.  ---proxy:secretToken:\"\u003csome secret\u003e\"service:type:NodePortingress:enabled:trueannotations:kubernetes.io/tls-acme:\"true\"hosts:[\u003cyour notebooks host\u003e]tls:- hosts:- \u003cyour notebooks host\u003esecretName:acme-tls-notebooksenabled:truehosts:[\u003cyour notebooks host\u003e]singleuser:storage:capacity:1Gidynamic:pvcNameTemplate:claim-{userid}{servername}volumeNameTemplate:vol-{userid}{servername}storageAccessModes:[\"ReadWriteMany\"]memory:limit:1Gguarantee:512Mcpu:limit:2guarantee:.02defaultUrl:\"/lab\"image:name:eginotebooks/single-usertag:c1b2a2ahub:image:name:eginotebooks/hubtag:c1b2a2aextraConfig:enable-lab:|-c.KubeSpawner.cmd = ['jupyter-labhub']volume-handling:|-from egispawner.spawner import EGISpawner c.JupyterHub.spawner_class = EGISpawnerextraEnv:JUPYTER_ENABLE_LAB:1EGICHECKIN_HOST:aai-dev.egi.euservices:status:url:\"http://status-web/\"admin:trueapi_token:\"\u003cmonitor token\u003e\"auth:type:customstate:enabled:truecryptoKey:\"\u003ca unique crypto key\u003e\"admin:access:trueusers:[\u003clist of EGI Check-in users with admin powers\u003e]custom:className:oauthenticator.egicheckin.EGICheckinAuthenticatorconfig:client_id:\"\u003cyour egi checkin_client_id\u003e\"client_secret:\"\u003cyour egi checkin_client_secret\u003e\"oauth_callback_url:\"https://\u003cyour notebooks host\u003e/hub/oauth_callback\"enable_auth_state:truescope:- openid- profile- email- offline_access- eduperson_scoped_affiliation- eduperson_entitlement","categories":"","description":"Internal Service Architecture","excerpt":"Internal Service Architecture","ref":"/providers/notebooks/architecture/","tags":"","title":"Architecture"},{"body":"The EGI Federated Cloud (FedCloud) is a multi-national cloud system that integrates community, private and/or public clouds into a scalable computing platform for research. The Federation pools resources from a heterogeneous set of cloud providers using a single authentication and authorisation framework that allows the portability of workloads across multiple providers, and enables bringing computing to data. The current implementation is focused on Infrastructure-as-a-Service (IaaS) services, but can be easily applied to Platform-as-a-Service (PaaS) and Software-as-a-Servcice (SaaS) layers.\nEach resource centre of the federated infrastructure operates a Cloud Management Framework (CMF) according to its own preferences and constraints and joins the federation by integrating this CMF with components of the EGI service portfolio. CMFs must at least be integrated with EGI Authentication and Authorization Infrastructure (AAI) so users can access services with a single identity, integration with other components and APIs to be provided are agreed by the community the resource centre provides services to.\nEGI follows a Service Integration and Management (SIAM) approach to manage the federation with processes that cover the different aspects of the IT Service Management. Providers in the federation keep complete control of their services and resources. EGI creates Virtual Organizations (VOs) for each research community, and EGI VO Operation Level Agreements (OLAs) establish a reliable, trust-based communication channel between the community and the providers, by agreeing on the services, their levels and the types of support.\nNote EGI VO OLAs are not legal contracts but, as agreements, they outline the clear intentions to collaborate and support research.  Federated IaaS The EGI FedCloud IaaS resource centres deploy a Cloud Management Framework (CMF) that provide users with an API-based service for management of Virtual Machines and associated Block Storage to enable persistence and Networks to enable connectivity of the Virtual Machines (VMs) among themselves and third party resources.\nThe IaaS federation is a thin layer that brings the providers together with:\n Federated authentication Resource discovery Central VM image catalogue Usage accounting Monitoring  The IaaS capabilities (VM, block storage, network management, etc.) must be provided via community agreed APIs (OpenStack and/or OCCI are supported at the moment) that allow integration with EGI Check-in for authentication and authorisation of users.\nNote Those providers that limit the interaction to web dashboards and do not expose APIs to direct consumption for users cannot be considered part of the EGI IaaS Cloud services.  Users and Community platforms built on top of the EGI IaaS can interact with the cloud providers at three different layers:\n Directly using the IaaS APIs or CLIs to manage individual resources. This option is recommended for pre-existing use cases with requirements on specific APIs. Using federated access tools that allow managing the complexity of dealing with different providers in a uniform way. These tools include:  Provisioning systems allow users to define infrastructure as code, then manage and combine resources from different providers, thus enabling the portability of application deployments between them (e.g. Infrastructure Manager or Terraform), and Cloud brokers provide matchmaking for workloads to available providers (e.g. the INDIGO-DataCloud Orchestrator).   Using the VMOps dashboard.  EGI provides ready-to-use software components to enable the federation for OpenStack. These components rely on public APIs of the IaaS system and use Check-in accounts for authenticating into the provider.\nImplementation Authentication and authorization Federated identity ensures that users of the federation can use a single account for accessing the resources.\nOpenID Connect Providers of the EGI Cloud support authentication with OAuth2 tokens provided by Check-in OpenID Connect Identity provider. Support builds on the AAI guide for SPs with detailed configuration provided at the EGI IaaS Service providers documentation.\nThe integration relies on the OpenStack Keystone OS-FEDERATION API.\nInformation discovery The Configuration Database contains the list of resource centres and their endpoints, while the AppDB Information System collects this information in a central service for discovery, providing a real-time view of the actual capabilities of federation participants (can be used by both human users and machine services).\nConfiguration Database The EGI Configuration Database is used to catalogue the static information of the production infrastructure topology (e.g. the list of resource centres and their endpoints).\nTo allow resource providers to expose IaaS federation endpoints, the following service types are avialable:\n org.openstack.horizon org.openstack.nova org.openstack.swift eu.egi.cloud.accounting eu.egi.cloud.vm-management.occi eu.egi.cloud.vm-metadata.marketplace  All providers must enter cloud service endpoints into the Configuration Database to enable integration with EGI.\nThe Cloud Info Provider extracts information from the resource centres using their native APIs and formats it following Glue, an OGC recommended standard. This information is pushed to the Argo Messaging System and consumed by AppDB to provide a central information discovery service that aggregates several other sources of information about the infrastructure.\nVirtual Machine Image management In a distributed, federated IaaS service, users need solutions for efficiently managing and distributing their VM images across multiple resource providers. EGI provides a catalogue of VM images (VMIs) that allows any user to share their VMI, and communities to select those VMIs relevant for distribution across providers. These images are automatically replicated at the providers supporting the community and converted as needed to ensure the correct instantiation when used.\nAppDB includes a Virtual Appliance Marketplace supporting Virtual Appliances (VAs), which are clean and lean virtual machine images designed to run on a virtualisation platform, that provide a software solution out-of-the-box, ready to be used with minimal or no set-up.\nAppDB allows representatives of research communities (VOs) to generate a VM image list that resource centres subscribe to. The subscription enables the periodic download, conversion and storage of those images to the image repository of the indicated resource centres, using HEPiX image list format. cloudkeeper provides this automated synchronisation between AppDB and the cloud provider.\nAccounting Federated Accounting provides an integrated view about resource/service usage: it pulls together usage information from the federated sites and services, integrates the data and presents them in such a way that both individual users as well as whole communities can monitor their own resource/service usage across the whole federation.\nUsage of resources is gathered centrally using EGI Accounting repository and available for visualisation at EGI Accounting portal.\nCloud Usage Record The federated cloud task force has agreed on a Cloud Usage Record, which inherits from the OGF Usage Record. This record defines the data that resource providers must send to EGI’s central Accounting repository.\nVersion 0.4 of the Cloud Accounting Usage Record was agreed at the FedCloud Face to Face in Amsterdam in January 2015. A summary table of the format is shown below:\n   Cloud Usage Record Property Type Null Definition     VMUUID varchar(255) No Virtual Machine's Universally Unique Identifier concatenation of CurrentTime, SiteName and MachineName   SiteName varchar(255) No GOCDB SiteName - GOCDB now has cloud service types and a cloud-only site is allowed.   CloudComputeService (NEW) varchar(255)  Name identifying cloud resource within the site. Allows multiple cloud resources within a sitei.e. a level of granularity.   MachineName varchar(255) No VM ID - the site name for the VM   LocalUserId varchar(255)  Local username   LocalGroupId varchar(255)  Local group name   GlobalUserName varchar(255)  Global identity of user (certificate DN)   FQAN varchar(255)  Use if VOs part of authorization mechanism   Status varchar(255)  Completion status - completed, started or suspended   StartTime datetime  Must be set when Status = started   EndTime datetime  Set to NULL until Status = completed   SuspendDuration datetime  Set when Status = suspended (Timestamp)   WallDuration int  WallClock time - actual time used   CpuDuration int  CPU time consumed (Duration)   CpuCount int  Number of CPUs allocated   NetworkType varchar(255)  Needs clarifying   NetworkInbound int  GB received   NetworkOutbound int  GB sent   PublicIPCount (NEW) int  Number of public IP addresses assigned to VM Not used.   Memory int  Memory allocated to the VM   Disk int  Size in GB allocated to the VM   BenchmarkType (NEW) varchar(255)  Name of benchmark used for normalization of times (eg HEPSPEC06)   Benchmark (NEW) Decimal  Value of benchmark of VM using ServiceLevelType benchmark’   StorageRecordId varchar(255)  Link to other associated storage record Need to check feasibility   ImageId varchar(255)  Every image has a unique ID associated with it. For images from the EGI FedCloud AppDB this should be VMCATCHER_EVENT_AD_MPURI; for images from other repositories it should be a vmcatcher equivalent; for local images - local identifier of the image.   CloudType varchar(255)   Type of cloud infrastructure: OpenNebula; OpenStack; Synnefo; etc.    Public IP Usage Record The fedcloud task force has agreed on an IP Usage Record. The format uses many of the same fields as the Cloud Usage Record. The Usage Record should be a \"snapshot\" of the number of IPs currently assigned to a user. A table defining v0.2 of the format is shown below:\n   Cloud Usage Record Property Type Null Definition Notes     MeasurementTime datetime No The time the usage was recorded. In the message format, must be a UNIX timestamp, i.e. the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970)   SiteName varchar(255) No The GOCDB site assigning the IP    CloudComputeService varchar(255) Yes See Cloud Usage Record    CloudType varchar(255) No See Cloud Usage Record    LocalUser varchar(255) No See Cloud Usage Record    LocalGroup varchar(255) No See Cloud Usage Record    GlobalUserName varchar(255) No See Cloud Usage Record    FQAN varchar(255) No See Cloud Usage Record    IPVersion byte No 4 or 6    IPCount int(11) No The number of IP addresses of IPVersion this user currently assigned to them     A JSON schema defining a valid Public IP Usage message can be found at: https://github.com/apel/apel/blob/9476bd86424f6162c3b87b6daf6b4270ceb8fea6/apel/db/__init__.py\nGPU Usage Record The fedcloud task force has agreed on an GPU Usage Record. The format uses many of the same fields as the Cloud Usage Record. A table defining Draft 4 – 24/02/2021 is shown below:\n   GPU Usage Record Property Type Null Definition      MeasurementMonth int No The month/year the reported usage should be assigned to. If the month/year is the current month/year, the usage should be up to the point of reporting.    MeasurementYear int No     AssociatedRecordType varchar(255) No The context in which the reported usage was used. I.e. “cloud” for an accelerator attached to a VM.    AssociatedRecord varchar(255) No VMUUID if AssociatedRecordType is “cloud”    GlobalUserName varchar(255) Yes See the definition of your AssociatedRecordType    FQAN varchar(255) No See the definition of your AssociatedRecordType    SiteName varchar(255) No See the definition of your AssociatedRecordType    Count decimal No A count of the Accelerators attached to the VM. At the moment Accelerators are not shared among VMs but it will change when Accelerator virtualization is applied, so we should have the field at decimal type instead of integer (e.g. Count = 0.5 when it is shared between two VMs).    Cores int(11) Yes Total number of cores. i.e. So if an Accelerator has 64 cores and a VM has 2 like that attached then we would report: Count=2 and Processors=128    ActiveDuration int(11) Yes Actual usage duration of the Accelerator in seconds for the given month/year (in case some systems could report actual usage). At the moment, ActiveDuration will be the same as the AvailableDuration due to the limitation of currently used technologies (impossible to get ACCELERATOR utilization from outside of the VM, no ACCELERATOR hot-plug into running VM) but it may change in near future so it is good to have the fields separately. Set to AvailableDuration if AcitveDuration is omitted from the record    AvailableDuration int(11) No Time accelerator was available in seconds for the given month/year (Wall)Time that a GPU was attached to a VM.    BenchmarkType varchar(255) Yes Name of benchmark used for normalization of times    Benchmark decimal Yes Value of benchmark of Accelerator    Type varchar(255) No High level description of accelerator, i.e. GPU, FPGA, Other    Model varchar(255) Yes model number, spec, some other concept that 2 ACCELERATORs with the same number of cores might be different etc     APEL and accounting portal Once generated, records are delivered to the central accounting repository using APEL SSM (Secure STOMP Messenger). SSM client packages can be obtained at https://apel.github.io. A Cloud Accounting Summary Usage Record has also been defined and summaries created on a daily basis from all the accounting records received from the Resource Providers are sent to the EGI Accounting Portal. The Accounting portal also runs SSM to receive these summaries and provides a web view of the accounting data received from the Resource Providers.\ncASO delivers an implmentation of the extrator probes for OpenStack.\nMonitoring The endpoints published in the Configuration Database are monitored via ARGO. Specific probes to check functionality and availability of services must be provided by service developers.\nThe current set of probes used for monitoring IaaS resources consists of:\n OCCI probes (eu.egi.cloud.OCCI-VM and eu.egi.cloud.OCCI-Context): OCCI-VM creates an instance of a given image by using OCCI, checks its status and deletes it afterwards. OCCI-Context checks that the OCCI interfaces correctly supports the standard and the FedCloud contextualization extension. Accounting probe (eu.egi.cloud.APEL-Pub): Checks if the cloud resource is publishing data to the Accounting repository TCP checks (org.nagios.Broker-TCP, org.nagios.CDMI-TCP, org.nagios.OCCI-TCP and org.nagios.CloudBDII-Check): Basic TCP checks for services. VM Marketplace probe (eu.egi.cloud.AppDB-Update): gets a predetermined image list from AppDB and checks its update interval. Perun probe (eu.egi.cloud.Perun-Check): connects to the server and checks the status by using internal Perun interface  Roadmap The TCB-Cloud board defines the roadmap for the technical evolution of the EGI Cloud. All the components are continuously maintained to:\n Improve their programmability, providing complete APIs specification in adequate format for facilitating the generation clients (e.g. following the OpenAPI initiative and Swagger). Lower the barriers to integrate and operate resource centres in the federation by a) minimizing the number of components used; b) contributing code to upstream distributions; and c) use only public APIs of the Cloud Management Frameworks.  Currently the EGI FedCloud TaskForce is focused on moving to a central operations model, where providers only need to integrate their system with EGI Check-in but do not need to deploy and configure the different tools (accounting, discovery, VMI management, etc.) locally but delegate this to a central EGI team.\n","categories":"","description":"The architecture of the EGI Federation\n","excerpt":"The architecture of the EGI Federation\n","ref":"/users/getting-started/architecture/","tags":"","title":"EGI Architecture"},{"body":"What is it? Block storage provides block-level storage volumes for use within virtual machines (VMs). Block storage volumes are raw, unformatted block devices, which can be mounted as devices in VMs.\nBlock storage volumes that are attached to a VM are exposed as storage volumes that persist independently from the life of the VM, and need to be explicitly destroyed when data is not needed anymore. Users can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive).\nThe content of block storage volumes can be accessed only from within the VM they are mounted to, and they can be mounted to a single VM at any given time.\nThe main features of block storage:\n Block storage is recommended for data that must be quickly accessible and requires long-term persistence. Block storage volumes are well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes. Users can create point-in-time snapshots of block storage volumes, which protect data for long-term durability, and they can be used as the starting point for new block storage volumes.  Note Block storage volumes can can only be mounted to VMs running at the same provider where the block storage is located.  Note Block storage usage is accounted for the entire block storage device, regardless how much of it is actually used.  Important There is a limit on the number of block storage devices you can attach on a VM and there is a limit to the maximum size of such virtual disks. These values will depend on the particular provider and your SLA.  Manage volumes The block storage in the EGI Cloud is offered via OpenStack deployments that implement the Cinder service.\nUsers can manage block storage using the OpenStack Horizon dashboard of a provider, from a command-line interface (CLI), or via the OpenStack Block Storage API.\nNote Please refer to OCCI How-To for details about using the legacy OCCI interface to manage block storage.  Manage from the command-line Multiple command-line interfaces (CLIs) are available to manage block storage:\n The OpenStack CLI The FedCloud Client is a high-level CLI for interaction with the EGI Federated Cloud (recommended) The Cinder CLI has some advanced features and administrative commands that are not available through the OpenStack CLI  The main FedCloud commands for managing volumes are detailed below.\nNote For more information see the documentation about volume management.  List volumes For example, to list the volumes in the site IN2P3-IRES via the Pilot VO (vo.access.egi.eu), use the following FedCloud command:\nLinux / Mac   Windows   PowerShell    To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n$ export EGI_SITE=IN2P3-IRES $ export EGI_VO=vo.access.egi.eu $ fedcloud openstack volume list Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+   To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e set EGI_SITE=IN2P3-IRES \u003e set EGI_VO=vo.access.egi.eu \u003e fedcloud openstack volume list Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+   To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e $Env:EGI_SITE=\"IN2P3-IRES\" \u003e $Env:EGI_VO=\"vo.access.egi.eu\" \u003e fedcloud openstack volume list Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list +---------------------------+--------+-----------+------+--------------------------------+ | ID | Name | Status | Size | Attached to | +---------------------------+--------+-----------+------+--------------------------------+ | aa711296-5cff-46ac-bbe... | Matlab | in-use | 50 | Attached to Moodle on /dev/vdb | | b0abc762-a503-129d-3c1... | | available | 30 | | +---------------------------+--------+-----------+------+--------------------------------+    Create volume To create a new volume use the FedCloud command below:\n$ fedcloud openstack volume create --size 10 my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume create --size 10 my-volume +---------------------+------------------------------------------------------------------+ | Field | Value | +---------------------+------------------------------------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-08-05T13:10:42.000000 | | description | None | | encrypted | False | | id | a711296-5cff-46ac-bbe3-58e00712ee3e | | multiattach | False | | name | my-volume | | properties | | | replication_status | None | | size | 10 | | snapshot_id | None | | source_volid | None | | status | creating | | type | ceph | | updated_at | None | | user_id | 1a3ef4b64714f86ac71f1c9512345678c157a94ae1b37f167b6a663baa3b915b | +---------------------+------------------------------------------------------------------+ The status of the new volume will probably be returned as creating. To check if the volume finished creating, look at the details of the volume, or list only the newly created volume (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 10 | | +--------------------------------------+-----------+-----------+------+-------------+ When the status of the volume is available the volume is ready to be attached to a VM.\nSee volume details To view details of a volume use the FedCloud command below:\nTip The volume can be specified either by its ID or by its name (if it has one).  $ fedcloud openstack volume show Matlab Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume show Matlab +------------------------------+------------------------------------------------------------+ | Field | Value | +------------------------------+------------------------------------------------------------+ | attachments | [{'server_id': 'a4ab00a1-d458-41a9-a091-ee707bc9357e', | | | 'attachment_id': '291111d3-f43c-494c-b64c-5c0abcda3d37', | | | 'attached_at': '2021-08-05T08:50:19.000000', | | | 'host_name': 'sbgcsrv17.in2p3.fr', | | | 'volume_id': '9a4000fb-0bcc-47e8-96fb-85a222295402', | | | 'device': '/dev/vdb', | | | 'id': '9a4000fb-0bcc-47e8-96fb-85a222295402'}] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2021-08-05T08:48:41.000000 | | description | | | encrypted | False | | id | 9a4000fb-0bcc-47e8-96fb-85a222295402 | | multiattach | False | | name | Matlab | | os-vol-tenant-attr:tenant_id | 7a910xxxxae74ed9yyyy7497zzzz9499 | | properties | | | replication_status | None | | size | 50 | | snapshot_id | None | | source_volid | None | | status | in-use | | type | ceph | | updated_at | 2021-08-05T08:50:20.000000 | | user_id | babzz8c3b4cxxxx6286dacyyyy01e5a3 | +------------------------------+------------------------------------------------------------+ Attach volume to VM Mapping block devices to VMs is described in detail in the OpenStack documentation.\nTo attach a volume to a VM use the FedCloud command below:\nNote To be able to attach a volume to a VM, the volume must not be attached to any VM (volume status must be available).  Caution The optional --device argument to specify the device name in the VM should not be used. It does not work properly, and will be removed in the near future.  $ fedcloud openstack server add volume my-server my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: server add volume my-server my-volume You can check that the volume got attached to the VM (and with what device name) by either looking at the details of the volume, the details of the VM, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-----------------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-----------------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | in-use | 10 | my-server on /dev/vdc | +--------------------------------------+-----------+-----------+------+-----------------------+ When the volume status is in-use the volume is attached to a VM, and it can be used from the VM.\nDetach volume from VM To detach a volume from a VM use the FedCloud command below:\n$ fedcloud openstack server remove volume my-server my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: server remove volume my-server my-volume You can check that the volume got detached by either looking at the details of the volume, the details of the VM, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 10 | | +--------------------------------------+-----------+-----------+------+-------------+ When the volume status is available the volume is not attached to any VM.\nResize volume To resize a volume set its size property to the desired size. For example, if there is a volume named my-volume with a size of 10GB, it can be resized using the FedCloud command below:\nTip Other volume properties can be altered in the same way.  Note To be able to resize a volume, the volume must not be attached to any VM (volume status must be available), unless the volume driver supports in-use extend.  $ fedcloud openstack volume set --size 20 my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume set --size 20 my-volume You can check that the volume got resized by either looking at the details of the volume, or by listing only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+-----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | available | 20 | | +--------------------------------------+-----------+-----------+------+-------------+ Snapshot a volume Users can create snapshots of a volume that can later be used to create other volumes or to rollback to a precedent point in time. Volume snapshots are pointers in the read-write history of a volume.\nTo take a snapshot of a volume, use the FedCloud command below:\nTip See also the documentation about the other snapshot management commands.  Note To be able to create a snapshot of a volume, the volume must not be attached to any VM (volume status must be available). To create a snapshot while the volume is attached to a VM, use the --force command flag, but be aware that there may be inconsistencies if the VM’s OS is not aware of the snapshot being taken.  $ fedcloud openstack volume snapshot create --volume my-volume my-snapshot --force Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume snapshot create --volume my-volume my-snapshot +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2021-08-06T16:08:37.631226 | | description | None | | id | 15149b42-032f-4cec-b6f3-41aa5c958081 | | name | my-snapshot | | properties | | | size | 10 | | status | creating | | updated_at | None | | volume_id | aa711296-5cff-46ac-bbe3-58e0712ee3e9 | +-------------+--------------------------------------+ The status of the snapshot will probably be returned as creating. To check if the snapshot is ready, look at the details of the snapshot, or list only the newly created snapshot (filter by snapshot name or ID):\n$ fedcloud openstack volume snapshot list --name my-snapshot Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume snapshot list --name my-snapshot +--------------------------------------+-------------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+-------------+-------------+-----------+------+ | 15149b42-032f-4cec-b6f3-41aa5c958081 | my-snapshot | None | available | 10 | +--------------------------------------+-------------+-------------+-----------+------+ When the status of the snapshot is available the snapshot is ready, and can be used to create new volumes or to restore the source volume to the point-in-time when the snapshot was taken.\nBackup a volume Users can also create backups of a volume, but backups can only be used later to create or replace other volumes. A volume backup is a copy of a volume saved to cold storage, which is cheaper than the performant storage used for volumes and snapshots.\nTo make a backup of a volume, use the FedCloud command below:\nTip See also the documentation of the other backup management commands.  Note Not all OpenStack deployments support volume backups.  Note Backups use as much storage as the source volume, albeit in a cheaper storage layer. This means backups take a long time to create (~4h for 500GB), and the space is accounted for the same way as for regular volumes (the entire size of the backed up volume, regardless of how much of the volume space is actually used). Thus backups should be deleted when no longer needed.  Note To be able to make a backup of a volume, the volume must not be attached to any VM (volume status must be available). To make a backup while the volume is attached to a VM, use the --force command flag, but be aware that there may be inconsistencies if the VM’s OS is not aware of the backup being taken.  $ fedcloud openstack volume backup create --name my-backup my-volume --force Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume backup create --name my-backup my-volume +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2021-08-06T16:08:37.631226 | | description | None | | id | 158bcb42-032f-4cec-b6f3-890a5c958081 | | name | my-backup | | properties | | | size | 10 | | status | creating | | updated_at | None | | volume_id | aa711296-5cff-46ac-bbe3-58e0712ee3e9 | +-------------+--------------------------------------+ The status of the backup will probably be returned as creating. To check if the backup has finished, look at the details of the backup, or list only the newly created backup (filter by backup name or ID):\n$ fedcloud openstack volume backup list --name my-backup Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume backup list --name my-backup +--------------------------------------+-----------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+-----------+-------------+-----------+------+ | 158bcb42-032f-4cec-b6f3-890a5c958081 | my-backup | None | available | 10 | +--------------------------------------+-----------+-------------+-----------+------+ When the status of the backup is available the backup operation is complete.\nDelete volume To delete a volume use the following FedCloud command:\nNote To be able to delete a volume, the volume must not be attached to any VM (volume status must be available).  $ fedcloud openstack volume delete my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume delete my-volume This starts deletion of the specified volume (the volume status changes to deleting):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+----------+------+-------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | deleting | 10 | | +--------------------------------------+-----------+----------+------+-------------+ Once deletion of the specified volume is complete, it will no longer show up in the list of volumes.\nAccess from your VMs Block storage volumes attached to a VM will appear as a block device in the VM.\nTo find out the device name that got assigned to a volume when it was attached to a VM, look at the details of the volume, or list only the volume in question (filter by volume name or ID):\n$ fedcloud openstack volume list --name my-volume Site: IN2P3-IRES, VO: vo.access.egi.eu, command: volume list --name my-volume +--------------------------------------+-----------+--------+------+-----------------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+-----------+--------+------+-----------------------+ | aa711296-5cff-46ac-bbe3-58e00712ee3e | my-volume | in-use | 10 | my-server on /dev/vdb | +--------------------------------------+-----------+--------+------+-----------------------+ In this example the device name is /dev/vdb. To validate this, run the following command in the VM:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 20G 0 disk └─vda1 252:1 0 20G 0 part / vdb 252:16 0 10G 0 disk Usually these devices are empty upon creation. The first time you attach them to a VM, you will need to partition the device and create filesystem(s) on it, by running the following command in the VM:\nTip Using a file system volume label is useful to avoid the need to find the out the device name, especially when multiple block storage volumes are attached to a VM. It is recommended to use a file system volume label in the VM that is the same as the name of the block storage volume.  Tip The filesystem type can be one supported by the Linux distribution in the VM, but xfs and ext4 are the most widely used.  Caution Only run this command the first time you use the device, as it deletes all data! Make sure you use the correct device name, otherwise you will destroy data on other devices!  $ sudo mkfs.ext4 -L my-volume /dev/vdb Once you created a filesystem on the device, you can mount it at any desired path by running the following command in the VM:\n$ sudo mount /dev/vdb1 /\u003cpath\u003e Continuing with the example above, if we check again the block devices by running the following command in the VM:\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 20G 0 disk └─vda1 252:1 0 20G 0 part / vdb 252:16 0 10G 0 disk └─vdb1 252:1 0 10G 0 part /\u003cpath\u003e If non-root users should be able to access the mounted volume similar to the way e.g. /tmp is accessible, set the sticky bit on the mount point, with this command in the VM:\n$ sudo chmod +t /\u003cpath\u003e If the desired behaviour is to mount the file system automatically on VM restart, add it to /etc/fstab. Using the LABEL parameter will ensure the correct volume is chosen if multiple volumes are attached:\nLABEL=my-volume /\u003cpath\u003e ext4 noatime,nodiratime,user_xattr,nofail 0 0\nNote The use of option nofail is recommended in order to skip (and not block on) mounting the file system volume if it is unavailable, e.g. in case of network issues. Remove this option from the fstab line if you want the VM to block the boot process if the volume is unavailable.  Note The use of option nobarrier is not recommended as volumes are accessed via a cache, and ignoring the correct ordering of journal commits may result in a corrupted file system in case of a hardware problem.  With that you can access /\u003cpath\u003e inside the VM, where all data stored on the volume will be available. Applications will not see any difference between a block storage device and a regular disk, thus no major changes should be required in the application logic.\nAccess via EGI Data Transfer The File Transfer service allows you to move any type of data files asynchronously from one storage to another. If you want to copy data from/to one VM running on the EGI cloud, you will need to run a compatible server (Webdav/https, GridFTP, xrootd, SRM, S3, GCloud) that can interact with the FTS3 software.\nAn easy way to provide a GridFTP server on your VM is to use the gridftp-le ready2go docker stack for deploying a GridFTP Docker Container with certificates from Let’s Encrypt. Take into account:\n  Security groups for the VM must allow ports 80, 2811 and the 50000-50200 range.\n  The VM must have a valid DNS entry (you can use FedCloud’s Dynamic DNS for getting one)\n  The default setup uses /srv as path to expose and maps users to the nobody user. Make sure that nobody is able to read (and write if needed) on that location or set the mapping to the appropriate users.\n  The Let’s Encrypt certificates may not be accepted by some of the EGI infrastructure endpoints, you may want to consider using IGTF certificates instead. Check your CA for instructions on how to get those.\n  You can add direct mappings for specific DNs by adding a /etc/localgridmap.conf file in your running container. See the example below to map the /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Enol Fernandez del Castillo DN to nobody. You can add as many lines as needed:\n\"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Enol Fernandez del Castillo\" nobody   You need to specify the environment variables in the docker-compose.yml file for obtaining the Let’s Encrypt certificate. An extra variable GLOBUS_HOSTNAME must be also set:\nenvironment:- TESTCERT- EMAIL=youremail@domain.com- DOMAIN=mygridftp.example.com- GLOBUS_HOSTNAME=mygridftp.example.com  If you are running on a site with MTU smaller than 1500 (e.g. CESNET-MCC), make sure that you set the MTU to a value smaller than the interface MTU(you can check this with ip addr). In your docker-compose.yml, add:\nnetworks:default:driver:bridgedriver_opts:com.docker.network.driver.mtu:1434  ","categories":"","description":"Block Storage offered by EGI Cloud providers\n","excerpt":"Block Storage offered by EGI Cloud providers\n","ref":"/users/online-storage/block-storage/","tags":"","title":"Block Storage"},{"body":"Moving the Site BDII to another machine   Ensure the new Site BDII is working fine and publishing all the necessary site information\n See MAN01 for general information about how to configure a Site BDII. In particular, remember that the Site BDII configuration must include the BDII node itself.    Put the old Site BDII in scheduled downtime in the Configuration Database for a couple of hours\n  Register the new service in the Configuration Database by adding a new Service Endpoint:\n select Site BDII in Service Type field fill in at least the hostname select Y for production and monitoring, N for beta service field    Properly modify the GIIS URL field with the new SITE-BDII ldap URL\n Note that the site name in the GIIS URL is case-sensitive    Mark the old Site BDII as not production and turn off its monitoring\n  Top BDIIs updates their sites list every hour so the new Site BDII should be published in less than an hour; instead Nagios updates the monitored hosts every 3 hours\n  When the new Site BDII is appeared on Nagios and the old one is disappeared, turn off the old Site BDII and remove it from the Configuration Database\n  Turning off a Site BDII co-hosted with other services   Stop and remove the service\nservice bdii stop yum remove glite-BDII glite-yaim-bdii   Delete some info-providers and ldif file ( /opt/glite/etc/gip/ldif/stub-site.ldif, /opt/glite/etc/gip/site-urls.conf, /opt/glite/etc/gip/provider/glite-info-provider-site, /opt/glite/etc/gip/provider/glite-info-provider-service-bdii-site-wrapper, /opt/glite/etc/gip/ldif/glite-info-site.ldif)\n  Reconfigure with yaim without specifying the BDII_site profile\n  ","categories":"","description":"Changing the Site BDII","excerpt":"Changing the Site BDII","ref":"/providers/high-throughput-compute/changing_site_bdii/","tags":"","title":"Changing the Site BDII"},{"body":"The integration of OpenStack service providers into the EGI Check-in is a two-step process:\n Test integration with the development instance of EGI Check-in. This will allow you to check complete the complete functionality of the system without affecting the production Check-in service. Once the integration is working correctly, register your provider with the production instance of EGI Check-in to allow members of the EGI User Community to access your service.  Registration into Check-in development instance Before your service can use the EGI Check-in OIDC Provider for user login, you must set up a client at the EGI Check-in ODIC client management page in order to obtain OAuth2.0 credentials and register one or more redirect URIs.\nMake sure that you fill in the following options:\n  Main tab:\n  Set redirect URL to https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect. Recent versions of OpenStack may deploy Keystone at /identity/, be sure to include that in the \u003cyour keystone endpoint\u003e part of the URL if needed.     Access tab:\n  Enable openid, profile, email, eduperson_entitlement in the Scope field Enable authorization code in the Grant Types field Enable Allow calls to the Introspection Endpoint? in Introspection field     Once done, you will get a client ID and client secret. Save them for the following steps\nKeystone setup Pre-requisites  Keystone must run as a WSGI application behind an HTTP server (Apache is used in this documentation, but any server should be possible if it has OpenID connect/OAuth2.0 support). Keystone project has deprecated eventlet, so you should be already running Keystone in such way. Keystone must be run with SSL You need to install mod_auth_openidc for adding support for OpenID Connect to Apache.  IGTF CAs EGI monitoring checks that your Keystone accepts clients with certificates from the IGTF CAs. Please ensure that your server is configured with the correct Certificate and Revocation path:\n For Apache HTTPd  HTTPd is able to use CAs and CRLs contained in a directory:\n  SSLCACertificatePath /etc/grid-security/certificates SSLCARevocationPath /etc/grid-security/certificates  For haproxy  CA and CRLS have to be bundled into one file.\nClient verification should be set as optional otherwise accepted CAs won't be presented to the EGI monitoring:\n# crt: concatenated cert, key and CA # ca-file: all IGTF CAs, concatenated as one file # crl-file: all IGTF CRLs, concatenated as one file # verify: enable optional X.509 client authentication bind XXX.XXX.XXX.XXX:443 ssl crt /etc/haproxy/certs/host-cert-with-key-and-ca.pem ca-file /etc/haproxy/certs/igtf-cas-bundle.pem crl-file /etc/haproxy/certs/igtf-crls-bundle.pem verify optional  For nginx  CA and CRLS have to be bundled into one file.\nClient verification should be set as optional otherwise accepted CAs won't be presented to the EGI monitoring:\nssl_client_certificate /etc/ssl/certs/igtf-cas-bundle.pem; ssl_crl /etc/ssl/certs/igtf-crls-bundle.pem; ssl_verify_client optional;  Managing IGTF CAs and CRLs  IGTF CAs can be obtained from UMD, you can find repository files for your distribution at EGI CA repository\nIGTF CAs and CRLs can be bundled using the examples command hereafter.\nPlease update CAs bundle after IGTF updates, and CRLs bundle after each CRLs update made by fetch-crl:\ncat /etc/grid-security/certificates/*.pem \u003e /etc/haproxy/certs/igtf-cas-bundle.pem cat /etc/grid-security/certificates/*.r0 \u003e /etc/haproxy/certs/igtf-crls-bundle.pem # Some CRLs files are not ending with a new line # Ensuring that CRLs markers are separated by a line feed perl -pe 's/----------/-----\\n-----/' -i /etc/haproxy/certs/igtf-crls-bundle.pem    Apache Configuration Include this configuration on the Apache config for the virtual host of your Keystone service, using the client ID and secret obtained above:\nOIDCResponseType \"code\" OIDCClaimPrefix \"OIDC-\" OIDCClaimDelimiter ; OIDCScope \"openid profile email eduperson_entitlement\" OIDCProviderMetadataURL https://aai-dev.egi.eu/oidc/.well-known/openid-configuration OIDCClientID \u003cclient id\u003e OIDCClientSecret \u003cclient secret\u003e OIDCCryptoPassphrase \u003csome crypto pass phrase\u003e OIDCRedirectURI https://\u003cyour keystone endpoint\u003e/v3/auth/OS-FEDERATION/websso/openid/redirect # OAuth for CLI access OIDCOAuthIntrospectionEndpoint https://aai-dev.egi.eu/oidc/introspect OIDCOAuthClientID \u003cclient id\u003e OIDCOAuthClientSecret \u003cclient secret\u003e # Increase Shm cache size for supporting long entitlements OIDCCacheShmEntrySizeMax 65536 \u003cLocation ~ \"/v3/auth/OS-FEDERATION/websso/openid\"\u003e AuthType openid-connect Require valid-user \u003c/Location\u003e \u003cLocation ~ \"/v3/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e If you have multiple keystone hosts, configure an alternative caching mechanism as per https://github.com/zmartzone/mod_auth_openidc/wiki/Caching\nFor example, using memcache\nOIDCCacheType memcache OIDCMemCacheServers \"memcache1 memcache2 memcache3\" Be sure to enable the mod_auth_oidc module in Apache, in Ubuntu:\nsudo a2enmod auth_openidc  Note If running Keystone behind a proxy, make sure to correctly set the X-Forwarded-Proto and X-Forwarded-Port request headers, e.g. for haproxy:\nhttp-request set-header X-Forwarded-Proto https if { ssl_fc } http-request set-header X-Forwarded-Proto http if !{ ssl_fc } http-request set-header X-Forwarded-Port %[dst_port]   Keystone Configuration Configure your keystone.conf to include in the [auth] section openid in the list of authentication methods:\n[auth] # This may change in your installation # add openid to the list of the methods you support methods = password, token, openid Add a [openid] section as follows:\n[openid] # this is the attribute in the Keystone environment that will define the # identity provider remote_id_attribute = HTTP_OIDC_ISS Add your horizon host as trusted dashboard to the [federation] section:\n[federation] trusted_dashboard = https://\u003cyour horizon\u003e/dashboard/auth/websso/ Finally copy the default template for managing the tokens in horizon to /etc/keystone/sso_callback_template.html. This template can be found in keystone git repository at https://github.com/openstack/keystone/blob/master/etc/sso_callback_template.html\ncurl -L https://raw.githubusercontent.com/openstack/keystone/master/etc/sso_callback_template.html \\  \u003e /etc/keystone/sso_callback_template.html Now restart your Apache (and Keystone if running in uwsgi) so you can configure the Keystone Federation support.\nKeystone Federation Support First, create a new egi.eu identity provider with remote id https://aai-dev.egi.eu/oidc/:\n$ openstack identity provider create --remote-id https://aai-dev.egi.eu/oidc/ egi.eu +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | 1cac7817dafb4740a249cc9ca6b14ea5 | | enabled | True | | id | egi.eu | | remote_ids | https://aai-dev.egi.eu/oidc/ | +-------------+----------------------------------+ Create a group for users coming from EGI Check-in, usual configuration is to have one group per VO you want to support.\n$ openstack group create ops +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | | | domain_id | default | | id | 89cf5b6708354094942d9d16f0f29f8f | | name | ops | +-------------+----------------------------------+ Add that group to the desired local project:\nopenstack role add member --group ops --project ops Define a mapping of users from EGI Check-in to the group just created and restrict with the OIDC-eduperson_entitlement the VOs you want to support for that group. Substitute the group ID and the allowed entitlements for the adequate values for your deployment:\n$ cat mapping.egi.json [ { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"89cf5b6708354094942d9d16f0f29f8f\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai-dev.egi.eu/oidc/\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:ops:role=vm_operator#aai.egi.eu$\" ] } ] } ] More recent versions of Keystone allow for more elaborated mapping, but this configuration should work for Mitaka and onwards\nCreate the mapping in Keystone:\n$ openstack mapping create --rules mapping.egi.json egi-mapping +-------+----------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-------+----------------------------------------------------------------------------------------------------------------------------------+ | id | egi-mapping | | rules | [{u'remote': [{u'type': u'HTTP_OIDC_SUB'}, {u'type': u'HTTP_OIDC_ISS', u'any_one_of': [u'https://aai-dev.egi.eu/oidc/']}, | | | {u'regex': True, u'type': u'OIDC-eduperson_entitlement', u'any_one_of': [u'^urn:mace:egi.eu:.*:ops:vm_operator@egi.eu$']}], | | | u'local': [{u'group': {u'id': u'89cf5b6708354094942d9d16f0f29f8f'}, u'user': {u'name': u'{0}'}}]}] | +-------+----------------------------------------------------------------------------------------------------------------------------------+ Finally, create the federated protocol with the identity provider and mapping created before:\n$ openstack federation protocol create \\  --identity-provider egi.eu \\  --mapping egi-mapping openid +-------------------+-------------+ | Field | Value | +-------------------+-------------+ | id | openid | | identity_provider | egi.eu | | mapping | egi-mapping | +-------------------+-------------+ Keystone is now ready to accept EGI Check-in credentials.\nHorizon Configuration Edit your local_settings.py to include the following values:\n# Enables keystone web single-sign-on if set to True. WEBSSO_ENABLED = True # Allow users to choose between local Keystone credentials or login # with EGI Check-in WEBSSO_CHOICES = ( (\"credentials\", _(\"Keystone Credentials\")), (\"openid\", _(\"EGI Check-in\")), ) Once horizon is restarted you will be able to choose \"EGI Check-in\" for login.\nCLI Access The OpenStack Client has built-in support for using OpenID Connect Access Tokens to authenticate. You first need to get a valid token from EGI Check-in (e.g. from https://aai-dev.egi.eu/fedcloud/) and then use it in a command like:\n$ openstack --os-auth-url https://\u003cyour keystone endpoint\u003e/v3 \\  --os-auth-type v3oidcaccesstoken --os-protocol openid \\  --os-identity-provider egi.eu \\  --os-access-token \u003cyour access token\u003e \\  token issue +---------+---------------------------------------------------------------------------------------+ | Field | Value | +---------+---------------------------------------------------------------------------------------+ | expires | 2017-05-23T11:24:31+0000 | | id | gAAAAABZJA3fbKX....nEMAPi-IsFOCkU9QWGTISYElzYJsI3z0SJGs7QsTJv4aJQq0JDJUBz6uE85SqXDj3 | | user_id | 020864ea9415413f9d706f6b473dbeba | +---------+---------------------------------------------------------------------------------------+ Additional VOs Configuration can include as many mappings as needed in the json file. Users will be members of all the groups matching the remote part of the mapping. For example this file has 2 mappings, one for members of ops and another for members of fedcloud.egi.eu:\n[ { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"66df3a7a0c6248cba8b729de7b042639\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [\"https://aai-dev.egi.eu/oidc/\"] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:ops:role=vm_operator#aai.egi.eu$\" ] } ] }, { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"e1c04284718f4e19bb0516e5534a24e8\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [\"https://aai-dev.egi.eu/oidc/\"] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^urn:mace:egi.eu:group:fedcloud.egi.eu:role=vm_operator#aai.egi.eu$\" ] } ] } ] Multiple OIDC providers If your OpenStack deployment needs to support multiple identity providers (besides EGI Check-in) you will need to configure mod_auth_openidc to support multiple providers and use an OAuth2.0 token introspection proxy like ESACO.\nmod_auth_openidc configuration First, create a directory to host each of the providers configuration, in our case we will use /var/lib/apache2/oidc/metadata, but adapt this to your specific needs. Ensure this directory is writable by the user running apache:\nmkdir -p /var/lib/apache2/oidc/metadata chown -R www-data:www-data /var/lib/apache2/oidc/metadata Set in your Apache configuration the OIDCMetadataDir pointing to that directory\nOIDCMetadataDir /var/lib/apache2/oidc/metadata You may remove the OIDCProviderMetadataURL, OIDCClientID and OIDCClientSecret options from the Apache configuration as these will be now set in new files created in the metadata directory. For every provider you will support, you need to create 3 files:\n  \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.provider with the OpenID Connect Discovery OP JSON metadata. The easiest way to create this file is getting its content from the OIDC server itself. For EGI Check-in:\ncurl https://aai-dev.egi.eu/oidc/.well-known/openid-configuration \u003e \\  /var/lib/apache2/oidc/metadata/aai-dev.egi.eu%2Foidc.provider   \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.client with the client credentials. For EGI Check-in (aai-dev.egi.eu%2Foidc.client):\n{ \"client_id\" : \"\u003cyour client id\u003e\", \"client_secret\" : \"\u003cyour secret id\u003e\" }   \u003curlencoded-issuer-value-with-https-prefix-and-trailing-slash-stripped\u003e.conf with any extra configuration for the provider. This may not be needed if all your providers are similar. For example to specify the scopes to use for Check-in, use a aai-dev.egi.eu%2Foidc.conf as follows:\n{ \"scope\": \"openid email profile eduperson_entitlement\" }   Now add for the providers you support new configuration in Apache to facilitate the use of the dashboard. This is for a configuration of an egi.eu identity provider with openid as protocol:\n\u003cLocation ~ \"/identity/v3/auth/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/websso\"\u003e AuthType openid-connect # This is your Redirect URI with a new iss=\u003cyour idp iss\u003e option added OIDCDiscoverURL https://openstack-test.test.fedcloud.eu/identity/v3/auth/OS-FEDERATION/websso/openid/redirect?iss=https%3A%2F%2Faai-dev.egi.eu%2Foidc%2F # Ensure that the user is authenticated with the expected iss Require claim iss:https://aai-dev.egi.eu/oidc/ Require valid-user \u003c/Location\u003e In your Horizon configuration, set the list of providers and their mappings:\n# this is the list that will show up in the dropdown menu WEBSSO_CHOICES = ( (\"credentials\", _(\"Keystone Credentials\")), (\"egi.eu\", _(\"EGI Check-in\")), (\"other-idp\", _(\"Other IdP\")), ) # this maps the options above to keystone's idps and protocols WEBSSO_IDP_MAPPING = { \"egi.eu\": (\"egi.eu\", \"openid\"), \"other-idp\": (\"other-idp.com\", \"openid\") } ESACO configuration ESACO will handle OAuth tokens when users hit your Keystone from API/CLI. It needs to run as a daemon that listens (by default) on port 8156. We will use docker for facilitating the deployment:\n  Create a yaml file with the configuration of the different providers (application.yaml):\noidc:clients:- issuer-url:https://aai-dev.egi.eu/oidc/client-id:\"\u003cyour check-in client id\u003e\"client-secret:\"\u003cyour check-in client secret\u003e\"- issuer-url:\u003canother idp\u003eclient-id:\"\u003cyour client id for second idp\u003e\"client-secret:\"\u003cyour client secret for second idp\u003e\"  Create a environment file with the ESACO credentials you want to use (esaco.env):\n# User name credential requested from clients introspecting tokens ESACO_USER_NAME=\u003cesaco user name\u003e # Password credential requested from clients introspecting tokens ESACO_USER_PASSWORD=\u003cesaco password\u003e   Run the ESACO server (adapt this as it better fits to run on your servers and make it run permanently):\ndocker run -p 8156:8156 -d -env-file=esaco.env \\  -v application.yml:/esaco/config/application.yml:ro \\  indigoiam/esaco:latest   Configure Keystone’s Apache to use ESACO as OAuth introspection endpoint:\n# point this to the host where ESACO is running OIDCOAuthIntrospectionEndpoint http://localhost:8156/introspect OIDCOAuthClientID \u003cesaco user name\u003e OIDCOAuthClientSecret \u003cesaco password\u003e OIDCIDTokenIatSlack 3600   Configure also the locations in Apache that should use OAuth:\n\u003cLocation ~ \"/identity/v3/OS-FEDERATION/identity_providers/egi.eu/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e \u003cLocation ~ \"/identity/v3/OS-FEDERATION/identity_providers/other_idp/protocols/openid/auth\"\u003e Authtype oauth20 Require valid-user \u003c/Location\u003e   Moving to EGI Check-in production instance Once tests in the development instance of Check-in are successful, you can move to the production instance. You should open a GGUS ticket for the request. Besides you will need to update your configuration as follows:\n  Update the remote-id of the identity provider:\nopenstack identity provider set --remote-id https://aai.egi.eu/oidc/ egi.eu   Update the HTTP_OIDC_ISS filter in your mappings, e.g.:\nsed -i 's/aai-dev.egi.eu/aai.egi.eu/' mapping.egi.json openstack mapping set --rules mapping.egi.json egi-mapping   Update Apache configuration to use aai.egi.eu instead of aai-dev.egi.eu:\nOIDCProviderMetadataURL https://aai.egi.eu/oidc/.well-known/openid-configuration OIDCOAuthIntrospectionEndpoint https://aai.egi.eu/oidc/introspect   Changes in the production settings If you want to make any changes to the client configuration of the production instance, first make the changes in the Check-in development environment and then open a GGUS ticket to sync the changes to production.  ","categories":"","description":"Authentication and Authorization integration\n","excerpt":"Authentication and Authorization integration\n","ref":"/providers/cloud-compute/openstack/aai/","tags":"","title":"Check-in"},{"body":"The Oneclient code and basic documentation are available on GitHub.\nThe official documentation is hosted on the Onedata homepage.\nUsing the web interface Using EGI Check-in it's possible to connect with your institute credentials.\nOn this page it’s possible to have an overview of all the spaces and their supporting providers.\nOn this capture, the information about the spaces supported by a specific provider is displayed.\nThe data space can be managed (i.e. uploading/downloading/managing files and metadata, managing space access) using the web browser.\nGenerating tokens for using Oneclient or APIs Important In order to be able to access your spaces using Oneclient or APIs, it’s required to generate an access token.  Tokens have to be generated from the EGI DataHub (Onezone) interface.\nThe access tokens can be created and managed using the EGI DataHub web interface.\nEnvironment variables The sections below assume you have defined the following variables in your environment:\n ONECLIENT_ACCESS_TOKEN: access token allowing to access all the spaces ONECLIENT_PROVIDER_HOST: name or IP of the Oneprovider the client should connect to.  Installing and testing Oneclient in a docker container Important In order to be able to use FUSE, the container should run in privileged mode.  A quick and simple solution for testing is to install the client on demand in a container for a supported Operating System flavor (mainly various CentOS and Ubuntu releases).\ndocker run -it --privileged centos:7 /bin/bash root@81dbd7e84438 /]# curl -sS http://get.onedata.org/oneclient.sh | bash # (...) Complete! Installation has been completed successfully. Run 'oneclient --help' for usage info. root@81dbd7e84438 /]# export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e root@81dbd7e84438 /]# export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu root@81dbd7e84438 /]# mkdir /tmp/space root@81dbd7e84438 /]# oneclient /tmp/space root@81dbd7e84438 /]# ls /tmp/space Here the data is mounted in /tmp/space, creating a file into it will push it to the Oneprovider and it will be accessible in the web interface and from other providers supporting the space.\nFor a real production usage it's preferable to use the Oneclient container as a source for a volume mounted into another container.\nTesting Oneclient in a Oneclient docker container with NFS or samba Docker containers for the Oneclient are available, the existing versions can be seen on the Oneclient docker hub.\nIt’s possible to use the most recent version by specifying the latest tag. We also recommend using the same version as shown on the Onezone and Oneprovider pages.\nexport ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu docker run -it --privileged -e ONECLIENT_ACCESS_TOKEN=$ONECLIENT_ACCESS_TOKEN -e ONECLIENT_PROVIDER_HOST=$ONECLIENT_PROVIDER_HOST onedata/oneclient:20.02.7 Connecting to provider 'plg-cyfronet-01.datahub.egi.eu:443' using session ID: '4138963898952098752'... Getting configuration... Oneclient has been successfully mounted in '/mnt/oneclient' Now the client will run in the background and the data will be available through samba/CIFS or nfs protocols:\n# Identifying the IP of the container docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" $(docker ps -ql) 172.17.0.2 So the data can be accessed at\n smb://172.17.0.2/onedata nfs://172.17.0.2/onedata  Testing Oneclient in a Oneclient docker container with local file access Another solution is to mount a local directory as a volume in the container, allowing to access both the working directory as well as the Onedata spaces, thus allowing to easily exchange files between a local directory and a Onedata space.\nIn order to do this we will open a bash shell in the container then we will mount manually the Onedata spaces.\nexport ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu docker run -it --privileged -e ONECLIENT_ACCESS_TOKEN=$ONECLIENT_ACCESS_TOKEN -e ONECLIENT_PROVIDER_HOST=$ONECLIENT_PROVIDER_HOST -v $PWD:/mnt/src --entrypoint bash onedata/oneclient:20.02.7 root@aca612a84fb4:/tmp# oneclient /mnt/oneclient Connecting to provider 'plg-cyfronet-01.datahub.egi.eu:443' using session ID: '1641165171427694510'... Getting configuration... Oneclient has been successfully mounted in '/mnt/oneclient'. root@aca612a84fb4:/tmp# ls /mnt/oneclient (...) root@aca612a84fb4:/tmp# ls /mnt/src (...) Now it's possible to use the following mount points:\n /mnt/oneclient: the Onedata spaces /mnt/src: the local directory (any absolute path could have been used instead of $PWD that points to the working directory)  Testing Oneclient in a Virtual Machine The following variables have to be exported:\n ONECLIENT_ACCESS_TOKEN: access token allowing to access all the spaces. ONECLIENT_PROVIDER_HOST: name or IP of the Oneprovider the client should connect to.  curl -sS http://get.onedata.org/oneclient.sh | bash export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu mkdir /tmp/space oneclient /tmp/space Testing Oneclient in a Vagrant box It's possible to quickly test Oneclient using Vagrant.\nvagrant init ubuntu/xenial64 vagrant up vagrant ssh curl -sS http://get.onedata.org/oneclient.sh | bash export ONECLIENT_ACCESS_TOKEN=\u003cACCESS_TOKEN_FROM_ONEZONE\u003e export ONECLIENT_PROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu mkdir /tmp/space oneclient /tmp/space ","categories":"","description":"Documentation related to [EGI DataHub](https://datahub.egi.eu/)","excerpt":"Documentation related to [EGI DataHub](https://datahub.egi.eu/)","ref":"/users/datahub/clients/","tags":"","title":"Clients"},{"body":"Every user of the EGI Notebooks catch-all instance has a 20GB persistent home to store any notebooks and associated data. The content of this home directory will be kept even if your notebook server is stopped (which can happen if there is no activity for more than 1 hour). Modifications to the notebooks environment outside the home directory are not kept (e.g. installation of libraries). If you need those changes to persist, let us know via a GGUS ticket to the Notebooks Support Unit. You can also ask for increasing the 20GB home via ticket.\nGetting data in/out Your notebooks have outgoing internet connectivity so you can connect to any external service to bring data in for analysis. As with input data, you can connect to any external service to deposit the notebooks output.\nThis is convenient for smaller datasets but not practical for larger ones, for those cases we can offer integration with several data services. These are not enabled in the catch-all instance but can be made available on demand.\nEGI DataHub DataHub provides a scalable distributed data infrastructure. It offers a tight integration with Jupyter and notebooks with specific drivers that make the DataHub Spaces accessible from any notebook.\nWhenever you log into the service, supported DataHub spaces will be available under the datahub folder. If you need support for any additional space, please open a ticket in GGUS to add it.\nAlternatively, you can also use the fs-onedatafs library from your code. For convenience, the ONEPROVIDER_HOST environment variable will point to the default oneprovider for the Notebooks and the ONECLIENT_ACCESS_TOKEN variable will contain a valid access token for the service.\nfrom fs.onedatafs import OnedataFS # create the OnedataFS driver using defaults from env odfs = OnedataFS(os.environ['ONEPROVIDER_HOST'], os.environ['ONECLIENT_ACCESS_TOKEN'], force_direct_io=True) # use it to open a file f = odfs.open(\"\u003cdatahub file path\u003e\") The ONEPROVIDER_HOST and ONECLIENT_ACCESS_TOKEN variables are obtained as part of the login process and made available in the notebooks environment automatically. You can also specify a different oneprovider host if needed.\nEUDAT B2DROP EUDAT B2DROP offers a WebDAV interface that can be used to mount your files from the notebooks. Files are accessed as any regular file from the notebooks interface or from your code. This feature requires users to create a client in B2DROP and provide the client’s credentials to the EGI notebooks service.\nD4Science Workspace D4Science VREs provide a shared workspace via a dedicated API. EGI Notebooks embedded in D4Science VREs will automatically show the user’s workspace at the workspace directory. You can browse and use as any regular file.\nShared folders The Notebooks service can enable shared folders for users, either in read-only or read-write mode. These are specially meant for community instances for easing the sharing of data between all the users of the service. In the catch-all instance the datasets directory serves as an example of such feature.\nOther services We are open for integration with other services for facilitating the access to input and output data. Please contact support _at_ egi.eu with your request so we can investigate the best way to support your needs.\n","categories":"","description":"Managing data on the Notebooks\n","excerpt":"Managing data on the Notebooks\n","ref":"/users/notebooks/data/","tags":"","title":"Data Management"},{"body":"The default environment includes a set of kernels that are automatically built from the EGI-Federation/egi-notebooks-images GitHub repository. These are the ones available:\n  Python: Default Python 3 kernel, it includes commonly used data analysis and machine learning libraries. Created from the jupyter/scipy-notebook stack.\n  DIRAC / Python 2: A python 2 kernel that includes a DIRAC installation for interacting with the EGI Workload Manager service.\n  Julia: The Julia programming language with the libraries described in jupyter/datascience-notebook.\n  R: The R programming language with several packages from the R ecosystem as provided by jupyter/r-notebook and some extra libraries.\n  Octave: The Octave programming language installed on its own conda environment (named octave).\n  If you want to add a new kernel, just let us know and we will discuss the best way to support your request.\n","categories":"","description":"EGI Notebooks default environment\n","excerpt":"EGI Notebooks default environment\n","ref":"/users/notebooks/kernels/default/","tags":"","title":"Default environment"},{"body":"Deploy cluster Through a “job wizard” interface the user can login to the Elastic Cloud Compute Cluster (EC3) portal and configure the virtual cluster with the related tools and applications to be deployed in the EGI Cloud. Click on the “Deploy your cluster” button to create a cluster in the EGI Cloud.\nThe cluster is composed of a front node, where a batch job scheduler is running, and a number of compute nodes. These compute nodes will be dynamically deployed and provisioned to fit increasing load, and un-deployed when they are in idle status. The installation and configuration of the cluster is performed by means of the execution of Ansible receipts.\nA wizard will guide the user during the configuration process of the cluster, allowing to configure details like the operating system, the characteristics of the nodes, the maximum number of nodes of the cluster or the pre-installed software packages. Specifically, the general wizard steps include:\n  LRMS selection: choose Torque from the list of LRMSs (Local Resource Management System) that can be automatically installed and configured by EC3.\n  Endpoint: the endpoints of the providers where to deploy the elastic cluster. The endpoints serving the vo.access.egi.eu VO are dynamically retrieved from the EGI Application DataBase using REST APIs.\n  Operating System: choose one of the available EGI base OS images available to create the cluster (e.g. CentOS7, or EGI Ubuntu 18.04 LTS).\n  Instance details: in terms of CPU and RAM to allocate for the front-end and the working nodes.\n  Cluster’s size and name: the name of the cluster and the maximum number of nodes of the cluster, without including the frontend. This value indicates the maximum number of working nodes that the cluster can scale. Initially, the cluster is created with the frontend and only one working node: the other working nodes are powered on on-demand.\n  Resume and Launch: a summary of the chosen cluster configuration. To start the deployment process, click the Submit button.\n  Note The configuration of the cluster may take some time. Please wait for its completion before starting to use the cluster!  When the frontend node of the cluster has been successfully deployed, the user will be notified with the credentials to access via SSH.\nThe cluster details are available by clicking on the “Manage your deployed clusters” link on the front page.\nAccessing the EC3 cluster To access the frontend of the elastic cluster:\n Download the SSH private key provided by the EC3 portal; Change its permissions to 600; Access via SSH providing the key as identity file for public key authentication.  ]$ ssh -i key.pem cloudadm@\u003cCLUSTER_PUBLIC_IP\u003e Last login: Mon Nov 18 11:37:29 2019 from torito.i3m.upv.es [cloudadm@server ~]$ Both the frontend and the working node are configured by Ansible. This process usually takes some time. User can monitor the status of the cluster configuration using the is_cluster_ready command-line tool:\n]# is_cluster_ready Cluster is still configuring. The cluster is successfully configured when the command returns the following message:\n]# is_cluster_ready Cluster configured! node state enabled time stable (cpu,mem) used (cpu,mem) total ]$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------- wn1 off enabled 00h04'20\" 0,0 1,-1 wn2 off enabled 00h04'20\" 0,0 1,-1 Operate the EC3 cluster To force CLUES to spawn a new node of the cluster, please use the command:\n]$ clues poweron wn1 node wn1 powered on The configuration triggers the execution of several ansible processes to configure the node and may take some time. To monitor the configuration of the node, you can use the is_cluster_ready command.\nTo avoid that clues powers off the node in case of inactivities, once the node is configured, you can disable the node as it follows:\n]$ clues disable wn1 node wn1 successfully disabled Log files  Cluster logs files are available in /var/tmp/.im/\u003ccluster_id\u003e IM log files are in /var/log/im/im.log CLUES log files are in /var/log/clues2/clues2.log  ","categories":"","description":"Deploy a virtual cluster with EC3 using the portal.\n","excerpt":"Deploy a virtual cluster with EC3 using the portal.\n","ref":"/users/cloud-compute/ec3/portal/","tags":"","title":"EC3 portal"},{"body":"The following guide is intended for researchers who want to use ECAS, a complete environment enabling data analysis experiments, in the EGI cloud.\nECAS (ENES Climate Analytics Service) is part of the EOSC-hub service catalog and aims to:\n provide server-based computation, avoid data transfer, and improve reusability of data and workflows.  It relies on Ophidia, a data analytics framework for eScience, which provides declarative, server-side, and parallel data analysis, jointly with an internal storage model able to efficiently deal with multidimensional data and a hierarchical data organization to manage large data volumes (“datacubes”), and on JupyterHub, to give users access to ready-to-use computational environments and resources.\nThanks to the Elastic Cloud Compute Cluster (EC3) platform, operated by the Polytechnic University of Valencia (UPV), researchers will be able to rely on the EGI Cloud Compute service to scale up to larger simulations without being worried about the complexity of the underlying infrastructure.\nThis guide will show how to:\n deploy an ECAS elastic cluster of VMs in order to automatically install and configure the whole ECAS environment services, i.e. JupyterHub, PyOphidia, several Python libraries such as numpy, matplotlib and Basemap; perform data intensive analysis using the Ophidia HPDA framework; access the ECAS JupyterHub interface to create and share documents containing live code, equations, visualizations and explanatory text.  Deploy an ECAS cluster with EC3 In the latest release of the EC3 platform, tailored to support the EGI Applications on Demand (AoD) service, a new Ansible receipt is now available for researchers interested to deploy ECAS cluster on the EGI Infrastuctrure. Additional details on how to configure and deploy an ECAS cluster on EGI resources are provided in the next sections.\nECAS in now available in the latest release of the EC3 platform supporting the EGI Applications on Demand (AoD). The next sections provide details on how to configure and deploy an ECAS cluster on EGI resources.\nConfigure and deploy the cluster To configure and deploy a Virtual Elastic Cluster using EC3, access the EC3 platform front page and click on the \"Deploy your cluster\" link as shown in the figure below:\nA wizard will guide you through the cluster configuration process. Specifically, the general wizard steps include:\n LRMS selection: choose ECAS from the list of LRMSs (Local Resource Management System) that can be automatically installed and configured by EC3.   Endpoint: the endpoints of the providers where to deploy the ECAS elastic cluster. The endpoints serving the vo.access.egi.eu VO are dynamically retrieved from the EGI Application DataBase using REST APIs.   Operating System: choose EGI CentOS7 as cluster OS.   Instance details, in terms of CPU and RAM to allocate for the front-end and the working nodes.   Cluster’s size and name: the name of the cluster and the maximum number of nodes of the cluster, without including the frontend. This value indicates the maximum number of working nodes that the cluster can scale to. Initially, the cluster is created with the frontend and only one working node: the other working nodes will be powered on on-demand.   Resume and Launch: a summary of the chosen cluster configuration. To start the deployment process, click the Submit button.  When the frontend node of the cluster has been successfully deployed, you will be notified with the credentials to access via SSH.\nThe cluster details are available by clicking on the \"Manage your deployed clusters\" link on the front page:\nNote The configuration of the cluster may take some time. Please wait for its completion before starting to use the cluster.  Accessing the cluster To access the frontend of the cluster:\n download the SSH private key provided by the EC3 portal; change its permissions to 600; access via SSH providing the key as identity file for public key authentication.  [user@localhost EC3]$ ssh -i key.pem cloudadm@\u003cYOUR_CLUSTER_IP\u003e Last login: Mon Nov 18 11:37:29 2019 from torito.i3m.upv.es [cloudadm@oph-server ~]$ sudo su - [root@oph-server ~]# Both the frontend and the working nodes are configured by Ansible. This process usually takes some time. You can monitor the status of the cluster configuration using the is_cluster_ready command-line tool:\n[root@oph-server ~]# is_cluster_ready Cluster is still configuring. The cluster is successfully configured when the command returns the following message:\n[root@oph-server ~]# is_cluster_ready Cluster configured! As SLURM is used as workload manager, it is possible to check the status of the working nodes by using the sinfo command, which provides information about Slurm nodes and partitions.\n[root@oph-server ~]# sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 1 down* oph-io2 debug* up infinite 1 idle oph-io1 Accessing the scientific eco-system ECAS provides two different ways to get access to its scientific eco-system: Ophidia client (oph_term) and JupyterHub.\nPerform some basic operations with Ophidia Run the Ophidia terminal as ophuser user.\nThe default parameters are already defined as environmental variables inside the .bashrc file:\nexport OPH_SERVER_HOST=\"127.0.0.1\" export OPH_SERVER_PORT=\"11732\" export OPH_PASSWD=\"abcd\" export OPH_USER=\"oph-test\" Create an empty container and a new datacube with random data and dimensions.\nNow, you can submit your first operation of data transformation: let’s reduce the whole datacube in a single value for grid point using the average along the time:\nLet’s have a look at the environment by listing the datacubes and containers in the session:\nBy default, the Ophidia terminal will use the last output datacube PID. So, you can use the oph_explorecube operator to visualize the first 100 values.\nFor further details about the Ophidia operators, please refer to the official documentation.\nAccessing the Jupyter interface To access the Jupyter interface, open the browser at https://\u003cYOUR_CLUSTER_IP\u003e:443/jupyter and log in to the system using the username and password specified in the jupyterhub_config.pyp configuration file (see the c.Authenticator.whitelist and c.DummyAuthenticator.password lines) located under the /root folder.\nFrom JupyterHub in ECAS you can do several things such as:\n create and run a Jupyter Notebook exploiting PyOphidia and other Python libraries for data manipulation, analysis and visualization (e.g. NumPy, matplotlib, Cartopy); browse the directories, download and update files in the home folder; execute operators and workflows directly from the Ophidia Terminal; access to a read-only data repository hosted in a Onedata space and perform any analysis on this shared data.  The ECAS space shared in the ECAS environment through the Onedata services is available at the onedata/ecas_provider/ECAS_space folder located under the /data directory.\nTo get started with the ECAS environment capabilities, open the ECAS_Basics.ipynb notebook available under the notebooks/ folder in the home directory.\nAccessing the Grafana UI This section will show how to monitor the ECAS environment and the resource usage and get aggregated information over time.\nTo access the Grafana monitoring interface, open the browser at https://\u003cYOUR_CLUSTER_IP\u003e:3000 and log in to the system using the admin username and the password specified in the .grafana_pwd file located under the /root folder.\nThe Grafana-based monitoring system provides two dashboards in order to monitor the ECAS cluster both at system and application level.\n The infrastructure dashboard provides information about the percentage of CPU, RAM, SWAP and disk used on each Node.js (the frontend and the working nodes).     frontend node working node           The application dashboard shows information about which operator/workflow is being executed and its current execution status and provides aggregated information over time (e.g. number of total, completed and failed workflows/tasks, hourly weighted average of running cores).  Destroy the cluster To destroy the running cluster use the delete action from the cluster management page.\nReferences  ECASLab CMCC ECASLab DKRZ Ophidia GitHub: ECAS-Lab GitHub: ansible role Ophidia cluster EC3 GitHub EC3  ","categories":"","description":"Using Elastic Cloud Computing Cluster (EC3) platform to create an ECAS environment.\n","excerpt":"Using Elastic Cloud Computing Cluster (EC3) platform to create an ECAS …","ref":"/users/cloud-compute/ec3/apps/ecas/","tags":"","title":"ECAS"},{"body":"Training events supported by the EGI Training Infrastructure are listed below:\n EGI Federated Cloud tutorial package (Software Carpentry Bootcamp, 17 July 2015 Feltham, London, UK). EGI Federated Cloud tutorial package (HPCS 2015 conference, 20-24 July 2015, Amsterdam, NL). EGI Federated Cloud for users (Training for MTA SZTAKI, 14 October 2015, Budapest, HU). Next Generation Sequencing Analysis Training Workshop (21 October, 2015, Thessaloniki, GR). Tutorials at the EGI Community Forum (10-12 November 2015, Bari, IT). EGI Technical Support for ENVRI+ Use Cases Workshop (May 2016, Zandvoord, NL). Running CHIPSTER, Galaxy, Jupyter Notebook on the EGI Federated Cloud (ELIXIR-FI workshop). EGI Federated Cloud for developers (DI4R, 28 September 2016, Krakow, PL). UberCloud - EGI webinar: Cloud for SMEs in CAE – OpenFOAM demo (20 October 2016); Webinar recording. EGI training (ENVRIplus week, 14-18 November 2016, Prague, CZ). MEDGENET-Workshop INAB (15 December 2016, Thessaloniki, GR). Cloud Tutorial at EUDAT summer School (03-07 July 2017, Heraklion, GR). CODATA-RDA Research Data Science Summer School (21 July, 2017, Trieste, IT). Scipion tutorial on Cloud (17-19 January 2017, Madrid, ES). CODATA-RDA Research Data Science Summer School (17 August 2018, Trieste, IT). NGSchool 2018. 3rd Int'l Summer School on Data Science (SSDS 2018). Training for PhD students at the University of Genoa (04 June 2019). Introduction to Jupyter and Open Science - Training (27 September 2019, Yervan). HPC graduate class (August - November 2019) at UNICAM (Brazil) NGSChool 2019 (October, 24-31) Open Science with Jupyter, Zenodo and Binder (tutorial), 04 Dec. 2019 Hercules European School (April 2020)  ","categories":"","description":"Training events\n","excerpt":"Training events\n","ref":"/users/training/events/","tags":"","title":"Events"},{"body":"This section offers an introduction to the EGI services, together with tutorials about how to set up, use, and combine these services.\nNote See the Service Providers section for details on how to integrate providers into the EGI Federation.  Request for information You can ask for more information about the public EGI services on our site.\n","categories":"","description":"Documentation for public EGI services","excerpt":"Documentation for public EGI services","ref":"/users/","tags":"","title":"User Guides"},{"body":"Use this section to get started quickly with internal EGI services:\n The complete list of internal EGI services supporting the coordination of the EGI Federation offers insight into how EGI is able to offer advanced public cloud services The Configuration Database records the topology of the sites in the EGI federation Service Monitoring tracks and controls the performance of the services Accounting tracks service and resource usage, providing insights and reports on consumption The Helpdesk lets users and providers report incidents and bugs, or request changes  ","categories":"","description":"Introduction to internal EGI services","excerpt":"Introduction to internal EGI services","ref":"/internal/getting-started/","tags":"","title":"Getting Started"},{"body":"The pages under this section depict how to join the EGI infrastructure as a service providers offering innovative services to the European Research Area.\n The Operations Start Guide will help you start with EGI Operations duties.\n  Interested in integrating your service with Check-in? Head to the Check-in for service providers! Interested in connecting your Identity Providers and allowing your users to access services via Check-in? Head to the Check-in for Identity Providers! Interested in joining the EGI Federated Cloud? Head to the section on Cloud Compute! Willing to deploy a Oneprovider? Head to DataHub section! And if you want learn about EGI Notebooks, head to the Notebooks section!  ","categories":"","description":"","excerpt":"The pages under this section depict how to join the EGI infrastructure …","ref":"/providers/getting-started/","tags":"","title":"Getting Started"},{"body":"Overview EGI is a federation of computing and storage resource providers united by a mission to support research and innovation.\nThe resources in the EGI infrastructure are offered by service providers that either run their own data centers or rely on community, private and/or public cloud services. These service providers offer:\n Single Sign-On via EGI Check-in allows users to login with their institutional (community) credentials Global image catalogue at AppDB with pre-configured virtual machine images Resource discovery features to easily understand which providers are supporting your community, and what are their capabilities Global accounting that aggregates and allows visualisation of usage information Monitoring of availability and reliability to ensure SLAs are met  The EGI infrastructure supports a multitude of science and research communities, each with their own virtualised resources built around open standards. The development of these communities is driven by by their own scientific requirements.\nTip See also an overview of the EGI FedCloud architecture, or read about the task force supporting it.  Accessing resources Access to resources (services) in the EGI infrastructure is based on OpenID Connect (OIDC), which replaces the legacy authentication and authorization based on X.509 certificates.\nNote Some services still rely on X.509 certificates, e.g. High Throughput Compute.  EGI uses Virtual Organisations (VOs) to control access to resources. VOs are fully managed by research communities, allowing communitites to manage their users and grant access to their services and resources. This means communities can either own their resources and use EGI services to share (federate) them, or can use the resources available in the EGI infrastructure for their scientific needs.\nBefore users can access an EGI service, they have to:\n Obtain a supported ID, by signing up with either EGI Check-in directly, or with one of the community identity providers from the EGI infrastructure. Enroll into one VO. Users need to be part of a VO before using EGI services. Explore the list of available VOs in the Operations Portal. Authenticate to EGI Check-in to obtain an OAuth2 access token (and optionally a refresh token). Manage or use the service by leveraging the access token, either implicitly (web interfaces and dashboards usually hide this from users) or explicitly (e.g. when using command-line tools).  Note See the EGI Check-in documentation for a detailed description of the Authentication and Authorization Infrastructure (AAI) of the EGI Federation, and to gain a better understanding of the concepts that act as building blocks for the AAI implementation.  Requesting resources Depending on the access conditions, a service (or an instance of the service) may be open for any user, or it may require requesting access (ordering). The EGI site together with the connected EGI Marketplace streamlines the ordering process.\nEGI services use the following types of access conditions:\n Wide access - Users can freely access the service. Login may be required but it is possible with various institutional accounts (through EduGAIN), or with social accounts (e.g. Google). For example you can create a test Virtual Machine or launch a Jupyter Notebook. Policy based - Users are granted access based on specific policies defined by the service providers. Access needs to be requested, and will be checked for such services. Example: Compute resources and tools allocated to researchers in medical imaging (Biomed VO). Pay-for-use - Services are provided for a fee. Example: FitSM In-house course  The EGI user community support team handles access requests (orders) for the Policy based and Pay-for-use access modes. They will respond to the request within maximum 5 work days. We normally contact you to have a short teleconference meeting to better understand your requirements, and to be able to identify resources and services that best match your needs. The meeting typically covers two topics:\n What is the background of your request? Scientific domain, partner countries, user bases, pay-for-use or not, etc. What are the technical details of your use case? How many CPU cores, how much RAM per CPU, which software services, and for how long do you need them, etc.  Contact us if you want to discuss further.\nCapacity allocation When EGI is able to support a request for resources, it can do so in two ways:\n We grant you access to an existing service, for example to compute resource pools (Virtual Organisations) that already exist in EGI for specific scientific disciplines or for researchers in specific regions. You can browse these in the EGI Operations Portal. If there is a suitable VO, we help you join it and use its services. We create a new VO for your community when none of the existing resource pools are suitable for your use case. The procedure is as follows:  We will contact our provider and negotiate resources for you If there are providers willing to support you, we will sign a Service Level Agreement (SLA) with you A new VO will be created for your community    Unused resources Users of the EGI services may gain opportunistic usage to unused resources. These are resources that are not dedicated to the user’s organization, but are accessible when the research center(s) have some spare resources. This enables the most efficient use of resources.\nNote Users should not rely on (unused) resources not dedicated to their organisation, as access can be revoked without warning, and data may be lost if not properly backed up.  ","categories":"","description":"Introduction to EGI services\n","excerpt":"Introduction to EGI services\n","ref":"/users/getting-started/","tags":"","title":"Getting Started"},{"body":"For collaboration purposes, it is best if you create a GitHub account and fork the repository to your own account. Once you do this, you will be able to push your changes to your GitHub repository for others to see and use, and you will be able to create pull requests (PRs) in the official EGI documentation repository based on branches in your fork.\nIf you are new to git and GitHub you are advised to start by the two following articles providing simple tutorials:\n Step by step guide to git Creating pull request with GitHub  GitHub official documentation is available at docs.github.com.\nTip The first-contributions is a repository allowing anyone to freely learn and test creating a real Pull Request to an existing GitHub repository.  Additional documentation about the main steps for working with GitHub is also available in this section.\nThe GitHub contribution flow In order to be able to send code update to the repository you need to:\n fork the repository to your GitHub account clone the repository on your local computer create a feature branch where you will commit your changes push the feature branch to the repository fork in your GitHub account open a Pull Request against the upstream repository  In this process three git repositories are used:\n The upstream repository: EGI-Federation/documentation Your fork, also named origin: \u003cyour_username\u003e/documentation A local clone of your fork, containing references to your fork, its origin and to the upstream repository  Add an SSH key to your GitHub account The most convenient way to authenticate with GitHub is to use SSH keys over the SSH protocol.\nYou can add an SSH public key to your GitHub account in the Settings on GitHub, at https://github.com/settings/keys.\nRefer to Connecting to GitHub with SSH for an extensive documentation on using SSH keys with GitHub.\nIt’s worth to mention that your ssh public keys can easily be retrieved using a URL like https://github.com/\u003cyour_username\u003e.keys.\nIn order to manage repositories over ssh, you will will have to clone them via SSH, not HTTPS.\nIf you already have a local clone of a repository created via HTTPS, you can switch it to SSH by following Switching remote URLs from HTTPS to SSH.\nStarting with the GitHub CLI The GitHub command-line interface greatly helps with working with GitHub repositories from a terminal.\nIt can be installed using the packages available on their homepage. There is also a manual.\nOnce installed you will have to start by setting up authentication.\n# Authenticate with GitHub, favor SSH protocol $ gh auth login $ gh config set git_protocol ssh Working with repositories The easiest way is to do it via the GitHub CLI that will also clone it locally. But it can also be done via the web interface, using the fork button and then cloning it locally manually.\nFork and clone This command will fork the repository to your GitHub account and clone a local copy for you to work with.\n$ gh repo fork EGI-Federation/documentation Clone existing fork If you want to clone an existing fork you should use:\n$ gh repo clone \u003cyour_username\u003e/documentation Validate the local clone If your local clone of you fork is correctly setup you should see references to the origin and upstream repositories.\n$ git remote -v origin git@github.com:\u003cyour_username\u003e/documentation (fetch) origin git@github.com:\u003cyour_username\u003e/documentation (push) upstream git@github.com:EGI-Federation/documentation.git (fetch) upstream git@github.com:EGI-Federation/documentation.git (push) Run the site locally The documentation site is built from the source files using Hugo. The repository README can be used as a reference for building instructions.\nRequirements  Hugo Node.js and other docsy theme dependencies:  postcss-cli autoprofixer    Installing dependencies To install npm+Node.js please check the official instructions.\nEverything has been tested with Node.js 12.\nThe dependencies of the docsy theme can be installed as follows:\n# From the root of the repository clone $ npm ci Hugo can be installed following the official documentation.\nHugo (extended) releases can be downloaded at the Hugo releases page.\nBuilding the site To build and run the site, from the repository root:\n$ git submodule update --init --recursive --depth 1 $ hugo --minify Testing the site locally To launch the site locally, from the repository root:\n$ hugo serve -D The site is available locally at: http://localhost:1313/.\nBranches and commits You should submit your patch as a git branch ideally named with a meaningful name related to the changes you want to propose. This is called a feature branch (sometimes also named topic branch). You will commit your modifications to this feature branch and submit a Pull Request (PR) based on the differences between the upstream main branch and your feature branch.\nCreate a feature branch Try to avoid committing changes to the main branch of your clone to simplify management, creating a dedicated feature branch helps a lot. Try to pick a meaningful name for the branch (my_nice_update in the example).\n# This should be done from the up-to-date main branch # Read furthermore to see documentation on updating a local clone $ git checkout -b my_nice_update Write changes The documentation being made of plain text files you are free to use whatever text editor or Integrated Development Environment (IDE) suits you, from neovim to Visual Studio Code.\nSome environments may provide you plugins helping with syntax or offering a preview, they are worth checking.\nBe sure to commit files with having been formated using Prettier as documented in our style guide.\nCommit changes It is the best practice to have your commit message have a summary line that includes the issue number, followed by an empty line and then a brief description of the commit. This also helps other contributors understand the purpose of changes to the code.\n#3 - platform_family and style * use platform_family for platform checking * update notifies syntax to \"resource_type[resource_name]\" instead of resources() lookup * GH-692 - delete config files dropped off by packages in conf.d * dropped debian 4 support because all other platforms have the same values, and it is older than \"old stable\" debian release # Select the modified files to be committed $ git add files1 path2/ # Commit the changes $ git commit -m \u003ccommit_message\u003e Push feature branch to the fork in preparation of a PR From inside a feature branch you can push it to your remote fork.\n# Ask git to keep trace of the link between local and remote branches $ git push --set-upstream Once done, the output will show a URL that you can click to generate a Pull Request (PR). Accessing GitHub upstream of forked repositories may also propose you to submit a PR.\nIf needed GitHub CLI can also be used to prepare the PR:\n$ gh pr create \u003cyour_username\u003e:\u003cfeature_branch\u003e --web Previewing a pull request If a repository maintainer adds the label safe for preview to a pull request it will be possible to preview it using a pull request-specific URL: https://docs.egi.eu/documentation/[PR_NUMBER]\nThe preview can be used as an alternative to testing a pull request locally, and the preview can easily be shared with other contributors.\nOnly collaborators having write permission to the repository are able to mark a pull request as safe for review.\n This should be carefully considered, especially for external and first time contributors.\n Update local feature branch with changes made on the PR Once you PR have been opened it will be reviewed, and reviewers can propose and commit changes to your PR. If you need to make further changes be sure to update the local clone with the remote changes.\n# Retrieve changes made on your PR in the upstream repository $ git pull Then you can commit new changes and push them to your remote fork.\nUpdate repository clone with the upstream changes # If you are still in a branch created for a previous PR, move to main $ git checkout main # Get the latest data from the upstream repository $ git fetch upstream # Update your local copy with this data $ git rebase upstream/main main # Update your remote GitHub fork with those changes $ git push Update local feature branch with changes made on the main branch In case the main branch evolved since the feature branch was created, it may be required to merge the new changes in the feature branch.\nIt can easily be done via the PR page on the GitHub web interface, but it can also be done in your repository clone using git rebase.\n# Retrieve changes made in the upstream repository $ git fetch upstream # Check out the feature branch $ git checkout feature_branch # Apply the new changes on main to your feature branch $ git rebase upstream/main In case some files have been changed on both sides you will will have to merge the conflicts manually.\nClone PR to edit/test/review locally It’s possible to clone a Pull Request to a local branch to test it locally. It’s done using the PR number.\n# List available PR and their identifiers. $ gh pr list # Clone specific PR, updating sudmodules $ gh pr checkout XX --recurse-submodules Once done it’s possible to build and run the site locally:\n# From the root of the repository clone # Here on MacOS X, adapt depending on your platform $ hugo serve -D The documentation will then be accessible on http://localhost:1313.\n People having write access to the repository hosting the branch related to the PR (ie. usually the PR author) will be able to add and edit files.\n # From the local clone of the repository $ gh pr checkout XXX --recurse-submodules $ vim yyy.zz $ git add yyy.zz $ git commit yyy.zz -m \u003ccommit_message\u003e $ git push Update a local clone of a PR # It will ask you to merge changes $ git pull Then you can refer to the README.md to see how to test it locally.\nIn case the PR got commits that were forced pushed you may have troubles, in that case it may be easier to delete the local branch and do another checkout of the PR.\nClean a local clone of a PR In case you have troubles updating the local clone, as it can happens if changes were forced pushed to it, it maybe easier to delete the local copy of the PR and recreate it.\n# Switch to main branch $ git checkout main # Check local branches $ git branch -vv # Delete a specific branch $ git branch -d \u003cbranch_name\u003e # If you need to force the deletion use -D $ git branch -D \u003cbranch_name\u003e Using stashes Sometimes we realise just before committing a change that we are not in the correct branch (ie. that we forgot to create a dedicated feature branch), when this happens git stash can be helpful.\n# Saving a change $ git stash save \u003coptional message\u003e # Creating the forgotten branch $ git checkout -b \u003cmy_feature_branch\u003e # Reviewing the saved changes, use TAB completion $ git stash show \u003cTAB\u003e # Applying the saved changes, use TAB completion $ git stash pop \u003cTAB\u003e # Review the changes to be committed $ git diff If you already committed your change(s) you may have to look at git reset.\n# Viewing the diff of the two last commits $ git log -n 2 -p # Reverting the last change, keeping the change in the local directory $ git reset HEAD^ ","categories":"","description":"First steps with Git and GitHub","excerpt":"First steps with Git and GitHub","ref":"/about/contributing/git/","tags":"","title":"Git and GitHub"},{"body":"This page contains information about integrating your identity provider (IdP) with Check-in in order to allow users in your community to access EGI tools and services.\nOrganisations who want to register their IdP in Check-in needs to fill this form in case the IdP is not publishing REFEDS R\u0026S and Sirtfi compliance in eduGAIN. A PDF scan of a printed and signed copy should be sent to operations_at_egi.eu\nIdentity Provider integration workflow To integrate your Identity Provider with the EGI Check-in service, you need to submit a GGUS ticket indicating your request. The responsible support unit is AAI Support. The integration follows a two-step process:\n Register your Identity Provider and test integration with the development instance of EGI Check-in. The development instance allows for testing authentication and authorisation to EGI services and resources without affecting the production environment of EGI. Note that the development instance is not connected to the production service and no information is shared between the two systems. Register your Identity Provider with the production instance of EGI Check-in to allow members of your Community to access production EGI services and resources protected by Check-in. This requires that your Identity Provider meets all the policy requirements and that integration has been thoroughly tested during Step 1.  The most important URLs for each environment are listed in the table below but more information can be found in the protocol-specific sections that follow.\n   Protocol Development environment Demo environment Production environment     SAML https://aai-dev.egi.eu/proxy/module.php/saml/sp/metadata.php/sso https://aai-demo.egi.eu/proxy/module.php/saml/sp/metadata.php/sso https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso   OpenID Connect See client registration See client registration See client registration    General requirements for integrating identity providers An institution or a community may connect their IdP with Check-in to allow their users to access EGI services, or any other services that have enabled Check-in as an authentication provider. This section presents the general requirements for integrating an IdP with EGI Check-in, while protocol-specific instructions are provided in the sections that follow.\nAttribute release requirements As a bare minimum, the IdP of a user’s Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community. The unique identifier must be accompanied with a minimum set of attributes which the Check-in Service Provider Proxy will attempt to retrieve from the user’s IdP. If this is not possible, the missing user attributes will be acquired and verified through the user registration process with the EGI Account Registry. The following table describes the data requested from the user’s Home Organisation, which are communicated to the Check-in SP as either SAML attributes or OIDC claims, depending on the protocol supported by the authenticating IdP.\n   Description Notes     At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier    Preferred name for display purposes For example to be used in a greeting or a descriptive listing   First name    Surname    Email address    Affiliation within Home Organisation or Community To be released only if relevant for accessing EGI services    Note that the above set of requested attributes, particularly the identifier, name, email and affiliation information, complies with the REFEDS R\u0026S attribute bundle.\nInformation about group membership and role information released by your IdP should follow the URN scheme below (see also AARC-G002):\n\u003cNAMESPACE\u003e:group:\u003cGROUP\u003e[:\u003cSUBGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e  where:\n \u003cNAMESPACE\u003e is in the form of urn:\u003cNID\u003e:\u003cDELEGATED-NAMESPACE\u003e[:\u003cSUBNAMESPACE\u003e*], where  \u003cNID\u003e is the namespace identifier associated with a URN namespace registered with IANA, as per RFC8141, ensuring global uniqueness. Implementers can and should use one of the existing registered URN namespaces, such as urn:geant and urn:mace; \u003cDELEGATED-NAMESPACE\u003e is a URN sub-namespace delegated from one of the IANA registered NIDs    to an organisation representing the e-infrastructure, research infrastructure or research collaboration.\n \u003cGROUP\u003e is the name of a VO, research collaboration or a top level arbitrary group. \u003cGROUP\u003e names are unique within the urn:mace:egi.eu:group namespace; zero or more \u003cSUBGROUP\u003e components represent the hierarchy of subgroups in the \u003cGROUP\u003e; specifying sub-groups is optional the optional \u003cROLE\u003e component is scoped to the rightmost (sub)group; if no group information is specified, the role applies to the VO \u003cGROUP-AUTHORITY\u003e is a non-empty string that indicates the authoritative source for the entitlement value. For example, it can be the FQDN of the group management system that is responsible for the identified group membership information  Example entitlement values expressing VO/group membership and role information:\nurn:geant:dariah.eu:group:egi-interop:role=member#aaiproxy.de.dariah.eu urn:geant:dariah.eu:group:egi-interop:role=vm_operator#aaiproxy.de.dariah.eu  Operational and security requirements The IdP needs to comply with additional requirements to achieve a higher level of assurance and allow its users to gain access to a wider set of EGI services. A first group of additional requirements are defined by the Sirtfi framework v1.0. Adherence to these requirements can be asserted either by publishing Sirtfi compliance in the eduGAIN metadata or by declaring it in this form. These requirements are in the areas of operational security, incident response, traceability and IdPs and users responsibility.\nBranding requirements Check-in provides a central Discovery Service (or “Where Are You From” - WAYF) page where users in your Home Organisation or Community will be automatically redirected when necessary to select to authenticate at your IdP. You can provide us with a logo of your Organisation or Community (in high-res PNG or preferably in svg format) to include a dedicated login button that will allow users to easily identify your IdP.\nSAML Identity Provider To allow users in your community to sign into federated EGI applications, you need to connect to the EGI AAI SP Proxy as a SAML Identity Provider (IdP). Users of the application will be redirected to the central Discovery Service page of the EGI AAI Proxy where they will able to select to authenticate at your IdP. Once the user is authenticated, the EGI AAI Proxy will return a SAML assertion to the application containing the information returned by your IdP about the authenticated user.\nMetadata registration SAML authentication relies on the use of metadata. Both parties (you as an IdP and the EGI AAI SP) need to exchange metadata in order to know and trust each other. The metadata include information such as the location of the service endpoints that need to be invoked, as well as the certificates that will be used to sign SAML messages. The format of the exchanged metadata should be based on the XML-based SAML 2.0 specification. Usually, you will not need to manually create such an XML document, as this is automatically generated by all major SAML 2.0 IdP software solutions (e.g., Shibboleth, SimpleSAMLphp). It is important that you serve your metadata over HTTPS using a browser-friendly SSL certificate, i.e. issued by a trusted certificate authority.\nTo exchange metadata, please send an email including the following information:\n entityID Metadata URL  Depending on the software you are using, the authoritative XML metadata URL for your IdP might be in the following form:\n https://your.idp.example.eu/idp/shibboleth (Shibboleth) https://your.idp.example.eu/simplesaml/module.php/saml2/idp/metadata.php (SimpleSAMLphp)  Note that if your IdP is part of a federation, then it would be preferred to send us the URL to a signed federation metadata aggregate. We can then cherry pick the appropriate entityID from that.\nYou can get the metadata of the EGI Check-in SP Proxy on a dedicated URL that depends on the integration environment being used:\n   Development environment Demo environment Production environment     https://aai-dev.egi.eu/proxy/module.php/saml/sp/metadata.php/sso https://aai-demo.egi.eu/proxy/module.php/saml/sp/metadata.php/sso https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso    For the production environment, it is recommended that you get the metadata for the EGI Check-in SP (entityID: https://aai.egi.eu/proxy/module.php/saml/sp/metadata.php/sso) from a signed eduGAIN metadata aggregate. For example, the following aggregates are provided by GRNET:\n GRNET federation's metadata eduGAIN SP metadata  Attribute release The SAML based Identity Provider of your Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community, along with a set of additional information as described in the following table (see also general attribute release requirements):\n   Description SAML attribute     At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier SubjectID (public) or eduPersonUniqueIdeduPersonPrincipalNameSubjectID (pairwise) or eduPersonTargetedID or SAML persistent identifier   Preferred name for display purposes displayName   First name givenName   Surname sn   Email address mail   Affiliation within Home Organisation or Community eduPersonScopedAffiliation   Group(s)/role(s) within Home Organisation or Community eduPersonEntitlement    OpenID Connect Identity Provider Users in your community can sign into federated EGI applications through the Check-in service using your OpenID Connect or OAuth 2.0 based Identity Provider.\nClient registration To enable your OIDC Identity Provider for user login, Check-in needs to be registered as a client in order to obtain OAuth 2.0 credentials, such as a client ID and client secret, and to register one or more redirect URIs. Once Check-in is registered as a client, your users will be redirected to the central Discovery Service page of Check-in when logging into EGI federated applications, where they will able to select to authenticate at your IdP. Once the user is authenticated, Check-in will be responsible for communicating the information returned by your IdP about the authenticated user to the connected application. Depending on the protocol, this information will be expressed through a SAML assertion, a set of OIDC claims or a (proxy) X.509 certificate.\nProvider configuration Check-in needs to obtain your OpenID Provider's configuration information, including the location of the Authorisation, Token and UserInfo endpoints. Your OpenID Provider is expected to make a JSON document available at the path formed by concatenating the string /.well-known/openid-configuration to the Issuer, following the OpenID Connect Discovery 1.0 specification.\nAttribute release The OpenID Connect or OAuth 2.0 based Identity Provider of your Home Organisation or Community is expected to release a non-reassignable identifier that uniquely identifies the user within the scope of that organisation or community, along with a set of additional information as described in the following table (see also general attribute release requirements):\n   Description OIDC claim     At least one of the following unique user identifiers:pseudonymous, non-targeted identifier;name-based, non-targeted identifier;pseudonymous, targeted identifier sub (public)N/Asub (pairwise)   Preferred name for display purposes name   First name given_name   Surname family_name   Email address email   Affiliation within Home Organisation or Community eduperson_scoped_affiliation   Group(s)/role(s) within Home Organisation or Community eduPerson_entitlement    Integration success stories  EGI AAI integration with ELIXIR  ","categories":"","description":"Check-in guide for Identity Providers","excerpt":"Check-in guide for Identity Providers","ref":"/providers/check-in/idp/","tags":"","title":"Identity Providers"},{"body":"Document control    Property Value     Title How to publish Site Information   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement Publishing site information in the Information Discovery System   Owner SDIS team    EGI profile for the use of the GLUE 2.0 Information Schema specifies how the GLUE 2.0 information schema should be used in EGI. It gives detailed guidance on what should be published, how the information should be interpreted, what kinds of uses are likely, and how the information may be validated to ensure accuracy.\nConfiguring a site BDII The site BDII needs to be configured to read from every node in the site which publishes information (meaning that it runs a so-called resource BDII). In YAIM this is defined with the BDII_REGIONS variable, which contains a list of node names which in turn refer to variables called BDII_\u003cNODE\u003e_URL which specify the LDAP URL of each resource BDII.\nSome services may have DNS aliases for multiple hosts, but the BDII_REGIONS must contain the real hostnames for each underlying node - the information in the resource BDII is different for each node, so reading it via an alias would produce inconsistent results. However, it will usually be desirable for the published endpoint URLs to contain the alias rather than the real hostname; that can often be defined with a YAIM variable for the service. For the site BDII itself this variable is SITE_BDII_HOST. (If multiple site or top BDIIs are configured identically their content will also be identical, so reading via an alias does not produce any inconsistencies.)\nMost services now publish themselves, so sites should check that all relevant services are included. In particular, VOMS servers have only published themselves comparatively recently so may be missing from the configuration. If the glite-CLUSTER node type is used this must also be included. Publication has been enabled for Argus in EMI 2, so this may also need to be added. Common services which do not currently publish are APEL and Squid. See the table below for more detailed information.\nIt is important to realise that the site BDII itself has a resource BDII, and this must be explicitly included in the configuration, e.g. with something like\nBDII_REGIONS=\"CE SE BDII\" (...) BDII_BDII_URL=\"ldap://$SITE_BDII_HOST:2170/mds-vo-name=resource,o=grid\" In the past it was common for the site BDII to be colocated with the CE so it did not need to be listed explicitly, but if installed on a dedicated node (which is now the recommended deployment) it must be included.\nTo check that all expected services are published the following command can be used:\n$ ldapsearch -x -h $SITE_BDII_HOST -p 2170 -b mds-vo-name=$SITE_NAME,o=grid \\  objectclass=GlueService \\  | perl -p00e 's/\\r?\\n //g' | grep Endpoint: (replacing SITE_BDII_HOST; and SITE_NAME with the values for your site), which should list all the service URLs.\nIn addition, most services should now be published in GLUE 2 format. There is no explicit configuration needed for GLUE 2, but one thing to be aware of is that the site name (and the other parts like o=grid) in the GIIS URL field in the GOCDB must have the correct case as GLUE 2 is case-sensitive.\nTo verify the GLUE 2 publication use the command:\n$ ldapsearch -x -h $SITE_BDII_HOST -p 2170 -b GLUE2DomainID=$SITE_NAME,o=glue \\  objectclass=GLUE2Endpoint \\  | perl -p00e 's/\\r?\\n //g' | grep URL: Some services, notably storage elements, may be missing or incomplete in GLUE 2 if they are older than the EMI 2 release. The following table shows the publishing status for gLite and WLCG node types (ARC and Unicore have a different structure).\n   Node type GLUE 1 GLUE 2 Notes     LCG-CE Yes No Obsolete   CREAM Yes Yes Full publication only in EMI 2   CLUSTER Yes Yes Full publication only in EMI 2   WMS Yes Yes    LB Yes Yes    DPM Yes EMI 2    dCache Yes EMI 2    StoRM Yes EMI 2    LFC Yes EMI 2    FTS Yes EMI 2 Channels not yet published in GLUE 2   Hydra EMI 2 EMI 2 Not yet released in EMI 2   AMGA Yes EMI 2    VOMS Yes Yes    MyProxy Yes Yes    Argus No EMI 2 Internal service, publication for deployment monitoring   Site BDII Yes Yes    Top BDII Yes Yes    R-GMA Yes No Obsolete   VOBOX Yes Yes    Apel No No Internal service, publishing not yet requested   Squid No No Configuration exists but not enabled   Nagios Yes Yes     Service-related documentation Federated Cloud BDII configuration For information about configuration of a Federated Cloud BDII, please look at the EGI Information System.\nGlueSite Object These are the existing well established attributes in the GlueSite object. All of these MUST remain.\n   Attribute Example Schema Notes     GlueSiteName RAL-LCG2 Free text, no whitespace Same as GOCDB name if in GOCDB, your choice.   GlueSiteUniqueID RAL-LCG2 Identical to your !GlueSiteName Same as GlueSiteName   GlueSiteWeb https://cern.ch/it Free Text Valid URL about the site.   GlueSiteLatitude 52.42 NN.NN Site Latitute.   GlueSiteLongitude 16.91 NN.NN Longitude of Site.   GlueSiteDescription Rutherford Lab Free Text A long name for the site.   GlueSiteLocation Dublin, Ireland Town, City, Country An decreasing resolution ending with Country, agree a country name within a country. i.e UK != United Kingdom. Scotland and the Balkans should write a dynamic provider.   !GlueSiteUserSupportContact mailto:helpdesk@example.com Valid URL URL for getting support. A ticket   system if available.      !GlueSiteSysAdminContact xmpp://admins@jabber.org Valid URL How to contact the admins.   !GlueSiteSecurityContact mailto:security@example.com Valid URL How to contact for security related matters.    The GlueSite object in the 1.3 Glue Schema contains an attribute GlueSiteOtherInfo. To quote.\n The attribute is to be used to publish data that does not fit any other attribute of the site entity. A name=value pair or an XML structure are example[s] of usage.\n All this extra configuration will be with in the static information for the glue site within the Grid Information Provider system.\nGuidelines for GlueSite Object A format for publishing useful information about sites within the !GlueSiteOtherInfo is needed, as shown in the following table.\n   Key Example Type Notes     GRID EGI [#validgrid List of valid grid names] Multiple ones can be defined.   WLCG_TIER 1 Tier level of site in WLCG context. Either 0, 1 , 2 , 3 , 4   WLCG_PARENT UK-T1-RAL Name of the higher (administrative) tier site in WLCG The WLCG_NAME of the site at a higher tier with WLCG   WLCG_NAME IT-ATLAS-federation [#lcgnames Valid WLCG Names] An official WLCG name.   WLCG_NAMEICON https://example.com/tier2.png Valid URL URL to WLCGNAME icon, ideally 80x80 pixels.   EGEE_ROC Russia Valid federated Operations Centre name Only applicable if your site is still part of a federated Operations Centre (“ROC” according to the old EGEE terminology). Name MUST match the Operations Centre name declared in GOCDB. Note. If the site is now part of a NGI, then EGI_NGI MUST be used (see below).   EGI_NGI NGI_CZ Valid NGI Must agree with the GOC DB   EGEE_SERVICE prod prod, pps or cert Which EGEE grid your site is part of, multiple attributes is okay. Obsolete in EGI.   OLDNAME Bristol text If your !GlueSiteName changes at some point please record your old name here.   ICON https://example.com/icon.png Valid URL Icon Image for your site, ideally 80x80 pixels   BLOG https://scotgrid.blogspot.com/feeds/posts/default Valid RSS or Atom Feed Your site blog if you have one   CONFIG yaim yaim, puppet, quattor, … The configuration tool(s) used at the site    Note. Keywords starting with one of the grid names are to some extent reserved for that grid.\nExample GlueSiteName: RAL-LCG2 GlueSiteOtherInfo: BLOG=https://example.com/blog/feed GlueSiteOtherInfo: EGI_NGI=NGI_UK GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: GRID=GRIDPP GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: ICON=https://example.com/images/tierOneSmall.png GlueSiteOtherInfo: WLCG_PARENT=CERN-PROD GlueSiteOtherInfo: WLCG_TIER=1 Distributed Tier1s and Tier2s Within an WLCG context for instance there are instances of distributed Tier2s and Tier1s. If separate component sites want to exist as a single WLCG tier then they might contain common values for their WLCGNAME.\nGlueSiteName: CSCS-LCG2 GlueSiteOtherInfo: CONFIG=yaim GlueSiteOtherInfo: EGI_NGI=NGI_CH GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: WLCG_NAME=CH-CHIPP-CSCS GlueSiteOtherInfo: WLCG_PARENT=FZK-LCG2 GlueSiteOtherInfo: WLCG_TIER=2 Note that WLCG_PARENT is an accounting unit defined in the MOU document, as shown in WLCG CRIC.\nEstablished Grid Name    Short Name Long Name URL     EGI European Grid Initiative https://www.egi.eu   EELA Europe and Latin America https://www.eu-eela.eu/   WLCG World LHC Computing Grid https://cern.ch/lcg   GRIDPP UK Particle Physics Grid https://www.gridpp.ac.uk   UKNGS National UK Grid Service https://www.ngs.ac.uk   OSG Open Science Grid (US) https://www.opensciencegrid.org/   NDGF Nordic DataGrid Facility https://www.ndgf.org/   LondonGrid London Grid https://www.gridpp.ac.uk/tier2/london/   NORTHGRID Northern (UK) Grid https://www.gridpp.ac.uk/northgrid/   SCOTGRID Scottish Grid https://www.scotgrid.ac.uk/   SOUTHGRID Southern (UK) Grid https://www.gridpp.ac.uk/southgrid/   Academic Grid Malaysia Malaysian Grid    UPM Campus Grid Universiti Putra Malaysia https://www.upm.edu.my/   AEGIS Academic and Educational Grid Initiative of Serbia https://www.aegis.rs/   BIGGRID Dutch e-science Grid https://www.biggrid.nl/   Consorzio Cometa Consorzio Multi-Ente per la promozione e l’adozione di Tecnologie di calcolo Avanzato (Italy) https://www.consorzio-cometa.it/en   D-Grid German Grid https://www.d-grid-gmbh.de/index.php?id=1\u0026amp;L=1   EUMED EU/Mediterranean Grid https://www.eumedgrid.eu/   GILDA Grid INFN Laboratory for Dissemination Activities (Italy) https://gilda.ct.infn.it/   GISELA Grid Initiative for e-Science virtual communities in Europe and Latin America https://www.gisela-grid.eu/   GRISU Griglia del Sud (Southern Italy Grid) https://www.grisu-org.it/   NEUGRID Neuroscience Grid https://neugrid4you.eu/background   RDIG Russian Data Intensive Grid https://grid-eng.jinr.ru/?page_id=43   SEE-GRID South Eastern European GRid-enabled eInfrastructure Development https://www.see-grid.org/     Important: The EGEE Grid name was decomissioned on [[Agenda-14-02-2011|14-02-2011]]. All sites need to replace this grid name with EGI.\n Being part of a grid is just a reference that your site is in some way associated with a particular Resource Infrastructure Provider either technically or as part of a collaboration. The list of Grids can be extended. Please contact operations@egi.eu to request changes.\nValid WLCG Names The WLCG names are the site names that appear within the LCG MOU concerning commitments to LHC computing.\n   WLCG Name Current GlueSiteName     CA-TRIUMF TRIUMF-LCG2   CERN CERN-PROD   DE-KIT FZK-LCG2   ES-PIC pic   FR-CCIN2P3 IN2P3-CC   IT-INFN-CNAF INFN-T1   NDGF NGDF-T1   NL-T1 SARA-MATRIX   TW-ASGC Taiwan-LCG2   UK-T1-RAL RAL-LCG2   US-FNAL-CMS USCMS-FNAL-W1   US-T1-BNL BNL-LCG2    For the tier two names please consult WLCG CRIC. The column marked Accounting Name are the WLCG Names which in the case of Tier2s are the GOCDB names. Use your site GOCDB name as your WLCG_NAME.\nAlso some tier2s live under more than 1 tier1 perhaps for different for different VOs. If your tier2 has more that one WLCG_PARENT then just add two distinct records to show this. Also some tier2s do not have a WLCGNAME at all.\nGlueSiteUniqueId: EENet GlueSiteName: EENet GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: EGI_NGI=NGI_NL GlueSiteOtherInfo: WLCG_TIER=2 GlueSiteOtherInfo: WLCG_PARENT=UK-T1-RAL GlueSiteOtherInfo: WLCG_PARENT=NL-T1 Valid EGI NGI Names The valid names are those published on GOCDB.\nYAIM Instructions YAIM will have to be updated for those sites using yaim. This will be done and submitted to sites in the normal way.\n   YAIM Variable and Value Resulting Glue Attribute and Value     SITE_NAME=RAL_LCG2 GlueSiteName: RAL-LCG2   SITE_DESC=“Rutherford Lab” GlueSiteDescription: Rutherford Lab   SITE_EMAIL= steve@example.com GlueSiteSysAdminContact: mailto:steve@example.com   SITE_SUPPORT_EMAIL= steve@example.com GlueSiteUserSupportContact: mailto:steve@example.com   SITE_SECURITY_EMAIL= steve@example.com GlueSiteSecurityContact: mailto:steve@example.com   SITE_LOC=“Soho, London, United Kingdom” GlueSiteLocation: Soho, London, United Kingdom   SITE_LONG=52.45 GlueSiteLongitude: 52.45   SITE_LAT=-12.34 GlueSiteLatitude: -12.34   SITE_WEB=“https://example.com/\" GlueSiteWeb: https://example.com/   SITE_OTHER_GRID=“EGI|WLCG” GlueSiteOtherInfo: GRID=EGI\nGlueSiteOtherInfo: GRID=WLCG   SITE_OTHER_EGEE_ROC=“UK/I” GlueSiteOtherInfo: EGEE_ROC=UK/I   SITE_OTHER_EGI_NGI=“NGI_CZ” GlueSiteOtherInfo: EGI_NGI=NGI_CZ   SITE_OTHER_EGEE_SERVICE=“prod” GlueSiteOtherInfo: EGEE_SERVICE=prod   SITE_OTHER_WLCG_TIER=2 GlueSiteOtherInfo: WLCG_TIER=2   SITE_OTHER*=\"|” GlueSiteOtherInfo: KEY=GlueSiteOtherInfo: KEY=    If multiple values for GlueSiteOtherInfo are needed, then just delimit your values with a |. The character | must be avoided in values.\nCheck your own GlueSite Object The information published can be checked through an ldap search:\n$ ldapsearch -x -H ldap://$SITE_BDII_HOST:2170 \\  -b 'Mds-Vo-Name=$SITE_NAME,o=Grid' \\  '(ObjectClass=GlueSite)' In addition, VAPOR is a tool which provides a GUI for different views of published information, including a LDAP view.\nSite information in GLUE 2 The GLUE 2 equivalent of the GlueSite object is the GLUE2AdminDomain. The same information should be present although in a slightly different format, and there are separate GLUE2Contact and GLUE2Location objects.\n","categories":"","description":"How to publish site information","excerpt":"How to publish site information","ref":"/providers/operations-manuals/man01_how_to_publish_site_information/","tags":"","title":"MAN01 How to publish site information"},{"body":"IaaS providers are very welcome to join the EGI Federated Cloud as a Resource Centres (RC) and joining the Federated Cloud Task Force to contribute to the design, creation and implementation of the federation.\nResource Centers are free to use any Cloud Management Framework (OpenStack, etc...) as long as they are able to integrate with the EGI Federation components as described in the Federated Cloud Architecture. At the moment this compliance is guaranteed for OpenStack.\nThe general minimal requirements are:\n Hardware requirements greatly depend on your cloud infrastructure, EGI components in general do lightweigth operations by interacting with your services APIs.  cloudkeeper requires enough disk space to download and convert images before uploading into your local catalogue. The number and size of images which will be downloaded depends on the communities you plan to support. For the piloting VO fedcloud.egi.eu, 100GB of disk should be enough.   Servers need to authenticate each other in the EGI Federated Cloud context using X.509 certificates. So a Resource Centre should be able to obtain server certificates for some services. User and research communities are called Virtual Organisations (VO). Resource Centres are expected to join:  ops and dteam VOs, used for operational purposes as per RC OLA a community-VO that supports EGI users (e.g. vo.access.egi.eu for piloting)   EGI provides packages for the following operating systems (others may work but we are not providing packages):  CentOS 7 (and in general RHEL-compatible) Ubuntu 16.04 (and in general Debian-based)    ","categories":"","description":"Requirements for integration","excerpt":"Requirements for integration","ref":"/providers/cloud-compute/requirements/","tags":"","title":"Requirements"},{"body":"Identity card    Property Value     Name Configuration Database   Description Central registry of the infrastructure topology   URL https://goc.egi.eu   Support Email gocdb-admin \u003cat\u003e mailman.egi.eu   Helpdesk Support Unit EGI Services and Service Components  I__ Configuration Database (GOCDB)   Configuration Database entry https://goc.egi.eu/portal/index.php?Page_Type=Site\u0026id=335   Supplier UKRI   Roadmap Roadmap   Release notes Release notes   Source code https://github.com/GOCDB   Issue tracker for developers https://github.com/GOCDB/gocdb/issues   License Apache 2   Privacy Policy Privacy policy    ","categories":"","description":"Technical details of the Configuration Database","excerpt":"Technical details of the Configuration Database","ref":"/internal/configuration-database/service-information/","tags":"","title":"Service information"},{"body":"Sign up You need to sign up for an account for accessing the EGI services. As part of this process you will be assigned a personal unique EGI ID which will be then used across all EGI tools and services. Follow the instructions below to get started:\n  Go to https://aai.egi.eu/signup. This will show you the identity provider discovery page: browse through the list of Identity Providers to find your Home Organisation, or, alternatively, type the name of your Home Organisation in the search box. Note that the names are localised based on the selected language.\n  Enter your login credentials to authenticate yourself with your Home Organisation\n  After successful authentication, you may be prompted by your Home Organisation to consent to the release of personal information to the EGI AAI Service Provider Proxy.\n  After successful authentication, you will be redirected to the EGI account registration form. On the introductory page, click Begin to start the registration process.\n  EGI requires some basic information from you, depending on the attributes released by your Identity Provider, you may need to provide the values of the missing attributes.\n  On the registration form, click Review Terms and Conditions (Acceptable Use Policy and Conditions of Use - EGI AUP)\n  If you agree to the Terms of Use, select the I Agree option.\nImportant You will not be able to agree to the terms until you review them.    Finally, click Submit to submit your request.\nImportant You will not be able to submit your request until you agree to the terms.    After submitting your request, Check-in will send you an email with a verification link. After you click that link, you’ll be taken to the request confirmation page.\nImportant If you do not find the email in your Inbox, please check your Spam or Junk folder for an email from “EGI AAI Notifications”. If you do find the email in these folders, mark the email as “safe” or “not spam” to ensure that you receive any future notifications about your EGI ID.    After reviewing your request, click Confirm and re-authenticate yourself using the Identity Provider you selected before.\n  In the case of the Sign Up registration, you need to wait for an EGI User Sponsor to approve your request to join the EGI User Community. Upon approval, EGI AAI will send you a notification email.\n  Note: After your registration has been completed, you can manage your profile through the EGI Account Registry portal.\nViewing user profile information The profile includes all the information related to the user. This information can be categorised as follows:\nBasic profile Includes the basic information about your profile:\n Name Identifiers Email addresses  VO/Group membership and roles Includes information about the Virtual Organisations and groups the user if member of and the roles assigned to the user within those Virtual Organisation.\nLinked identities Information about identites linked to your account. Check the guide for linking accounts for more information.\n","categories":"","description":"Register an account with Check-in to get access to EGI services\n","excerpt":"Register an account with Check-in to get access to EGI services\n","ref":"/users/check-in/signup/","tags":"","title":"Sign up for an EGI Account"},{"body":"According to AARC-G002 the information about the groups a user is a member of is commonly used by Service Providers in order to authorise user access to protected resources.The entity responsible for disseminating this information is the EGI Check-in AAI proxy and the format used is that of a URN namespace, called eduPersonEntitlement, that is uniformly interpreted across infrastructures.\nThe general form of the eduPersonEntitlement string is:\n\u003cNAMESPACE\u003e:group:\u003cVO\u003e[:\u003cGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e\nAs a result, an eduPersonEntitlement string informing the Service Provider that the user has the role Associate in the vo.example.eu VO (modelled as a COU) is:\nurn:mace:egi.eu:group:vo.example.eu:role=associate#aai.egi.eu\nEntitlement Construction For the case of the CO Person with a profile/canvas, like the one provided above, we expect to get entitlements for all the entries listed under the tab Role Attributes. Additionally, we will get entitlements for all the General Purpose(GP) Groups enlisted under the tab Groups. These GP Groups have no prefix, neither CO: nor CO:COU, and no postfix, neither active nor all.\nVO(COU) For each entry in the table Role Attributes, that is in status Active or Grace Period, we create one eduPersonEntitlement for each different Role and for the Affiliation. For example, the CO Person from above is affiliated as Member to the VO vo.example.eu and has been assigned the role of an Associate. This will generate two entitlements as:\nurn:mace:egi.eu:group:vo.example.eu:role=associate#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.eu:role=member#aai.egi.eu\nVO Groups (sub COUs) There are occasions where we need a VO to be organized in subgroups. For example vo.example.eu contains the sub-COU vo.example-sub.eu.\nThe CO Person is affiliated as member and with the Role of Support in the VO sub-group vo.example-sub.eu:\nIn such occasions the eduPersonEntitlement will have the following structure:\nurn:mace:egi.eu:group:vo.example.eu:vo.example-sub.eu:role=support#aai.egi.eu\nurn:mace:egi.eu:group:vo.example.eu:vo.examples-sub.eu:role=member#aai.egi.eu\n","categories":"","description":"Expressing VO group membership and role information\n","excerpt":"Expressing VO group membership and role information\n","ref":"/users/check-in/vos/expressing-vo-information/","tags":"","title":"VO membership information"},{"body":"What is it? The EGI Federated Cloud (FedCloud) Task Force gathers together scientific communities, R\u0026D projects, and technology and resource providers so they can design the tools and services that support the federation of providers, share best practices, and offer user support and training in a collaborative fashion. This enables community cloud solutions to develop faster, with a lower cost, and with a more sustainable future.\nObjectives The task force members:\n Capture requirements from user communities needing federated cloud services. Identify, integrate and enhance open source tools and services that enable cloud federations for research and education. Develop and maintain tools and services to fill gaps in third-party solutions to reach production quality cloud federations. Provide consultancy for communities on how to build a federated cloud that meets custom community demands. Provide training and support for existing and potential users of cloud federations about topics such as:  How to port existing applications to the cloud, or develop cloud-native applications How to operate services in the cloud How to join a cloud federation as a provider   Facilitate the reuse of tools and services across participating cloud federations to lower total cost of development and to improve cloud sustainability. Promote Platform-as-a-Service (PaaS) and Software-as-a-Service (SaaS) environments that are proven to be robust and reusable, to communities that interact with federated Infrastructure-as-a-Service (IaaS) providers in the EGI infrastructure. Provide service management and security oversight for participating clouds and cloud federations. Act as a discussion forum where cloud federations can be discussed and specific questions can be analysed with experts. Organise dissemination and marketing events, such as workshops and conferences relating to the topics of the collaboration.  Tip If you are interested in joining the EGI FedCloud Task Force, send an email to fedcloud-tf \u003cat\u003e mailman.egi.eu and introduce yourself.  Note We hold bi-weekly meetings on Tuesdays at 11.00 CE(S)T. Check our Indico category for minutes and upcoming events.  ","categories":"","description":"The task force behind the EGI Federation\n","excerpt":"The task force behind the EGI Federation\n","ref":"/users/getting-started/task-force/","tags":"","title":"EGI Task Force"},{"body":"To access the web interface of the EGI Configuration Database (GOCDB), users can either:\n Use EGI Check-in with an institutional account, or Use an X.509 digital certificate installed in the internet browser, or the local machine’s certificate store.  Users can access the system as soon as they are authenticated. However, they will only be able to update information based on their roles. More information about roles and associated permission is available in the Users and roles section of the documentation.\nApplications requesting a specific role have to be validated by parent roles or administrators. Once granted, users can access and/or modify relevant information, according to the roles granted to them.\nUsing institutional account via EGI Check-in In order to be able to access the Configuration Database with their institutional account, users need to:\n Have their Identity Provider (IdP) federated in EGI Check-in (via eduGAIN or directly). Have created an EGI Check-in account.  Important In the case the user cannot use an IdP compliant with REFEDS R\u0026S and REFEDS Sirtfi, the user will have to request joining a specific group, by performing the steps below. Using a compliant IdP is the preferable solution.\n User should ask to join the GOCDB user group. The access request will be managed by the EGI Operations team.   Using an X.509 digital certificate To access the Configuration Database using a digital certificate, first obtain a certificate from one of the recognised EU-Grid-PMA Certification Authorities (CAs), then install it in your browser of choice (or import it into the certificate store of your local machine, on Windows).\nNote X.509 certificates do not support single or double quotes in the certificate’s Distinguished Name (DN). The DN below is rejected because of the single quote:\n/C=UK/O=STFC/OU=SomeOrgUnit/CN=David Mc'Donald\nThis is in accordance with RFC1778, which also disallows single quotes in all Relative Distinguished Name (RDN) components, and the OGF Certificate Authority Working Group (CAOPS) who strongly discourage any type of quote in a certificate DN as specified by their Grid Certificate Profile document.\n ","categories":"","description":"Accessing the Configuration Database","excerpt":"Accessing the Configuration Database","ref":"/internal/configuration-database/access/","tags":"","title":"Access"},{"body":"What is it? EGI Accounting tracks and reports usage of EGI services, offering insights and control over resource consumption. EGI Federation members can use it to account for the resource usage of their own services.\nEGI Accounting consists of two main components:\n The Accounting Repository is where all accounting data is collected by a network of message brokers that transfer usage data from hosts and services. The Accounting Portal allows filtering and displaying resource usage information.  Note Documentation for the Accounting Repository is available in the EGI Wiki, documentation for the Accounting Portal is also available in the EGI Wiki.  ","categories":"","description":"Resource usage accounting for EGI services\n","excerpt":"Resource usage accounting for EGI services\n","ref":"/internal/accounting/","tags":"","title":"Accounting"},{"body":"There are two different processes handling the accounting integration:\n cASO, which connects to the OpenStack deployment to get the usage information, and, ssmsend, which sends that usage information to the central EGI accounting repository.  They should be run by cron periodically, settings below run cASO every hour and ssmsend every six hours.\nUsing the VM Appliance cASO configuration is stored at /etc/caso/caso.conf. Most default values should be ok, but you must set:\n  site_name (line 12), with the name of your site as defined in GOCDB.\n  projects (line 20), with the list of projects you want to extract accounting from.\n  credentials to access the accounting data (lines 28-47, more options also available). Check the cASO documentation for the expected permissions of the user configured here.\n  The mapping from EGI VOs to your local projects /etc/caso/voms.json, following this format: :\n{ \"vo name\": { \"projects\": [ \"project A that accounts for the vo\", \"project B that accounts for the VO\" ] }, \"another vo\": { \"projects\": [\"project C that accounts for the VO\"] } }   cASO will write records to /var/spool/apel from where ssmsend will take them.\nSSM configuration is available at /etc/apel. Defaults should be OK for most cases. The cron file uses /etc/grid-security for the CAs and the host certificate and private keys (/etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem).\nRunning the services Both caso and ssmsend are run via the root user crontab. For convenience there are two scripts /usr/local/bin/caso-extract.sh and /usr/local/bin/ssm-send.sh that run the docker container with the proper volumes.\n","categories":"","description":"Accounting integration\n","excerpt":"Accounting integration\n","ref":"/providers/cloud-compute/openstack/accounting/","tags":"","title":"Accounting"},{"body":"The EGI Applications on Demand (AoD) service is EGI’s response to the requirements of researchers who are interested in using applications in a on-demand fashion together with the compute and storage environment needed to compute and store data.\nYou can order and access the service through the EGI Marketplace.\nService Description The service combines compute and storage cloud with Application-development and hosting frameworks to run custom scientific applications, and/or to turn those into applications into online data analysis services that can be accessed by scientists worldwide.\nThe portfolio of applications is currently composed by a readily available set of applications relevant to different scientific and research areas. This portfolio is open to be extended thanks to the contributions of users of the service. If you are interested in this, please get in touch with us: support (at) egi.eu\nIntended user groups The main target groups of this service are:\n Application developers who want to make their applications and tools accessible in a scalable way for researchers internationally. Algorithm developers (researchers) who want to run their own codes at scale in a compute cloud.  The main target groups of the applications that are already hosted in this service are:\n Researchers and innovators who want to run a specific scientific application that is already available in the platform.  Scientific applications The scientific applications that are already available in this service are:\n Chipster a user-friendly analysis software for high-throughput data. It contains over 300 analysis tools for next generation sequencing (NGS), microarray, proteomics and sequence data. The application is available through the Science Software on Demand Service (SSoD). Instructions to run the application are available here. NAMD a parallel molecular dynamics code designed for high-performance simulation of large bio-molecular systems. The application is available through the EC3 portal. ECAS a complete environment enabling data analysis experiments from the ENES Climate Analytics Service.  The service includes:\n  Cloud compute and storage resources to host and scale up scientific applications.\n  Cloud access and application-hosting frameworks (to run and to operate your own scientific application in the cloud environment): that offer integrated development environments to port custom applications with cloud resources.\n  VMOps dashboard: a graphical environment for the management of Virtual Machines (VM) in the federated network of clouds that enable the Applications on Demand service. User documentation is available here.\n  Elastic Cloud Compute Cluster (EC3): a portal that allows the creation of elastic virtual clusters in the cloud. Those clusters can then host your scientific application either directly, or via Apache Mesos, Chronos, Kubernetes, Marathon, OSCAR or SLURM. Instructions for application developers are available here.\n  Science Software on Demand (SSoD): a programmable interface of a RESTful API Server to provide an easy access PaaS layer by leveraging recent Web technologies. Instructions for application developers are available here\n  Requirements and user registration The service is open for any scientific software developer who needs a scalable and user-friendly application execution and hosting environment to offer data/compute intensive scientific applications online.\nAccess requires acceptance of Acceptable Use Policy (AUP) and Conditions of the 'EGI Applications on Demand Service'.\nAcknowledgment Users of the service are asked to provide appropriate acknowledgement of the use in scientific publications. The following acknowledgement text can be used for this purpose (you can adapt to match the exact providers in your case):\nThis work used advanced computing resources from the 100%IT, CESGA, CLOUDIFIN, CYFRONET-CLOUD, GSI-LCG2, IFCA-LCG2, IN2P3-IRES, INFN-CATANIA-STACK, INFN-PADOVA-STACK, SCAI, TR-FC1-ULAKBIM, UA-BITP and UNIV-LILLE resource centres of the EGI federation. The services are co-funded by the EGI-ACE project (grant number 101017567).\n When requesting access to AoD users are guided through a lightweight registration process. Members of the EGI support team will perform a lightweight vetting process to validate the users' requests before granting the access to the resources.\nService grant Once granted access, each user will have a grant with a predefined quota of resources, which can be used to run the application of choice. This grant includes:\n up to 4 CPU cores, 8 GB of RAM, 100GB of block storage.  The grant to run applications is initially valid for 6 months and can be extended/renewed upon request.\nHow can you access the service?  Login to the EGI Marketplace with the EGI AAI Check-In service. Setup a profile, including details about your affiliation and role within a research institute/project/team. Navigate the marketplace top-menu and click on the category: Applications. Click on the Applications on Demand service and submit an order for one of the available applications. When the request is approved, run the requested application(s) as described below.  Please check the EGI Marketplace guide for further details.\nReferences Main scientific paper describing the service and status:\n EGI Applications On Demand Service - Catering for the computational needs of the long tail of science (May 2017). IWSG2017 Proceeding.  Presentations about the service:\n Slideset about the Applications on Demand (AoD) service introduced at IWSG 2017 (June 2017). Webinar to introduce the Applicatios on Demand (AoD) service to NGIs/USTs representatives, RI architects, resource providers and researchers (June 2017). Slideset about the status report of the platform at the EGI Conference 2016. Slideset about the status report of the EGI platform at the DI4R Conference 2016. Overview of the EGI Infrastructure for serving the long tail (EGI Community Forum, November 2015). Poster and animated slides from Demo at EGI Community Forum, November 2015 (Winner of best demo prize). Slideset about the authentication and authorization model adopted (from Nov. 2015). Slideset about the concept of the EGI long-tail of science platform (from Nov. 2014).  ","categories":"","description":"The [EGI Applications on Demand](https://www.egi.eu/services/applications-on-demand/) (AoD) service\n","excerpt":"The [EGI Applications on …","ref":"/users/applications-on-demand/","tags":"","title":"Applications on Demand (AoD)"},{"body":"Authentication OpenID Connect is the main authentication protocol used on the EGI Cloud. It replaces the legacy VOMS-based authentication for all OpenStack providers.\nAuthentication to web based services (like the AppDB) will redirect you to the EGI Check-in authentication page. Just select your institution or social login and follow the regular authentication process.\nAccess to APIs or via command-line interfaces (CLI) requires the use of OAuth2.0 tokens and interaction with the OpenStack Keystone OS-FEDERATION API. The process for authentication is as follows:\n Obtain a valid OAuth2.0 access token from Check-in. Access tokens are short-lived credentials that can be obtained by recognised Check-in clients once a user has been authenticated. Interchange the Check-in access token for a valid unscoped Keystone token. Discover available projects from Keystone using the unscoped token. Use the unscoped Keystone token to get a scoped token for a valid project. Scoped tokens will allow the user to perform operations on the provider.  Authorisation Cloud Compute service is accessed through Virtual Organisations (VOs). Users that are members of a VO will have access to the providers supporting that VO: they will be able to manage VMs, block storage and object storage available to the VO. Resources (VMs and storage) are shared across all members of the VO, please do not interfere with the VMs of other users if you are not entitled to do so (specially do not delete them).\nSome users roles have special consideration in VOs:\n Users with VO Manager, VO Deputy or VO Expert Role have extra privileges in the AppDB for managing the Virtual Appliances to be available at every provider. Check the Virtual Machine Image Management documentation for more information.  Pilot VO The vo.access.egi.eu Virtual Organisation serves as a test ground for users to try the Cloud Compute service and to prototype and validate applications. It can be used for up to 6 month by any new user.\nWarning  After the 6-month long membership in the vo.access.egi.eu VO, you will need to move to a production VO, or establish a new VO. The resources are not guaranteed and may be removed without notice by providers. Back-up frequently to avoid losing your work!   For joining this VO, just place an order in the EGI Marketplace and once approved you will be able to interact with the infrastructure.\nOther VOs Pre-existing VOs of EGI can be also used on IaaS cloud providers. Consult with your VO manager or browse the existing VOs at the EGI Operations Portal.\nCheck-in and access tokens Access tokens can be obtained via several mechanisms, usually involving the use of a web server and a browser. Command-line clients/APIs without access to a browser or interactive prompt for user authentication can use refresh tokens. A refresh token is a special token that is used to generate additional access tokens. This allows you to have short-lived access tokens without having to collect credentials every single time one expires. You can request this token alongside the access and/or ID tokens as part of a user’s initial authentication flow.\nIf you need to obtain these kind of tokens for using it in command-line tools or APIs, you can easily do so with the special fedcloud client. You can access the FedCloud Check-in client and click on 'Authorise' to log in with your Check-in credentials to obtain:\n a client ID (fedcloud) a refresh token  Refresh tokens Refresh tokens should be treated with care! This is a secret that can be used to impersonate you in the infrastructure. It is recommended not to store them in plain text.  Alternatively, you can use the oidc-agent tool that is able to manage your tokens locally, or the fedcloud client executed inside EGI Notebooks.\nDiscovering projects in Keystone The access token will provide you access to a cloud provider, but you may have access to several different projects within that provider (a project can be considered equivalent to a VO allocation). In order to discover which projects are available you can do that using the Keystone API.\nYou can use the fedcloud client to simplify the discovery of projects.\n# Get a list of sites (also available in [AppDB](https://appdb.egi.eu)) fedcloud site list # Get list of projects that you are allowed to access # You can either specify the name of the account in your oidc-agent configuration # or directly a valid access token fedcloud endpoint projects --site=\u003cname of the site\u003e \\  [--oidc-agent-account \u003caccount name\u003e|--oidc-access-token \u003caccess token\u003e] # You can also use environment variables for the configuration export EGI_SITE=\u003cname of the site\u003e export OIDC_ACCESS_TOKEN=\u003cyour access token\u003e fedcloud endpoint projects # or with oidc-agent export OIDC_AGENT_ACCOUNT=\u003caccount name\u003e fedcloud enpoint projects Using the OpenStack API Once you know which project to use, you can use your regular openstack cli commands for performing actual operations in the provider:\nfedcloud openstack image list --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e For third-party tools that can use token based authentication in OpenStack, use the following command:\nexport OS_TOKEN=$(fedcloud openstack --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e \\  token issue -c id -f value) Legacy X.509 AAI Warning OpenID Connect is the preferred federated identity technology on EGI Cloud. Use of X.509 certificates should be limited to legacy applications.  VOMS uses X.509 proxies extended with VO information for authentication and authorisation on the providers. You can learn about X.509 certificates and VOMS in the Check-in documentation.\nVOMS configuration Valid configuration for fedcloud.egi.eu is available on the FedCloud client VM as generated by the fedcloud-ui installation script.\nVOMS client expects your certificate and private key to be available at $HOME/.globus/usercert.pem and $HOME/.globus/userkey.pem respectively.\nAccess the providers VOMS authentication differs from one provider to another depending on the technology used. There are 3 different cases handled automatically by the rOCCI-cli. For accessing native OpenStack sites there are two different plugins available for Keystone that are installed with a single library:\npip install openstack-voms-auth-type For Keystone-VOMS based installations (Keystone URL ending on /v2.0), just define the location of your proxy and v2voms as authorisation plugin:\nopenstack --os-auth-url https://\u003ckeystone-url\u003e/v2.0 \\  --os-auth-type v2voms --os-x509-user-proxy /tmp/x509up_u1000 \\  token issue +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-02-04T12:41:25+0000 | | id | gAAAAABcWCTlMoz6Jx9IHF5hj-ZOn-CI17CfX81FTn7yy0ZJ54jkza7QNoQTRU5-KRJkphmes55bcoSaaBRnE3g2clFgY-MR2GVUJZRkCmj9TXsLZ-hVBWXQNENiX9XxUwnavj7KqDn4b9B1K22ijTrjdDVkcdpvMw | | user_id | 9310054c2b6f4fd28789ee08c2351221 | +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+ For those Keystone installations supporting only v3, specify v3voms as authorisation plugin, egi.eu as identity provider, mapped as protocol, and the location of your proxy:\nopenstack --os-auth-url https://\u003ckeystone url\u003e/v3 \\  --os-auth-type v3voms --os-x509-user-proxy /tmp/x509up_u1000 \\  --os-identity-provider egi.eu --os-protocol mapped \\  token issue +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-02-04T12:45:32+0000 | | id | gAAAAABcWCXcXGUDpHUYnI1IDLW3MnEpDzivw_OPaau8DQDYxA7gK9XsmOqZh1pL5Uqqs8aM-tHowdJQnJURww2-UhmQVqk5PxbjdnvLeqtXPYURCLaSsbmhkQg6kB311c_ZA1jfgdT-pG6fZz3toeH66SEFX-H0bThSUy0KFLhcZVkrZIbYgTsAOIzFkTfLjOgTw_tNChS8 | | user_id | 50fa8516b2554daeae652619ba9ebf96 | +---------+---------------------------------------------------------------------------------------------------------------------------------------- ","categories":"","description":"Authentication and Authorisation on EGI Cloud\n","excerpt":"Authentication and Authorisation on EGI Cloud\n","ref":"/users/cloud-compute/auth/","tags":"","title":"Authentication and Authorisation"},{"body":"","categories":"","description":"Integration with Check-in for IdPs and SPs","excerpt":"Integration with Check-in for IdPs and SPs","ref":"/providers/check-in/","tags":"","title":"Check-in"},{"body":"Chipster is a user-friendly software for analysing high-throughput sequencing and microarray data, provided as part of EGI's Applications on Demand service.\nThe software contains over 400 analysis tools and a large collection of reference genomes.\nUsers can save and share automatic analysis workflows, and visualize data interactively using for example the built-in genome browser.\nEGI Chipster The Chipster testbed configured at CESGA offers:\n 8 vCPU cores, 32GB of RAM, 1TB of block storage in /data, Software and tools are in available under the /cvmfs/tools_* partition, Chipster (v3.16.3).  For accessing this testbed you need to be a member of the Applications on Demand.\nCreate/Review a temporary account Go to Chipster entry in Science Software on Demand Portal and log in with your EGI Check-In account\nThe first time you log in, you need to generate a temporary password by clicking on the \"Execute\" button, as shown below.\nPlease refresh the web page after your password has been successfully generated. Once the page is reloaded (see screenshot below) you are presented with a link to access the Chipster server that you can access with the new credentials.\nIf your account has expired the Science Gateway will automatically generate a new password for you. Just click on the \"Show form\" button to manage your credentials.\nAcknowledgment Please provide appropriate acknowledgement of the use of this service in your scientific publications. Here is an example:\nThis work used the EGI Applications on Demand service, which is co-funded by the EOSC-hub project (grant number 777536)\n ","categories":"","description":"Chipster on Applications on Demand\n","excerpt":"Chipster on Applications on Demand\n","ref":"/users/applications-on-demand/chipster/","tags":"","title":"Chipster"},{"body":"Overview The FTS3 service offers a command-line client to ease the interaction with the service.\nPrerequisites The client software is available for RHEL 6 and 7 derivatives.\nPlease note that the RHEL 6 support is ending the 30/11/2020 and the implementation for RHEL 8 is on-going.\nUsers from other distributions should refer to the RESTFul API section.\nInstallation The CLI can be installed from the EPEL repositories for RHEL 7 with the following package:\nyum install fts-rest-cli -y Commands This section describes some of the commands that can be issues via the FTS CLI. As per the API, in order to authenticate to the FTS REST server you need an X.509 User certificate, please refer to this section\nfor more information.\nCheck the full documentation about the FTS CLI\nfts-rest-whoami This command can be used to check, as the name suggests, who are we for the server.\nUsage fts-rest-whoami [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example $ fts-rest-whoami -s https://fts3-public.cern.ch:8446 User DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi VO: dteam VO id: 6b10f4e4-8fdc-5555-baa2-7d4850d4f406 Delegation id: 9ab8068853808c6b Base id: 01874efb-4735-4595-bc9c-591aef8240c9 fts-rest-delegate This command can be used to (re)delegate your credentials to the FTS3 server.\nUsage fts-rest-delegate [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) -f/--force : Force the delegation -H/--hours : Duration of the delegation in hours (default: 12) Example $ fts-rest-delegate -s https://fts3-public.cern.ch:8446 Delegation id: 9ab8068853808c6b fts-rest-transfer-submit This command can be used to submit new jobs to FTS3. It supports simple and bulk submissions. The bulk format is as follows:\n{ \"files\": [ { \"sources\": [ \"gsiftp://source.host/file\" ], \"destinations\": [ \"gsiftp://destination.host/file\" ], \"metadata\": \"file-metadata\", \"checksum\": \"ADLER32:1234\", \"filesize\": 1024 }, { \"sources\": [ \"gsiftp://source.host/file2\" ], \"destinations\": [ \"gsiftp://destination.host/file2\" ], \"metadata\": \"file2-metadata\", \"checksum\": \"ADLER32:4321\", \"filesize\": 2048, \"activity\": \"default\" } ] } Usage fts-rest-transfer-submit [options] SOURCE DESTINATION [CHECKSUM] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) -b/--blocking : Blocking mode. Wait until the operation completes. -i/--interval : Interval between two poll operations in blocking mode. -e/--expire : Expiration time of the delegation in minutes. --delegate-when-lifetime-lt : Delegate the proxy when the remote lifetime is less than this value (in minutes) -o/--overwrite : Overwrite files. -r/--reuse : Enable session reuse for the transfer job. --job-metadata : Transfer job metadata. --file-metadata : File metadata. --file-size : File size (in bytes) -g/--gparam : Gridftp parameters. -t/--dest-token : The destination space token or its description. -S/--source-token : The source space token or its description. -K/--compare-checksum : Deprecated: compare checksums between source and destination. -C/--checksum-mode : Compare checksums in source, target, both or none. --copy-pin-lifetime : Pin lifetime of the copy in seconds. --bring-online : Bring online timeout in seconds. --timeout : Transfer timeout in seconds. --fail-nearline : Fail the transfer is the file is nearline. --dry-run : Do not send anything, just print the json message. -f/--file : Name of configuration file --retry : Number of retries. If 0, the server default will be used. If negative, there will be no retries. -m/--multi-hop : Submit a multihop transfer. --cloud-credentials : Use cloud credentials for the job (i. E. Dropbox). --nostreams : Number of streams --ipv4 : Force ipv4 --ipv6 : Force ipv6 Example fts-rest-transfer-submit -s https://fts3-public.cern.ch:8446 \\  gsiftp://source.host/file gsiftp://destination.host/file Job successfully submitted. Job id: 7e02b4fa-d568-11ea-9c80-02163e018681 $ fts-rest-transfer-submit -s https://fts3-public.cern.ch:8446 -f test.json Job successfully submitted. Job id: 9a28d204-d568-11ea-9c80-02163e018681 fts-rest-transfer-status This command can be used to check the current status of a given job.\nUsage fts-rest-transfer-status [options] JOB_ID Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example fts-rest-transfer-status -s https://fts3-public.cern.ch:8446 \\  7e02b4fa-d568-11ea-9c80-02163e018681 Request ID: 7e02b4fa-d568-11ea-9c80-02163e018681 Status: FAILED Client DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi Reason: One or more files failed. Please have a look at the details for more information Submission time: 2020-08-03T09:05:36 Priority: 3 VO Name: dteam fts-rest-transfer-cancel This command can be used to cancel a running job. It returns the final state of the cancelled job. Please, mind that if the job is already in a final state (FINISHEDDIRTY, FINISHED, FAILED), this command will return this state. You can additionally cancel only a subset appending a comma-separated list of file IDs.\nUsage fts-rest-transfer-cancel [options] Options -h/--help : Show this help message and exit -v/--verbose : Verbose output. -s/--endpoint : Fts3 rest endpoint. -j/--json : Print the output in json format. --key : The user certificate private key. --cert : The user certificate. --capath : Use the specified directory to verify the peer --insecure : Do not validate the server certificate --access-token : Oauth2 access token (supported only by some endpoints, takes precedence) Example fts-rest-transfer-cancel -s https://fts3-public.cern.ch:8446 9a28d204-d568-11ea-9c80-02163e018681 CANCELED ","categories":"","description":"Documentation related to EGI Data Transfer Clients","excerpt":"Documentation related to EGI Data Transfer Clients","ref":"/users/data-transfer/clients/","tags":"","title":"Clients"},{"body":"The EGI documentation is a static site built using Hugo from Markdown source files. Hugo uses goldmark to parse and render markdown, which is compliant with CommonMark and GitHub Flavored Markdown (also based on CommonMark).\nThe EGI documentation is organized into sections and pages. Read below to uderstand when to use each of these, and how to create new sections and add new pages to a section.\nImportant Avoid using pages in the documentation for now, create a distinct section for every page, because currently there is no way to automatically validate links in pages.  Sections Sections are those pages that can have subpages. They always appear in bold in the left-side navigation tree: Sections are also pages, meaning that selecting them in the navigation tree will show their content.\nCreating sections To create a new section, create a folder under /content/\u003clanguage\u003e/, add a file named _index.md to it, then author content for it as described below.\nNote Sections immediately under /content/\u003clanguage\u003e/ can show up in the top-level navigation bar. See below for details on how to control this.  Pages Pages are Markdown files that contain the documentation about a specific topic. They hold the content for a section (in which case are named _index.md and the containing folder is the section), or a stand-alone page that is immediately under a section (the containing folder is the section).\nThis is how stand-alone pages appear in the left-side navigation tree: Creating pages Creating a documentation page is done by creating a Markdown file (with .md extension) under the relevant section (in the section’s folder).\nNote When authoring pages please observe and adhere to the Style Guide.  Page metadata Each page needs some metadata that controls where the page appears and how its content will be rendered. The beginning of the Markdown file contains a front matter in YAML, holding the metadata of the page:\n---title:\"Style\"linkTitle:\"Style Guide\"description:\"Style guide for EGI documentation\"type:docsweight:30---The parameter weight controls the order of the pages on the same level in the left-side navigation tree. Pages will appear in ascending order of their weight.\nThe above metadata produces the following result (relevant elements highlighed): Add page to top navigation bar Pages can be added to the top navigation bar by using a front matter like this:\n--- title: \"About\" description: \"About EGI Documentation\" type: docs menu: main: weight: 50 --- Pages will be added to the top navigation bar in ascending order of their menu weight, from left to right.\nIf you also want to add an icon to the entry in the top navigation bar:\n--- title: \"About\" description: \"About EGI Documentation\" type: docs menu: main: weight: 50 pre: \u003ci class='fa fa-info'\u003e\u003c/i\u003e --- Embedding images (or other content) Hugo organizes content for each page into a subfolder with the same name as the page’s filename. This allows authors to easily keep track of the resources used by each page.\nLet’s assume we have a section named About with a subpage Concepts, using the following hierarchy of files:\n/documentation /content /en /about _index.md concepts.md map.png /concepts metadata.png Embedding an image in the page of the section (the file _index.md) can be done with:\n![Image title](map.png) Embedding an image in a subpage can be done by editing the Markdown file of the page (concepts.md in our case):\n![Image title](metadata.png) ![Image title](../map.png)  Note For subpages, the (relative) path to the image already includes a folder with the same name as the file’s base name.  Linking to pages You can include hyperlinks in the documentation that will link to any documentation page, or to external resources.\nAssuming we have the same section named About with a subpage Concepts as used above, and the file _index.md contains this:\nThis is a link to a [page](concepts) This is another link to the same [page](concepts.md) while the page concepts.md contains this:\nThis is link to a [section](../) This another link to a [section](../../about) the links in these pages exemplify how to link to a different page, be it a section or a stand-alone page.\nNote When linking to a stand-alone page, if you omit the file extension, .md will be assumed.  Linking to documentation headings You can also include hyperlinks in the documentation that will link to any heading (aka chapter) from any documentation page.\nTo link to a heading in a documentation page, you need to point the hyperlink to the anchor created automatically by Markdown for the targeted heading.\nNote You can derive the anchor from the name of the target heading, by converting it to lowercase, removing non-alphanumeric characters, replacing spaces with dashes, and keeping exactly one dash separating each subsequent word pair in the anchor.  Below you have an example of a page with sample headings and links to those headings:\nThis is a link to a [chapter](#some-chapter-title) This is a link to another [subchapter](#some-subchapter-title) ## Some chapter title ... ### Some subchapter title ... Assuming we have the same section named About with a subpage Concepts as used above, and the file _index.md contains this:\n## Section chapter This is a link to a [page chapter](concepts#page-chapter) This is another link to the same [page chapter](concepts.md#page-chapter) while the page concepts.md contains this:\n## Page chapter This s link to a [section chapter](../#section-chapter) This s link to a [section chapter](../../about#section-chapter) the links in these pages exemplify how to link to headings in a different page or section.\nGlossary The EGI Glossary is available on the EGI Glossary space.\n ","categories":"","description":"Concepts used when writing documentation","excerpt":"Concepts used when writing documentation","ref":"/about/concepts/","tags":"","title":"Concepts"},{"body":"The EGI Docker VA is a ready-to-use Virtual Machine Image with docker and docker-compose pre-installed.\nYou can start that image as any other VA available from AppDB:\n  Go to the EGI Docker image entry in AppDB.\n  Check the identifiers of the endpoint, image and flavor you want to use at the provider.\n  Use a ssh key when, so you can log into the VM once it's instantiated.\n  Once up, just ssh in the VM and start using docker as usual.\n  Using docker from inside the VM You can log in with user ubuntu and your ssh key:\nssh -i \u003cyourprivatekey\u003e ubuntu@\u003cyour VM ip\u003e Once in, you can run any docker command, e.g.:\nubuntu@fedcloud_vm:~$ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b901d36b6f2f: Pull complete 0a6ba66e537a: Pull complete Digest: sha256:8be990ef2aeb16dbcb9271ddfe2610fa6658d13f6dfb8bc72074cc1ca36966a7 Status: Downloaded newer image for hello-world:latest Hello from Docker. This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker Hub account: https://hub.docker.com For more examples and ideas, visit: https://docs.docker.com/userguide/ Docker-compose can be also used to execute applications with more than one container running together, follow this documentation to learn more.\n","categories":"","description":"Run containers on the EGI Cloud on a single VM with Docker\n","excerpt":"Run containers on the EGI Cloud on a single VM with Docker\n","ref":"/users/cloud-container-compute/docker/","tags":"","title":"Docker VM"},{"body":"You can find here documentation on how to deploy a sample SLURM cluster, which you can then adapt to create other kind of clusters easily.\nGetting started We will use docker for running EC3, direct installation is also possible and described at EC3 documentation. First get the docker image:\ndocker pull grycap/ec3 And check that you can run a simple command:\n$ docker run grycap/ec3 list name state IP nodes ------------------------ For convenience we will create a directory to keep the deployment configuration and status together.\nmkdir ec3-test cd ec3-test You can list the available templates for clusters with the templates command:\n$ docker run grycap/ec3 templates name kind summary ---------------------------------------------------------------------------------------------------------------------- blcr component Tool for checkpointing applications. [...] sge main Install and configure a cluster SGE from distribution repositories. slurm main Install and configure a cluster using the grycap.slurm ansible role. slurm-repo main Install and configure a cluster SLURM from distribution repositories. [...] We will use the slurm template for configuring our cluster.\nSite details EC3 needs some information on the site that you are planning to use to deploy your cluster:\n authentication information network identifiers VM image identifiers  We will use the fedcloud client to discover the required information. Set your credentials as shown in the authentication guide and create the autorisation files needed for ec3 (in this case for CESGA with VO vo.access.egi.eu):\nfedcloud ec3 init --site CESGA --vo vo.access.egi.eu This will generate an auth.dat file with your credentials to access the site and a templates/refresh.radl with a refresh token to allow long running clusters to be managed on the infrastructure.\nLet’s get also some needed site information. Start getting the available networks, we will need both a public and private network:\n$ fedcloud openstack --site CESGA --vo vo.access.egi.eu network list +--------------------------------------+----------------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+----------------------+--------------------------------------+ | 12ffb5f7-3e54-433f-86d0-8ffa43b52025 | net-vo.access.egi.eu | 754342b1-92df-4fc8-9499-2ee8b668141f | | 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 | public00 | 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 | +--------------------------------------+----------------------+--------------------------------------+ Then, get the list of images available:\n$ fedcloud openstack --site CESGA --vo vo.access.egi.eu image list +--------------------------------------+----------------------------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+----------------------------------------------------------+--------+ | 9d22cb3b-e6a3-4467-801a-a68214338b22 | Image for CernVM3 [CentOS/6/QEMU-KVM] | active | | b03e8720-d88a-4939-b93d-23289b8eed6c | Image for CernVM4 [CentOS/7/QEMU-KVM] | active | | 06cd7256-de22-4e9d-a1cf-997b5c44d938 | Image for Chipster [Ubuntu/16.04/KVM] | active | | 8c4e2568-67a2-441a-b696-ac1b7c60de9c | Image for EGI CentOS 7 [CentOS/7/VirtualBox] | active | | abc5ebd8-f65c-4af9-8e54-a89e3b5587a3 | Image for EGI Docker [Ubuntu/18.04/VirtualBox] | active | | 22064e93-6af9-430b-94a1-e96473c5a72b | Image for EGI Ubuntu 16.04 LTS [Ubuntu/16.04/VirtualBox] | active | | d5040b3e-ef33-4959-bb88-5505e229f579 | Image for EGI Ubuntu 18.04 [Ubuntu/18.04/VirtualBox] | active | | 79fadf3f-6092-4bb7-ab78-9a322f0aad33 | cirros | active | +--------------------------------------+----------------------------------------------------------+--------+ For our example we will use the EGI CentOS 7 with id 8c4e2568-67a2-441a-b696-ac1b7c60de9c.\nFinally, with all this information we can create the images template for EC3 that specifies the site configuration for our deployment. Save this file as templates/centos.radl:\ndescription centos-cesga ( kind = 'images' and short = 'centos7-cesga' and content = 'CentOS7 image at CESGA' ) network public ( provider_id = 'public00' and outports contains '22/tcp' ) network private (provider_id = 'net-vo.access.egi.eu') system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' and disk.0.os.credentials.username = 'centos' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and ec3_max_instances = 5 and # maximum number of worker nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' and disk.0.os.credentials.username = 'centos' ) Note we have used public00 as public network and opened port 22 to allow ssh access. The private network uses net-vo.access.egi.eu. We have two kind of VMs in almost every deployment: the front, that runs the batch system, and the wn, that will execute the jobs. In our example, both will use the same CentOS image, which is specified with the disk.0.image.url = 'ost://fedcloud-osservices.egi.cesga.es/8c4e2568-67a2-441a-b696-ac1b7c60de9c' line: ost refers to OpenStack, fedcloud-osservices.egi.cesga.es is the hostname of the URL obtained above with fedcloud endpoint list and 8c4e2568-67a2-441a-b696-ac1b7c60de9c is the ID of the image in OpenStack. The size of the VM is also specified.\nLaunch cluster We are ready now to deploy the cluster with ec3 (this can take several minutes):\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 launch mycluster slurm ubuntu refresh -a auth.dat Creating infrastructure Infrastructure successfully created with ID: 74fde7be-edee-11ea-a6e9-da8b0bbd7c73 Front-end configured with IP 193.144.46.234 Transferring infrastructure Front-end ready! We can check the status of the deployment:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 list name state IP nodes ---------------------------------------------- mycluster configured 193.144.46.234 0 And once configured, ssh to the front node. The is_cluster_ready command will report whether the cluster is fully configured or not:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 ssh mycluster Warning: Permanently added '193.144.46.234' (ECDSA) to the list of known hosts. Last login: Thu Sep 3 14:07:46 2020 from torito.i3m.upv.es $ bash cloudadm@slurmserver:~$ is_cluster_ready Cluster configured! cloudadm@slurmserver:~$ EC3 will deploy CLUES, a cluster management system that will power on/off nodes as needed depending on the load. Initially all the nodes will be off:\nnode state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn2 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn3 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn4 off enabled 00h03'55\" 0,0.0 1,1073741824.0 wn5 off enabled 00h03'55\" 0,0.0 1,1073741824.0 SLURM will also report nodes as down:\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 5 down* wn[1-5] As we submit a first job, some nodes will be powered on to meet the request. You can also start them manually with clues poweron.\ncloudadm@slurmserver:~$ srun hostname srun: Required node not available (down, drained or reserved) srun: job 2 queued and waiting for resources srun: job 2 has been allocated resources wn1.localdomain cloudadm@slurmserver:~$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1 idle enabled 00h07'45\" 0,0.0 1,1073741824.0 wn2 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn3 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn4 off enabled 00h52'25\" 0,0.0 1,1073741824.0 wn5 off enabled 00h52'25\" 0,0.0 1,1073741824.0 cloudadm@slurmserver:~$sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up infinite 4 down* wn[2-5] debug* up infinite 1 idle wn1 Destroying the cluster Once you are done with the cluster and want to destroy it, you can use the destroy command. If your cluster was created more than one hour ago, your credentials to access the site will be expired and need to refreshed first with fedcloud ec3 refresh:\n$ fedcloud ec3 refresh # refresh your auth.dat $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 list # list your clusters name state IP nodes ---------------------------------------------- mycluster configured 193.144.46.234 0 $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy mycluster -a auth.dat -y WARNING: you are going to delete the infrastructure (including frontend and nodes). Success deleting the cluster! ","categories":"","description":"Getting started with Elastic Cloud Compute Cluster on EGI Cloud with the Command Line Interface\n","excerpt":"Getting started with Elastic Cloud Compute Cluster on EGI Cloud with …","ref":"/users/cloud-compute/ec3/cli/","tags":"","title":"EC3 CLI"},{"body":"Welcome to the home page of Service Providers. This section provides documentation aimed at Service Providers across the EGI Federation.\nAny service provider can join the EGI Federation and in doing so benefit from being part of the major worldwide e-Infrastructure that is EGI. From a technical point of view, the Federation may be joined by integrating your resources with federation services such as Check-in, the EGI Federation Authentication and Authorisation Infrastructure (AAI) service. This may be done at no cost, and resources can immediately benefit from EGI federation services.\nIn order to make a difference in helping to define the strategic direction of the EGI Federation and its federation services, countries or international organisaionts can join the EGI Council. More details may be found on the EGI site.\nSee the users section for User-centric documentation.\n","categories":"","description":"Documentation for service providers","excerpt":"Documentation for service providers","ref":"/providers/","tags":"","title":"Service Provider Guides"},{"body":"What is it? Grid storage enables storage of files in a fault-tolerant and scalable environment, and sharing it with distributed teams. Your data can be accessed through multiple protocols, and can be replicated across different providers to increase fault-tolerance. Grid storage gives you complete control over what data you share, and with whom you share the data.\nThe main features of grid storage:\n Access highly-scalable storage from anywhere Control the data you share Organise your data using a flexible, hierarchical structure  Grid storage file access is based on the gridFTP and WebDav/HTTP protocols, together with XRootD and legacy SRM (under deprecation at some of the endpoints).\nSeveral grid storage implementations are available in the EGI Cloud, the most common being:\n dCache DPM StoRM  Endpoint discovery The grid storage endpoints that are available to a user’s Virtual Organizations are discoverable via the EGI Information System (BDII).\nThe lcg-infosites command can be used to obtain VO-specific information on existing grid storages, using the following syntax:\n$ lcg-infosites --vo voname -[v] -f [site name] [option(s)] [-h| --help] [--is BDII] For example, to list the Storage Elements (SEs) available to the biomed VO, we could issue the following command:\n$ lcg-infosites --vo biomed se Avail Space(kB) Used Space(kB) Type SE ------------------------------------------ 280375465082 n.a SRM ccsrm.ihep.ac.cn 10995116266 11 SRM cirigridse01.univ-bpclermont.fr Access from the command-line Access to grid storage via a command-line interface (CLI) requires users to obtain a valid X.509 user VOMS proxy. Please refer to the Check-in documentation for more information.\nNote Integration via OpenID Connect to the EGI Check-in service is under piloting at some of the endpoints of the EGI Cloud infrastructure , but it has not yet reached the production stage.  The CLI widely used to access grid-storage is gfal2, which is available for installation both on RHEL and Debian compatible systems.\nIn particular, gfal2 provides an abstraction layer on top of several storage protocols (XRootD, WebDAV, SRM, gsiftp, etc), offerint a convenient API that can be used over different protocols.\nThe gfal2 CLI can be installed as follows (for RHEL compatible systems):\n$ yum install gfal2-util gfal2-all where gfal2-all will install all the plug-ins (to deal with all the available protocols).\nBelow you can find examples of the usual commands needed to access storage via gfal2. For a complete list of available commands, and the guide on how to use them, please refer to the gfal2-util documentation.\nNote In the examples below, the used gsiftp protocol can be replaced by any other supported protocol.  List files on a given endpoint $ gfal-ls gsiftp://dcache-door-doma01.desy.de/dteam 1G.header-1 domatest gb SSE-demo test tpctest Create a folder $ gfal-mkdir gsiftp://dcache-door-doma01.desy.de/dteam/test Copy a local file $ gfal-copy test.json gsiftp://dcache-door-doma01.desy.de/dteam/test Copying file:///root/Documents/test.json [DONE] after 0s Copy files between storages $ gfal-copy gsiftp://prometheus.desy.de/VOs/dteam/public-file gsiftp://dcache-door-doma01.desy.de/dteam/test Copying gsiftp://prometheus.desy.de/VOs/dteam/public-file [DONE] after 3s Download a file to a local folder $ gfal-copy gsiftp://prometheus.desy.de/VOs/dteam/public-file /tmp Copying gsiftp://prometheus.desy.de/VOs/dteam/public-file [DONE] after 0s Delete a file $ gfal-rm gsiftp://dcache-door-doma01.desy.de/dteam/test/public-file gsiftp://dcache-door-doma01.desy.de/dteam/test/public-file DELETED Access via EGI Data Transfer The EGI Data Transfer service provides mechanisms to optimize the transfer of files between EGI Online Storage endpoints. Both a graphical user interface (GUI) and command-line interfaces (CLI) are available to perform bulk movement of data. Please check out the related documentation for more information.\nIntegration with Data Management frameworks Grid storage access, most of the time, is hidden from users by the integration with the Data Management Frameworks (DMFs) used by Collaborations and Experiments.\nFor example, the EGI Workload Manager provides a way to efficiently access grid storage endpoints in order to read/store files, and to catalogue the existing file and related metadata.\nWhen running computation via the EGI Workload Manager, users do not actually access the storage directly. However, users can retrieve the output of the computation once it has been stored on the grid.\n","categories":"","description":"Grid Storage offered by EGI HTC providers\n","excerpt":"Grid Storage offered by EGI HTC providers\n","ref":"/users/online-storage/grid-storage/","tags":"","title":"Grid Storage"},{"body":"Templates We will build a torque cluster on one of the EGI Cloud providers using EC3. Create a directory to store EC3 configuration and init it with the fedcloud client:\nmkdir -p torque cd torque fedcloud ec3 init --site \u003cyour site\u003e --vo \u003cyour vo\u003e We will use the following templates:\n torque (from ec3 default templates) nfs (from ec3 detault templates), ubuntu-1604 (user’s template), cluster_configure (user’s template)  You can find the content below (make sure that you adapt them to your needs):\ntemplates/ubuntu-1604.radl specifies the VM image to use in the deployment:\ndescription ubuntu-1604 ( kind = 'images' and short = 'Ubuntu 16.04' and content = 'FEDCLOUD Image for EGI Ubuntu 16.04 LTS [Ubuntu/16.04/VirtualBox]' ) system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 4 and memory.size \u003e= 8196 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://\u003curl\u003e/\u003cimage_id\u003e' and disk.0.os.credentials.username = 'ubuntu' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048m and ec3_max_instances = 10 and # maximum number of working nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://\u003curl\u003e/\u003cimage_id\u003e' and disk.0.os.credentials.username = 'ubuntu' ) templates/cluster_configure.radl customises the torque deployment to match our needs:\nconfigure front ( @begin --- - vars: - USERS: - { name: user01, password: \u003cPASSWORD\u003e } - { name: user02, password: \u003cPASSWORD\u003e } [..] tasks: - user: name: \"{{ item.name }}\" password: \"{{ item.password }}\" shell: /bin/bash append: yes state: present with_items: \"{{ USERS }}\" - name: Install missing dependences in Debian system apt: pkg={{ item }} state=present with_items: - build-essential - mpich - gcc - g++ - vim become: yes when: ansible_os_family == \"Debian\" - name: SSH without password include_role: name: grycap.ssh vars: ssh_type_of_node: front ssh_user: \"{{ user.name }}\" loop: '{{ USERS }}' loop_control: loop_var: user - name: Updating the /etc/hosts.allow file lineinfile: path: /etc/hosts.allow line: 'sshd: XXX.XXX.XXX.*' become: yes - name: Updating the /etc/hosts.deny file lineinfile: path: /etc/hosts.deny line: 'ALL: ALL' become: yes @end ) configure wn ( @begin --- - vars: - USERS: - { name: user01, password: \u003cPASSWORD\u003e } - { name: user02, password: \u003cPASSWORD\u003e } [..] tasks: - user: name: \"{{ item.name }}\" password: \"{{ item.password }}\" shell: /bin/bash append: yes state: present with_items: \"{{ USERS }}\" - name: Install missing dependences in Debian system apt: pkg={{ item }} state=present with_items: - build-essential - mpich - gcc - g++ - vim become: yes when: ansible_os_family == \"Debian\" - name: SSH without password include_role: name: grycap.ssh vars: ssh_type_of_node: wn ssh_user: \"{{ user.name }}\" loop: '{{ USERS }}' loop_control: loop_var: user - name: Updating the /etc/hosts.allow file lineinfile: path: /etc/hosts.allow line: 'sshd: XXX.XXX.XXX.*' become: yes - name: Updating the /etc/hosts.deny file lineinfile: path: /etc/hosts.deny line: 'ALL: ALL' become: yes @end ) Create the cluster Deploy the cluster using ec3 docker image:\n$ docker run -it -v $PWD:/root/ -w /root \\  grycap/ec3 launch torque_cluster \\  torque nfs ubuntu-1604 refresh cluster_configure \\  -a auth.dat Creating infrastructure Infrastructure successfully created with ID: 529c62ec-343e-11e9-8b1d-300000000002 Front-end state: launching Front-end state: pending Front-end state: running IP: 212.189.145.XXX Front-end configured with IP 212.189.145.XXX Transferring infrastructure Front-end ready! To access the cluster, use the command:\n$ docker run -ti -v $PWD:/root/ -w /root grycap/ec3 ssh torque_cluster Warning: Permanently added '212.189.145.140' (ECDSA) to the list of known hosts. Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 3.13.0-164-generic x86_64) * Documentation: https://help.ubuntu.com/ Last login: Tue Feb 19 13:04:45 2019 from servproject.i3m.upv.es Configuration of the cluster Enable Password-based authentication Change settings in /etc/ssh/sshd_config\n# Change to no to disable tunnelled clear text passwords PasswordAuthentication yes and restart the ssh daemon:\nsudo service ssh restart Configure the number of processors of the cluster $ cat /var/spool/torque/server_priv/nodes wn1 np=XX wn2 np=XX [...] To obtain the number of CPU/cores (np) in Linux, use the command:\n$ lscpu | grep -i CPU CPU op-mode(s): 32-bit, 64-bit CPU(s): 16 On-line CPU(s) list: 0-15 CPU family: 6 Model name: Intel(R) Xeon(R) CPU E5520 @ 2.27GHz CPU MHz: 2266.858 NUMA node0 CPU(s): 0-3,8-11 NUMA node1 CPU(s): 4-7,12-15 Test the cluster Create a simple test script:\n$ cat test.sh #!/bin/bash #PBS -N job #PBS -q batch #cd $PBS_O_WORKDIR/ hostname -f sleep 5 Submit to the batch queue:\nqsub -l nodes=2 test.sh Destroy the cluster To destroy the running cluster, use the command:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy torque_cluster WARNING: you are going to delete the infrastructure (including frontend and nodes). Continue [y/N]? y Success deleting the cluster! ","categories":"","description":"Using Elastic Cloud Computing Cluster (EC3) platform to create elastic virtual clusters on the EGI Cloud.\n","excerpt":"Using Elastic Cloud Computing Cluster (EC3) platform to create elastic …","ref":"/users/cloud-compute/ec3/apps/htc/","tags":"","title":"HTC"},{"body":"Notebooks running on EGI can access other existing computing and storage services from EGI or other e-Infrastructures. For data services, check data section of the documentation\nEGI services: access tokens Most services integrated with EGI Check-in can handle valid access tokens for authorising users. These are short-lived (normally less than 1-hour) and need to be renewed for longer usage. EGI Notebooks provides a ready to use access token that can be accessed from your notebooks and is automatically refreshed so you can always have a valid one.\nThe token is available at /var/run/secrets/egi.eu/access_token and you can use it for example to access cloud providers of the EGI cloud. See the following sample code where a list of VMs is obtained for CESGA:\nfrom keystoneauth1.identity import v3 from keystoneauth1 import session from novaclient import client with open(\"/var/run/secrets/egi.eu/access_token\") as f: access_token = f.read() auth = v3.OidcAccessToken(auth_url=\"https://fedcloud-osservices.egi.cesga.es:5000/v3\", identity_provider=\"egi.eu\", protocol=\"openid\", project_id=\"3a8e9d966e644405bf19b536adf7743d\", access_token=access_token) sess = session.Session(auth=auth) nova = client.Client(session=sess, version=2) nova.servers.list() A valid ID token is also available at /var/run/secrets/egi.eu/id_token.\nfedcloud client A direct benefit of the integration with access tokens in EGI Notebooks is that you can easily work with the fedcloud client. Once logged into the EGI Notebooks open a terminal and run:\nexport OIDC_ACCESS_TOKEN=`cat /var/run/secrets/egi.eu/access_token` fedcloud token check If the fedcloud command is not available, please follow the getting started.\nD4Science If you are using a Notebooks instance integrated with D4Science, you can easily invoke DataMiner or any other D4Science functionality as the service will provide the GCUBE_TOKEN environment variable with a valid token.\nThis code will print the list of DataMiner methods available within your VRE:\nimport os from owslib.wps import WebProcessingService # init http header parameter and base folders for gCube REST API gcube_vre_token_header = {'gcube-token': os.environ[\"GCUBE_TOKEN\"]} # init WPS access for DataMiner algorithms dataminer_url = 'http://dataminer-prototypes.d4science.org/wps/WebProcessingService' wps = WebProcessingService(dataminer_url, headers=gcube_vre_token_header) for process in wps.processes: print('- Name: ', process.title) DataMiner algorithms can be invoked also from Notebooks, this code shows a sample:\nfrom owslib.wps import ComplexDataInput, monitorExecution # define processid and inputs processid = 'org.gcube.dataanalysis.wps.statisticalmanager.synchserver.mappedclasses.transducerers.WOFOST_CLOUD_V0_2_1' inputs = [ ('ClassToRun', 'nl.wur.wofostsystem.App'), ('FileInput', ComplexDataInput( 'https://data.d4science.org/shub/E_eVhZTzBWWktOaVJxQjJkdTUxR3FHaTFFdE9BTDYrZkZxQnFWcGMyaVVJbXptejdDOEFpSVNmam82RllkRUJ6cA==', mimeType=\"text/xml\") ) ] # execute the process execution = wps.execute(processid, inputs) monitorExecution(execution, sleepSecs=5, download=True) print(execution.status) Note that inputs that point to a URL should be specified using the ComplextDataInput class as shown above.\nOther third-party services We are open for integration with other services that may be relevant for your research. Please contact support _at_ egi.eu with your request so we can investigate the best way to support your needs.\n","categories":"","description":"Access new services from the Notebooks\n","excerpt":"Access new services from the Notebooks\n","ref":"/users/notebooks/integration/","tags":"","title":"Integration with other services"},{"body":"There are several container management tools available, on the EGI Cloud we use Kubernetes as the default platform for our service. This guide explains how to get a running scalable Kubernetes deployment for your applications with EC3.\nGetting started Before getting your kubernetes cluster deployed, you need to get access to the Cloud Compute service, check the Authentication and Authorisation guide for more information. You should also get fedcloud client installed to get EC3 templates needed to start deployment.\nYour kubernetes deployment needs to be performed at an specific provider (site) and project. Discover them using fedcloud as described in the EC3 tutorial.\nEC3 Templates EC3 relies on a set of templates that will determine what will be deployed on the infrastructure. fedcloud helps you to get an initial set of templates for your kubernetes deployment:\nmkdir k8s cd k8s fedcloud ec3 init --site \u003cyour site\u003e --vo \u003cyour vo\u003e You will also need a base image template for the deployment. Please refer to the EC3 tutorial to create such file. Below you can see an example for IFCA-LCG2 site with project related to vo.access.egi.eu:\ndescription ubuntu-ifca ( kind = 'images' and short = 'ubuntu18-ifca' and content = 'Ubuntu 18 image at IFCA-LCG2' ) network public ( provider_id = 'external' and outports contains '22/tcp' ) network private (provider_id = 'provider-2008') system front ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and disk.0.os.name = 'linux' and disk.0.image.url = 'ost://api.cloud.ifca.es/723171cb-53b2-4881-ae37-a7400ce0665b' and disk.0.os.credentials.username = 'cloudadm' ) system wn ( cpu.arch = 'x86_64' and cpu.count \u003e= 2 and memory.size \u003e= 2048 and ec3_max_instances = 5 and # maximum number of working nodes in the cluster disk.0.os.name = 'linux' and disk.0.image.url = 'ost://api.cloud.ifca.es/723171cb-53b2-4881-ae37-a7400ce0665b' and disk.0.os.credentials.username = 'cloudadm' ) Now you are ready to deploy the cluster using launch command of ec3 with:\n  the name of the deployment, k8s in this case\n  a list of templates: kubernetes for configuring kubernetes, ubuntu for specifying the image and site details, refresh to enable credential refresh and elasticity\n  the credentials to access the site with -a auth.dat\n  $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 \\  launch k8s kubernetes ubuntu refresh -a auth.dat Creating infrastructure Infrastructure successfully created with ID: b9577c34-f818-11ea-a644-2e0fc3c063db Front-end configured with IP 193.144.46.249 Transferring infrastructure Front-end ready! Your kubernetes deployment is now ready, log in with the ssh command of ec3:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 ssh k8s Warning: Permanently added '193.144.46.249' (ECDSA) to the list of known hosts. Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-109-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage * Kubernetes 1.19 is out! Get it in one command with: sudo snap install microk8s --channel=1.19 --classic https://microk8s.io/ has docs and details. * Canonical Livepatch is available for installation. - Reduce system reboots and improve kernel security. Activate at: https://ubuntu.com/livepatch Last login: Wed Sep 16 14:35:35 2020 from 158.42.104.204 $ bash cloudadm@kubeserver:~$ You can interact with the kubernetes cluster with the kubectl command:\ncloudadm@kubeserver:~$ sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kubeserver.localdomain Ready master 23h v1.18.3 The cluster will only have one node (the master) and will start new nodes as you create pods. Alternatively you can poweron nodes manually:\ncloudadm@kubeserver:~$ clues status node state enabled time stable (cpu,mem) used (cpu,mem) total ----------------------------------------------------------------------------------------------- wn1.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn2.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn3.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn4.localdomain off enabled 00h32'22\" 0,0 1,1073741824 wn5.localdomain off enabled 00h32'22\" 0,0 1,1073741824 cloudadm@kubeserver:~$clues poweron wn1.localdomain node wn1.localdomain powered on cloudadm@kubeserver:~$sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kubeserver.localdomain Ready master 24h v1.18.3 wn1.localdomain Ready \u003cnone\u003e 6m49s v1.18.3 Exposing services outside the cluster Kubernetes uses services for exposing an applications via the network. The services can rely on Load Balancers supported at the underlying cloud provider, which is not always feasible on the EGI Cloud providers. As an alternative solution we use an ingress controller which allows us to expose services using rules based on hostnames.\nHelm allows you to quickly install the nginx based ingress. Add the helm repos:\ncloudadm@kubeserver:~$ sudo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx cloudadm@kubeserver:~$ sudo helm repo add stable https://kubernetes-charts.storage.googleapis.com/ cloudadm@kubeserver:~$ sudo helm repo update Create a configuration file (ingress.yaml), get the externalIP using ip addr:\ncontroller:tolerations:- effect:NoSchedulekey:node-role.kubernetes.io/masterservice:type:NodePortexternalIPs:- 192.168.10.3defaultBackend:tolerations:- effect:NoSchedulekey:node-role.kubernetes.io/masterand install:\ncloudadm@kubeserver:~$ sudo helm install ingress -n kube-system -f ingress.yaml ingress-nginx/ingress-nginx Now you are ready to expose your services using a valid hostname. Use the EGI Cloud Dynamic DNS service for getting hostnames if you need. Assign as IP the public IP of the master node. Once you have a hostname assigned to the master IP, the ingress will be able to reply to requests already:\n$ curl ingress.test.fedcloud.eu \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e404 Not Found\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e404 Not Found\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003enginx/1.19.2\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e The following example yaml creates a service and exposes at that ingress.test.fedcloud.eu host:\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginxreplicas:1template:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.14.2ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:name:my-nginxlabels:app:nginxspec:ports:- port:80protocol:TCPselector:app:nginx---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:ingress-testspec:rules:- host:ingress.test.fedcloud.euhttp:paths:- pathType:Prefixpath:\"/\"backend:serviceName:my-nginxservicePort:80Now the ingress will redirect request to the NGINX pod that we have just created:\n$ curl ingress.test.fedcloud.eu \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Volumes Volumes on these deployments can be supported with NFS volume driver. You can either manually configure the server on one of the nodes or use EC3 to deploy it and configure it for you. Create a templates/nfs.radl to do so:\ndescription nfs ( kind = 'component' and short = 'Tool to configure shared directories inside a network.' and content = 'Network File System (NFS) client allows you to access shared directories from Linux client.' ) system front ( ec3_templates contains 'nfs' and disk.0.applications contains (name = 'ansible.modules.grycap.nfs') ) configure front ( @begin - tasks: - name: Create /volume for supporting NFS file: path: /volumes state: directory mode: '0777' - roles: - role: grycap.nfs nfs_mode: 'front' nfs_exports: - path: \"/volumes\" export: \"wn*.localdomain(rw,async,no_root_squash,no_subtree_check,insecure) kubeserver.localdomain(rw,async,no_root_squash,no_subtree_check,insecure)\" @end ) system wn (ec3_templates contains 'nfs') configure wn ( @begin - tasks: - name: Install NFS common apt: name: nfs-common state: present @end ) if you have a running cluster, you can add the NFS support by reconfiguring the cluster:\n$ docker run -it -v $PWD:/root/ -w /root grycap/ec3 \\  reconfigure k8s -a auth.dat -t nfs Reconfiguring infrastructure Front-end configured with IP 193.144.46.249 Transferring infrastructure Front-end ready! And then install the NFS driver in kubernetes with helm:\ncloudadm@kubeserver:~$ sudo helm install nfs-provisioner \\  stable/nfs-client-provisioner \\  --namespace kube-system \\  --set nfs.server=192.168.10.9 \\  --set storageClass.defaultClass=true \\  --set nfs.path=/volumes \\  --set tolerations[0].effect=NoSchedule,tolerations[0].key=node-role.kubernetes.io/master Now you are ready to create a PVC and attach it to a pod, see this example:\n---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:test-pvcspec:storageClassName:nfs-clientaccessModes:- ReadWriteOnceresources:requests:storage:3Gi---apiVersion:v1kind:Podmetadata:name:pvc-podnamespace:defaultspec:restartPolicy:Nevervolumes:- name:volpersistentVolumeClaim:claimName:test-pvccontainers:- name:testimage:\"busybox\"command:[\"sleep\",\"1d\"]volumeMounts:- name:volmountPath:/volumeOnce you apply the yaml, you will see the new PVC gets bounded to a PV created in NFS:\ncloudadm@kubeserver:~$ sudo kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-39f970de-eaad-44d7-b49f-90dc9de54a14 3Gi RWO nfs-client 9m46s Destroying the cluster Once you don’t need the cluster anymore, you can undeploy with the destroy command of EC3:\n$ fedcloud ec3 refresh # refresh your credentials to interact with the cluster $ docker run -it -v $PWD:/root/ -w /root grycap/ec3 destroy k8s -y -a auth.dat WARNING: you are going to delete the infrastructure (including frontend and nodes). Success deleting the cluster! ","categories":"","description":"Run containers on the EGI Cloud with Kubernetes\n","excerpt":"Run containers on the EGI Cloud with Kubernetes\n","ref":"/users/cloud-container-compute/k8s/","tags":"","title":"Kubernetes"},{"body":"Linking new identities to your EGI Account Identity linking allows you to access EGI resources with your existing personal EGI ID, using any of the login credentials you have linked to your account. You can use any of your organisational or social login credentials for this purpose. To link a new organisational or social identity to your EGI account:\n  Enter the following URL in a browser: https://aai.egi.eu/registry\n  Click Login and authenticate using any of the login credentials already linked to your EGI account\n  Navigate to My EGI User Community Account page in one of the following ways:\n hover over your display name next to the gear icon on the top right corner of the page; or, alternatively, select EGI User Community from the list of available Collaborations and then click My EGI User Community Account from the People menu    Under the Organisational Identities section of your profile page, expand Actions menu and click Link New Identity.\n  On the introductory page for Identity Linking, click Begin\n  You will need to sign in using the login credentials from the institutional/social identity provider you want to link to your account.\nWarning It is very important to escape the identity provider selection, cached in the discovery page, before picking the new one.    After successful authentication, the new Identity Provider will be available under the Organizational Identities tab and you’ll be able to access EGI resources with your existing personal EGI ID using the login credentials of the identity provider you selected in Step 6.\n  Linking your certificate to your EGI Account Certificate linking allows you to add the subject DN of your certificate to your existing personal EGI ID. For this you need to import your certificate to your browser.\nTo link a subject DN to your EGI account:\n  Enter the following URL in a browser: https://aai.egi.eu/registry\n  Click Login and authenticate using any of the login credentials already linked to your EGI account\n  Navigate to My EGI User Community Account page in one of the following ways:\n hover over your display name next to the gear icon on the top right corner of the page; or, alternatively, select EGI User Community from the list of available Collaborations and then click My EGI User Community Account from the People menu    Under the Organisational Identities section of your profile page, expand Actions menu and click Link New Identity.\n  On the introductory page for Identity Linking, click Begin\n  Continuously, you will need to sign in using the IGTF Certificate Proxy.\nWarning It is very important to escape the identity provider selection, cached in the discovery page, before picking the new one.    Then select the certificate you want to link to your account from the popup window.\n  After successful authentication you will be redirected back to your EGI Account. Also, you’ll be able to access EGI resources with your existing personal EGI ID using IGTF Certificate Proxy and your certificate.\n  To verify that the subject DN is added to your EGI account scroll down to Organisational Identities and click on view button in the row where the source is https://edugain-proxy.igtf.net/simplesaml/saml2/idp/metadata.php.\n  Then scroll down to Certificates and you should see the subject DN of your certificate.\n  ","categories":"","description":"Linking additional organisational/social identities to your EGI Account\n","excerpt":"Linking additional organisational/social identities to your EGI …","ref":"/users/check-in/linking/","tags":"","title":"Linking identities"},{"body":"Document control    Property Value     Title Service Intervention Management   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement Managing Service interventions   Owner SDIS team    Service Intervention A service intervention is defined as an action which will involve or lead to the possibility of a loss, or noticeable degradation of a service. Depending on the planning of the outage, we have two types of intervention:\n Scheduled interventions: planned and agreed in advance Unscheduled interventions: unplanned, usually triggered by an unexpected failure  How to manage an intervention Interventions are recorded through the Configuration Database. For more information, have a look at the description.\nScheduled interventions  Scheduled interventions MUST be declared at least 24 h in advance, specifying reason and duration. Existing scheduled interventions CAN be extended, provided that it’s done 24 hours in advance.  Unscheduled interventions  Any intervention declared less than 24 h in advance will be considered unscheduled. Sites MUST declare unscheduled interventions as soon as they are detected to inform the users. Unscheduled interventions CAN be declared up to 48 hours in the past (retroactive information to the user community).  Required information The required information to fill in when declaring an intervention is:\n Severity (Outage or Warning) Description Timezone Starting and ending dates Affected site / Affected services and endpoints  Recommendations  For interventions that impact end users, the downtime SHOULD be declared 5 working days in advance, specifying reason and duration. A post−mortem SHOULD be included in the downtime report.  Notifications Intervention notifications (through broadcasts, RSS feeds, etc) as specified in the following procedures are automatically sent when declaring a downtime in Configuration Database: at declaration time, 24 h in advance and 1 h before the intervention.\nSuspension policy Sites on downtime for more than 1 month will be suspended/uncertified. AT_RISK downtime declarations are only for providing warnings to users, and are ignored for calculating site availability (actual status will be used).\n","categories":"","description":"How to service interventions","excerpt":"How to service interventions","ref":"/providers/operations-manuals/man02_service_intervention_management/","tags":"","title":"MAN02 Service intervention management"},{"body":"EGI offers MATLAB on the cloud using the MATLAB Integration for Jupyter and shares data and code with other EGI users. MATLAB is also available directly on compute services offered by the members of the EGI Federation.\nMATLAB on the EGI infrastructure can be used by scientists and engineers for Open Science by enabling them to share their data and code using computational notebooks.\nBefore getting started MATLAB Licenses To use the MATLAB Integration on the EGI Notebooks service, a supported MATLAB license is required. You can also use the toolboxes linked to your MATLAB license.\nIf you are unsure of your MATLAB license type, contact your System Administrator or MathWorks support.\nTutorials If you are not familiar or have limited experience with MATLAB, MATLAB Onramp provides a free, self-paced tutorial. Additional Onramps are available for other topics using MATLAB.\nGetting started   Once your server has started up (after selecting the MATLAB environment), click on the MATLAB icon\n  A dialog for handling MATLAB licensing will appear.\n  Individual and Campus Wide MATLAB Licenses\nIf you have access to a MATLAB Individual License or a MATLAB Campus Wide License, use the Online License Manager tab to log in using your MathWorks account (does not need to match the email used on the EGI Identity Provider) and click on next.\n  Concurrent/Network Licenses\nIf you have Network Licenses, enter the port@hostname of your License Server on the Network License Manager tab of the login screen. Before doing so, please check with your system administrator about allowing the EGI access to your on-prem Network License Manager (License Server).\n  Free trial MATLAB Licenses If you do not have a MATLAB license and would like to try it out, you can download a free MATLAB trial license here. Note that the trial license is for MATLAB only and does not have any toolboxes.\n  More information on MATLAB licensing with this integration can be found here.\n  Following this, the MATLAB IDE will appear in your browser.\n  To facilitate sharing your research output, you can use MATLAB Live Scripts combining rich text, equations, images, code and inline output all in one document. Live Scripts can be accessed from the Live Editor tab. Here is an example of a tutorial Live Script from the EGI webinar showing steps required to analyze some public COVID-19 data with MATLAB. Some other examples of Live Scripts can be found here.\n  Limitations Browser based MATLAB has some differences as compared to MATLAB on the desktop. For more information, see the limitations here. Additionally, Simulink is not supported on the EGI at this time.\nRestarting, stopping or signing out of the MATLAB session At any time, clicking on this symbol will bring up a dialog to stop, restart or sign out of the current MATLAB session.\nOpen Science using MATLAB on EGI Notebooks You can access publicly available datasets via the EGI DataHub and analyze them in-the-cloud directly using MATLAB, without the need for time consuming downloads. Your MATLAB code can also be shared with your community in a variety of interoperable and open formats.\nAnalyzing public datasets using MATLAB on EGI This short video and this webinar explains how to access EGI Data Services from MATLAB in detail. Data from different data providers can be accessed from the DataHub. More information on Data Management from Notebooks and persistent storage can be found here.\nMATLAB support for data formats MATLAB supports several scientific and standard data formats including web data and those from specialized hardware.\nSharing data and code You can share your data and analyses by sharing your EGI provided persistent storage space with others. Users belonging to a specific community can request community Notebooks.\nInteroperability with Python and other languages You can collaborate with users of other languages by calling these languages (eg. Python) from MATLAB for conducting specific analyses. You can also save data in formats compatible with other languages.\nHere are some other ways in which MATLAB enables Open Science\nUsing MATLAB on other EGI services In addition to MATLAB on EGI Notebooks, you can also use your MATLAB license to access MATLAB on any of the Institutions participating in the EGI Federation. To run your compute-intensive MATLAB code faster or to run code in parallel, you will need MATLAB Parallel Server and MATLAB Parallel Computing Toolbox on your MATLAB license. If you have access to a Campus Wide License , you can now scale up to use all available workers on the HPC cluster of your choice.\nTo access MATLAB on any site of the EGI Federation, follow these steps.\n  Check if you have access to MATLAB Parallel Server and Parallel Computing Toolbox\n  Ask the system administrator of the site about MATLAB as part of EGI\n  Contact Shubo Chakrabarti at MathWorks or Enol Fernandez at EGI for setting up access to MATLAB at the site.\n  ","categories":"","description":"MATLAB in EGI Notebooks\n","excerpt":"MATLAB in EGI Notebooks\n","ref":"/users/notebooks/kernels/matlab/","tags":"","title":"MATLAB"},{"body":"This documentation covers how to install and configure a OneData OneProvider in order to join a new or existing EGI DataHub space. In particular two types of installations are available, depending if the provider wants to support the space with an empty storage or if existing data should be exposed via the Oneprovider.\nRequirements for production installation  Oneprovider  RAM: 32GB CPU: 8 vCPU Disk: 50GB SSD To be adjusted for the dataset and usage scenario   For high Input/Output operations per second (IOPS)  High performance backend storage (CEPH) Low latency network    Network Requirements  The following ports need to be open on the local and site firewall:  80, 443, 9443, 6665 (for data transfer)    Installation and attach empty storage to the EGI DataHub The installation of a new Oneprovider is performed using the onedatify installation script which will setup the components using Docker and Docker-compose.\nThis simple installation script can be generated from the EGI DataHub interface.\nFirstly, you need to login to the EGI DataHub and using the Data menu you either select an existing space or create a new one.\nSecondly, you can select on the space menu the Providers section and click on the Add Support button on the top right corner.\nYou should then select on the page the tab: Deploy your own provider and there you will have to copy the command already configured with the correct parameters for the OneZone to use (datahub.egi.eu) and the space to join.\nRun the command on the host Paste the copied command in the terminal on the Oneprovider machine as superuser.\nIf necessary, the Onedatify script will ask for permission to install all necessary dependencies including Docker and Docker Compose.\nAfter the dependency installation is complete, the script will ask several questions and suggest default settings:\nThe progress can be monitored on a separate terminal using the following command:\nonedatify logs After the deployment is complete, a message will be shown, including connection details for the administration panel of the Oneprovider:\nThis administration panel at port 9443 can be used to administer the Oneprovider.\nInstallation and expose existing data to the EGI DataHub The installation of a new OneProvider to expose existing datasets to an EGI DataHub space is similar to the installation with an empty storage.\nWhen adding support to an existing or new space you should select from the EGI DataHub user interface the tab : Expose Existing dataset and there you will have to copy the command already configured with the correct parameters for the Onezone (datahub.egi.eu) and the space to join.\nRun the command on the host Paste the copied command in the terminal on the Oneprovider machine as superuser, and follow the instructions as for the case of an empty storage.\nThe only difference is that at the end of the installation and configuration process the existing files will be automatically imported to the OneProvider.\nYou can monitor the import activity from the administration panel at port 9443.\n","categories":"","description":"Documentation for installation/configuration of OneProvider to join EGI DataHub spaces","excerpt":"Documentation for installation/configuration of OneProvider to join …","ref":"/providers/datahub/oneprovider/","tags":"","title":"DataHub OneProvider"},{"body":"This manual provides information on how to set up a Resource Centre providing cloud resources in the EGI infrastructure. Integration with FedCloud requires a working OpenStack installation as a pre-requirement. EGI supports any recent OpenStack version (tested from OpenStack Mitaka).\nEGI expects the following OpenStack services to be available and accessible from outside your site:\n Keystone Nova Cinder Glance Neutron Swift (if providing Object Storage)  FedCloud components are distributed through CMD (Cloud Middleware Distribution) or docker container images available in dockerhub. These docker containers come pre-packaged and ready to use in the EGI FedCloud Appliance so you do not need to install any extra components on your site but just run a VM and configure it approprietely to interact with your services.\nThe integration is performed by a set of EGI components that interact with the OpenStack services APIs:\n Authentication of EGI users into your system is performed by configuring the native OpenID Connect support of Keystone. Support for legacy VOs using VOMS requires the installation of the Keystone-VOMS Authorization plugin to allow users with a valid VOMS proxy to obtain tokens to access your OpenStack deployment. cASO collects accounting data from OpenStack and uses SSM to send the records to the central accounting database on the EGI Accounting service (APEL) cloud-info-provider registers the RC configuration and description through the ARGO Messaging Service to facilitate service discovery cloudkeeper (and cloudkeeper-os) synchronises with EGI AppDB so new or updated images can be provided by the RC to user communities (VO).  Not all EGI components need to share the same credentials. They are individually configured, you can use different credentials and permissions if desired.\nInstallation options EGI distributes the integration components as:\n A Virtual Appliance (VA) that uses Docker containers to bundle all of the components in a single VM and just needs minor configuration to get started RPM and DEB Packages in the CMD distribution  FedCloud Virtual Appliance The EGI FedCloud Appliance is available at AppDB as an OVA file. You can easily extract the VMDK disk by untaring and optionally converting it to your preferred format with qemu-img:\n# get image and extract VMDK $ curl $(curl \"https://appdb.egi.eu/store/vm/image/fc90d1aa-b0ae-46a0-b457-96f6f7a7d446:7875/json?strict\" | jq -r .url) | \\  tar x \"*.vmdk\" # convert to qcow2 $ qemu-img convert -O qcow2 FedCloud-Appliance.Ubuntu.*.vmdk fedcloud-appliance.qcow2 The appliance running at your OpenStack must:\n Have a host certificate to send the accounting information to the accounting repository. DN of the host certificate must be registered in GOCDB with service type eu.egi.cloud.accounting. The host certificate and key in PEM format are expected in /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem respectively. Have enough disk space for handling the VM image replication (~ 100GB for fedcloud.egi.eu VO). By default these are stored at /image_data. You can mount a volume at that location.  Upgrading the OpenStack Appliance From 2018.05.07 or newer to 2021.03.12 Configuration changes:\n Removes BDII, service is no longer in use A cloud-info-provider cron is added Uses AMS for pushing accounting records. New configuration file for ssmsend is available  From 2017.08.09 to 2018.05.07 Configuration changes:\n This upgrade moves the voms.json file to the respective caso and cloudkeeper-os directories under /etc/ No other changes in configuration are needed  From 20160403 to 2017.08.09 There are several major changes between these versions, namely:\n atrope has been deprecated and cloudkeeper is used instead. The configuration cannot be reused directly and the new services need to be configured as described above caso is upgraded to version 1.1.1, the configuration file has some incompatible changes. A new bdii.service is available for managing the process is available.  CMD Packages The CMD-OS repository provides packages that have gone through a quality assurance process for the supported distributions. Packages are available via the EGI repository.\nOpen Ports The following services must be accessible to allow access to an OpenStack-based FedCloud site (default ports listed below, can be adjusted to your installation)\n   Port Application Note     5000/TCP OpenStack/Keystone Authentication to your OpenStack.   8776/TCP OpenStack/cinder Block Storage management.   8774/TCP OpenStack/nova VM management.   9696/TCP OpenStack/neutron Network management.   9292/TCP OpenStack/glance VM Image management.    Outgoing ports The EGI Cloud components require the following outgoing connections open:\n   Port Host Note     443/TCP msg.argo.grnet.gr ARGO Messaging System (used to send accounting records by SSM).   8443/TCP msg.argo.grnet.gr AMS authentication (used to send accounting records by SSM).   443/TCP vmcaster.appdb.egi.eu AppDB image lists (used by cloudkeeper).   8080/TCP cephrgw01.ifca.es Swift server hosting EGI images (used by cloudkeeper).    Images listed in AppDB may be hosted in other servers besides cephrgw01.ifca.es. Check the specific VO-wide image lists for details.\nPermissions This is an overview of the expected account permissions used in an OpenStack site, these accounts can be merged as needed for your deployment:\n   Component Permission     cloud-info Member of all projects supporting EGI VOs   accounting Member of all projects and able to list users (allowed to identity:list_users in keystone)   cloud-keeper Permission to manage the images for all the projects supporting EGI VOs   Other users Automatically created by Keystone and permission set as configured in the mappings    ","categories":"","description":"Integration of OpenStack providers\n","excerpt":"Integration of OpenStack providers\n","ref":"/providers/cloud-compute/openstack/","tags":"","title":"OpenStack"},{"body":"OpenStack providers in the EGI infrastructure offer services and features via OpenStack APIs, and the command-line interface (CLI), both integrated with EGI Check-in accounts.\nThe extensive OpenStack user documentation includes details on every OpenStack project, but most providers offer:\n Keystone, for identity Nova, for VM management Glance, for VM image management Cinder, for block storage Swift, for object storage Neutron, for network management Horizon, as a web dashboard  The web-dashboard of the individual providers can be used to manage and use services. It can be accessed using EGI Check-in credentials directly: select OpenID Connect or EGI Check-in in the Authenticate using drop-down menu of the login screen.\nTip You can quickly find the dashboards of all providers in the EGI infrastructure that are accessible to you (use the correct VO) with the FedCloud Client:\n$ fedcloud endpoint list --service-type org.openstack.horizon --site ALL_SITES The same way you can also discover other types of resources, just use the correct resource type:\n org.openstack.horizon for dashboards org.openstack.nova for virtual machines org.openstack.swift for object storage   Note For more advanced information discovery, including resources not based on OpenStack deployments, check out the EGI architecture summary.  ","categories":"","description":"How to interact with OpenStack providers\n","excerpt":"How to interact with OpenStack providers\n","ref":"/users/getting-started/openstack/","tags":"","title":"Using OpenStack Providers"},{"body":"In this section you can find the common operational activities related to keep the service available to our users.\nInitial set-up Notebooks VO The resources used for the Notebooks deployments are managed with the vo.notebooks.egi.eu VO. Operators of the service should join the VO, check the entry at the operations portal and at AppDB.\nClients installation In order to manage the resources you will need these tools installed on your client machine:\n fedcloudclient for discovering sites and managing tokens, terraform to create the VMs at the providers, ansible to configure the VMs and install kubernetes at the providers, terraform-inventory to get the list of hosts to use from terraform.  Get the configuration repository All the configuration of the notebooks is stored at a git repository available in keybase. You'll need to be part of the opslife team in keybase to access. Start by cloning the repo:\ngit clone keybase://team/opslife/egi-notebooks Kubernetes We use terraform and ansible to build the cluster at one of the EGI Cloud providers. If you are building the cluster for the first time, create a new directory on your local git repository from the template, add it to the repo, and get terraform ready:\ncp -a template \u003cnew provider\u003e git add \u003cnew provider\u003e cd \u003cnew provider\u003e/terraform terraform init Using the fedcloud you can get set the right environment for interacting with the OpenStack APIs of a given site:\neval \"$(fedcloud site show-project-id --site CESGA --vo vo.notebooks.egi.eu)\" Whenever you need to get a valid token for the site and VO, you can obtain it with:\nOS_TOKEN=$(fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu \\  token issue -c id -f value) First get the network IDs and pool to use for the site:\n$ fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu network list +--------------------------------------+-------------------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-------------------------+--------------------------------------+ | 1aaf20b6-47a1-47ef-972e-7b36872f678f | net-vo.notebooks.egi.eu | 6465a327-c261-4391-a0f5-d503cc2d43d3 | | 6174db12-932f-4ee3-bb3e-7a0ca070d8f2 | public00 | 6af8c4f3-8e2e-405d-adea-c0b374c5bd99 | +--------------------------------------+-------------------------+--------------------------------------+ In this case we will use public00 as the pool for public IPs and 1aaf20b6-47a1-47ef-972e-7b36872f678f as the network ID. Check with the provider which is the right network to use. Use these values in the terraform.tfvars file:\nip_pool = \"public00\" net_id = \"1aaf20b6-47a1-47ef-972e-7b36872f678f\" You may want to check the right flavors for your VMs and adapt other variables in terraform.tfvars. To get a list of flavors you can use:\n$ fedcloud openstack --site CESGA --vo vo.notebooks.egi.eu flavor list +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ | 26d14547-96f2-4751-a686-f89a9f7cd9cc | cor4mem8hd40 | 8192 | 40 | 0 | 4 | True | | 42eb9c81-e556-4b63-bc19-4c9fb735e344 | cor2mem2hd20 | 2048 | 20 | 0 | 2 | True | | 4787d9fc-3923-4fc9-b770-30966fc3baee | cor4mem4hd40 | 4096 | 40 | 0 | 4 | True | | 58586b06-7b9d-47af-b9d0-e16d49497d09 | cor24mem62hd60 | 63488 | 60 | 0 | 24 | True | | 635c739a-692f-4890-b8fd-d50963bff00e | cor1mem1hd10 | 1024 | 10 | 0 | 1 | True | | 6ba0080d-d71c-4aff-b6f9-b5a9484097f8 | small | 512 | 2 | 0 | 1 | True | | 6e514065-9013-4ce1-908a-0dcc173125e4 | cor2mem4hd20 | 4096 | 20 | 0 | 2 | True | | 85f66ce6-0b66-4889-a0bf-df8dc23ee540 | cor1mem2hd10 | 2048 | 10 | 0 | 1 | True | | c4aa496b-4684-4a86-bd7f-3a67c04b1fa6 | cor24mem50hd50 | 51200 | 50 | 0 | 24 | True | | edac68c3-50ea-42c2-ae1d-76b8beb306b5 | test-bigHD | 4096 | 237 | 0 | 2 | True | +--------------------------------------+----------------+-------+------+-----------+-------+-----------+ Finally ensure your public ssh key is also listed in the cloud-init.yaml file and then you are ready to deploy the cluster with:\nterraform apply Your VMs are up and running, it's time to get kubernetes configured and running with ansible.\nThe following ansible role needs to be installed first:\nansible-galaxy install grycap.kubernetes and then:\ncd .. # you should be now in \u003cnew provider\u003e ANSIBLE_TRANSFORM_INVALID_GROUP_CHARS=silently TF_STATE=./terraform \\  ansible-playbook --inventory-file=$(which terraform-inventory) \\  playbooks/k8s.yaml Interacting with the cluster As the master will be on a private IP, you won't be able to directly interact with it, but you can still ssh into the VM using the ingress node as a gateway host (you can get the different hosts with TF_STATE=./terraform terraform-inventory --inventory)\n$ ssh -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -q egi@\u003cingress ip\u003e\" \\  -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null egi@\u003cmaster ip\u003e egi@k8s-master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 33m v1.15.7 k8s-nfs Ready \u003cnone\u003e 16m v1.15.7 k8s-w-ingress Ready \u003cnone\u003e 16m v1.15.7 egi@k8s-master:~$ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE certs-man 2 Wed Jan 8 15:56:58 2020 DEPLOYED cert-manager-v0.11.0 v0.11.0 cert-manager cluster-ingress 3 Wed Jan 8 15:56:53 2020 DEPLOYED nginx-ingress-1.7.0 0.24.1 kube-system nfs-provisioner 3 Wed Jan 8 15:56:43 2020 DEPLOYED nfs-client-provisioner-1.2.8 3.1.0 kube-system Modifying/Destroying the cluster You should be able to change the number of workers in the cluster and re-apply terraform to start them and then execute the playbook to get them added to the cluster.\nAny changes in the master, NFS or ingress VMs should be done carfully as those will probably break the configuration of the kubernetes cluster and of any application running on top.\nDestroying the cluster can be done with a single command:\nterraform destroy Notebooks deployments Once the k8s cluster is up and running, you can deploy a notebooks instance. For each deployment you should create a file in the deployments directory following the template provided:\ncp deployments/hub.yaml.template deployments/hub.yaml Each deployment will need a domain name pointing to your ingress host, you can create one at the FedCloud dynamic DNS service.\nThen you will need to create an OpenID Connect client for EGI Check-in to authorise users into the new deployment. You can create a client by going to the Check-in demo OIDC clients management. Use the following as redirect URL: https://\u003cyour host domain name\u003e/hub/oauth_callback.\nIn the Access tab, add offline_access to the list of scopes. Save the client and take note of the client ID and client secret for later.\nFinally you will also need 3 different random strings generated with openssl rand -hex 32 that will be used as secrets in the file describing the deployment.\nGo and edit the deployment description file to add this information (search for # FIXME NEEDS INPUT in the file to quickly get there)\nFor deploying the notebooks instance we will also use ansible:\nANSIBLE_TRANSFORM_INVALID_GROUP_CHARS=silently \\  TF_STATE=./terraform ansible-playbook \\  --inventory-file=$(which terraform-inventory) playbooks/notebooks.yaml The first deployment trial may fail due to a timeout caused by the downloading of the container images needed. You can retry after a while to re-deploy.\nIn the master you can check the status of your deployment (the name of the deployment will be the same as the name of your local deployment file):\n$ helm status hub LAST DEPLOYED: Thu Jan 9 08:14:49 2020 NAMESPACE: hub STATUS: DEPLOYED RESOURCES: ==\u003e v1/ServiceAccount NAME SECRETS AGE hub 1 6m46s user-scheduler 1 3m34s ==\u003e v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hub ClusterIP 10.100.77.129 \u003cnone\u003e 8081/TCP 6m46s proxy-public NodePort 10.107.127.44 \u003cnone\u003e 443:32083/TCP,80:30581/TCP 6m45s proxy-api ClusterIP 10.103.195.6 \u003cnone\u003e 8001/TCP 6m45s ==\u003e v1/ConfigMap NAME DATA AGE hub-config 4 6m47s user-scheduler 1 3m35s ==\u003e v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hub-db-dir Pending managed-nfs-storage 6m46s ==\u003e v1/ClusterRole NAME AGE hub-user-scheduler-complementary 3m34s ==\u003e v1/ClusterRoleBinding NAME AGE hub-user-scheduler-base 3m34s hub-user-scheduler-complementary 3m34s ==\u003e v1/RoleBinding NAME AGE hub 6m46s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE continuous-image-puller-flf5t 1/1 Running 0 3m34s continuous-image-puller-scr49 1/1 Running 0 3m34s hub-569596fc54-vjbms 0/1 Pending 0 3m30s proxy-79fb6d57c5-nj8n2 1/1 Running 0 2m22s user-scheduler-9685d654b-9zt5d 1/1 Running 0 3m30s user-scheduler-9685d654b-k8v9p 1/1 Running 0 3m30s ==\u003e v1/Secret NAME TYPE DATA AGE hub-secret Opaque 3 6m47s ==\u003e v1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE continuous-image-puller 2 2 2 2 2 \u003cnone\u003e 3m34s ==\u003e v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hub 1 1 1 0 6m45s proxy 1 1 1 1 6m45s user-scheduler 2 2 2 2 3m32s ==\u003e v1/StatefulSet NAME DESIRED CURRENT AGE user-placeholder 0 0 6m44s ==\u003e v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE jupyterhub notebooktest.fedcloud-tf.fedcloud.eu 80, 443 6m44s ==\u003e v1beta1/PodDisruptionBudget NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE hub 1 N/A 0 6m48s proxy 1 N/A 0 6m48s user-placeholder 0 N/A 0 6m48s user-scheduler 1 N/A 1 6m47s ==\u003e v1/Role NAME AGE hub 6m46s NOTES: Thank you for installing JupyterHub! Your release is named hub and installed into the namespace hub. You can find if the hub and proxy is ready by doing: kubectl --namespace=hub get pod and watching for both those pods to be in status 'Running'. You can find the public IP of the JupyterHub by doing: kubectl --namespace=hub get svc proxy-public It might take a few minutes for it to appear! Note that this is still an alpha release! If you have questions, feel free to 1. Read the guide at https://z2jh.jupyter.org 2. Chat with us at https://gitter.im/jupyterhub/jupyterhub 3. File issues at https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues Updating a deployment Just edit the deployment description file and run ansible again. The helm will be upgraded at the cluster.\n","categories":"","description":"Getting the service up and running","excerpt":"Getting the service up and running","ref":"/providers/notebooks/operations/","tags":"","title":"Service Operations"},{"body":"This page contains information about connecting services to EGI Check-in in order to allow user login through Check-in and to receive users' attributes. Check-in is connected to a wide range of academic and social Identity Providers that users can choose from in order to access your service.\nServices eligible for integration EGI Operations, as owner of the Check-in service, must approve every request for integration of new services with Check-in. The approval (or non-approval) is based on some prerequisites, the relevance of the service for the EGI community and the available resources to support the integration. The prerequisites are described in the following sections.\nEGI at any time can prevent a service provider to access the Check-in service\nServices federated in EGI All the services that are operated by Resource Providers federated in EGI federation and that abide to the RC OLA, and consequently to the relevant security policies of EGI, can be connected with Check-in. Fulfilling all the relevant EGI policies makes the service eligible in receiving attributes released by Check-in.\nServices not federated in EGI A service not part of the EGI federation can be integrated with Check-in if the organisation providing the service complies with the EGI security requirements relevant to the service providers.\nBy accepting the policies a service provider assures that they will operate the service in good faith, without deliberately exposing the user to security risks, without claiming intellectual property on the data owned by the user, and protecting sensitive data generated by the interaction of the user with the service.\nThe policies that the service provider will have to accept are available in the EGI Policies and procedures page and specifically are:\n EGI Security Policy Service Operations Security Policy Traceability and Logging Policy Security Incident Response Policy Policy on the Processing of Personal Data  Service Provider integration workflow To integrate your Service Provider with the EGI Check-in service, you need to create a registration request using the EGI Federation Registry Portal. You can also use the Federation Registry portal to request the reconfiguration or deregistration of an existing deployed service. Service registration requests typically require approval by an administrator. Please refer to the Federation Registry Documentation for more information.\nThe integration follows a two-step process:\n Register your Service Provider and test integration with the demo instance of EGI Check-in by selecting the “Demo” integration environment during registration through the EGI Federation Registry Portal. The demo instance allows for testing authentication and authorisation through the academic and social Identity Providers connected to Check-in without affecting the production Check-in service. Note that while the demo instance has identical functionality to the production instance, no information is shared between the two systems.  You can also test new features of Check-in that are not available in production yet, by registering your Service Provider and testing integration with the development instance of Check-in. In the development instance service requests can be self-reviewed without the need to wait for approval from an administrator. As with the demo instance, the development instance allows for testing authentication and authorisation without affecting the production Check-in service. NB: the list of supported Identity Providers in the development instance is limited. Therefore, we recommend using any of the social identity providers or the EGI SSO to test the login workflow when using the development instance.   Register your Service Provider with the production instance of EGI Check-in by selecting the “Production” integration environment during registration through the EGI Federation Registry Portal. The production instance allows access to your service through the academic and social Identity Providers connected to Check-in. This requires that your service meets all the eligibility criteria and that integration has been thoroughly tested during Step 1.  The most important URLs for each environment are listed in the table below but more information can be found in the protocol-specific sections that follow.\n   Protocol Development environment Demo environment Production environment     SAML https://aai-dev.egi.eu/proxy/saml2/idp/metadata.php https://aai-demo.egi.eu/proxy/saml2/idp/metadata.php https://aai.egi.eu/proxy/saml2/idp/metadata.php   OpenID Connect https://aai-dev.egi.eu/oidc/.well-known/openid-configuration https://aai-demo.egi.eu/oidc/.well-known/openid-configuration https://aai.egi.eu/oidc/.well-known/openid-configuration    General Information EGI Check-in supports two authentication and authorisation protocols that you can choose from:\n Security Assertion Markup Language (SAML) 2.0 OpenID Connect - an extension to OAuth 2.0  Regardless of which of the two protocols you are going to use, you need to provide the following information to connect your service to EGI Check-in:\n Name of the service (in English and optionally in other languages supported by the service) Short description of the service Site (URL) for localised information about the service; the content found at the URL SHOULD provide more complete information than what provided by the description Contact information of the following types:  Helpdesk/Support contact information (for redirecting user) Administrative Technical Security/incident response   Privacy statement URL: The privacy policy is used to document the data collected and processed by the service. You can use the Privacy Policy template Logo URL (optional for showing in catalogues); if provided, logos SHOULD:  use a transparent background where appropriate to facilitate the usage of logos within a user interface use PNG, or GIF (less preferred), images use HTTPS URLs in order to avoid mixed-content warnings within browsers have a size larger than 40000 and smaller than 50000 characters when encoded in base64   Country of the service Compliance with the EGI Policies and the GÉANT Data Protection Code of Conduct  SAML Service Provider To enable federated access to a web-based application, you can connect to the EGI Check-in IdP as a SAML Service Provider (SP). Users of the application will be redirected to Check-in in order to log in, and Check-in can authenticate them using any of the supported backend authentication mechanisms, such as institutional IdPs registered with eduGAIN or Social Providers. Once the user is authenticated, EGI Check-in will return a SAML assertion to the application containing information about the authenticated user.\nMetadata registration SAML authentication relies on the use of metadata. Both parties (you as a SP and the EGI Check-in IdP) need to exchange metadata in order to know and trust each other. The metadata include information such as the location of the service endpoints that need to be invoked, as well as the certificates that will be used to sign SAML messages. The format of the exchanged metadata should be based on the XML-based SAML 2.0 specification. Usually, you will not need to manually create such an XML document, as this is automatically generated by all major SAML 2.0 SP software solutions (e.g., Shibboleth, SimpleSAMLphp, and mod_auth_mellon). It is important that you serve your metadata over HTTPS using a browser-friendly SSL certificate, i.e. issued by a trusted certificate authority.\nYou can get the metadata of the EGI Check-in IdP Proxy on a dedicated URL that depends on the integration environment being used:\n   Development environment Demo environment Production environment     https://aai-dev.egi.eu/proxy/saml2/idp/metadata.php https://aai-demo.egi.eu/proxy/saml2/idp/metadata.php https://aai.egi.eu/proxy/saml2/idp/metadata.php    To register your SAML SP, you must submit a service registration request at Federation Registry. Your request should include the general information about your service (see General Information) and the SP’s metadata and entity ID.\nMetadata Metadata provided by your SP should contain a descriptive name of the service that your SP represents in at least English. It is recommended to also provide the name in other languages which are commonly used in the geographic scope of the deployment. The name should be placed in the \u003cmd:ServiceName\u003e in the \u003cmd:AttributeConsumingService\u003e container.\nIt is recommended that the \u003cmd:IDPSSODescriptor\u003e element included in your SP metadata contains both an AuthnRequestsSigned and an WantAssertionsSigned attribute set to true.\nYour SP metadata should also contain contact information for support and for a technical contact. The \u003cmd:EntityDescriptor\u003e element should contain both a \u003cmd:ContactPerson\u003e element with a contactType of \"support\" and a \u003cmd:ContactPerson\u003e element with a contactType of \"technical\". The \u003cmd:ContactPerson\u003e elements should contain at least one \u003cmd:EmailAddress\u003e. The support address may be used for generic support questions about the service, while the technical contact may be contacted regarding technical interoperability problems. The technical contact must be responsible for the technical operation of the service represented by your SP.\nAttributes The EGI Check-in IdP is guaranteed to release a minimal subset of the REFEDS R\u0026S attribute bundle to connected Service Providers without administrative involvement, subject to user consent. The following attributes constitute a minimal subset of the R\u0026S attribute bundle:\n Persistent, non-reassignable, non-targeted, opaque, globally unique EGI user ID (eduPersonUniqueId); this is always scoped @egi.eu Email address (mail) Display name (displayName) OR (givenName AND sn)  A more extensive list of all the attributes that may be made available to Service Providers is included in the User Attribute section.\nAttribute-based authorisation EGI Check-in provides information about the authenticated user that may be used by Service Providers in order to control user access to resources. This information is provided by the EGI Check-in IdP in the SAML attribute assertion. The table below lists the SAML attributes that are relevant for user authorisation:\n   Description SAML Attribute     VO/group membership/roles of the authenticated user eduPersonEntitlement   Capabilities eduPersonEntitlement   GOCDB roles eduPersonEntitlement   Identity Assurance eduPersonAssurance    References  Shibboleth Service Provider Documentation SimpleSAMLphp Service Provider QuickStart Simple SAML 2.0 service provider with mod_auth_mellon Apache module  OpenID Connect Service Provider Service Providers can be integrated with EGI Check-in using OpenID Connect (OIDC) as an alternative to the SAML2 protocol. To allow this, the EGI Check-in IdP provides an OpenID Connect (OAuth2) API based on MITREid Connect, which has been certified by the OpenID Foundation. Interconnection with the EGI Check-in OIDC Provider allows users to sign in using any of the supported backend authentication mechanisms, such as institutional IdPs registered with eduGAIN or Social Providers. Once the user has signed in, EGI Check-in can return OIDC Claims containing information about the authenticated user.\nClient registration Before your service can use the EGI Check-in OIDC Provider for user login, you must submit a service registration request using Federation Registry in order to obtain OAuth 2.0 credentials. The client configuration should include the general information about your service, as described in General Information section.\nObtaining OAuth 2.0 credentials You need OAuth 2.0 credentials, which typically include a client ID and client secret, to authenticate users through the EGI Check-in OIDC Provider.\nYou can specify the client ID and secret when creating/editing your client or let them being automatically generated during registration (recommended).\nTo find the ID and secret of your client, do the following:\n Select your client from the Manage Services Page. Look for the Client ID in the Protocol tab. Select the Display/edit client secret: option from the Protocol tab.  Note You can copy these values using the green copy button next to the desired field.  Setting one or more Redirection URIs The Redirection URI(s) that you set when creating/editing your client determine where the EGI Check-in OIDC Provider sends responses to your authentication requests. Note that the Redirection URI MUST use the https scheme; the use of http Redirection URIs is only allowed in the development environment.\nTo find the Redirection URI(s) for your client, do the following:\n Open the Manage Services Find the redirect URIs for your client listed under the Protocol column of the overview table or Edit the particular client and look for the Redirect URI(s) in the Protocol tab.  Setting additional information about the client It is strongly suggested that you add a short description and a logo for the client. Lastly, you need to set the email addresses of one or more contacts.\nClaims The EGI Check-in UserInfo Endpoint is an OAuth 2.0 Protected Resource that returns specific information about the authenticated end user as Claim Values. To obtain the requested Claims about the End-User, the Client makes a request to the UserInfo Endpoint using an Access Token obtained through OpenID Connect Authentication. The scopes associated with the Access Token used to access the EGI Check-in UserInfo Endpoint will determine what Claims will be released. These Claims are represented by a JSON object that contains a collection of name and value pairs for the Claims.\nThe following scope values can be used to request Claims from the EGI Check-in UserInfo Endpoint:\n   Scope Claims     openid sub   profile namegiven_namefamily_namepreferred_username   email emailemail_verifiedvoperson_verified_email   eduperson_scoped_affiliation eduperson_scoped_affiliation   eduperson_entitlement eduperson_entitlement    A more extensive list of all the attributes that may be made available to Service Providers is included in the User Attribute section.\nGrant Types Check-in supports the following OpenID Connect/OAuth2 grant types:\n Authorization Code: used by Web Apps executing on a server. Token Exchange: used by clients to request and obtain security tokens in support of delegated access to resources. Device Code: used by devices that lack a browser to perform a user-agent based OAuth flow.  Endpoints The most important OIDC/OAuth2 endpoints are listed below:\n   Endpoint Development environment Demo environment Production environment     Provider configuration https://aai-dev.egi.eu/oidc/.well-known/openid-configuration https://aai-demo.egi.eu/oidc/.well-known/openid-configuration https://aai.egi.eu/oidc/.well-known/openid-configuration   Client registration https://aai-dev.egi.eu/oidc Contact EGI Check-in support for registering your client Contact EGI Check-in support for registering your client   Authorisation https://aai-dev.egi.eu/oidc/authorize https://aai-demo.egi.eu/oidc/authorize https://aai.egi.eu/oidc/authorize   Token https://aai-dev.egi.eu/oidc/token https://aai-demo.egi.eu/oidc/token https://aai.egi.eu/oidc/token   Device Code https://aai-dev.egi.eu/oidc/devicecode https://aai-demo.egi.eu/oidc/devicecode https://aai.egi.eu/oidc/devicecode   JSON Web Key(jwt) https://aai-dev.egi.eu/oidc/jwk https://aai-demo.egi.eu/oidc/jwk https://aai.egi.eu/oidc/jwk   User Info https://aai-dev.egi.eu/oidc/userinfo https://aai-demo.egi.eu/oidc/userinfo https://aai.egi.eu/oidc/userinfo   Introspection https://aai-dev.egi.eu/oidc/introspect https://aai-demo.egi.eu/oidc/introspect https://aai.egi.eu/oidc/introspect    Authorization Endpoint The Authorization Endpoint performs Authentication of the end user. This is done by sending the User Agent to the Authorization Server's Authorization Endpoint for Authentication and Authorization, using request parameters defined by OAuth 2.0 and additional parameters and parameter values defined by OpenID Connect.\nThe request parameters of the Authorization endpoint are:\n client_id: ID of the client that ask for authentication to the Authorization Server. redirect_uri: URI to which the response will be sent. scope: A list of attributes that the application requires. state: Opaque value used to maintain state between the request and the callback. response_type: value that determines the authorization processing flow to be used. For Authorization Code grant set response_type=code. This way the response will include an authorization code.  Token Endpoint To obtain an Access Token, an ID Token, and optionally a Refresh Token, the Client sends a Token Request to the Token Endpoint.\nDepending on the grant type, the following parameters are required:\nAuthorization Code    Parameter Presence Values     grant_type Required authorization_code   code Required The value of the code in the response from authorization endpoint.   redirect_uri Required URI to which the response will be sent (must be the same as the request to authorization endpoint)    Proof Key for Code Exchange (PKCE) The Proof Key for Code Exchange (PKCE, pronounced pixie) extension (RFC 7636) describes a technique for public clients (clients without client_secret) to mitigate the threat of having the authorization code intercepted. The technique involves the client first creating a secret, and then using that secret again when exchanging the authorization code for an access token. This way if the code is intercepted, it will not be useful since the token request relies on the initial secret.\nClient configuration To enable PKCE you need to go to the Manage Services Page and create/edit a client. In “Protocol” tab under “Token Endpoint Authentication Method” select “No authentication” and in “Crypto” tab under “Proof Key for Code Exchange (PKCE) Code Challenge Method” select “SHA-256 hash algorithm”.\nProtocol Flow Because the PKCE-enhanced Authorization Code Flow builds upon the standard Authorization Code Flow, the steps are very similar.\nFirst, the client creates and records a secret named the code_verifier. The code_verifier is a high-entropy cryptographic random STRING using the unreserved characters [A-Z] / [a-z] / [0-9] / “-” / “.” / “_” / “~”, with a minimum length of 43 characters and a maximum length of 128 characters. Then the client creates a code_challenge derived from the code_verifier by using one of the following transformations on the code verifier:\n plain code_challenge = code_verifier S256 code_challenge = BASE64URL-ENCODE(SHA256(ASCII(code_verifier)))  If the client is capable of using S256, it MUST use S256. Clients are permitted to use plain only if they cannot support S256 for some technical reason.\nNote There are various tools that generate these values such as https://tonyxu-io.github.io/pkce-generator/  Then the code_challenge is sent in the Authorization Request along with the transformation method (code_challenge_method).\nExample request:\nGET \"${AUTHORISATION_ENDPOINT}? client_id=${CLIENT_ID}\u0026scope=openid%20profile%20email \u0026redirect_uri=${REDIRECT_URI}\u0026response_type=code \u0026code_challenge=${CODE_CHALLENGE}\u0026code_challenge_method=S256\"  Note You can find the Authorisation Endpoint in the Endpoints table.  The Authorization Endpoint responds as usual but records code_challenge and the code_challenge_method.\nExample response:\nHTTP/1.1 302 Found Location: ${REDIRECT_URI}? code=fgtLHT The client then sends the authorization code in the Access Token Request as usual but includes the code_verifier secret generated in the first request.\nExample request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\  -d \"grant_type=authorization_code\" \\  -d \"code=${CODE}\" \\  -d \"client_id=${CLIENT_ID}\" \\  -d \"redirect_uri=${REDIRECT_URI}\" \\  -d \"code_verifier=${CODE_VERIFIER}\" | python -m json.tool  Note You can find the Token Endpoint in the Endpoints table.  The authorization server transforms code_verifier and compares it to code_challenge from the first request. Access is denied if they are not equal.\nExample response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUlMyNTYifQ...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUlMyNTYifQ...\", \"scope\": \"openid email profile\", \"token_type\": \"Bearer\" } Refresh request The following request allows obtaining an access token from a refresh token using the grant_type value refresh_token:\n   Parameter Presence Values     client_id Required The identifier of the client.   client_secret Required The secret value of the client.   grant_type Required refresh_token   refresh_token Required The value of the refresh token   scope Required This parameter should contain openid at least    Example request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\  -u \"${CLIENT_ID}\":\"${CLIENT_SECRET}\" \\  -d \"grant_type=refresh_token\" \\  -d \"refresh_token=${REFRESH_TOKEN}\" \\  -d \"scope=openid%20email%20profile\" | python -m json.tool;  Note You can find the Token Endpoint in the Endpoints table.  Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWx...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYW...\", \"refresh_token\": \"eyJhbGciOiJub25...\", \"scope\": \"openid profile email\", \"token_type\": \"Bearer\" } Refresh Request with PKCE To combine the refresh token grant type with PKCE you need to make the following request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\  -d \"client_id=${CLIENT_ID}\" \\  -d \"grant_type=refresh_token\" \\  -d \"refresh_token=${REFRESH_TOKEN}\" \\  -d \"scope=openid%20email%20profile\" | python -m json.tool;  Note You can find the Token Endpoint in the Endpoints table.  Token Exchange To get a token from client B using a token issued for client A, the parameters of the request are:\n   Parameter Presence Values     grant_type Required urn:ietf:params:oauth:grant-type:token-exchange   audience Optional Define the logical name of the service that the token will be used for   subject_token Required The value of the access token   subject_token_type Required urn:ietf:params:oauth:token-type:access_token (because this feature accepts access tokens only)   scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected    Example request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\  -u \"${CLIENT_B_ID}\":\"${CLIENT_B_SECRET}\" \\  -d \"grant_type=urn:ietf:params:oauth:grant-type:token-exchange\" \\  -d \"audience=tokenExchange\" \\  -d \"subject_token=${ACCESS_TOKEN_A}\" \\  -d \"subject_token_type=urn:ietf:params:oauth:token-type:access_token\" \\  -d \"scope=openid%20profile%20offline_access\" | python -m json.tool;  Note You can find the Token Endpoint in the Endpoints table.  Example response:\n{ \"access_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJvaWRjIiwiYWxnIjoiUl...\", \"refresh_token\": \"eyJhbGciOiJub25lIn0.eyJleHAiO...\", \"scope\": \"openid profile offline_access\", \"token_type\": \"Bearer\" } Device Code The device code flow enables OAuth clients on (input-constrained) devices to obtain user authorization for accessing protected resources without using an on-device user-agent, provided that they have an internet connection.\n1. Device Authorization Request The client initiates the authorization flow by requesting a set of verification codes from the authorization server by making an HTTP “POST” request to the device authorization endpoint. The client constructs the request with the following parameters:\n   Parameter Presence Values     client_id Required The identifier of the client   scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected    Example request:\ncurl -X POST \"${DEVICE_CODE_ENDPOINT}\" \\  -H \"Content-Type: application/x-www-form-urlencoded\" \\  -d \"client_id=${CLIENT_ID}\" \\  -d \"scope=openid%20email%20profile\" | python -m json.tool  Note You can find the Device Code Endpoint in the Endpoints table.  Example response:\n{ \"device_code\": \"c4341bd6-5e82-4f9c-9f6f-5842409d48db\", \"expires_in\": 600, \"user_code\": \"IEJSJB\", \"verification_uri\": \"https://aai.egi.eu/oidc/device\" } 2. User Interaction After receiving a successful Authorization Response, the client displays or otherwise communicates the user_code and the verification_uri to the end user and instructs them to visit the URI in a user agent on a secondary device (for example, in a browser on their mobile phone), and enter the user code.\n3. Device Access Token Request After displaying instructions to the user, the client makes an Access Token Request to the token endpoint. The request contains the following parameters:\n   Parameter Presence Values     grant_type Required urn:ietf:params:oauth:grant-type:device_code   device_code Required The device verification code, device_code from the Device Authorization Response   client_id Required The identifier of the client    client_secret Required The secret value of the client   scope Optional Define one or more scopes that are contained in the original token; otherwise all scopes will be selected    Example request:\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\  -H \"Content-Type: application/x-www-form-urlencoded\" \\  -d \"grant_type=urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Adevice_code\" \\  -d \"device_code=${DEVICE_CODE}\" \\  -d \"client_id=${CLIENT_ID}\" \\  -d \"client_secret=${CLIENT_SECRET}\" \\  -d \"scope=openid%20profile\" | python -m json.tool  Note You can find the Token Endpoint in the Endpoints table.  Example response:\n{ \"access_token\": \"eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJhZG1pbiIs...\", \"expires_in\": 3599, \"id_token\": \"eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiI5MDM0Mi...\", \"scope\": \"openid profile\", \"token_type\": \"Bearer\" } Device Code with PKCE To combine Device Code flow with PKCE you need to make the following requests:\n1 - Device Authorization Request:\ncurl -X POST \"${DEVICE_CODE_ENDPOINT}\" \\  -H \"Content-Type: application/x-www-form-urlencoded\" \\  -d \"client_id=${CLIENT_ID}\" \\  -d \"scope=openid%20email%20profile\" \\  -d \"code_challenge=${CODE_CHALLENGE}\" \\  -d \"code_challenge_method=S256\" | python -m json.tool  Note You can find the Device Code Endpoint in the Endpoints table.  2 - Device Access Token Request\ncurl -X POST \"${TOKEN_ENDPOINT}\" \\  -H \"Content-Type: application/x-www-form-urlencoded\" \\  -d \"grant_type=urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Adevice_code\" \\  -d \"device_code=${DEVICE_CODE}\" \\  -d \"client_id=${CLIENT_ID}\" \\  -d \"code_verifier=${CODE_VERIFIER}\" | python -m json.tool  Note You can find the Token Endpoint in the Endpoints table.  Claims-based authorisation EGI Check-in provides information about the authenticated user that may be used by Service Providers in order to control user access to resources. This information is provided by the EGI Check-in OIDC Provider in the form of OIDC claims. The table below lists the claims that are relevant for user authorisation:\n   Description OIDC Claim     VO/group membership/roles of the authenticated user eduperson_entitlement   Capabilities eduperson_entitlement   GOCDB roles eduperson_entitlement   Identity Assurance eduperson_assurance    Example OIDC Client In this guide we will demonstrate how to install and configure a Simple OIDC Client.\nInstall simple-oidc-client-php This guide assumes the Apache HTTP server has been installed and the document root is /var/www/html\nMove to the apache document root and download and extract simple-oidc-client-php-v2.0.0.zip.\nConfigure Client Login to the EGI Federation Registry\nThen create a new service or edit your existing service. In General tab fill all the required fields. For Integration Environment select Demo. In Protocol Specific tab select as Protocol the OIDC Service and then in the Redirect URI(s) insert your simple-oidc-client-php URL (e.g. http://localhost/simple-oidc-client-php/refreshtoken.php). This URL must link to refreshtoken.php which is located in simple-oidc-client-php directory. Next, in Scope select the scopes that your service needs. Then, submit the form and and self approve it. Finally you should get a pair of Client ID and Client Secret.\nConfigure simple-oidc-client-php Now that you have everything you need, you can configure your login settings. Go to your terminal and open config.php with your favorite text editor.\nExample:\nvi simple-oidc-client-php/config.php Let’s go quickly through the settings:\n title required, the title on the navigation bar img required, the source of the logo scope_info optional, a message that informs the user for the application requirements issuer required, the base URL of our IdentityServer instance. This will allow oidc-client to query the metadata endpoint so it can validate the tokens client_id required, the ID of the client we want to use when hitting the authorization endpoint client_secret optional, a value the offers better security to the message flow pkceCodeChallengeMethod optional, a string that defines the code challenge method for PKCE. Choose between plain or S256. redirect_url required, the redirect URL where the client and the browser agree to send and receive correspondingly the code scopesDefine required, defines the scopes the client supports refresh_token_note optional, info for the refresh token access_token_note optional, info for the access token manage_token_note optional, message the informs the user where can manage his tokens manageTokens optional, URL of the manage tokens service sessionName required, define the name of the cookie session sessionLifetime required, define the duration of the session. This must be equal to the validity time of the access token.  You must change the followings options based on your Service configuration you setup earlier:\n issuer client_id client_secret redirect_url scopesDefine sessionName (based on the installation path of the portal)  An example configuration follows:\n\u003c?php // index.php interface configuration $title = \"Generate Tokens\"; $img = \"https://clickhelp.co/images/feeds/blog/2016.05/keys.jpg\"; $scope_info = \"This service requires the following permissions for your account:\"; // Client configuration $issuer = \"https://aai-demo.egi.eu/oidc/\"; $client_id = \"CHANGE_ME\"; $client_secret = \"CHANGE_ME\"; // comment if you are using PKCE // $pkceCodeChallengeMethod = \"S256\"; // uncomment to use PKCE $redirect_url = \"http://localhost/simple-oidc-client-php/refreshtoken.php\"; // add scopes as keys and a friendly message of the scope as value $scopesDefine = array( 'openid' =\u003e 'log in using your identity', 'email' =\u003e 'read your email address', 'profile' =\u003e 'read your basic profile info', ); // refreshtoken.php interface configuration $refresh_token_note = \"NOTE: New refresh tokens expire in 12 months.\"; $access_token_note = \"NOTE: New access tokens expire in 1 hour.\"; $manage_token_note = \"You can manage your refresh tokens in the following link: \"; $manageTokens = $issuer . \"manage/user/services\"; $sessionName = \"simple-oidc-client-php\"; $sessionLifetime = 60*60; // must be equal to access token validation time in seconds Integrating Science Gateways with RCauth for obtaining (proxy) certificates In order for Science Gateways (VO portals) to obtain RFC proxy certificates derived from personal end-entity certificates, an EGI Science Gateway can make use of the IGTF-approved IOTA-type RCauth.eu online CA. The actual integration goes via an intermediary service, called a Master Portal. EGI is running two Master Portal instances, one development, one production instance.\n   Endpoint Development environment Production environment     Provider configuration https://masterportal-pilot.aai.egi.eu/mp-oa2-server/.well-known/openid-configuration https://aai.egi.eu/mp-oa2-server/.well-known/openid-configuration   Client registration https://masterportal-pilot.aai.egi.eu/mp-oa2-server/register https://aai.egi.eu/mp-oa2-server/register   Authorisation https://masterportal-pilot.aai.egi.eu/mp-oa2-server/authorize https://aai.egi.eu/mp-oa2-server/authorize   Token https://masterportal-pilot.aai.egi.eu/mp-oa2-server/token https://aai.egi.eu/mp-oa2-server/token   JSON Web Key(jwt) https://masterportal-pilot.aai.egi.eu/mp-oa2-server/certs https://aai.egi.eu/mp-oa2-server/certs   User Info https://masterportal-pilot.aai.egi.eu/mp-oa2-server/userinfo https://aai.egi.eu/mp-oa2-server/userinfo    Registering a client at the Master Portal In order to register a new client for your VO portal go to:\n EGI Development instance: https://masterportal-pilot.aai.egi.eu/mp-oa2-server/register EGI Production instance: https://aai.egi.eu/mp-oa2-server/register  Note Make sure to store the client_id and client_secret in a secure place  In order to get the client approved, send an email to the administrator of the EGI Master Portal using EGI Check-in support.\nDetailed information For further and detailed instructions on the integration flow, see the generic RCAuth.eu MasterPortal VOPortal integration guide\nSSH key authentication for proxy retrieval The EGI MasterPortal also allows users to authenticate using SSH key pair, in order to retrieve proxy certificates from the MasterPortal. Users need to first upload the public key via a self-service portal, https://aai.egi.eu/sshkeys/. About once a week they need to follow a web-flow to ensure a long-lived proxy certificate is present in MasterPortal, e.g. by going to https://aai.egi.eu/vo-portal/. They can then obtain a proxy certificate by doing\nssh proxy@ssh.aai.egi.eu and storing the output in /tmp/x509up_u$(id -u)\nGeneric information for users on how to do this can be found at Instructions for end users on how to use the SSH key authN for proxy retrieval. Alternatively VO portals could implement such functionality themselves by using the API described at the Master Portal sshkey endpoint description.\nUser attributes This section defines the attributes that can be made available to services connected to Check-in.\n1. EGI ID    attribute name EGI ID     description An identifier for the user, unique among all EGI accounts and never reused   SAML Attribute(s) 1.3.6.1.4.1.5923.1.1.1.13 (eduPersonUniqueId)   OIDC scope openid   OIDC claim(s) sub   OIDC claim location ID tokenUserinfo endpointIntrospection endpoint   origin Check-in assigns this attribute on user registration   changes No   multiplicity No   availability Always   example ef72285491ffe53c39b75bdcef46689f5d26ddfa00312365cc4fb5ce97e9ca87@egi.eu   notes Use EGI ID within your application as the unique-identifier key for the user   status Stable    2. Display Name    attribute name Display Name     description The user’s full name, in a displayable form   SAML Attribute(s) urn:oid:2.16.840.1.113730.3.1.241 (displayName)   OIDC scope profile   OIDC claim(s) name   OIDC claim location Userinfo endpoint   origin Provided by user’s Identity Provider   changes Yes   multiplicity Single-valued   availability Always   example John Doe   notes -   status Stable    3. Given Name    attribute name Given Name     description The user’s first name   SAML Attribute(s) urn:oid:2.5.4.42 (givenName)   OIDC scope profile   OIDC claim(s) given_name   OIDC claim location Userinfo endpoint   origin Provided by user’s Identity Provider   changes Yes   multiplicity Single-valued   availability Always   example John   notes -   status Stable    4. Family Name    attribute name Family Name     description The user’s last name   SAML Attribute(s) urn:oid:2.5.4.4 (sn)   OIDC scope profile   OIDC claim(s) family_name   OIDC claim location Userinfo endpoint   origin Provided by user’s Identity Provider   changes Yes   multiplicity Single-valued   availability Always   example Doe   notes -   status Stable    5. Username    attribute name Username     description The username by which the user wishes to be referred to   SAML Attribute(s) urn:oid:0.9.2342.19200300.100.1.1 (uid)   OIDC scope profile   OIDC claim(s) preferred_username   OIDC claim location ID tokenUserinfo endpointIntrospection endpoint   origin Check-in assigns this attribute on user registration   changes No   multiplicity Single-valued   availability Always   example jdoe   notes The Service Provider MUST NOT rely upon this value being unique   status Stable    6. Email Address    attribute name Email Address     description The user’s email address   SAML Attribute(s) urn:oid:0.9.2342.19200300.100.1.3 (mail)   OIDC scope email   OIDC claim(s) email   OIDC claim location Userinfo endpointIntrospection endpoint   origin Provided by user’s Identity Provider   changes Yes   multiplicity Single-valued   availability Always   example john.doe@example.org   notes This MAY NOT be unique and is NOT suitable for use as a primary key   status Stable    7. Verified email flag    attribute name Verified email flag     description True if the user’s email address has been verified; otherwise false   SAML Attribute(s) See Verified email list   OIDC scope email   OIDC claim(s) email_verified   OIDC claim location Userinfo endpointIntrospection endpoint   origin Check-in assigns this attribute on user registration   changes Yes   multiplicity Single-valued   availability Always   example true   notes This claim is available only in OpenID Connect   status Stable    8. Verified email list    attribute name Verified email list     description A list of user’s email addresses that have been verified   SAML Attribute(s) urn:oid:1.3.6.1.4.1.25178.4.1.14 (voPersonVerifiedEmail)   OIDC scope email   OIDC claim(s) voperson_verified_email   OIDC claim location Userinfo endpointIntrospection endpoint   origin Check-in or the user’s Identity Provider   changes Yes   multiplicity Multi-valued   availability Not always   example john.doe@example.orgjdoe@example.com   notes -   status Experimental    9. Affiliation    attribute name Affiliation     description The user’s affiliation within a particular security domain (scope)   SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.9 (eduPersonScopedAffiliation)   OIDC scope eduperson_scoped_affiliation   OIDC claim(s) eduperson_scoped_affiliation   OIDC claim location Userinfo endpointIntrospection endpoint   origin Check-in assigns this attribute on user registration   changes Yes   multiplicity Multi-valued   availability Always   example member@example.org   notes Service Providers are encouraged to validate the scope of this attribute   status Stable    10. Groups    attribute name Groups     description The user’s group/VO membership/role information expressed as entitlements   SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement)   OIDC scope eduperson_entitlement   OIDC claim(s) eduperson_entitlement   OIDC claim location Userinfo endpointIntrospection endpoint   origin Group memberships are managed by group administrators   changes Yes   multiplicity Multi-valued   availability Not always   example urn:mace:egi.eu:aai.egi.eu:vm_operator@fedcloud.egi.eu   notes -   status Stable    11. Capabilities    attribute name Capabilities     description This attribute describes the resource or child-resource a user is allowed to access, optionally specifying certain actions the user is entitled to perform, as described in AARC-G027   SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement)   OIDC scope eduperson_entitlement   OIDC claim(s) eduperson_entitlement   OIDC claim location Userinfo endpointIntrospection endpoint   origin Capabilities are managed by Check-in   changes Yes   multiplicity Multi-valued   availability Not always   example urn:mace:egi.eu:res:rcauth#aai.egi.eu   notes -   status Stable    12. GOCDB Roles    attribute name GOCDB Roles     description The user’s GOCDB role information expressed as entitlements   SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.7 (eduPersonEntitlement)   OIDC scope eduperson_entitlement   OIDC claim(s) eduperson_entitlement   OIDC claim location Userinfo endpointIntrospection endpoint   origin The roles are managed in GOCDB   changes Yes   multiplicity Multi-valued   availability Not always   example urn:mace:egi.eu:goc.egi.eu:100453G0:GRIDOPS-CheckIn:Site+Administrator@egi.eu   notes -   status Stable    13. Assurance    attribute name Assurance     description Assurance of the identity of the user, following REFEDS Assurance Framework (RAF) and the EGI AAI Assurance Profiles. The following RAF values are qualified and automatically set for all Community identities:$PREFIX$$PREFIX$/ID/unique$PREFIX$/ID/eppn-unique-no-reassign$PREFIX$/IAP/low$PREFIX$/ATP/ePA-1m$PREFIX$/ATP/ePA-1dFollowing RAF values are set if the currently used authentication provider asserts (or otherwise qualifies to) them:$PREFIX$/IAP/medium$PREFIX$/IAP/highThe following compound profiles are asserted if the user qualifies to them$PREFIX$/profile/cappuccino$PREFIX$/profile/espresso   SAML Attribute(s) urn:oid:1.3.6.1.4.1.5923.1.1.1.11 (eduPersonAssurance)   OIDC scope eduperson_assurance   OIDC claim(s) eduperson_assurance   OIDC claim location Userinfo endpointIntrospection endpoint   origin Check-in assigns this attribute on user registration   changes Yes   multiplicity Multi-valued   availability Not always   example [https://aai.egi.eu/LoA#Low, https://refeds.org/assurance/IAP/low]   notes -   status Stable    14. CertEntitlement    attribute name CertEntitlement     description Provides information about the user’s certificate subject(s) and the associated VO(s)   SAML Attribute(s) Not available   OIDC scope cert_entitlement   OIDC claim(s) cert_entitlement   OIDC claim location Userinfo endpointIntrospection endpoint   origin VO/group management tools integrated with Check-in   changes Yes   multiplicity Multi-valued   availability Not always   example {\"cert_entitlement\": [{\"cert_subject_dn\": \"/C=GR/O=HellasGrid/...\",\"cert_iss\": \"/C=GR/O=HellasGrid/...\",\"eduperson_entitlement\": \"urn:mace:egi.eu:group:checkin-integration:role=VO-Admin#aai.egi.eu\"}]}   notes This is available only for DIRAC   status Stable    15. SSH Public Key    attribute name SSH Public Key     description Provides information about the user’s SSH public key(s)   SAML Attribute(s) urn:oid:1.3.6.1.4.1.24552.500.1.1.1.13 (sshPublicKey)   OIDC scope ssh_public_key   OIDC claim(s) ssh_public_key   OIDC claim location Userinfo endpoint   origin Added SSH public key(s) in user’s Check-in Profile   changes Yes   multiplicity Multi-valued   availability Not always   example ssh-rsa AAAAB3NzaC...qxxEEipdnZ nikosev@grnet-hq.admin.grnet.gr   notes -   status Experimental    User authorisation The following information about the authenticated user can be provided by EGI Check-in in order to control user access to resources:\n VO/group membership and role information about the authenticated user Capabilities Identity Assurance GOCDB roles  VO/group membership and role information Background VO/group membership and role information is encoded in entitlements (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN.\nSyntax An entitlement value expressing group membership and role information has the following syntax (components enclosed in square brackets are OPTIONAL):\nurn:mace:egi.eu:group:\u003cGROUP\u003e[:\u003cSUBGROUP\u003e*][:role=\u003cROLE\u003e]#\u003cGROUP-AUTHORITY\u003e where:\n \u003cGROUP\u003e is the name of a VO, research collaboration or a top level arbitrary group. \u003cGROUP\u003e names are unique within the urn:mace:egi.eu:group namespace; zero or more \u003cSUBGROUP\u003e components represent the hierarchy of subgroups in the \u003cGROUP\u003e; specifying sub-groups is optional the optional \u003cROLE\u003e component is scoped to the rightmost (sub)group; if no group information is specified, the role applies to the VO \u003cGROUP-AUTHORITY\u003e is a non-empty string that indicates the authoritative source for the entitlement value. For example, it can be the FQDN of the group management system that is responsible for the identified group membership information  Example:\nurn:mace:egi.eu:group:fedcloud.egi.eu:role=vm_operator#aai.egi.eu Capabilities Background The user’s capability information is encoded in entitlements (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN following the syntax defined in AARC-G027.\nSyntax An entitlement value expressing a capability has the following syntax (components enclosed in square brackets are OPTIONAL):\n\u003cNAMESPACE\u003e:res:\u003cRESOURCE\u003e[:\u003cCHILD-RESOURCE\u003e]...[:act:\u003cACTION\u003e[,\u003cACTION\u003e]...]#\u003cAUTHORITY\u003e where:\n  \u003cNAMESPACE\u003e is controlled by the e-infrastructure, research infrastructure or research collaboration that manages the capability. The \u003cNAMESPACE\u003e of capabilities managed by Check-in is set to urn:mace:egi.eu, while, generally, it is in the form of urn:\u003cNID\u003e:\u003cDELEGATED-NAMESPACE\u003e[:\u003cSUBNAMESPACE\u003e]... where:\n  \u003cNID\u003e is the namespace identifier associated with a URN namespace registered with IANA2, ensuring global uniqueness. Implementers SHOULD use one of the existing registered URN namespaces, such as urn:mace[MACE].\n  \u003cDELEGATED-NAMESPACE\u003e is a URN sub-namespace delegated from one of the IANA registered NIDs to an organisation representing the e-infrastructure, research infrastructure or research collaboration. It is RECOMMENDED that a publicly accessible URN value registry for each delegated namespace be provided.\n    The literal string \"res\" indicates that this is a resource-specific entitlement as opposed to, for example, an entitlement used for expressing group membership AARC-G002.\n  \u003cRESOURCE\u003e is the name of the resource. Whether the name should be unique is an implementation decision.\n  An optional list of colon-separated \u003cCHILD-RESOURCE\u003e components represents a specific branch of the hierarchy of resources under the identified \u003cRESOURCE\u003e.\n  An optional list of comma-separated \u003cACTION\u003es MAY be included, which, if present, MUST be prefixed with the literal string “act”. This component MAY be used for further specifying the actions a user is entitled to do at a given resource. Note that the list of \u003cACTION\u003es is scoped to the rightmost child-resource; if no child-resource information is specified, actions apply to the top level resource. The interpretation of a capability without actions specified is an implementation detail.\n  \u003cAUTHORITY\u003e is a mandatory and non-empty string that indicates the authoritative source of the capability. This SHOULD be used to further specify the exact issuing instance. For example, it MAY be the FQDN of the service that issued that specific capability. The \u003cAUTHORITY\u003e is specified in the f-component RFC8141 of the URN; thus, it is introduced by the number sign (\"#\") character and terminated by the end of the URN. All characters must be encoded according to RFC8141. Hence, the \u003cAUTHORITY\u003e MUST NOT be considered when determining equivalence (Section 3 in RFC8141) of URN-formatted capabilities. The \u003cAUTHORITY\u003e of capabilities managed by Check-in is typically set to aai.egi.eu.\n  Example:\nurn:mace:egi.eu:res:rcauth#aai.egi.eu Identity Assurance Based on the authentication method selected by the user, the EGI proxy assigns a Identity Assurance, which is conveyed to the SP through both the eduPersonAssurance attribute and the Authentication Context Class (AuthnContextClassRef) of the SAML authentication response. EGI Check-in uses Assurance Profiles which distinguish between three Identity Assurance levels, similarly to the eID Assurance Framework (eIDAF). Each level is represented by a URI as follows:\n Low: Authentication through a social identity provider or other low identity assurance provider: https://aai.egi.eu/LoA#Low Substantial: Password/X.509 authentication at the user's home IdP: https://aai.egi.eu/LoA#Substantial High: Substantial + multi-factor authn (not yet supported, TBD): https://aai.egi.eu/LoA#High  Moreover, EGI Check-in follows the REFEDS Assurance framework (RAF). The EGI Check-in conveys any RAF values provided by the IdP directly to the SP, through the aforementioned methods. Furthermore, Check-in will append into the User’s profile any additional LoA, if the user is eligible for it. For example, a user having a Verified Email is eligible for the RAF value https://refeds.org/assurance/IAP/low\nSome EGI SPs have been configured to provide limited access (or not to accept at all) credentials with the Low LoA.\nNote: When logging in through the EGI SSO IdP, the LoA is determined based on the selected authentication mechanism as follows:\n Username/password credentials → Low X.509 certification → Substantial  GOCDB Roles Background GOCDB roles, as per GOCDB documentations, are encoded (eduPersonEntitlement attribute values in SAML or eduperson_entitlement claim in OIDC). These entitlements are typically used to indicate access rights to protected resources. Entitlements are multi-valued, with each value formatted as a URN.\nSyntax An entitlement value expressing GOCDB roles has the following syntax (components enclosed in square brackets are OPTIONAL):\nurn:mace:egi.eu:goc.egi.eu:\u003cPRIMARY_KEY\u003e:\u003cON_ENTITY\u003e:\u003cUSER_ROLE\u003e@egi.eu where:\n \u003cPRIMARY_KEY\u003e is the primary key for the user role, e.g. “123G0” \u003cON_ENTITY\u003e is the name of the entity on which the user role applies to, e.g. “GRIDOPS-GOCDB” \u003cUSER_ROLE\u003e is the user’s role, e.g. “Site Operations Manager”  Example:\nurn:mace:egi.eu:goc.egi.eu:100453G0:GRIDOPS-CheckIn:Site+Administrator@egi.eu ","categories":"","description":"Check-in guide for Service Providers","excerpt":"Check-in guide for Service Providers","ref":"/providers/check-in/sp/","tags":"","title":"Service Providers"},{"body":"General recommendations  All files and folders should be lower case EGI Services should be named exactly as in the EGI Services Portfolio Acronyms should be used only when it makes sense Service names should never be replaced by acronyms When introducing services, link to the public page of the service, if any:  [EGI Cloud Compute](https://www.egi.eu/services/cloud-compute/) Markdown writing guidelines Documentation pages have to be written in markdown, compliant with CommonMark and GitHub Flavored Markdown.\nBasic rules  Headings must start at level 2 (##), as level 1 (#) is the title of the page Lines should be wrapped at 80 characters Sentences must be separated by one space only Indent is made with spaces, not with tabs Bullet lists should be using - not * Numbered lists should be using 1. for each line (automatic numbering) Indent secondary (and following) level lists with 2 spaces Lines must end with a Line Feed character (\\n) Files must end with an empty line Shell examples should include a prompt ($ or \u003e) in front of commands, to make it easy to understand which is the command and which is the output Commands in shell examples should be broken into multiple lines of 80 characters or less, using a trailing backslash character (\\) on each line that continues on the next Never break command output in shell examples to multiple lines, instead use style exceptions when necessary  Tip Syntax examples that can be used in the files are documented in the shortcodes section.  Automating formatting and checking Style should be enforced via the usage of Prettier. Prettier can be integrated with various editors.\n With VIM/neovim it can be used via a plugin like ALE as described in the official documentation. With VisualStudio Code please see the official documentation  Configuration is provided in .prettierrc, options can be set as follows:\n--print-width 80 --tab-width 2 --prose-wrap always When a contribution is received (via a pull request), the proposed changes are checked using various linters.\nGeneral writing guidelines Follow the guidelines below to ensure readability and consistency of the EGI documentation. These are based on the OpenStack documentation writing style guidelines, released under a Creative Commons license.\nTip Short and simple sentences are easier to read and understand.  Use standard English Use standard British English (UK) throughout all technical publications. When in doubt about the spelling of a word, consult the Merriam-Webster’s Collegiate Dictionary and the IBM developerWorks editorial style guide.\nBe clear and concise Follow the principles of minimalism. If you can describe an idea in one word, do not use two words. Eliminate all redundant modifiers, such as adjectives and adverbs.\nWrite objectively Do not use humor, jargon, exclamation marks, idioms, metaphors, and other colloquialisms.\nDescribe the most common use case first Put the most common case in the main clause and at the beginning of a paragraph or section. You can introduce additional use cases by starting a sentence with “however” or “if”.\nWrite in active voice In general, write in active voice rather than passive voice. Active voice identifies the agent of action as the subject of the verb, usually the user. Passive voice identifies the recipient (not the source) of the action as the subject of the verb.\nActive-voice sentences clarify the performer of an action and are easier to understand than passive-voice sentences. Passive voice is usually less engaging and more complicated than active voice. When you use passive voice, the actions and responses of the software can be difficult to distinguish from those of the user. In addition, passive voice usually requires more words than active voice.\nExamples       Do not use Use     After the software has been installed, the computer can be started. After you install the software, start the computer.   The Configuration is saved when you click OK. Click OK to save the configuration.   A server is created by you. Create a server.      However, passive voice is acceptable in the following situations:\n Using active voice sounds like you are blaming the user. For example, you can use passive voice in an error message or troubleshooting content when the active subject is the user.  Example       Do not use Use     If the build fails, you probably omitted the flavor. If the build fails, the flavor might have been omitted.       The agent of action is unknown, or you want to de-emphasize the agent of action and emphasize the object on which the action is performed.  Example       Do not use Use     The product, OS, or database returns the messages. The messages are returned [by the database].       Recasting the sentence in active voice is wordy or awkward.  Example       Do not use Use     In 2009, engineers developed a software that simplifies the installation. A software that simplifies the installation was developed in 2009.      Write in second person Users are more engaged with documentation when you use second person (that is, you address the user as “you”).\nWriting in second person has the following advantages:\n Second person promotes a friendly tone by addressing users directly. Using second person with the imperative mood (in which the subject you is understood) and active voice helps to eliminate wordiness and confusion about who or what initiates an action, especially in procedural steps. Using second person also avoids the use of gender-specific, third-person pronouns such as he, she, his, and hers. If you must use third person, use the pronouns they and their, but ensure that the pronoun matches the referenced noun in number. Use first person plural pronouns (we, our) judiciously. These pronouns emphasize the writer or EGI rather than the user, so before you use them, consider whether second person or imperative mood is more “user-friendly.” However, use “we recommend” rather than “it is recommended” or “EGI recommends”.  Tip You can use “we” in the place of EGI if necessary.  Do not use first person to avoid naming the product or to avoid using passive voice. If the product is performing the action, use third person (the product as an actor). If you want to de-emphasize the agent of action and emphasize the object on which the action is performed, use passive voice.\nThe first-person singular pronoun “I” is acceptable in the question part of FAQs and when authors of blogs or signed articles are describing their own actions or opinions.\nImportant Do not switch person (point of view) in the same guide or on the same page.  Examples       Do not use Use     Creating a server involves specifying a name, flavor, and image. To create a server, specify a name, a flavor, and image.   To create a server, the user specifies a name, flavor, and image. To create a server, you specify a name, flavor, and image.      Use the present simple tense Users read documentation to perform tasks or gather information. For users, these activities take place in their present, so the present tense is appropriate in most cases. Additionally, the present tense is easier to read than the past or future tense.\nExample       Do not use Use     The product will prompt you to verify the deletion. After you log in, your account will then begin the verification process. The product prompts you to verify the deletion. After you log in, your account begins the verification process.      Tip Use the future tense only when you need to emphasize that something will occur later (from the users’ perspective).  Do not humanize inanimate objects Do not give human characteristics to non-human subjects or objects.\nExample       Do not use Use     This guide assumes… This guide describes…      Avoid personification Do not express your fears or feelings in technical writing. Avoid the adverbs such as “probably”, “hopefully”, “basically”, and so on.\nAvoid ambiguous titles Each title should include a clear description of the page’s or chapter’s subject.\nExample       Do not use Use     Update metadata Update object metadata      Eliminate needless politeness Do not use “please” and “thank you” in technical documentation.\nWrite positively Write in a positive tone. Positive sentences improve readability. Try to avoid the following words as much as possible:\nExamples       Do not use Use     damage affect   catastrophic serious   bad serious (or add explanation)   fail unable to   kill cancel or stop   fatal serious   destroy remove or delete   wrong incorrect or inconsistent      Do not use contractions Generally, do not contract the words.\nExamples       Do not use Use     can’t cannot   don’t do not      Do not overuse this, that, these, and it Use these pronouns sparingly. Overuse contributes to readers’ confusion. To fix the ambiguity, rephrase the sentence.\nExample       Do not use Use     The monitoring system should perform regular checks to verify that the Ceph cluster is healthy. This can be achieved using the Ceph health command. The monitoring system performs regular checks to ensure the Ceph cluster is functioning correctly. Use the Ceph health command to run a health check.      Tip You can also fix the ambiguity by placing a noun modifier immediately after the pronoun.  Do not split infinitives Do not place modifiers between “to” and the verb. Typically, placing an adverb or an adjective between “to” and a verb adds ambiguity to a sentence.\nAvoid prepositions at the end of sentences As much as possible, avoid trailing prepositions in sentences by avoiding phrasal verbs.\nExample       Do not use Use     The image registration window will open up. The image registration window opens.      To fix the verb-preposition constructions, replace them with active verbs.\nExamples       Do not use Use     written up composed   pop up appear      Use consistent terminology Use consistent terms across all content. Avoid multiple variations or spellings to refer to the same service, function, UI element, and so on.\nUse spelling and grammar checking tools Run text through spelling and grammar checking tools, if available. Correcting mistakes, especially to larger sections of new content, helps eliminate rework later.\nLists When reading a document for the first time, users scan through pages stopping only on the content that stands out, such as titles, lists, links, diagrams, and so on. Lists help to organize options, as well as help readers to find information easily.\nWhen listing items, follow these guidelines:\n Use a bulleted list for options. Create a bulleted list when you need to describe more than three options. Use a numbered list for steps. Use a definition list to explain terms or describe command-line parameters, options, or arguments. Use a colon at the end of the sentence that introduces a list. Use the same grammatical structure (aka parallel structure) for all items in a list. Start each option with a capital letter.  When listing options in a paragraph, add and or or before the last item in a list. Use a serial (Oxford) comma before these conjunctions if they connect three or more items.\nPunctuation in lists In bulleted lists:\n If you list individual words or phrases, do not add a period at the end of each list item. If you use full sentences, add a period at the end of each sentence. If your list includes both individual words or phrases and full sentences, be consistent and either add or do not add periods to all items.  In numbered lists:\n Add periods at the end of steps. If an item of a numbered list is followed by a code block that illustrates how to perform the step, use a colon at the end.  Adding exceptions for style violations Successfully passing the checks is a firm requirement, but for the following cases it is possible to add exceptions and bypass some checks in Markdown files:\n When in-line HTML must be used (e.g. in tables, or when no other proper solution is available) When the same procedure needs to be described for multiple platforms, and the automatic checker flags it as duplicate content  Important Exceptions should only be used when there are no other choices, and should be confined to the smallest possible block of Markdown code.  Dealing with in-line HTML tags In some specific cases it is impossible to use anything but in-line HTML tags:\n Presentation page leveraging bootstrap CSS classes or other advanced features Using special formatting for the information presented (e.g. a list in a table cell)  Blocks with in-line HTML tags should be preceded by a HTML comment starting with markdownlint-disable to disable the no-inline-html check, as in the following example:\nTip When having a table is not absolutely necessary, use a different construct to present the information.  \u003c!-- markdownlint-disable no-inline-html --\u003e | Action | OCCI | OpenStack | This is a very long column with important data | | ----------- | ------------------------ | ---------------------- | ---------------------------------------------- | | List images | `occi -a list -r os_tpl` | `openstack image list` | \u003cul\u003e\u003cli\u003eLorem\u003c/li\u003e\u003cli\u003eipsum\u003c/li\u003e\u003c/ul\u003e | \u003c!-- markdownlint-enable no-inline-html --\u003e  Tip Do not forget to follow up with a HTML comment starting with markdownlint-enable to re-enable the no-inline-html check.  Important Always use the tag that is providing the proper semantic: e.g. for a list use \u003cul\u003e and \u003cli\u003e, not \u003cbr /\u003e.  Dealing with duplicate content When the same procedure needs to be described for multiple platforms, or when the same code has to be exemplified for multiple languages, it is possible that the automatic checkers will flag these as duplicates.\nFor example, describing the following procedure will result in duplicates being reported:\n{{\u003ctabpanex\u003e}} {{\u003ctabxheader=\"Linux\"\u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/engine/install/#server), then run the following commands: ```shell $ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash ``` {{\u003c/tabx\u003e}} {{\u003ctabxheader=\"Mac\"\u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/desktop/mac/install/), then run the following commands: ```shell $ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash ``` {{\u003c/tabx\u003e}} {{\u003ctabxheader=\"Windows\"\u003e}} To run the FedCloud client in a container, make sure [Docker is installed](https://docs.docker.com/desktop/windows/install/), then run the following commands: ```shell \u003e docker pull tdviet/fedcloudclient \u003e docker run -it tdviet/fedcloudclient bash ``` {{\u003c/tabx\u003e}} {{\u003c/tabpanex\u003e}} This type of content should be preceded by a HTML comment that disables the check for duplicates, and followed by another HTML comment that enables it again.\n\u003c!-- // jscpd:ignore-start --\u003e ...content with duplicates here... \u003c!-- // jscpd:ignore-end --\u003e ","categories":"","description":"Style guide for EGI documentation","excerpt":"Style guide for EGI documentation","ref":"/about/contributing/style/","tags":"","title":"Style Guide"},{"body":"An overview of the use cases and possible deployment scenarios of the EGI DataHub.\nTransparent data access  Clients use one ore more providers to access data Data can be accessed over multiple protocols  Federation of service providers  Heterogeneous backend storage Common interfaces (Web, REST, POSIX, CDMI) Common AAI with Check-in Discovery of Datasets in the EGI DataHub  Smart caching  Site A hosts data and computing resources Site B hosts only data Site X uses data from A and B without pre-staging Pre-staging can also be done using APIs Data is accessed locally “à la” POSIX with FUSE  Publication of datasets  PID minting Publishing, discovery and access to datasets  Integrating DataHub and EGI Notebooks ","categories":"","description":"[EGI DataHub](https://datahub.egi.eu/) Use Cases","excerpt":"[EGI DataHub](https://datahub.egi.eu/) Use Cases","ref":"/users/datahub/usecases/","tags":"","title":"Use Cases"},{"body":"Overview The WebFTS interface offers end users a way to graphically execute and monitor file transfer between storages using different protocols. You can check CERN WebFTS documentation and FAQ for more details.\nCredential Delegation In order to move files between storage endpoints, named also Storage Elements (SE), the authentication is done using an X.509 certificate which needs to be installed on the browser. The Integration with EGI Check-in is under development and will be available at a later date in the production instance.\nA pop-up window appears when you choose the “My jobs” or “Submit a transfer” tabs. There, you will need to paste the private RSA key of your certificate. The private key WILL NOT BE TRANSMITTED ANYWHERE. It is only used locally (within the user’s browser) to generate the proxies needed to have access to the FTS services.\nTo obtain the private key, you can run in the console:\nopenssl pkcs12 -in yourCert.p12 -nocerts -nodes | openssl rsa This command requires openssl.\nIf you need a delegation with VOMS credentials (required to access some types of Storage Elements), you will need to introduce the name of the virtual organization (VO) you belong to.\nFrom the right button where the remaining time for the current delegation is shown, you can remove the current delegation and delegate again (the delegation window will appear).\nSubmit a transfer Open the “Submit a transfer” tab and Load origin and destination storage elements as endpoints. If one endpoint URL is not known, there is an autocompletion once 3 characters have been typed. You should also specify the protocol, the address of the endpoint and the path. (Ex: gsiftp://lxfsra10a01.cern.ch/dpm/cern.ch/home/).\nBrowse the content and select all the files you want to transfer. CTRL and SHIFT keys can be used for selecting multiple entries at once.\nIf you need to filter the list of files and folders, you can use the available filters: name, size and date. Once you have loaded both endpoints and selected the files, the transfer buttons will be enabled. Click the button with the correct direction. A success or error message will appear above the endpoints' containers.\nYou can now check the status of you transfer in the “My jobs” tab.\nListing your transfers Open the “My jobs” tab. If you have done any transfers recently, they will appear there. If you click on an individual transfer, you can see its state and any errors.\nYou can resubmit transfer jobs, but you would need to delegate your credentials if you did not do it before. You can do this at any state by clicking the “Resubmit” button.\nThe transfer states are: YELLOW if still running, GREEN if successfully completed and RED if something went wrong.\nThe transfers that are not completed have a “Cancel” button on the left of the “Resubmit” button. Clicking this button will cancel that transfer.\n","categories":"","description":"Documentation related to EGI Data Transfer Web Interface","excerpt":"Documentation related to EGI Data Transfer Web Interface","ref":"/users/data-transfer/webfts/","tags":"","title":"WebFTS"},{"body":"Virtual Organisations (VOs) in Check-in are represented as Collaborative Organisation Units (COUs).\nIn order to join a Virtual Organisation you must have an EGI account. If you don’t have, then first sign up for an EGI account.\n  Login to Check-in registry with your EGI account.\n  Expand the People drop down menu and click Enroll.\n  Click the Begin link of the Enrollment flow of the VO you want to join\n  Click the Begin button to start the Enrollment flow\n  a. If there are no pending petitions the enrollment flow will continue as usual.\nb. If there is one pending petition:\nb1. If its status is in Pending Approval you will see a page similar to this:\nWhere you can:\n  Click the Notify Approver(s) Again button and a reminder email will be sent to approver(s)\n  Click Proceed with new enrollment, so a new enrollment flow will start\n  Click Delete and proceed with new enrollment. In this case, the pending petition will be deleted and a new enrollment flow will be created.\n  b2. If its status is not in Pending Approval, Pending Confirmation or Finalized you will see a page similar to this:\nWhere you can:\n  Click the Resume button and continue the enrollment flow\n  Click Proceed with new enrollment, so a new enrollment flow will start\n  Click Delete and proceed with new enrollment. In this case, the pending petition will be deleted and a new enrollment flow will start.\n  c. If there is more than one pending petition related to the enrollment flow:\nWhere you can:\n  Click the View Button of each petition and review it. (see (b) use case above)\n  Click Proceed with new enrollment and a new enrollment flow will start\n    ","categories":"","description":"Joining a Virtual Organisation (VO) in Check-in\n","excerpt":"Joining a Virtual Organisation (VO) in Check-in\n","ref":"/users/check-in/joining-virtual-organisation/","tags":"","title":"Joining a Virtual Organisation"},{"body":"For monitoring purposes, each service endpoints registered into the Configuration Database, and having the flags production and monitored should include the endpoint URL information in order to be contacted by the corresponding service-specific nagios probe.\nThe information needed for service type are:\n SRM: the value of the attribute GlueServiceEndpoint published in the Configuration Database or BDII (e.g. httpg://se.egi.eu:8444/srm/managerv2) Cloud:  org.openstack.nova: The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3 org.openstack.swift:The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3 eu.egi.cloud.accounting: for the host sending the records to the accounting repositority   Other service types: the value of the attribute GlueServiceEndpoint published in the BDII  It is also possible to register additional endpoints for every services, they will also be monitored if the “Monitored” flag is set.\nFor having more information about managing the Service endpoints in the Configuration Database, please consult the service endpoints documentation.\nRetrieving the information For retrieving the queue URL from the BDII, you can use the command lcg-infosites, to be executed from an UI. Be sure to query a production Top BDII: you can either use the one provided by your Operations Centre or choose one from the Configuration Database\nFor example:\n$ export LCG_GFAL_INFOSYS=egee-bdii.cnaf.infn.it:2170 $ lcg-infosites --vo ops ce | grep nikhef 5680 15 0 0 0 dissel.nikhef.nl:2119/jobmanager-pbs-infra 5680 17 1 1 0 gazon.nikhef.nl:8443/cream-pbs-infra 5680 15 0 0 0 juk.nikhef.nl:8443/cream-pbs-infra 5680 15 0 0 0 klomp.nikhef.nl:8443/cream-pbs-infra 5680 16 0 0 0 stremsel.nikhef.nl:8443/cream-pbs-infra In order to find the GlueServiceEndpoint URL of your SRM service, you can launch a LDAP query to your Site BDII (or directly to the SRM service):\n$ ldapsearch -x -LLL -H ldap://sbdii01.ncg.ingrid.pt:2170 \\  -b \"mds-vo-name=NCG-INGRID-PT,o=grid\" \\  '(\u0026(objectClass=GlueService)(GlueServiceType=SRM))' \\  GlueServiceEndpoint dn: GlueServiceUniqueID=httpg://srm01.ncg.ingrid.pt:8444/srm/managerv2,Mds-Vo-name=NCG-INGRID-PT,o=grid GlueServiceEndpoint: httpg://srm01.ncg.ingrid.pt:8444/srm/managerv2 In a similar way, by just changing the value of GlueServiceType, you can retrieve the endpoint URLs of other services.\nAn alternative way for retrieving the GlueServiceEndpoint URL is using the GLUE2 information browser provided by VAPOR: select your NGI, then your site and hence the Storage service; click on the endpoint details button for finding the URL associated to the SRM interface.\nFilling the information in URLs information are completely missing Editing the services information  Site overview  This is the home page regarding your site. You need to fill in the URL information.\n Click on a service for displaying its page (e.g. the CREAM-CE).\n  Editing a service   Click on the EDIT button on the top right corner\n  Adding a Service URL   fill in the Service URL field with the queue URL\n  Reviewing the site  Now the CREAM-CE service endpoint contains the required queue information.\n Proceed in a similar way for the other services.\n Additional endpoints information In case you need to register an additional endpoint for a service, go on the service summary page and add the proper information. In the example below it is shown the case of a computing element.\n Service summary page  This is the service summary page.\n You need to click on the Add endpoint button for registering additional endpoint URLs.\n  Adding an endpoint   Fill in the proper information and don’t forget to select the “Monitored” flag for making nagios to detect the new endpoint.\n  Reviewing the endpoint description  The summary page of the endpoint just added should look like this one.\n Reviewing the service description  And this is the summary page of the service reporting the information about all its endpoints registered: the first one in the Grid Information section and the additional ones in the Service Endpoints section.\nExamples webdav In order to properly monitor your webdav endpoint:\n you should register a new service endpoint with the webdav service type, separated from the SRM one; fill in the webdav URL containing also the VO ops folder, for example: https://darkstorm.cnaf.infn.it:8443/webdav/ops or https://hepgrid11.ph.liv.ac.uk/dpm/ph.liv.ac.uk/home/ops/  it corresponds to the value of GLUE2 attribute GLUE2EndpointURL (containing the used port and without the VO folder);   verify that the webdav URL (for example: https://darkstorm.cnaf.infn.it:8443/webdav) is properly accessible.  GridFTP In order to properly monitor your gridftp endpoint for ops VO\n register a new service endpoint, associating the storage element hostname to the service type globus-GRIDFTP, with the “production” flag disabled; in the “Extension Properties” section of the service endpoint page, fill in the following fields:  Name: SE_PATH Value: /dpm/ui.savba.sk/home/ops (this is an example, set the proper path)   check if the tests are OK (it might take some hours for detecting the new service endpoint) and then switch the production flag to “yes”  SURL value for SRM The SURL value needed by the SRM monitoring probes is the following structure:\nsrm://\u003chostname\u003e:\u003cport\u003e/srm/managerv2?SFN=\u003cGlueSAPath or GlueVOInfoPath\u003e\nFor example:\nsrm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/pnfs/in2p3.fr/data/dteam/\n As explained in previous sections, you can retrieve the port number from the GlueServiceEndpoint URL information. If your SE provides GlueSAPath information, use that. To retrieve it:  $ ldapsearch -x -LLL -H \u003cldap://sbdii01.ncg.ingrid.pt:2170\u003e \\  -b \"mds-vo-name=NCG-INGRID-PT,o=grid\" \\  '(\u0026(objectClass=GlueSA)(GlueSAAccessControlBaseRule=VO:ops))' \\  GlueSAPath GlueChunkKey dn: GlueSALocalID=opsdisk:replica:online,GlueSEUniqueID=srm01.ncg.ingrid.pt,Mds-Vo-name=NCG-INGRID-PT,o=grid GlueChunkKey: GlueSEUniqueID=srm01.ncg.ingrid.pt GlueSAPath: /gstore/t2others/ops  if your SE doesn’t provide GlueSAPath information, use instead the GlueVOInfoPath one:  $ ldapsearch -x -LLL -H \u003cldap://ntugrid5.phys.ntu.edu.tw:2170\u003e \\  -b \"Mds-Vo-name=TW-NTU-HEP,o=grid\" \\  (\u0026(objectClass=GlueVOInfo)(GlueVOInfoAccessControlBaseRule=VO:ops)) \\  GlueVOInfoLocalID GlueChunkKey dn: GlueVOInfoLocalID=ops:SRR,GlueSALocalID=SRR:SR:replica:*****,GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw,Mds-Vo-name=TW-NTU-HEP,o=grid GlueVOInfoPath: /dpm/phys.ntu.edu.tw/home/ops GlueChunkKey: GlueSALocalID=SRR:SR:replica:***** GlueChunkKey: GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw GlueVOInfoLocalID: ops:SRR dn: GlueVOInfoLocalID=ops:data01,GlueSALocalID=data01:replica:online,GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw,Mds-Vo-name=TW-NTU-HEP,o=grid GlueVOInfoPath: /dpm/phys.ntu.edu.tw/home/ops GlueChunkKey: GlueSALocalID=data01:replica:online GlueChunkKey: GlueSEUniqueID=ntugrid6.phys.ntu.edu.tw GlueVOInfoLocalID: ops:data01  Pay attention to use the storage path for the ops VO On GOCDB, in the “Extension Properties” section of the SRM service endpoint page, fill in the following fields:  Name: SURL Value: the actual SURL value, for example: srm://srm01.ncg.ingrid.pt:8444/srm/managerv2?SFN=/gstore/t2others/ops    ","categories":"","description":"Adding service endpoints information into Configuration Database","excerpt":"Adding service endpoints information into Configuration Database","ref":"/internal/configuration-database/adding-service-endpoint/","tags":"","title":"Adding service endpoints"},{"body":"Overview The EGI Data Transfer service offers API both for Users and Admins, in this section we are focusing on the User API. Two APIs are available to users:\n RESTFul API Python Easy Bindings  In both cases users need a way to be authenticated and authorised and this is explained in the next section.\nAuthentication \u0026 Authorisation Warning Users have to authenticate using a X.509 User certificate. The integration with EGI Check-in in order to authenticate via OIDC tokens\nis under development and will be later made available in production endpoints.  During the authentication phase, credentials are delegated to the FTS service, which will contact the storages to steer the data transfers on behalf of the users.\nThe FTS service supports both plain X.509 proxies than VOMS X.509 proxies extended with VO information for authentication and authorisation.\nLearn about VOMS configuration and proxy creation.\nRESTFul API The User RESTFul APIs can be used to submit transfers jobs (collections of single transfers), monitor and cancel existing transfers. Please check the CERN documentation for the full API details. Here we will provide some examples usage using the Curl client.\nChecking how the server sees the identity of the user curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\  --cacert $X509_USER_PROXY https://fts3-public.cern.ch:8446/whoami { \"dn\": [ \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi\", \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi/CN=proxy\" ], \"vos_id\": [ \"6b10f4e4-8fdc-5555-baa2-7d4850d4f406\" ], \"roles\": [], \"delegation_id\": \"9ab8068853808c6b\", \"user_dn\": \"/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi\", \"level\": { \"transfer\": \"vo\" }, \"is_root\": false, \"base_id\": \"01874efb-4735-4595-bc9c-591aef8240c9\", \"vos\": [ \"dteam\" ], \"voms_cred\": [ \"/dteam/Role=NULL/Capability=NULL\" ], \"method\": \"certificate\" } Getting a list of jobs running Filtered by VO\ncurl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\  --cacert $X509_USER_PROXY https://fts3-public.cern.ch:8446/jobs?vo_name=dteam [ { \"cred_id\": \"1426115d1660de4d\", \"user_dn\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=ftssuite/CN=737188/CN=Robot: fts3 testsuite\", \"job_type\": \"N\", \"retry\": -1, \"job_id\": \"94560e74-7ca3-11e9-97dd-02163e00d613\", \"cancel_job\": false, \"job_state\": \"FINISHED\", \"submit_host\": \"fts604.cern.ch\", \"priority\": 3, \"source_space_token\": \"\", \"max_time_in_queue\": null, \"job_metadata\": { \"test\": \"test_bring_online\", \"label\": \"fts3-tests\" }, \"source_se\": \"mock://somewhere.uk\", \"bring_online\": 120, \"reason\": null, \"space_token\": \"\", \"submit_time\": \"2019-05-22T15:09:22\", \"retry_delay\": 0, \"dest_se\": \"mock://somewhere.uk\", \"internal_job_params\": \"nostreams:1\", \"overwrite_flag\": false, \"copy_pin_lifetime\": -1, \"verify_checksum\": \"n\", \"job_finished\": null, \"vo_name\": \"dteam\" } ] Cancelling a job curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\  --cacert $X509_USER_PROXY \\  -X DELETE \\  https://fts3-pilot.cern.ch:8446/jobs/a40b82b7-1132-459f-a641-f8b49137a713 Getting expiration time of delegated credentials curl --capath /etc/grid-security/certificates -E $X509_USER_PROXY \\  --cacert $X509_USER_PROXY \\  https://fts3-public.cern.ch:8446/delegation/9ab8068853808c6b { \"voms_attrs\": [ \"/dteam/Role=NULL/Capability=NULL\" ], \"termination_time\": \"2020-07-31T22:50:28\" } Python Bindings The Python bindings for FTS can be installed from the EPEL package repository (EL6 and EL7 packages are available) with Python 2.7 being supported.\nyum install python-fts -y For using the bindings, you need to import fts3.rest.client.easy, although for convenience it can be renamed as something else:\nimport fts3.rest.client.easy as fts3 In the following code snippets, an import as above is assumed.\nIn order to be able to do any operation, information about the state of the user credentials and remote endpoint needs to be kept. That’s the purpose of a Context.\ncontext = fts3.Context(endpoint, ucert, ukey, verify=True) The endpoint to use corresponds to the FTS instance REST server and it must have the following format:\nhttps://\\\u003chost\u003e:\\\u003cport\u003e\nfor instance https://fts3-public.cern.ch:8446\nIf you are using a proxy certificate, you can either specify only user_certificate, or point both parameters to the proxy. user_certificate and user_key can be safely omitted, and the program will use the values defined in the environment variables X509_USER_PROXY or X509_USER_CERT + X509_USER_KEY.\nIf verify is False, the server certificate will not be verified.\nHere are some examples about creating a context, submitting a job with a single transfer and getting the job status:\n# pretty print the json outputs \u003e\u003e\u003e import pprint \u003e\u003e\u003e pp = pprint.PrettyPrinter(indent=4) # creating the context \u003e\u003e\u003e context = fts3.Context(\"https://fts3-public.cern.ch:8446\") # printing the whoami info \u003e\u003e\u003e pp.pprint (fts3.whoami(context)) { u'base_id': u'01874efb-4735-4595-bc9c-591aef8240c9', u'delegation_id': u'9ab8068853808c6b', u'dn': [ u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi', u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi/CN=proxy'], u'is_root': False, u'level': { u'transfer': u'vo'}, u'method': u'certificate', u'roles': [], u'user_dn': u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi', u'voms_cred': [u'/dteam/Role=NULL/Capability=NULL'], u'vos': [u'dteam'], u'vos_id': [u'6b10f4e4-8fdc-5555-baa2-7d4850d4f406']} # creating a new transfer and submiting a job \u003e\u003e\u003e transfer = fts3.new_transfer( ... 'gsiftp://source/path', 'gsiftp://destination/path', ... checksum='ADLER32:1234', filesize=1024, ... metadata='Submission example' ... ) \u003e\u003e\u003e job = fts3.new_job([transfer]) \u003e\u003e\u003e job_id = fts3.submit(context, job) \u003e\u003e\u003e print job_id b6191212-d347-11ea-8a47-fa163e45cbc4 # get the job status \u003e\u003e\u003e pp.pprint(fts3.get_job_status(context, job_id)) { u'bring_online': -1, u'cancel_job': False, u'copy_pin_lifetime': -1, u'cred_id': u'9ab8068853808c6b', u'dest_se': u'gsiftp://destination', u'http_status': u'200 Ok', u'internal_job_params': u'nostreams:1', u'job_finished': u'2020-07-31T16:05:55', u'job_id': u'b6191212-d347-11ea-8a47-fa163e45cbc4', u'job_metadata': None, u'job_state': u'FAILED', u'job_type': u'N', u'max_time_in_queue': None, u'overwrite_flag': False, u'priority': 3, u'reason': u'One or more files failed. Please have a look at the details for more information', u'retry': -1, u'retry_delay': 0, u'source_se': u'gsiftp://source', u'source_space_token': u'', u'space_token': u'', u'submit_host': u'fts-public-03.cern.ch', u'submit_time': u'2020-07-31T16:05:54', u'user_dn': u'/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Manzi', u'verify_checksum': u't', u'vo_name': u'dteam'} Full documentation is also available.\n","categories":"","description":"Documentation for EGI Data Transfer API","excerpt":"Documentation for EGI Data Transfer API","ref":"/users/data-transfer/api/","tags":"","title":"API"},{"body":"Most if not all operations can be performed using the Onedata API.\nThe official documentation is here.\nImportant In order to be able to access the Onedata APIs, an access token is required. See below for instructions on how to generate one.  Getting an API access token Tokens have to be generated from the EGI DataHub (Onezone) interface as documented in Generating tokens for using Oneclient or APIs or using a command-line call as documented hereafter.\nBear in mind that a single API token can be used with both Onezone, Oneprovider and other Onedata APIs.\nIt’s possible to retrieve the CLIENT_ID, CLIENT_SECRET and REFRESH_TOKEN using a special OIDC client connected to Check-in. See Check-in documentation for more information.\nCLIENT_ID=\u003cCLIENT_ID\u003e CLIENT_SECRET=\u003cCLIENT_SECRET\u003e REFRESH_TOKEN=\u003cREFRESH_TOKEN\u003e # Retrieving an OIDC token from Check-in curl -X POST -u \"$CLIENT_ID\":\"$CLIENT_SECRET\" \\  -d \"client_id=$CLIENT_ID\u0026$CLIENT_SECRET\u0026grant_type=refresh_token\u0026refresh_token=$REFRESH_TOKEN\u0026scope=openid%20email%20profile\" \\  'https://aai.egi.eu/oidc/token' | python -m json.tool; # Token is in the access_token field of the response The following variables should be set:\n OIDC_TOKEN: OpenID Connect Access token. ONEZONE_HOST: name or IP of the Onezone host (to use Onezone API).  ONEZONE_HOST=https://datahub.egi.eu OIDC_TOKEN=\u003cOIDC_ACCESS_TOKEN\u003e curl -H \"X-Auth-Token: egi:$OIDC_TOKEN\" -X POST \\  -H 'Content-type: application/json' \\  \"$ONEZONE_HOST/api/v3/onezone/user/tokens/named\" -d '{ \"name\": \"REST and CDMI access token\", \"type\": { \"accessToken\": {} }, \"caveats\": [ { \"type\": \"interface\", \"interface\": \"rest\" } ] }' Data access via CDMI and REST API Below are example commands to learn how to access DataHub files and folders via CDMI and REST API using the command-line interface.\nFor more information please check the Onedata CDMI documentation and the Onedata Oneprovider REST API\nCommon configuration Follow instructions above to get an API access token, and configure environment variables:\nexport DATAHUB_TOKEN=\u003cDATAHUB_ACCESS_TOKEN\u003e export ONEPROVIDER_HOST=plg-cyfronet-01.datahub.egi.eu Having jq installed is useful for better formatting of the json output.\nCDMI Configure a header to be passed in some operations.\nexport CDMI_VSN_HEADER='X-CDMI-Specification-Version: 1.1.1' See examples on how to list a folder, and file download/upload using CDMI:\n# List files in a folder curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  -H \"$CDMI_VSN_HEADER\" \\  \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/?children\" | jq . # Download \"helloworld.txt\" from DataHub to \"downloadtest.txt\" on your computer curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/helloworld.txt\" \\  -o downloadtest.txt # Upload \"helloworld.txt\" from your computer to \"uploadtest.txt\" on DataHub curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  -H \"$CDMI_VSN_HEADER\" \\  -X PUT \"https://$ONEPROVIDER_HOST/cdmi/PLAYGROUND/uploadtest.txt\" \\  -T helloworld.txt REST API See examples on how to list a folder, and file download/upload using REST API:\n# Get base folder ID curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  -X POST \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/lookup-file-id/PLAYGROUND\" # Add the folder ID to an environment variable export DIR_ID=\u003cID_FROM_PREVIOUS_COMMAND\u003e # List files inside the folder with DIR_ID curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  -X GET \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$DIR_ID/children\" \\  | jq . # Add the ID of the file that you want to download export FILE_ID=\u003cID_FROM_PREVIOUS_COMMAND\u003e # Download file with FILE_ID from DataHub to \"helloworld.txt\" on your computer curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  -X GET \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$FILE_ID/content\" \\  -o helloworld.txt # Upload \"helloworld.txt\" on your local computer to \"uploadtest.txt\" on DataHub curl -H \"X-Auth-Token: $DATAHUB_TOKEN\" \\  -X POST \\  \"https://$ONEPROVIDER_HOST/api/v3/oneprovider/data/$DIR_ID/children?name=uploadtest.txt\" \\  -H \"Content-Type: application/octet-stream\" -d \"@helloworld.txt\" Data access from Python If your application is written in Python please check the documentation for the OnedataFS Python library\nTesting the API with the REST client A docker container with clients acting as wrappers around the API calls is available: onedata/rest-cli. It's very convenient for discovering and testing the Onezone and Oneprovider API.\ndocker run -it onedata/rest-cli # Exporting env for Onezone API export ONEZONE_HOST=https://datahub.egi.eu export ONEZONE_API_KEY=\u003cACCESS_TOKEN\u003e # Checking current user onezone-rest-cli getCurrentUSer | jq '.' # Listing all accessible spaces onezone-rest-cli listEffectiveUserSpaces | jq '.' docker run -it onedata/rest-cli # Exporting env for Oneprovider API export ONEPROVIDER_HOST=https://plg-cyfronet-01.datahub.egi.eu export ONEPROVIDER_API_KEY=\u003cACCESS_TOKEN\u003e # Listing all spaces supported by the Oneprovider oneprovider-rest-cli getAllSpaces | jq '.' # Listing content of a space oneprovider-rest-cli listFiles path='EGI Foundation/' oneprovider-rest-cli listFiles path='EGI Foundation/CS3_dataset' Printing the raw REST calls of a wrapped command Raw REST calls (used with curl) can be printed using the --dry-run switch.\ndocker run -it onedata/rest-cli export ONEZONE_HOST=https://datahub.egi.eu export ONEZONE_API_KEY=\u003cACCESS_TOKEN\u003e # Listing all accessible spaces onezone-rest-cli listEffectiveUserSpaces | jq '.' # Printing the curl command without running it onezone-rest-cli listEffectiveUserSpaces --dry-run Working with PID / Handle It’s possible to mint a Permanent Identifier (PID) for a space or a subdirectory of a space using a handle service (like Handle.net) that is registered in the Onezone (EGI DataHub).\nOnce done, accessing the PID using its URL will redirect to the Onedata share allowing to retrieve the files.\nPrerequisites: access to a Handle service registered in the Onezone. See the Handle Service API documentation for documentation on registering a new Handle service or ask a Onezone administrator to authorize you to use an existing Handle service already registered in the Onezone.\nThe following variables should be set:\n API_ACCESS_TOKEN: Onedata API access token ONEZONE_HOST: name or IP of the Onezone host (to use Onezone API). ONEPROVIDER_HOST: name or IP of the Oneprovider host (to use Oneprovider API)  # Getting the IDs of the available Handle Services curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/user/handle_services\" HANDLE_SERVICE=\u003cHANDLE_SERVICE_ID\u003e # Getting details about a specific Handle service curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/user/handle_services/$HANDLE_SERVICE\" # Listing all spaces curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/user/effective_spaces/\" | jq '.' # Displaying details of a space curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/spaces/$SPACE_ID\" | jq '.' # Listing content of a space curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEPROVIDER_HOST/api/v3/oneprovider/files/EGI%20Foundation/\" | jq '.' # Creating a share of a subdirectory of a space DIR_ID_TO_SHARE=\u003cDIR_ID\u003e curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  -X POST -H 'Content-Type: application/json' \\  -d '{\"name\": \"input\"}' \"$ONEPROVIDER_HOST/api/v3/oneprovider/shares-id/$DIR_ID_TO_SHARE\" | jq '.' # Displaying the share SHARE_ID=\u003cSHARED_ID\u003e curl -sS --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/shares/$SHARE_ID\" | jq '.' # Registering a handle # Proper Dublin Core metadata is required # It can be created using https://nsteffel.github.io/dublin_core_generator/generator_nq.html cat metadata.xml # Escape double quotes and drop line return METADATA=$(cat metadata.xml | sed 's/\"/\\\\\"/g' | tr '\\n' ' ') # On handle creation the created handles is provided in the Location header curl -D - --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  -H \"Content-type: application/json\" -X POST \\  -d '{\"handleServiceId\": \"'\"$HANDLE_SERVICE_ID\"'\", \"resourceType\": \"Share\", \"resourceId\": \"'\"$SHARE_ID\"'\", \"metadata\": \"'\"$METADATA\"'\"}' \\  \"$ONEZONE_HOST/api/v3/onezone/user/handles\" # Listing handles curl --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/user/handles\" # Displaying a handle HANDLE_ID=\u003cHANDLE_ID\u003e curl --tlsv1.2 -H \"X-Auth-Token: $API_ACCESS_TOKEN\" \\  \"$ONEZONE_HOST/api/v3/onezone/user/handles/$HANDLE_ID\" ","categories":"","description":"Documentation of [EGI DataHub](https://datahub.egi.eu/) APIs","excerpt":"Documentation of [EGI DataHub](https://datahub.egi.eu/) APIs","ref":"/users/datahub/api/","tags":"","title":"API"},{"body":"","categories":"","description":"The [EGI Check-in service](https://www.egi.eu/services/check-in/)\n","excerpt":"The [EGI Check-in service](https://www.egi.eu/services/check-in/)\n","ref":"/users/check-in/","tags":"","title":"Check-in"},{"body":"This documentation covers how to join the EGI Cloud federation as a provider. If you are interested in joining please first contact EGI operations team at operations@egi.eu, expressing interest and providing few details about:\n  the projects you may be involved in as cloud provider.\n  the user communities you want to support (AKA Virtual Organisations or VOs). You can also support the ‘long-tail of science’ through the access.egi.eu VO.\n  the technologies (Cloud Management Framework) you want to provide.\n  details on the current status of your deployment (to be installed or already installed, already used or not, how it is used, who uses the services…)\n  Integration of cloud stacks into EGI FedCloud follows a well-defined path, with certain steps which need to be taken, depending on the cloud stack in question. By integration here, we refer to the proper interoperation with EGI infrastructure services such as accounting, monitoring, authentication and authorisation, etc. These configurations make your site discoverable and usable by the communities you wish to support, and allow EGI to support you in operational and technical matters.\nIntegration of these services implies specific configuration actions which you need to take on your site. These aim to be unintrusive and are mostly to facilitate access to your site by the communities you wish to support, without interfering with normal operations. This can be summarised essentially as:\n Network configuration Permissions configuration AAI configuration Accounting configuration Information system integration VM and appliance repository configuration  If at any time you experience technical difficulties or need support, please open a ticket or discuss the matter with us on the forum\nYou can follow find integration guides for your cloud management in this documentation.\n","categories":"","description":"IaaS Service providers documentation","excerpt":"IaaS Service providers documentation","ref":"/providers/cloud-compute/","tags":"","title":"Cloud Compute"},{"body":"Command-line tools The various public EGI services can be managed and used/accessed with a wide variety of command-line interface (CLI) tools. The documentation of each service contains a summary of the CLIs that can be used wih that service, together with recommendations on which one to use in what context.\nThe FedCloud client The FedCloud client is a high-level Python package for a command-line client designed for interaction with the OpenStack services in the EGI infrastructure.\nTip The FedCloud client is the recommended command-line interface to use with most EGI services.  FedCloud client has the following modules (features):\n Check-in allows checking validity of access tokens and listing Virtual Organisations (VOs) of a token Endpoint can search endpoints in the Configuration Database and extract site-specific information from unscoped/scoped tokens Sites allows management of site configurations OpenStack can perform commands on OpenStack services deployed to sites EC3 allows deploying elastic cloud compute clusters  Installation The FedCloud client can be installed with the pip3 Python package manager (without root or aministrator privileges).\nLinux / Mac   Windows    To install the FedCloud client:\n$ pip3 install fedcloudclient This installs the latest version of the FedCloud client, together with its required packages (like openstackclient). It will also create executables fedcloud and openstack, adding them to the bin folder corresponding to your current Python execution environment ($VIRTUAL_ENV/bin for executing pip3 in a Python virtual environment, ~/.local/bin for executing pip3 as user (with --user option), and /usr/local/bin when executing pip3 as root).\n As there are non-pure Python packages needed for installation, the Microsoft C++ Build Tools is a prerequisite, make sure it’s installed with the following options selected:\n C++ CMake tools for Windows C++ ATL for latest v142 build tools (x86 \u0026 x64) Testing tools core features - Build Tools Windows 10 SDK (\u003clatest\u003e)  In case you prefer to use non-Microsoft alternatives for building non-pure packages, please see here.\nTo install the FedCloud client:\n\u003e pip3 install fedcloudclient This installs the latest version of the FedCloud client, together with its required packages (like openstackclient). It will also create executables fedcloud and openstack, adding them to the bin folder corresponding to your current Python execution environment.\n  Check if the installation is correct by executing the client:\n$ fedcloud --version Installing EGI Core Trust Anchor certificates Some sites in the EGI infrastructure use certificates issued by Certificate Authorities (CAs) that are not included in the default OS distribution. If you receive error message “SSL exception connecting to…”, install the EGI Core Trust Anchor Certificates by running the following commands:\n$ wget https://raw.githubusercontent.com/tdviet/python-requests-bundle-certs/main/scripts/install_certs.sh $ bash install_certs.sh  Note The above script does not work on all Linux distributions. Change python to python3 in the script if needed, see the README for more details, or follow the official instructions for installing EGI Core Trust Anchor certificates in production environments.  Using via Docker container The FedCloud client can also be used without installation, by running it in a Docker container. In this case, the EGI Core Trust Anchor certificates are preinstalled.\nLinux   Mac   Windows    To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n$ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash   To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n$ docker pull tdviet/fedcloudclient $ docker run -it tdviet/fedcloudclient bash   To run the FedCloud client in a container, make sure Docker is installed, then run the following commands:\n\u003e docker pull tdviet/fedcloudclient \u003e docker run -it tdviet/fedcloudclient bash    Once you have a shell running in the container with the FedCloud client, usage is the same as from the command-line.\nUsing from EGI Notebooks EGI Notebooks are integrated with access tokens so it simplifies using the FedCloud client. First make sure that you follow the installation steps above. Then, below are the commands that you need to run inside a terminal in JupyterLab:\nexport OIDC_ACCESS_TOKEN=`cat /var/run/secrets/egi.eu/access_token` fedcloud token check Please follow instructions below to learn how to use the fedcloud command.\nUsing from the command-line The FedCloud client has these subcommands:\n fedcloud token for checking access tokens (see token subcommands) fedcloud endpoint for querying the Configuration Database (see endpoint subcommands) fedcloud site for manipulating site configurations (see site subcommands) fedcloud openstack or fedcloud openstack-int for performing OpenStack commands on sites (see openstack subcommands) fedcloud ec3 for provisioning elastic cloud compute clusters (see cluster subcommands)  Note See also the complete documentation or read and contribute to the source code.  Performing any OpenStack command on any site requires only three options: the site, the VO and the command. For example, to list virtual machine (VM) images available to members of VO fedcloud.egi.eu on the site CYFRONET-CLOUD, run the following command:\n$ fedcloud openstack image list --vo fedcloud.egi.eu --site CYFRONET-CLOUD Authentication Many of the FedCloud client commands need access tokens for authentication. Users can choose whether to provide access tokens directly (via option --oidc-access-token), or generate them on the fly with oidc-agent (via option --oidc-agent-account) or from refresh tokens (via option --oidc-refresh-token, which must be provided together with option --oidc-client-id and option --oidc-client-secret).\nTip Users of EGI Check-in can get a Check-in client ID, client secret, and refresh token, as well as all the information needed to obtain access tokens for their FedCloud client, by visiting Check-in FedCloud client.  Tip To provide access tokens automatically via oidc-agent, follow these instructions to register a client, then pass the client name (account name used during client registration) to the FedCloud client via option --oidc-agent-account.  Important Refresh tokens have long lifetime (one year in EGI Check-in), so they must be properly protected. Exposing refresh tokens via environment variables or command-line options is considered insecure and will be disabled in the near future in favor of using oidc-agent.  If multiple methods of getting access tokens are given at the same time, the FedCloud client will try to get an access token from the oidc-agent first, then obtain one using the refresh token.\nThe default authentication protocol is openid. Users can change the default protocol via the option --openstack-auth-protocol. However, sites may have the protocol fixed in the site configuration (e.g. oidc for the site INFN-CLOUD-BARI).\nThe default OIDC identity provider is EGI Check-in (https://aai.egi.eu/oidc). Users can set another OIDC identity provider via option --oidc-url.\nNote Remember to also set the identity provider’s name accordingly for OpenStack commands, by using the option --openstack-auth-provider.  Environment variables Most of the FedCloud client options can be set via environment variables:\nTip To save a lot of time, set the frequently used options like site, VO, etc. using environment variables.  Tip When you want commands to work on all sites in the EGI infrastructure, use ALL_SITES for the --site parameter (pass it directly or via an anvironment variable).     Environment variable Command-line option Default value     OIDC_AGENT_ACCOUNT --oidc-agent-account    OIDC_ACCESS_TOKEN --oidc-access-token    OIDC_REFRESH_TOKEN --oidc-refresh-token    OIDC_CLIENT_ID --oidc-client-id    OIDC_CLIENT_SECRET --oidc-client-secret    OIDC_URL --oidc-url https://aai.egi.eu/oidc   OPENSTACK_AUTH_PROTOCOL --openstack-auth-protocol openid   OPENSTACK_AUTH_PROVIDER --openstack-auth-provider egi.eu   OPENSTACK_AUTH_TYPE --openstack-auth-type v3oidcaccesstoken   EGI_SITE --site    EGI_VO --vo     Getting help The FedCloud client can display help for the commands and subcommands it supports. Try running the following command to see the commands supported by the FedCloud client:\n$ fedcloud --help Usage: fedcloud [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. --help Show this message and exit. Commands: ec3 EC3 related comands endpoint endpoint command group for interaction with GOCDB and... openstack Executing OpenStack commands on site and VO openstack-int Interactive OpenStack client on site and VO site Site command group for manipulation with site... token Token command group for manipulation with tokens Similarly, you can see help for e.g. the openstack subcommand by running the command below:\n$ fedcloud openstack --help Usage: fedcloud openstack [OPTIONS] OPENSTACK_COMMAND... Executing OpenStack commands on site and VO Options: --oidc-client-id TEXT OIDC client id --oidc-client-secret TEXT OIDC client secret --oidc-refresh-token TEXT OIDC refresh token --oidc-access-token TEXT OIDC access token --oidc-url TEXT OIDC URL [default: https://aai.egi.eu/oidc] --oidc-agent-account TEXT short account name in oidc-agent --openstack-auth-protocol TEXT Check-in protocol [default: openid] --openstack-auth-type TEXT Check-in authentication type [default: v3oidcaccesstoken] --openstack-auth-provider TEXT Check-in identity provider [default: egi.eu] --site TEXT Name of the site or ALL_SITES [required] --vo TEXT Name of the VO [required] -i, --ignore-missing-vo Ignore sites that do not support the VO -j, --json-output Print output as a big JSON object --help Show this message and exit.  Note Most commands support multiple levels of subcommands, you can get help for all of them using the same principle as above.  Using from Python The FedCloud client can be used as a library for developing other services and tools for EGI services. Most of the functionalities can be called directly from Python code without side effects.\nAn usage example is available on GitHub. Just copy/download the code, add your access token and execute python demo.py to see how it works.\nUsing in scripts The FedCloud client can also be used in scripts for simple automation, either for setting environment variables for other tools, or to process outputs from OpenStack commands.\nSetting environment variables for external tools Some FedCloud commands generate output that contains shell commands to set environment variables with the returned result, as exemplified below.\nLinux / Mac   Windows   PowerShell    Run a command to get details of a project:\n$ export EGI_SITE=IISAS-FedCloud $ export EGI_VO=eosc-synergy.eu $ fedcloud site show-project-id export OS_AUTH_URL=\"https://cloud.ui.savba.sk:5000/v3/\"; export OS_PROJECT_ID=\"51f736d36ce34b9ebdf196cfcabd24ee\"; Run the same command but set environment variables with the returned values:\n$ eval $(fedcloud site show-project-id) The environment variables will have their values set to what the command returned:\n$ echo $OS_AUTH_URL https://cloud.ui.savba.sk:5000/v3/ $ echo $OS_PROJECT_ID 51f736d36ce34b9ebdf196cfcabd24ee   Run a command to get details of a project:\n\u003e set EGI_SITE=IISAS-FedCloud \u003e set EGI_VO=eosc-synergy.eu \u003e fedcloud site show-project-id set OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ set OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee If you copy the returned output and execute it as commands in a command prompt:\n\u003e set OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ \u003e set OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee The environment variables will have their values set to what the command returned:\n\u003e set OS_AUTH_URL OS_AUTH_URL=https://cloud.ui.savba.sk:5000/v3/ \u003e set OS_PROJECT_ID OS_PROJECT_ID=51f736d36ce34b9ebdf196cfcabd24ee   Run a command to get details of a project:\n\u003e $Env:EGI_SITE = \"IISAS-FedCloud\" \u003e $Env:EGI_VO = \"eosc-synergy.eu\" \u003e fedcloud site show-project-id $Env:OS_AUTH_URL=\"https://cloud.ui.savba.sk:5000/v3/\"; $Env:OS_PROJECT_ID=\"51f736d36ce34b9ebdf196cfcabd24ee\"; Run the same command but set environment variables with the returned values:\n\u003e fedcloud site show-project-id | Out-String | Invoke-Expression The environment variables will have their values set to what the command returned:\n\u003e $Env:OS_AUTH_URL https://cloud.ui.savba.sk:5000/v3/ \u003e $Env:OS_PROJECT_ID 51f736d36ce34b9ebdf196cfcabd24ee    Processing output from OpenStack commands The fedcloud openstack subcommand’s output can be converted to JavaScript Object Notation (JSON) format by using the --json-output option. This is useful for further machine processing of the command output.\nTip JSON output can be processed with a tool like jq, which can slice, filter, map, and transform structured data. It acts as a filter: it takes an input and produces an output. Check out the tutorial for using it to extract data from JSON sources.  $ export EGI_SITE=IISAS-FedCloud $ export EGI_VO=eosc-synergy.eu $ fedcloud openstack flavor list --json-output [ { \"Site\": \"IISAS-FedCloud\", \"VO\": \"eosc-synergy.eu\", \"command\": \"flavor list\", \"Exception\": null, \"Error code\": 0, \"Result\": [ { \"ID\": \"0\", \"Name\": \"m1.nano\", \"RAM\": 64, \"Disk\": 1, \"Ephemeral\": 0, \"VCPUs\": 1, \"Is Public\": true }, { \"ID\": \"2e562a51-8861-40d5-8fc9-2638bab4662c\", \"Name\": \"m1.xlarge\", \"RAM\": 16384, \"Disk\": 40, \"Ephemeral\": 0, \"VCPUs\": 8, \"Is Public\": true }, ... ] } ] # The following jq command selects flavors with VCPUs=2 and prints their names $ fedcloud openstack flavor list--json-output | \\  jq -r '.[].Result[] | select(.VCPUs == 2) | .Name' m1.medium  Note Note that --json-output option can be used only with those OpenStack commands that have outputs. Using this parameter with commands with no output (e.g. setting properties) will generate an unsupported parameter error.  ","categories":"","description":"EGI command line interface\n","excerpt":"EGI command line interface\n","ref":"/users/getting-started/cli/","tags":"","title":"Command Line Interface"},{"body":"What is it? The EGI Configuration Database (GOCDB) is a central registry that records topology information about all sites participating in the EGI infrastructure.\nThe configuration database also provides different rules and grouping mechanisms for filtering and managing the information associated to resources. This can include entities such as operations and resource centres, service endpoints and their downtimes, contact information and roles of staff responsible for operations at different levels.\nThe configuration database is used by all the actors (end-users, site managers, NGI managers, support teams, VO managers), by other tools, and by third party middleware to discover information about the infrastructure topology.\nNote Documentation for the configuration database (work in progress) is available in the EGI Wiki.  ","categories":"","description":"Topology and configuration registry for sites in EGI infrastructure\n","excerpt":"Topology and configuration registry for sites in EGI infrastructure\n","ref":"/internal/configuration-database/","tags":"","title":"Configuration Database"},{"body":"Thank you for taking the time to contribute to this project. The maintainers greatly appreciate the interest of contributors and rely on continued engagement with the community to ensure this project remains useful. We would like to take steps to put contributors in the best possible position to have their contributions accepted. Please take a few moments to read this short guide on how to contribute.\nNote Before you start contributing to the EGI documentation, please familiarize yourself with the concepts used by documentation authors. When authoring pages, please observe and adhere to the Style Guide.  Tip We also welcome contributions regarding how to contribute easier and more efficiently.  Feedback and questions If you wish to discuss anything related to the project, please open a GitHub issue or start a topic on the EGI Community Forum.\nNote The maintainers will move issues from GitHub to the community forum when longer, more open-ended discussion would be beneficial, including a wider community scope.  Contribution process All contributions have to go through a review process, and contributions can be made in two ways:\n For simple contributions navigate to the documentation page you want to improve, and click the  Edit this page link in the top-right corner (see also the GitHub documentation). You will be guided through the required steps. Be sure to save your changes quickly as the repository may be updated by someone else in the meantime. For more complex contributions or when you want to preview and test changes locally you should fork the repository as documented on the Using Git and GitHub page.  Contributing via PRs Note If you need to discuss your changes beforehand (e.g. adding a new section or if you have any doubts), please consult the maintainers by creating a GitHub issue.\nYou can also create an issue by navigating to a documentation page, and clicking the  Create documentation issue link in the top-right corner.\n Before proposing a contribution via the so-called Pull Request (PR) workflow, there should be an open issue describing the need for your contribution (refer to this issue number when you submit the PR). We have a three-step process for contributions:\n Fork the project if you have not done so yet, and commit changes to a feature branch. Building the documentation locally is described in the README. Create a GitHub PR from your feature branch, following the instructions in the PR template. Perform a code review with the maintainers on the PR.  Tip Rebase your fork’s main branch on the EGI documentation repository’s main branch, before you create new feature branches from it.  PR requirements  If the PR is not finalised mark it as draft using the GitHub web interface, so it is clear it should not be reviewed yet. Explain your contribution in plain language. To assist the maintainers in understanding and appreciating your PR, please use the template to explain why you are making this contribution, rather than just what the contribution entails.  Code review process Code review takes place in GitHub pull requests (PRs). See this article if you’re not familiar with GitHub PRs.\nOnce you open a PR, automated checks will verify the style and syntax of your changes and maintainers will review your code using the built-in code review process in GitHub PRs.\nThe process at this point is as follows:\n Automated syntax and formatting checks are run using GitHub Actions, successful checks are a hard requirement, but maintainers will help you address reported issues. Maintainers will review your changes and merge it if no changes are necessary. Your change will be merged into the repository’s main branch. If a maintainer has feedback or questions on your changes, they will set request changes in the review and provide an explanation.  Release cycle The documentation is using a rolling release model, all changes merged to the main branch are directly deployed to the live production environment.\nThe main branch is always available. Tagged versions may be created as needed following semantic versioning when applicable.\nCommunity EGI benefits from a strong community of developers and system administrators, and vice-versa. If you have any questions or if you would like to get involved in the wider EGI community you can check out:\n EGI Community Forum EGI site  ","categories":"","description":"Contributing to EGI documentation","excerpt":"Contributing to EGI documentation","ref":"/about/contributing/","tags":"","title":"Contributing"},{"body":"Here you can find documentation about the EGI DataHub for service providers\n","categories":"","description":"Documentation for EGI DataHub Service Providers","excerpt":"Documentation for EGI DataHub Service Providers","ref":"/providers/datahub/","tags":"","title":"DataHub"},{"body":"Definition A downtime is a period of time for which a service is declared to be inoperable. Downtimes may be scheduled (e.g. for software/hardware upgrades), or unscheduled (e.g. power outages). The Configuration Database stores the following information about downtimes (non exhaustive list):\n The downtime classification (Scheduled or unscheduled) The severity of the downtime The date at which the downtime was added The start and end of the downtime period A description of the downtime The entities affected by the downtime  Manipulating downtimes Viewing downtimes There are different pages in the Configuration Database where downtimes are listed:\n Active \u0026 Imminent, linked from the main menu, that allows users to see currently active downtimes and downtimes planned in the coming weeks. Downtime Calendar, linked from the main menu, that allows users to view and filter all downtimes. Site details, where all the downtimes associated to the site are listed Service endpoint details, where all the downtimes associated to the service endpoint are listed. Service group details, where all the downtimes associated to the service group are listed.  Each downtime has its own page providing details, accessible by clicking on the Downtime Id link or similar in downtime listing pages.\nSubscribing to downtimes The EGI Operations Portal provides a publicly-accessible page allowing to view and filter downtimes: Operations Portal.\nAuthenticated users can subscribe to downtimes affecting sites selected using a filter. The downtimes notifications can be sent by email, RSS and iCal, allowing to easily integrate with your calendar.\nAdding downtimes Provided you have proper permissions (check the permissions matrix section), you can add a downtime by clicking on the Add Downtime link in the sidebar.\nThis is done in 2 steps:\n Enter downtime information Specify the full list of impacted services in case there is more than one or select an site to select all the sites associated services.  Please note  All dates have to be entered in UTC or using the Site Timezone. A downtime can be retrospectively added if its start date is less than 48h in the past (giving a 2 day window to add). downtime classification (scheduled/unscheduled) is determined automatically (see Scheduled or unscheduled section)  Editing downtime information  To edit a downtime, simply click the edit link on top of the downtime’s details page. A downtime can be retrospectively updated if its start date is less than 48h in the past (giving a 2 day window to modify). Note there are limitations to downtime editing, especially if it has already started, or is due to start in the next 24hrs or is finished. See downtime shortening and extension section for more details.  Removing downtimes To delete a downtime, simply click the delete link on top of the downtime’s details page. For integrity reasons, it is only possible to remove downtimes that have not started.\nGood practices and further understanding Scheduled or unscheduled Depending on the planning of the intervention, downtimes can be:\n Scheduled: planned and agreed in advance Unscheduled: planned or unplanned, usually triggered by an unexpected failure or at a short term notice  EGI defines precise rules about what should be declared as scheduled or unscheduled, based on how long in advance the downtime is declared. These rules are described in MAN02 Service intervention management and are enforced as follows:\n All downtimes declared less than 24h in advance will be automatically classified as UNSCHEDULED All other downtimes will be classified as SCHEDULED  Notes  A downtime can be retrospectively declared and/or updated if its start date is less than 48h in the past (giving a 2 day window to add/modify). Although 24h in advance is enough for the downtime to be classified as SCHEDULED, it is good practice to declare it at least 5 working days before it starts.  WARNING or OUTAGE? When declaring a downtime, you will be presented the choice of a “severity”, which can be either WARNING or OUTAGE. Please consider the following definitions:\n  WARNING means the resource is considered available, but the quality of service might be degraded. Such downtimes generate notifications, but are not taken into account by monitoring and availability calculation tools. In case of a service failure during the WARNING period an OUTAGE downtime has to be declared, cancelling the rest of the WARNING downtime.\n  OUTAGE means the resource is considered as unavailable. Such downtimes will be considered as in maintenance by monitoring and availability calculation tools.\n  Downtime shortening and extension Limitation rules to downtime extensions are enforced as follows:\n Scheduled downtimes due to start in 24 hours cannot be edited in any way, but can be deleted. Other downtimes that have not yet started can be edit and deleted.  They can be shortened or moved, i.e. They can be edited such that:  Both start and end time are still in the future The duration remains the same or is decreased     Ongoing downtimes can not be deleted. A downtime cannot be edited once it has finished, nor can a new downtime be added more than 48 hours into the past.  If for any reason a downtime already declared needs to be extended, the procedure is to add another adjacent downtime, before or after.\n","categories":"","description":"Managing and consulting downtimes","excerpt":"Managing and consulting downtimes","ref":"/internal/configuration-database/downtimes/","tags":"","title":"Downtimes"},{"body":"This section documents how to run some applications and use the existing tools with EC3.\nHow to run scientific applications in EC3 NAMD cluster To deploy NAMD clusters, please select one of the available LRMS (Local Resource Management System) and choose NAMD from the list of applications.\nHow to use generic tools/practices in EC3 ECAS cluster Check the dedicated ECAS documentation.\nKubernetes Check the Cloud Container Compute documentation.\nMesos + Marathon + Chronos To deploy a virtual cluster with Marathon, Mesos, and Chronos as an orchestration, please select Mesos + Marathon + Chronos from the list of available LRMS.\nOSCAR cluster To deploy Serverless computing for data-processing applications in EGI, please select OSCAR from the list of LRMS (Local Resource Management System). Use cases of applications that use the OSCAR framework for event-driven high-throughput processing of files:\n Plants Classification, an application that performs plant classification using Lasagne/Theano. ImageMagick, a tool to manipulate images. Radiomics, a use case about the handling of Rheumatic Heart Disease (RHD) through image computing and Artificial Intelligence (AI).  SLURM cluster To deploy SLURM clusters, please select SLURM from the list of available LRMS. See also the dedicated guide on HTC clusters\n","categories":"","description":"How to run scientific applications and tools with EC3\n","excerpt":"How to run scientific applications and tools with EC3\n","ref":"/users/cloud-compute/ec3/apps/","tags":"","title":"EC3 applications and tools"},{"body":"In this section we describe how users can easily discover the EGI Applications on Demand (AoD) service published in the EOSC portal Marketplace and submit a service-order request to deploy an elastic virtual cluster on the HNSciCloud pilot services. The virtual cluster will be deployed and configured with the requested software packages using the Elastic Cloud Compute Cluster (EC3) portal.\nSubmit a service-order request through the EOSC Portal Marketplace The EOSC Portal Marketplace is a single entry point where users can navigate the EOSC service catalogue, discover a service of interest, get information about it, and place an order in a few clicks by specifying the service requested along with the quantity, quality and duration.\nAll the available services of the EOSC Marketplace are grouped in 8 main categories and no authentication is required to browse and discover the list of services published in the marketplace.\nExoscale vouchers can be used when selecting he Elastic Cloud Compute Cluster (EC3) platform from the list of available services by submitting a service-order request.\nPlacing orders in the EOSC Marketplace requires the user to authenticate. There are two login options: use your personal academic credentials or use EGI Check-in service. The first time you login into the marketplace requires some extra information to be provided to configure the marketplace account.\nTwo different service options are available for EC3:\nTo deploy the virtual cluster on the HNSciCloud pilot services, select the voucher option and click on Next and provide additional information for profiling the order (e.g. the motivation for requesting the service).\nUsers with an existing Exoscale voucher to redeem can provide the voucherID during the submission request. Otherwise, a valid voucherID will be provided once the order if processed and accepted.\nOnce the service order is submitted, the marketplace will send a notification via email. Users can at any time the status of their orders through the user dashboard. This dashboard shows the history of service order(s) submitted.\nThe service order request will be processed within 3 working days. The user will be notified of the outcome of the evaluation via email.\nVoucher redemption In case the service order is accepted:\n The user will be notified by the Marketplace with the instructions to redeem the voucher (in case it was not provided), generate a client and secret keys and access the Elastic Cloud Compute Cluster (EC3) portal. To redeem an Exoscale voucher, open the provided voucher link included in the email sent by the Marketplace within a web browser. A typical link looks like:https://portal.exoscale.com/register?coupon=XXXXXXX Enter the email address and password you wish to use. Accept the terms and hit sign up.   A validation email is sent. Check out your mailbox and click on the verification link.   Choose “for team projects” and fill your details. Choose your “Company or team name” and submit the form.  Access to the Exoscale dashboard Access the Exoscale dashboard and check the tenant settings (click in the User profile on the left)\nCheck the voucher credit Access the Exoscale dashboard. The voucher credit is shown in the top-right of the dashboard. In case voucher credit is going to expire, a notification email will be sent by the dashboard.\nDeploy a virtual cluster on the Exoscale commercial resources Through a “job wizard” interface (see Figures below) the user can login to the Elastic Cloud Compute Cluster (EC3) portal and configure the virtual cluster with the related tools and applications to be deployed on top of Infrastructure as a Service (IaaS). The cluster is composed by a front node, where a batch job scheduler is running, and a number of compute nodes. These compute nodes will be dynamically deployed and provisioned to fit increasing load, and un-deployed when they are in idle status. The installation and configuration of the cluster is performed by means of the execution of Ansible receipts.\nAcknowledgement and report feedback The user MUST acknowledge the EOSC-hub and the HNSciCloud projects in the scientific publications/presentations benefiting from the service. The following acknowledgement statement can be used for this purpose:\n\"This work used the EGI Applications on Demand service, which is co-funded by the EOSC-hub project (grant number 777536). The HNSciCloud project (grant number 687614) is also sponsoring the service, allowing users to access the HNSciCloud services pilot for limited scale usage using the voucher scheme provided by Exoscale.\"\nReport feedback To report feedback on the Exoscale vouchers, please fill in the following web form: https://forms.gle/cVT7JRd4TFxZiYmQ8\n","categories":"","description":"Exoscale vouchers on Applications on Demand\n","excerpt":"Exoscale vouchers on Applications on Demand\n","ref":"/users/applications-on-demand/exoscale/","tags":"","title":"Exoscale vouchers"},{"body":"Information discovery provides a real-time view about the actual images and flavors available at the OpenStack for the federation users. It runs as a single python application cloud-info-provider that pushes information through the Argo Messaging Service (AMS)\nBDII is deprecated Cloud providers no longer need to provide BDII as the Argo Messaging Service is used instead for transferring information  You can either run the service by yourself or rely on central operations of the cloud-info-provider. In both cases you must register your host DN in the GOCDB entry for the org.openstack.nova.\nCatch-all operations EGI can manage the operation of the cloud-info-provider for the site so you don’t need to do it. In order for your site to be included in the centrally operated cloud-info-provider, you need to create a Pull Request at the EGI-Federation/fedcloud-catchall-operations repository adding your site configuration in the sites directory with a file like this:\nendpoint:\u003cyour endpoint as declared in GOCDB\u003egocdb:\u003cyour site name as declared in GOCDB\u003evos:# a list of VOs you support in your deployment as follows- auth:project_id:\u003clocal OpenStack project identifier\u003ename:\u003cname of the vo\u003e- auth:project_id:\u003clocal OpenStack project identifier for second VO\u003ename:\u003cname of another vo\u003eOnce PR is merged, the service will be reconfigured and your site should start publishing information.\nLocal operations You can operate by yourself the cloud-info-provider. The software can be obtained as RPMs, debs and python packages from the GitHub releases page.\nThe cloud-info-provider needs a configuration file where your site is described, see the sample OpenStack configuration for the required information. The authentication parameters for your local OpenStack and the AMS are passed as command-line options:\ncloud-info-provider-service --yaml-file \u003cyour site description.yaml\u003e \\  --middleware openstack \\  --os-auth-url \u003cyour keystone URL\u003e \\  [ any other options for authentication ] --format glue21 \\  --publisher ams \\  --ams-cert \u003cyour host certificate\u003e \\  --ams-key \u003cyour host secret key\u003e \\  --ams-topic \u003cyour endpoint topic\u003e For authentication, you should be able to use any authentication method supported by keystoneauth, for username and password use: --os-password and --os-username. Check the complete list of options with cloud-info-provider-service --help.\nThe AMS topic has the format: SITE_\u003cSITE_NAME\u003e_ENDPOINT_\u003cGOCDB_ID\u003e, where \u003cSITE_NAME\u003e is the name of the site as declared in GOCDB and \u003cGOCDB_ID\u003e is the ID of the endpoint in GOCDB. For example, this endpoint would have a topic like: SITE_IFCA-LCG2_ENDPOINT_7513G0.\nYou should periodically run the cloud-info-provider (e.g. with a cron every 5 minutes) to push the information for consumption by clients.\nUsing the EGI FedCloud Appliance The appliance provides a ready-to-use cloud-info-provider configuration if you want to operate it by yourself. Once you have downloaded the appliance check the following files:\n  /etc/cloud-info-provider/openstack.rc: the configuration of the account used to log into your OpenStack and the location of the host certificate that will be used to authenticate to the AMS.\n  /etc/cloud-info-provider/openstack.yaml: the cloud-info-provider configuration. You need to enter the details about the VOs/projects that the site is supporting.\n  The appliance has a cron job that will connect to the configured OpenStack API and send messages every 5 minutes.\n","categories":"","description":"cloud info provider configuration\n","excerpt":"cloud info provider configuration\n","ref":"/providers/cloud-compute/openstack/cloud-info/","tags":"","title":"Information System"},{"body":"This section documents how to integrate a new application in EC3.\nAbout The process to integrate a new application in EC3 is described by the following process:\n Describe the application to be integrated with Ansible, the open-source automation engine that automates software provisioning, configuration management, and application deployment. Use the Ansible receipt to create a new RADL template  Documentations  Ansible documentation EC3 documentation RADL  Contacts  Miguel Caballer at: micafer1 \u003cat\u003e upv \u003cdot\u003e es Amanda Calatrava at: amcaar \u003cat\u003e i3m \u003cdot\u003e upv \u003cdot\u003e es  ","categories":"","description":"How to integrate a new application in EC3\n","excerpt":"How to integrate a new application in EC3\n","ref":"/users/cloud-compute/ec3/developers/","tags":"","title":"Instructions for developers"},{"body":"This section contains documentation about the internal EGI services. These services are being operated centrally on behalf of EGI, and are supporting the coordination of the EGI Federation.\nNote See the User Guides section for documentation about public EGI services, and the Service Providers section for details on how to integrate providers into the EGI Federation.  Guidelines for software development We provide Guidelines for software development to be considered when developing a product for the EGI Federation.\nRequest for information You can ask for more information about the internal EGI services on our site.\n","categories":"","description":"Documentation for internal EGI services","excerpt":"Documentation for internal EGI services","ref":"/internal/","tags":"","title":"Internal Services"},{"body":"   Property Value     Title Tool Intervention Management   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement How to manage central operational tool unscheduled downtimes   Owner SDIS team    The purpose of this document is to describe the intervention in case of unscheduled failure of central operational tool.\nScope This manual only applies to unscheduled downtimes of central operational tools. The list of central operational tool is available here.\nNote: Scheduled downtimes are management according to existing procedures (MAN02).\nAnnouncements All announcements should be sent with the Operations Portal Broadcast tool.\nWhen using Operations Portal Broadcast tool the following groups should be included:\n LCG Rollout Mailing List Operators Mailing lists OSG Mailing list Tool Admins Mailing List WLCG Tier 1 contacts NGI managers VO managers VO users Site administrators Operation tools  Notice: Individual notification templates together with targets are predefined in Operations Portal Broadcast tool. Administrators are advised to use such predefined templates.\nProcedure In the following sections several relevant scenarios are covered.\nCase 1: short “undetected” downtime Description: Service fails and recovers before administrator manages to react (e.g. short power or network outage).\nAction: Administrator announces the failure by using the following template:\nSubject: [SERVICE_NAME] unscheduled downtime Message: Dear all, [SERVICE_NAME] experienced unscheduled downtime between [START] and [END]. [DETAILED_FAILURE_DESCRIPTION] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] Case 2: long “detected” outage Service fails and administrator detects the problem. The problem takes at least 1 hour time to recover. In the sections below individual situations are described.\n1. Outage Description: Service failure is detected.\nAction: Administrator announces the failure by using the following template:\nSubject: [SERVICE_NAME] outage Message: Dear all, [SERVICE_NAME] is experiencing unscheduled downtime. [ADDITIONAL_INFO] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] 2. Extended downtime Description: Service recovery is delayed. Update should be sent at least every 24h.\nAction: The administrator announces that recovery is taking longer by using the following template:\nSubject: [SERVICE_NAME] extended outage Message: Dear all, Outage of [SERVICE_NAME] is extended. [ADDITIONAL_INFO] Apologies for any inconvenience caused. Best Regards [SERVICE_TEAM] Note: In this template [ADDITIONAL_INFO] should indicate the estimated time of recovery.\n3. Recovery Description: Service is recovered.\nActions: At the time of recovery the administrator announces the recovery by using the following template:\nSubject: [SERVICE_NAME] recovery Message: Dear all, [SERVICE_NAME] is back online. [ADDITIONAL_INFO] Best Regards [SERVICE_TEAM] 4. Post mortem analysis Description: Service failure required further time to investigate the source of the problem. This action is required only if the post mortem analysis is needed.\nActions: The administrator announces the post mortem analysis of failure by using the following notification template:\nSubject: [SERVICE_NAME] outage analysis Message: Dear all, [SERVICE_NAME] experienced unscheduled downtime between [START] and [END]. [DETAILED_FAILURE_DESCRIPTION] Best Regards [SERVICE_TEAM] ","categories":"","description":"How to manage central operational tool unscheduled downtimes","excerpt":"How to manage central operational tool unscheduled downtimes","ref":"/providers/operations-manuals/man04_tool_intervention_management/","tags":"","title":"MAN04 Tool Intervention Management"},{"body":"This page contains information about using Check-in for managing your Virtual Organisation (VO). For joining a VO please look at Joining Virtual Organisation.\nBackground In simple terms a Virtual Organisation (VO) is just a group of users. In EGI VOs are created to group researchers who aim to share resources across the EGI Federation to achieve a common goal as part of a scientific collaboration. For a more formal definition of VO please look at the EGI Glossary.\nYou can browse existing VOs in the EGI Operations Portal. For each VO you can click on the Details link to get more information. You can join an existing VO either using the enrollment URL or emailing VO managers.\nIf you are interested in creating your own VO, please see instructions in the section below.\nVO management VOs in Check-in are represented as Collaborative Organisation Units (COUs). A COU is more than just a group. It is the concept of groups combined with membership management and advanced enrolment workflows. COUs can also be organised in a hierarchical structure for creating groups or subgroups within a VO.\nIt is assumed that VO managers and members have already registered their EGI Check-in account (A step-by-step guide is provided in this link.\nRegistering your VO Any person who can authenticate to the Operations Portal using their EGI Check-in account can register a new VO.\nThe person initiating the registration is called the VO manager. After the VO is set up and operational, the VO manager is the person who is primarily responsible for the operation of the VO and for providing sufficient information about VO activities for EGI and for VO members (to both people and sites).\nA step-by-step guide for the VO registration process is provided in the procedure PROC14 VO Registration.\nViewing VO members   Login to Check-in registry using any of the login credentials already linked to your EGI account.\n  To view the existing members, expand the People drop down menu and click on My VO-NAME Population (for example, My vo.example.org Population)\n  Then you are able to see all the VO members.\n  Accepting new VO members Users can request membership in your VO by following the VO enrollment URL. The enrollment URL has the following form:\nhttps://aai.egi.eu/registry/co_petitions/start/coef:## where ## is the unique numeric identifier for the enrollment flow of your VO.\nOnce a user submits a VO membership petition, all VO managers are notified with an email containing a link to the petition. Any of the VO managers can then review the petition and either approve or deny the request.\nThe VO enrollment URL can be found through the EGI Check-in Registry:\n  Login to Check-in registry using any of the login credentials already linked to your EGI account.\n  Expand the People drop down menu and click Enroll.\n  Copy the Begin link of the Enrollment flow of the VO you want the user to join and send it to the user\n  Once the user submits the VO membership request, the Role Attributes section of their profile page will include the new VO membership role in Pending Approval status\n  Once the VO manager accepts the new member, the Role Attributes section of the user’s profile page will include the VO membership role in Active status\n  Managing VO groups VO groups can only be created by Check-in platform administrators. Please contact checkin-support \u003cAT\u003e mailman.egi.eu indicating the following information for every (sub)group that you need to add/remove to/from your VO:\n VO name Group name Group description Optional, Group manager(s), i.e. the Check-in identifiers (in the form of \"xxxxxxx@egi.eu\") of one or more users responsible for managing the VO group members. Group managers can also appoint other users as (sub)group managers. The manager(s) of the VO (or any parent group) are implicitly managers of the group. You can provide additional Check-in user identifiers to extend the list of group managers. Optional, Parent VO group name (in the case of a hierarchical group, e.g. \u003cVO\u003e –\u003e \u003cPARENT_GROUP\u003e –\u003e \u003cGROUP\u003e)  Known limitation: Group names must be unique so the names you suggest may need to be adjusted by the Check-in administrators to guarantee their uniqueness.\nAdding members to VO groups   Login to Check-in registry using any of the login credentials already linked to your EGI account.\n  Then expand the People drop down menu and click My \u003cVO-NAME\u003e Population (for example, My vo.example.org Population)\n  Find the user you want to add to the VO Group and click Edit.\n  Click Add at the Role Attributes section of the user profile\n  Fill in the fields in the form and click Add. The user now is a member of the new VO group. For more information about Affiliation and Role fields you can see below at section Managing Affiliation and Role of VO Member\n  Removing members From the VO members list (see Viewing VO members above):\n  Click Edit on the person that is going to be removed.\n  Under Role Attributes click Delete on the right of the COU entry of interest (for example, vo.example.com). On success the selected row will be removed. In this example we removed the vo.geoss.eu that we previously added.\n  Managing Affiliation and Role of VO Member User’s Affiliation to a VO, as defined in RFC4512, has eight permissible values. These are faculty, student, staff, alum, member, affiliate, employee, library-walk-in. EGI Check-in assigns to all user’s the affiliation Member by default, during the VO(COU) enrollment process. This value is immutable for the user but editable for the VO administrator. As a result, if there is a change of status the administrator can always step in and change it appropriately. Additionally, the user’s Role in a VO is the EGI User Community Title column, in Co Person Role’s View. This column can be either a custom text value; or a value chosen from a drop down list. The drop down list administration is an EGI Check-in CO administrator task and can not be managed by any VO admin.\nUpdate User’s VO affiliation   Navigate to Co Person Role view   Choose Affiliation from drop down list   Update User’s VO Role   Navigate to Co Person Role view   Choose Role from drop down list, if available, or add custom text if no list is present.   Subsequently, EGI Check-in uses the CO Person’s group membership and role information in order to construct the eduPersonEntitlement values, in short entitlements. These URN-formatted attributes can be used for representing group membership, as well as to indicate rights to resources. According to the AARC-G002 specification, a user that is a member of the VO vo.example.org, and has the role supervisor, obtains the following entitlements:\n  urn:mace:egi.eu:group:vo.example.org:role=member#aai.egi.eu\n  urn:mace:egi.eu:group:vo.example.org:role=supervisor#aai.egi.eu\n  Managing COU Admin members COU Admin Groups are used to determine COU Administrators. Admin Groups are automatically created when a COU is created. The default name for COU admin groups is\nCO:COU:\u003cCOU_Name\u003e:admins\nFor example CO:COU:vo.example.org:admins\n A CO Person can be a member, an owner, both, or neither. Specifically:  A COU admins group member can manage COU members: Approve or decline membership petitions   Manage members' roles  A COU admins group owner has permission to add and remove members to and from the group, i.e. manage the list of CO Persons who can manage the COU members    A COU admins group owner can manage the admins group member as follows:\n  Login to Check-in registry using any of the login credentials already linked to your EGI account.\n  To view the available groups expand the Groups drop down list and click All Groups\nExpand the Filter section and find the COU admin group you are interested in. For the case of the service-integration COU with type the string service-integration in the text box with the placeholder Name.Then we click on Filter button\n  Locate Admins group click on Edit action\n  Expiration Policy VO membership expires within a given time, typically a year after the VO member joins the VO. VO members receive a notification from EGI Check-in Notifications with the subject “vo.example.org membership will expire soon” warning them that their membership will expire four weeks before expiration. The notification email is sent on a weekly basis and includes all the instructions needed by VO members in order to reapply for a membership. If the VO member does not take any action to renew their membership, a final notification email is sent when the VO membership expires. Please note that a user with expired membership is not eligible for VO membership entitlements and as a result the user will not have access to VO resources relying on these entitlements.\nAssign COU member admin role From the steps defined above:\n  Follow Manage Group Memberships link\n  Filter out the CO Person you need to apply for the admin role. Use Given, Family Name, Email, Identifier or a combination of the former.\n  Remove COU admin role From the steps defined above:\n  Under Group Members tab, click on Delete action for the CO Person that needs to be removed from Admins group\n  VO membership API Check-in provides a REST API that allows clients to manage membership information only for the VOs they are authoritative for.\nFeatures:\n Members of the VO are identified via their EGI Check-in ePUID Membership can be limited to a specified period Different membership status values are supported, namely Active, Expired, Deleted Check-in automatically changes the membership status from Active to Expired beyond the validity period  Authentication The REST client is authenticated via username/password credentials transmitted over HTTPS using the Basic Authentication scheme. More sophisticated authentication mechanisms, such as OpenID Connect/OAuth 2.0 access tokens, may be supported in the future.\nMethods   Adding a user to a VO requires specifying the user’s EGI Check-in ePUID, the name of the VO (e.g. vo.access.egi.eu in the case of LToS), the status (Active) and the valid from/through dates. All these parameters are mandatory. Here is an example using curl (see example add.json file below):\ncurl -vX POST https://aai.egi.eu/api/v1/VoMembers \\  --user \"example-client\":\"veryverysecret\" \\  --data @add.json \\  --header \"Content-Type: application/json\" ad.json:\n{ \"RequestType\": \"VoMembers\", \"Version\": \"1.0\", \"VoMembers\": [ { \"Version\": \"1.0\", \"VoId\": \"vo.access.egi.eu\", \"Person\": { \"Type\": \"CO\", \"Id\": \"01234567890123456789@egi.eu\" }, \"Status\": \"Active\", \"ValidFrom\": \"2017-05-21\", \"ValidThrough\": \"2017-06-21\" } ] }   Retrieving the VO membership information for a given EGI Check-in ePUID:\ncurl -vX GET https://aai.egi.eu/api/v1/VoMembers/01234567890123456789@egi.eu \\  --user \"example-client\":\"veryverysecret\" output:\n[ { \"id\": 85, \"epuid\": \"01234567890123456789@egi.eu\", \"vo_id\": \"vo.access.egi.eu\", \"valid_from\": \"2017-05-20T22:00:00.000Z\", \"valid_through\": \"2017-06-21T22:00:00.000Z\", \"status\": \"Active\" } ] Beyond the valid_through date, the status will be automatically changed to Expired. So, when querying for VO membership information, it’s important to check that the status is actually set to Active for each of the identified VOs (see the vo_id attribute)\n  Updating existing VO membership record:\ncurl -vX PUT https://aai.egi.eu/api/v1/VoMembers \\  --user \"example-client\":\"veryverysecret\" \\  --data @update.json \\  --header \"Content-Type: application/json\" The request body is the same as the one used for adding new members but update requires using PUT instead of POST.\n  Removing VO member:\nSame as the update but requires setting the membership status to Deleted\n  ","categories":"","description":"Managing a Virtual Organisation (VO) in Check-in\n","excerpt":"Managing a Virtual Organisation (VO) in Check-in\n","ref":"/users/check-in/vos/","tags":"","title":"Managing a Virtual Organisation"},{"body":"What is it? Object storage is a standalone service that stores data as individual objects, organized into containers. It is a highly scalable, reliable, fast, and inexpensive data storage. It has a simple web services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web.\nThe main features of object storage:\n Storage containers and objects have unique URLs, which can be used to access, manage, and share them. Data can be accessed from anywhere, using standard HTTP requests to a REST API (e.g. VMs running in the EGI Cloud or in other cloud provider’s cloud, from any browser/laptop, etc.) Access can be public or can be restricted using access control lists. There is virtually no limit to the amount of data you can store, only the space used is accounted for.  Concepts To use object storage effectively, you need to understand the following key concepts and terminology:\nStorage containers Storage containers (aka buckets) are the fundamental holders of data. Every object is stored in a storage container. You can store any number of objects in a storage container.\nStorage containers have an unique name and act as the root folders of the storage space.\nEach storage container has a unique URL (that includes the name) by which anyone can refer to it.\nObjects Objects are the fundamental entities stored in object storage. Objects consist of object data and metadata. The data portion is opaque to object storage. The metadata is a set of name-value pairs that describe the object. These include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type. You can also specify custom metadata at the time the object is stored.\nAn object is uniquely identified within a storage container by a key (name) and a version.\nEach object has a unique URL, based on the storage container’s URL (that includes the key, and optionally the version) by which anyone can refer to it.\nPermissions Storage containers and objects can be shared by sharing their URLs. However, access to a storage container or to an object is controlled by access control lists (ACLs). When a request is received against a resource, object storage checks the corresponding ACL to verify that the requester has the necessary access permissions.\nNote It is possible to set permissions so that the storage container or object can be accessed publicly.  Usage from your application The object storage in the EGI Cloud is offered via OpenStack deployments that implement the Swift service.\nUsers can manage object storage using the OpenStack Horizon dashboard of a provider or from the command-line (CLI). More advanced usage include access via the S3 protocol, via the OpenStack Object Store API, or using the EGI Data Transfer service.\nNote Available object storage resources can be discovered in the Configuration Database (GOCDB).  Access from the command-line Multiple command-line interfaces (CLIs) are available to manage object storage:\n The OpenStack CLI The FedCloud Client is a high-level CLI for interaction with the EGI Federated Cloud (recommended) The Swift CLI has some advanced features that are not available through the OpenStack CLI The Davix Client, developed at CERN for RHEL and Debian environments, is another alternative for working with S3-compatible object storage  Access with the FedCloud CLI The main FedCloud commands for managing storage containers and storage objects are described below.\nNote See here for documentation on all storage container-related commands, and here for all object-related commands.  List storage containers For example, to access to the SWIFT endpoint at IFCA-LCG2 via the Pilot VO (vo.access.egi.eu), and list the available storage containers, use the FedCloud command below:\nLinux / Mac   Windows   PowerShell    To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n$ export EGI_SITE=IFCA-LCG2 $ export EGI_VO=vo.access.egi.eu $ fedcloud openstack container list +------------------+ | Name | +------------------+ | test-egi | +------------------+   To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e set EGI_SITE=IN2P3-IRES \u003e set EGI_VO=vo.access.egi.eu \u003e fedcloud openstack container list +------------------+ | Name | +------------------+ | test-egi | +------------------+   To avoid passing the site, VO, etc. each time, you can use FedCloud CLI environment variables to set them once and reuse them with each command invocation.\n\u003e $Env:EGI_SITE=\"IN2P3-IRES\" \u003e $Env:EGI_VO=\"vo.access.egi.eu\" \u003e fedcloud openstack container list +------------------+ | Name | +------------------+ | test-egi | +------------------+    Create new storage container To create a new storage container named test-egi, use the follwoing FedCloud command:\n$ fedcloud openstack container create test-egi +---------+-----------+------------------------------------------------------+ | account | container | x-trans-id | +---------+-----------+------------------------------------------------------+ | v1 | test-egi | tx000000000000000000afc-005f845160-2bb3ed4-RegionOne | +---------+-----------+------------------------------------------------------+ Create new object by uploading a file To upload a file as a new object into a storage container named test-egi, use the following FedCloud command:\nTip The newly created object can have a different name than the file being uploaded, use the --name command flag for this.  Tip Multiple files can be uploaded at once, but in that case the resulting objects will have the same names as the uploaded files.  $ fedcloud openstack object create test-egi file1.txt +-----------+-----------+----------------------------------+ | object | container | etag | +-----------+-----------+----------------------------------+ | file1.txt | test-egi | 5bbf5a52328e7439ae6e719dfe712200 | +-----------+-----------+----------------------------------+ List objects in a storage container To list the objects in a storage container use the FedCloud command below:\n$ fedcloud openstack object list test-egi +-----------+ | Name | +-----------+ | file1.txt | +-----------+ Download (the content of) an object To download an object named file1.txt located in storage container test-egi, and save its content to a file use the FedCloud command below:\nTip The object can be saved into a file named differently than the object itselft, by using the --filename command flag.  Tip Multiple files can be downloaded at once, but in that case the resulting files will have the same names as the downloaded objects.  $ fedcloud openstack object save test-egi file1.txt Add metadata to an object You can add/update object metadata, stored as key-value pairs among the object properties. E.g. to add a property named key1 with the value value2 to an object named file1.txt located in the storage container named test-egi, you can use the FedCloud command below:\n$ fedcloud openstack object set \\  --property key1=value2 test-egi file1.txt Remove metadata from an object You can also remove metadata from objects. E.g. to remove the property named key1 from the object named file1.txt located in the storage container named test-egi, you can use the FedCloud command below:\nNote Only metadata added by users can be removed (system properties cannot be removed).  $ fedcloud openstack object unset \\  --property key test-egi file1.txt Remove an object from a storage container To delete an object named file1.txt from the storage container test-egi, use the following FedCloud command:\nCaution Deleting object from storage containers is final, there is no way to recover deleted objects. Unlike in AWS S3, objects in OpenStack storage containers cannot be protected against deletion.  $ fedcloud openstack object delete test-egi file1.txt Removing an entire container To delete a storage container, including all objects in it, use the FedCloud command below.\nTip You can add the -r option to recursively remove sub-containers.  Caution Deleting all objects from a storage container is final, there is no way to recover deleted objects. Unlike in AWS S3, objects in OpenStack storage containers cannot be protected against deletion.  $ fedcloud openstack container delete test-egi Access via the S3 protocol The OpenStack Swift service is compatible with the S3 protocol, therefore when properly configured, it can be accessed as any other S3-compatible object store.\nNote The S3 protocol was created by Amazon Web Services (AWS) for their object storage, called Simple Storage Service (S3), but it was adopted as the de-facto standard to access object storage offered by other providers.  In order to access the storage via S3, you need EC2-compatible credentials from the OpenStack deployment. Use the following command:\n$ fedcloud openstack ec2 credentials create +------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+------------------------------------------------------------------------------------------------------------------------------------------+ | access | zxxxxxxxxxxxxxxxxxxxxxxxxxx | | links | {'self': 'https://api.cloud.ifca.es:5000/v3/users/5495cd688ad7401b8e87b46bdea92f33/credentials/OS-EC2/xxxxxxxxxxxxxxxxx'} | | project_id | 999f045cb1ff4684a15ebb338af69460 | | secret | xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx | | trust_id | None | | user_id | xxxxxxxxxxxxxxxxxxxxxxxxxxxx | +------------+------------------------------------------------------------------------------------------------------------------------------------------+  Important Save the access and secret values, as those are needed in subsequent commands that use the S3 protocol.  To list containers/objects via the S3 protocol, use the command:\n$ davix-ls --s3accesskey 'access' --s3secretkey 'secret' \\  --s3alternate s3s://api.cloud.ifca.es:8080/swift/v1/test-egi davix-get, davix-put and davix-del are also available to download, store and delete objects from the storage.\nAccess via EGI Data Transfer The EGI Data Transfer service can move files to and from object storages that are compatible with the S3 protocol. You will have to upload the EC2 access keys to the EGI Data Transfer service, which will be able to generate properly signed URLs for the objects in the storage.\nNote Please contact support at support \u003cat\u003e egi.eu for more details.  ","categories":"","description":"Object Storage offered by EGI Cloud providers\n","excerpt":"Object Storage offered by EGI Cloud providers\n","ref":"/users/online-storage/object-storage/","tags":"","title":"Object Storage"},{"body":"EGI provides a training instance of the Notebooks service for training events.\nTo get started:\n  Go to https://training.notebooks.egi.eu.\nNote This instance may not use the same software version as in production and may not be always available, as it is typically configured only for specific training events.    Start the authentication process by clicking on Start your notebooks! button\n  Select the Identity Provider you belong to from the discovery page. If this is the first time you access an EGI service, Check-in will guide you through a registration process.\n  You will see the Jupyter interface once your personal server is started\n  Launching a notebook Click on the New \u003e Python 3 option to launch your notebook with Python 3 kernel. When you create this notebook, a new tab will be presented with a notebook named Untitled.ipynb. You can easily rename it by right-clicking on the current name.\nStructure of a notebook The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking either the “Play” button in the toolbar, or Cell -\u003e Run in the menu bar.\nThe execution behaviour of a cell is determined by the cell’s type.\nThere are three types of cells: cells, markdown, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be “Code”, initially).\nCode cells A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel.\nWhen a code cell is executed, its content is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell’s output. The output is not limited to text, with many other possible forms of output are also possible, including figures and HTML tables.\nMarkdown cells You can document the computational process in a literate way, alternating descriptive text with code, using rich text. This is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc.\nIf you want to provide structure for your document, you can also use markdown headings. Markdown headings consist of 1 to 6 hash # signs followed by a space and the title of your section. The Markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.\nWhen a Markdown cell is executed, the Markdown code is converted into the corresponding formatted rich text. Markdown allows arbitrary HTML code for formatting.\nRaw cells Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook.\nKeyboard shortcuts All actions in the notebook can be performed with the mouse, but keyboard shortcuts are also available for the most common ones. These are some of the most common:\n Shift-Enter: run cell. Execute the current cell, show any output, and jump to the next cell below. If Shift-Enter is invoked on the last cell, it creates a new cell below. This is equivalent to clicking the Cell -\u003e Run menu item, or the Play button in the toolbar. Esc: Command mode. In command mode, you can navigate around the notebook using keyboard shortcuts. Enter : Edit mode. In edit mode, you can edit text in cells.  Hands-on We pre-populate your home directory with some sample notebooks to get started, below you can find links to other notebooks that we have used in past trainings that may be useful to explore the system:\n A very basic notebook to get started Getting data and doing a simple plot. Connect to NOAA's GrADS Data Server to plot wind speed. Installing new libraries. Interact with Check-in  ","categories":"","description":"Notebooks for training events\n","excerpt":"Notebooks for training events\n","ref":"/users/notebooks/training/","tags":"","title":"Training instance"},{"body":"Users of the EGI Cloud create Virtual Machines (VMs) on the providers. Those VMs are started from images: templates for the root volume of the running instances, i.e. operating system and applications available initially on a VM. The AppDB collects the Virtual Machine Images available on the service as Virtual Appliances (VA).\nAny user can register new Virtual Appliances at the AppDB, these are then managed by special VO members that curate which appliances are available to their VO.\nAppDB Cloud Marketplace The AppDB is a browsable catalogue of Virtual Appliances that users can start at the providers. You can find below a set of reference guides for the catalogue:\n How to register a VA?: any registered user can register VAs in AppDB for anyone to download or for making them available at the EGI Cloud providers once a VO adds it to the VO-wide image list. Once registered, VAs can be managed as described in the VA management guide. VO managers select VAs to be available at the providers following the VO-wide image list management.  Check the full list of Cloud marketplace guides and Cloud marketplace FAQ for more information about the AppDB features.\nCustom images Packaging your application in a custom VM image is a suggested solution in one of the following cases:\n your particular OS flavor is not available at AppDB; installation of your application is very complex and time-consuming for being performed during contextualization; or you want to reduce the number of 'moving-parts' of your software stack and follow an immutable infrastructure approach for deploying your application.  Custom VM images can be crafted in different ways. The two main possibilities are:\n start from scratch, creating a virtual machine, installing an OS and the software on top of it, then taking the virtual machine OS disk as custom image; or dump an existing disk from a running VM or physical server and modify it, if needed, to run on a virtualisation platform.  In this guide we will focus on the first option, because it tends to produce cleaner images and reduces the risks of hardware conflicts. Snapshotting may be also restricted by the cloud providers or by security policies.\nAdvantages:\n Possibility to build the virtual disk directly from a legacy machine, dumping the contents of the disk. Possibility to speed-up the deployment for applications with complex and big installation packages. This because you do not need to install the application at startup, but the application is already included in the machine.  Disadvantages:\n Building a virtual disk directly from a legacy machine poses a set of compatibility issues with hardware drivers, which usually differs from a virtual and physical environment and even between different virtual environments. You need to keep your machine updated. Outdated VM disk images may take a long time to startup due to the need to download and install the latest OS updates. If you are using special drivers or you are not packaging correctly the disk, your custom VM image may not run (or run slowly) on different cloud providers based on different virtualisation technologies. VM images on public clouds are sometimes public, thus be aware of installing proprietary software on custom images, since other users may be able to run the image or download it. In general, the effort to implement this solution is higher than the basic contextualization.  Image size and layout The larger the VM image, the longer it will take to be distributed to the providers and the longer it will take to be started on the infrastructure. As a general rule, always try to make images as smaller as possible following these guidelines:\n  DO NOT include (big) data in your image. There are other mechanisms for accessing data from your VM (block/object storage, CVMFS)\n  DO NOT include (big) empty space or swap in your image. Extra space for your computation or swap can be added with block storage once the VM is booted or using VM flavors that have extra disk allocated for your VM.\n  DO NOT install un-needed software. Tools like GUI are of no-use in most cases since you will have no access to the graphical console of the VM.\n  DO adjust the size of the images as much as possible. As stated above, empty space can be allocated on runtime easily.\n  DO use compressed image formats, like qcow2 or vmdk (used in OVA) to minimize the size of the image. Preferred format for images in EGI is OVA as it's standardised.\n  DO fill with 0 the empty disk space of your image so when compressed it can be significantly reduced, e.g. using:\ndd if=/dev/zero of=/bigemptyfile bs=4096k rm -rf /bigemptyfile   DO use a single partition (no /boot, no swap) for the disk layout and avoid LVM. This will allow the cloud provider to easily resize your partition when instantiated and to modify files in it if needed.\n  Contextualization and credentials Danger Do NOT include any credentials on your images.  You should never include any kind of credentials on your images, instead you should use contextualization. cloud-init is a tool that will simplify the contextualization process for you. This is widely available as packages in major OS distributions and is supported by all the providers of the EGI Cloud and most of the commercial providers.\ncloud-init documentation contains detailed examples on how to create users, run scripts, install packages and several other actions supported by the tool.\nFor complex setups, especially when applications involve multiple VMs it may be useful to use cloud-init to bootstrap some Configuration Management Software that will manage the configuration of the VMs during runtime.\nSecurity  Always remove all default passwords and accounts from your VM. Disable all services unless necessary for the intended tasks. Make sure the firewall configuration (iptables for Linux, also on IPv6) is minimally open. Put no shared credentials (passwords) in any image.  You should also follow the best practice guides for each service that's exposed to the outside world. See for example guides for:\n ssh tomcat  See also AWS security Best Practices\nTools Whenever possible, automate the process of creating your images. This will allow you to:\n get reproducible results avoid tedious manual installation steps quickly produce updated versions of your images.  EGI uses packer as a tool for automating the creation of our base images. This tool can use VirtualBox as a hypervisor for the creation of the images and guarantees identical results under different platforms and providers.\nCheck the fedcloud-vmi-templates GitHub repository with all the packer recipes used to build our images and re-use them as needed for your images.\n","categories":"","description":"Managing VM images in the EGI Cloud\n","excerpt":"Managing VM images in the EGI Cloud\n","ref":"/users/cloud-compute/vmi/","tags":"","title":"Virtual Machine Image Management"},{"body":"With High-Throughput Compute you can run computational jobs at scale on the EGI infrastructure. It allows you to analyse large datasets and execute thousands of parallel computing tasks.\nHigh-Throughput Compute is provided by a distributed network of computing centres, accessible via a standard interface and membership of a virtual organisation. EGI offers more than 1,000,000 cores of installed capacity, supporting over 1.6 million computing jobs per day.\nThis service supports research and innovation at all scales: from individuals to large collaborations.\nMain characteristics of the service:\n Access to high-quality computing resources Integrated monitoring and accounting tools to provide information about the availability and resource consumption Workload and data management tools to manage all computational tasks Large amounts of processing capacity over long periods of time Faster results for your research Shared resources among users, enabling collaborative research  Service management Miscellaneous collection of documentation related to High-Throughput Compute.\n Changing the Site BDII  ","categories":"","description":"Execute thousands of computational tasks to analyse large datasets","excerpt":"Execute thousands of computational tasks to analyse large datasets","ref":"/providers/high-throughput-compute/","tags":"","title":"High-Throughput Compute"},{"body":"The EGI Cloud service offers a multi-cloud IaaS federation that brings together research clouds as a scalable computing platform for data and/or compute driven applications and services for research and science.\nThis documentation focuses on using the service. Those resource providers willing to integrate into the service, please check the EGI Federated Cloud Integration documentation.\nCloud Compute gives you the ability to deploy and scale virtual machines on-demand. It offers computational resources in a secure and isolated environment controlled via APIs without the overhead of managing physical servers.\nCloud Compute service is provided through a federation of IaaS cloud sites that offer:\n Single Sign-On via EGI Check-in, users can login into every provider with their institutional credentials and use modern industry standards like OpenID Connect. Global VM image catalogue at AppDB with pre-configured Virtual Machine images that are automatically replicated to every provider based on your community needs. Resource discovery features to easily understand which providers are supporting your community and what are their capabilities. Global accounting that aggregates and allows visualisation of usage information across the whole federation. Monitoring of Availability and Reliability of the providers to ensure SLAs are met.  The flexibility of the Infrastructure as a Service can benefit various use cases and usage models. Besides serving compute/data intensive analysis workflows, Web services and interactive applications can be also integrated with and hosted on this infrastructure. Contextualisation and other deployment features can help application operators fine tune services in the cloud, meeting software (OS and software packages), hardware (number of cores, amount of RAM, etc.) and other types of needs (e.g. orchestration, scalability).\nSince the opening of the EGI Federated Cloud, the following usage models have emerged:\n Service hosting: the EGI Federated Cloud can be used to host any IT service as web servers, databases, etc. Cloud features, as elasticity, can help users to provide better performance and reliable services.  Example: NBIS Web Services, Peachnote analysis platform.   Compute and data intensive applications: for those applications needing considerable amount of resources in terms of computation and/or memory and/or intensive I/O. Ad-hoc computing environments can be created in the EGI cloud providers to satisfy extremly intensive HW resource requirements.  Example: VERCE platform, The Genetics of Salmonella Infections, The Chipster Platform.   Datasets repository: the EGI Cloud can be used to store and manage large datasets exploiting the large amount of disk storage available in the Federation. Disposable and testing environments: environments for training or testing new developments.  Example: Training infrastructure    Eager to test this service? Have a look at how to create your first Virtual Machine in EGI.\n","categories":"","description":"[EGI Cloud Compute](https://www.egi.eu/services/cloud-compute/) (FedCloud)\n","excerpt":"[EGI Cloud Compute](https://www.egi.eu/services/cloud-compute/) …","ref":"/users/cloud-compute/","tags":"","title":"Cloud Compute"},{"body":"Connect to CheckIn an IdP federated in an hub and spoke federations I get an error similar to: “Error - No connection between institution and service” (SURFconext example) In case of a “hub and spoke” federation the federation coordinator may require that the IdP administrators explicitly request to connect to a SP and let their users to authenticate on these SP.\nIn most of the cases this is not a configuration problem neither for the CheckIn service nor for the Identity provider. The connection needs to be implemented in the hub and spoke IdP Proxy.\nOne example of such federation is SURFconext, the national IdP federation for research and education in the Netherlands operated by SURFnet. If you are using credentials from a Dutch IdP in eduGAIN, you or your IdP administrators need to request the connection. The following steps will lead you to perform the connection:\n Connect to SURFconext dashboard Search for “EGI AAI Service provider proxy”  If the service does not show in the search, you need to ask SURFnet to add it in the dashboard, please write to support at surfconext dot nl   In the dashboard, near the “EGI AAI Service provider proxy” there should be a “Connect” button, this will create a service ticket and the SURFconext team will make the connection active. After you received confirmation that the “EGI AAI Service provider proxy” is accessible, you will be able to login in CheckIn  Authentication error with ADFS-based Identity Providers Why do I get the error below after successfully authenticating at my Home IdP? opensaml::FatalProfileException at (https://aai.egi.eu/registry.sso/SAML2/POST) SAML response reported an IdP error. Error from identity provider: Status: urn:oasis:names:tc:SAML:2.0:status:Responder The Responder error status is typically returned from ADFS-based IdP implementations (notably Microsoft ADFS 2.0 and ADFS 3.0) that cannot properly handle Scoping elements. Check-in can be configured to omit the scoping element from the authentication requests sent to such IdPs in order to allow successful logins. Please contact the CheckIn support team and include a screenshot of your error.\nI have linked an IGTF X.509 certificate to my Check-in identity but the information is inaccurate or incomplete What can I do? To update your certificate information, follow these steps to log into your Check-in profile page using your IGTF certificate:\n Click here to access your profile page   Warning This may log you out of any service you have accessed with Check-in on this browser!  2. On the Check-in identity provider discovery page, select IGTF\nWarning If prompted to log in with a different identity provider, click CHOOSE ANOTHER ACCOUNT and then select IGTF. Alternatively, you can click here for your convenience  ","categories":"","description":"Frequently Asked Questions\n","excerpt":"Frequently Asked Questions\n","ref":"/users/check-in/faq/","tags":"","title":"FAQ"},{"body":"How do I install library X? You can install new software easily on the notebooks using conda or pip. The %conda and %pip magics can be used in a cell of your notebooks to do so, e.g. installing rdkit:\n%conda install rdkit Once installed you can import the library as usual.\nWarning Any modifications to the libraries/software of your notebooks will be lost when your notebook server is stopped (automatically after 1 hour of inactivity)!  Can I request library X to be installed permanently? Yes! Just let us know what are your needs. You can contact us via:\n Opening a ticket in the EGI Helpdesk, or Creating a GitHub Issue  We will analyse your request and get back to you.\n","categories":"","description":"Frequently Asked Questions\n","excerpt":"Frequently Asked Questions\n","ref":"/users/notebooks/faq/","tags":"","title":"FAQ"},{"body":"Site endpoints must be registered in EGI Configuration Management Database (GOCDB). If you are creating a new site for your cloud services, check the PROC09 Resource Centre Registration and Certification procedure. Services can also coexist within an existing (grid) site.\nExpected Services These are the expected services for a working site:\n  If offering native OpenStack access (nova), register: org.openstack.nova for the Nova endpoint of the site. The endpoint URL must contain the Keystone v3 URL: https://hostname:port/url/v3. Set the Host DN so the cloud-info-provider can be enabled in the AMS.\n  If offering native OpenStack access (swift), register: org.openstack.swift for the swift endpoint of the site. The endpoint URL field must contain the Keystone v3 URL: https://hostname:port/url/v3.\n  eu.egi.cloud.accounting for the host sending the records to the accounting repository (executing SSM send).\n  Deprecated services Deprecated services for cloud providers:\n  Site-BDII. This service collects and publishes site's data for the Information System. Existing sites should already have this registered.\n  eu.egi.cloud.vm-metadata.vmcatcher for the VMI replication mechanism. Register here the host providing the replication (i.e. the host with cloudkeeper installation)\n  If offering OCCI interface, eu.egi.cloud.vm-management.occi for the OCCI endpoint offered by the site. The endpoint URL must follow this syntax:\nhttps://hostname:port/?image=\u003cimage_name\u003e\u0026resource=\u003cresource_name\u003e\nwhere \u003cimage_name\u003e and \u003cresource_name\u003e cannot contain spaces. These attributes map to os_tpl and resource_tpl respectively and will be the ones used for monitoring purposes.\n  ","categories":"","description":"Registration of service endpoints in GOCDB\n","excerpt":"Registration of service endpoints in GOCDB\n","ref":"/providers/cloud-compute/registration/","tags":"","title":"GOCDB Registration"},{"body":"What is it? Support for EGI services is available through the EGI Helpdesk.\nThe EGI Helpdesk is a distributed tool with central coordination, which provides the information and support needed to troubleshoot product and service problems. Users can report incidents, bugs or request changes.\nThe support activities are grouped into first and second level support.\nNote Documentation for the EGI Helpdesk is available in the EGI Wiki.  ","categories":"","description":"EGI Helpdesk \n","excerpt":"EGI Helpdesk \n","ref":"/internal/helpdesk/","tags":"","title":"Helpdesk"},{"body":" EGI DataHub service  Overview slides Community Forum EGI Webinar and YouTube video   System requirements Official Onedata documentation  Onedata homepage Getting started Source code    ","categories":"","description":"Related links","excerpt":"Related links","ref":"/users/datahub/links/","tags":"","title":"Links"},{"body":"Overview Each instance of the FTS3 service, offers a Web monitoring interface, that can be accessed by end users in order to monitor their submitted transfers and obtain statistics.\nFeatures The Web monitoring can be accessed without user authentication, only access to the transfer log files needs an X.509 user certificate installed on the browser.\nOverview page The Overview page offers a way to access the information about the transfers submitted and executed in the last 6 hours. Users can filter transfers per Virtual Organization, source or destination storage or JobId.\nJob details page By selecting a specific job the information about the job details are displayed. Each transfer part of the job is listed with his own information. From this page it’s also possible to access the transfer logs (upon authentication).\nOptimizer page The Optimizer page shows Optimizer information about a specific link, detailing the throughput evolution and the parallel transfer/stream per link at a given time.\n","categories":"","description":"Documentation related to EGI Data Transfer Monitoring","excerpt":"Documentation related to EGI Data Transfer Monitoring","ref":"/users/data-transfer/monitoring/","tags":"","title":"Monitoring"},{"body":"Here you can find documentation about the internal architecture and operations of the EGI Notebooks service.\n","categories":"","description":"Architecture and Operations of EGI Notebooks","excerpt":"Architecture and Operations of EGI Notebooks","ref":"/providers/notebooks/","tags":"","title":"Notebooks"},{"body":"In addition to the formatting support provided by Markdown, Hugo adds support for shortcodes, which are Go templates for easily including or displaying content (images, notes, tips, advanced display blocks, etc.).\nFor reference, the following shortcodes are available:\n Hugo’s shortcodes Docsy theme shortcodes  Highighted paragraphs This is achieved using Docsy shortcodes.\nPlaceholders The following code:\n{{% pageinfo %}} This is a placeholder. {{% /pageinfo %}} Will render as:\nThis is a placeholder.\n Information messages The following code:\n{{% alert title=\"Note\" color=\"info\" %}} This is a Note. {{% /alert %}} Will render as:\nNote This is a Note.  Warning messages The following code:\n{{% alert title=\"Important\" color=\"warning\" %}} This is a warning. {{% /alert %}} Will render as:\nImportant This is a warning.  Code or shell snippets The code or instructions should be surrounded with three backticks, followed by an optional highlighting type parameter.\nThe supported languages are dependant on the syntax highlighter, which depends itself on the mardkown parser.\nNote Hugo uses the goldmark parser, which relies on Prism syntax highlighting.  The following Markdown creates a shell excerpt:\n```shell $ ssh-keygen -f fedcloud $ echo $HOME ``` Will render as:\n$ ssh-keygen -f fedcloud $ echo $HOME  Tip If you click the Copy button in the top-right corner of a shell example, all commands in that block are copied to the clipboard. The prompt in front of each command, and any command output is not copied.  Note In case the command(s) in your shell example cause the introduction of a horizontal scroll bar, consider breaking the command(s) into multiple lines with trailing backslashes (\\). However, you should never break command output to multiple lines, as that makes understanding the output, and recognizing it in real life, very difficult.  Code in multiple languages This is also achieved using Docsy shortcodes.\nWhen you need to include code snippets, and you want to provide the same code in multiple programming languages, you can use a tabbed pane for code snippets:\n{{\u003ctabpane\u003e}} {{\u003ctabheader=\"C++\"lang=\"C++\"\u003e}} #include \u003ciostream\u003e int main() { std::cout \u003c\u003c \"Hello World!\" \u003c\u003c std::endl; } {{\u003c/tab\u003e}} {{\u003ctabheader=\"Java\"lang=\"Java\"\u003e}} class HelloWorld { static public void main( String args[] ) { System.out.println( \"Hello World!\" ); } } {{\u003c/tab\u003e}} {{\u003ctabheader=\"Kotlin\"lang=\"Kotlin\"\u003e}} fun main(args : Array\u003cString\u003e) { println(\"Hello, world!\") } {{\u003c/tab\u003e}} {{\u003ctabheader=\"Go\"lang=\"Go\"\u003e}} import \"fmt\" func main() { fmt.Printf(\"Hello World!\\n\") } {{\u003c/tab\u003e}} {{\u003c/tabpane\u003e}} Will render as:\nC++  Java  Kotlin  Go   #include \u003ciostream\u003eint main() { std::cout \u003c\u003c \"Hello World!\" \u003c\u003c std::endl; } class HelloWorld { static public void main( String args[] ) { System.out.println( \"Hello World!\" ); } } fun main(args : Array\u003cString\u003e) { println(\"Hello, world!\") } package main import \"fmt\" func main() { fmt.Printf(\"Hello World!\\n\") }  Content with multiple variants When you need to include multiple variants of the same content, other than code snippets in multiple programing languages, you can use the follwing shortcode:\n{{\u003ctabpanex\u003e}} {{\u003ctabxheader=\"Linux\"\u003e}} You can list all files in a folder using the command: ```shell ls -a -l ``` {{\u003c/tabx\u003e}} {{\u003ctabxheader=\"Mac\"\u003e}} To get a list of all files in a folder, press **Cmd** + **Space** to open a spotlight search, type terminal, then press Enter. In the terminal window then run the command: ```shell ls -a -l ``` {{\u003c/tabx\u003e}} {{\u003ctabxheader=\"Windows\"\u003e}} You can list all files in the current folder using the command: ```shell dir ``` or you can use PowerShell: ```powershell Get-ChildItem -Path .\\ ``` {{\u003c/tabx\u003e}} {{\u003c/tabpanex\u003e}} Will render as:\nLinux   Mac   Windows    You can list all files in a folder using the command: ```shell ls -a -l ```   To get a list of all files in a folder, press **Cmd** + **Space** to open a spotlight search, type terminal, then press Enter. In the terminal window then run the command: ```shell ls -a -l ```   You can list all files in the current folder using the command: ```shell dir ``` or you can use PowerShell: ```powershell Get-ChildItem -Path .\\ ```    Tip You can include any valid markdown content in each tab, including code or shell snippets.  ","categories":"","description":"Helpers for writing EGI documentation","excerpt":"Helpers for writing EGI documentation","ref":"/about/contributing/shortcodes/","tags":"","title":"Shortcodes"},{"body":"VM Images are replicated using cloudkeeper, which has two components:\n fronted (cloudkeeper-core) dealing the with image lists and downloading the needed images, run periodically with cron backend (cloudkeeper-os) dealing with your glance catalogue, running permanently.  Using the VM Appliance Every 4 hours, the appliance will perform the following actions:\n download the configured lists in /etc/cloudkeeper/image-lists.conf and verify its signature check any changes in the lists and download new images synchronise this information to the configured glance endpoint  First you need to configure and start the backend. Edit /etc/cloudkeeper-os/cloudkeeper-os.conf and add the authentication parameters from line 117 to 136.\nThen add as many image lists (one per line) as you would like to subscribe to /etc/cloudkeeper/image-lists.conf. Use URLs with your AppDB token for authentication, check the following guides for getting such token and URLs:\n how to access to VO-wide image lists, and how to subscribe to a private image list.  Finally, you need to provide a /etc/cloudkeeper-os/mapping.json that configures the mapping of VOs supported in your OpenStack. The file should contain a json document that follows this format:\n{ \"\u003cVO_NAME\u003e\": { \"project\": \"\u003cid of project in OpenStack for VO_NAME\u003e\" }, \"\u003cVO_NAME_2\u003e\": { \"project\": \"\u003clocal name of project in OpenStack for VO_NAME_2\u003e\", \"domain\": \"\u003cdomain name for project in OpenStack\u003e\" } } Note that you can either specify a project ID or the project name with the domain name in the mapping. Add as many VOs as you are supporting.\nRunning the services cloudkeeper-os should run permanently, there is a cloudkeeper-os.service for systemd in the appliance. Manage as usual:\nsystemctl \u003cstart|stop|status\u003e cloudkeeper-os cloudkeeper core is run every 4 hours with a cron script.\n","categories":"","description":"cloudkeeper and AppDB integration\n","excerpt":"cloudkeeper and AppDB integration\n","ref":"/providers/cloud-compute/openstack/cloudkeeper/","tags":"","title":"VM Image synchronisation"},{"body":"The EGI Application Database (AppDB) includes a web GUI for management of Virtual Machines (VMs) on the federated infrastructure.\nThis GUI is available for a set of selected VOs. If your VO is not listed and you are interested in getting support, please open a ticket or contact us at support _at_ egi.eu.\nMain user features  User identification with Check-in, with customised view of the VAs and resource providers based on the VO membership of the user. Management of VMs in topologies, containing one or more instances of a given VA. Attachment of additional block storage to the VM instances. Start/Stop VMs without destroying the VM (for all VMs of a topology or for individual instances within a topology) Single control of topologies across the whole federation.  Quick start   Log into the VMOps dashboard using EGI Check-in.\n  Click on \"Create a new VM Topology\" to start the topology builder, this will guide you through a set of steps:\n  Select the Virtual Appliance you want to start, these are the same shown in the AppDB Cloud Marketplace, you can use the search field to find your VA;\n  select the VO to use when instantiating the VA;\n  select the provider where to instantiate the VA; and finally\n  select the template (VM instance type) of the instance that will determine the number of cores, memory and disk space used in your VM.\n    Now you will be presented with a summary page where you can further customise your VM by:\n  Adding more VMs to the topology\n  Adding block storage devices to the VMs\n  Define contextualisation parameters (e.g. add new users, execute some script)\n    Click on \"Launch\" and your deployment will be submitted to the infrastructure.\n  The topology you just created will appear on your \"Topologies\" with all the details about it, clicking on a VM of a topology will give you details about its status and IP. VMOps will create a default cloudadm user for you and create ssh-key pair for login (you can create as many users as needed with the contextualisation options of the wizard described above).\nVMOps was presented in one of the EGI Webinars in 2020. The indico page contains more details and there is also a video recording available on YouTube.\n","categories":"","description":"The VMOPs Dashboard for management of VMs on the federation\n","excerpt":"The VMOPs Dashboard for management of VMs on the federation\n","ref":"/users/cloud-compute/vmops/","tags":"","title":"VMOps Dashboard"},{"body":"Infrastructure Manager is a tool that streamlines the access and the usability of IaaS clouds by automating the configuration, deployment, and monitoring of Virtual Appliances.\nIt supports APIs from a large number of Cloud Management Frameworks (CMFs) making user applications cloud-agnostic. In addition it integrates a contextualization system to enable the installation and configuration of user specific applications.\nIt is a service that features a web-based GUI, a XML-RPC API, a REST API and a command-line interface. For detailed information about Infrastructure Manager please visit its documentation pages.\nIM is integrated with the EGI Check-In Service. A very easy way to deploy your first Virtual Machine in the EGI Federation is using the web-based GUI. Please see tutorial here.\nInfrastructure Manager was presented in one of the EGI Webinars. See more details on the indico page and the video recording available on YouTube.\n","categories":"","description":"How to use Infrastructure Manager (IM)\n","excerpt":"How to use Infrastructure Manager (IM)\n","ref":"/users/cloud-compute/im/","tags":"","title":"Infrastructure Manager"},{"body":"The EGI Cloud Container Compute service allows you to run container-based applications on the providers of the EGI Federated Cloud. There are two main ways of executing containers:\n  Using docker (or a similar container runtime) on a VM, so you can just interact directly with the container runtime to run your applications. This fits simpler applications that can easily fit on one node and are composed by a small number of containers.\n  Using a container orchestration platform, e.g. kubernetes on a set of VMs to manage the applications in an automated way for you. This is usually suited for more complex applications that spawn several nodes and are composed of several containers that need to cooperate to deliver the expected functionality.\n  Follow the guides below to learn more about them.\nThe EGI Cloud Container Compute service was presented in one of the EGI Webinars. See more details on the indico page and a video recording on YouTube.\n","categories":"","description":"Run containers on the EGI Cloud\n","excerpt":"Run containers on the EGI Cloud\n","ref":"/users/cloud-container-compute/","tags":"","title":"Cloud Container Compute"},{"body":"Setting up GPU flavors Support for GPU can be added to flavors using the PCI passthrough feature in OpenStack. This allows to plug any kind of PCI device to the Virtual Machines.\nAs a summary of the OpenStack documentation, these are the steps needed to add a GPU enabled flavor (be aware this may need tuning to your specific hardware/configuration!):\n On computing node, get vendor/product ID of your hardware: lspci | grep NVDIA to get pci slot of GPU, then virsh nodedev-dumpxml pci_xxxx_xx_xx_x On computing node, unbind device from host kernel driver. Unbinding is system dependent, and can be done in many ways, e.g.:  if the kernel does not uses the devices (no GPU drivers included in kernel, or drivers disable in GRUB), nothing to unbind via pci-stub grubby --args=\"pci-stub.ids=10de:11fa\" --update-kernel DEFAULT (see RedHat manual, section 12.1, step 1-2; where the pci-stub.ids value is vendor_ID: product_id from lspci. via echo command: echo $dev \u003e /sys/bus/pci/devices/$dev/driver/unbind where $dev is the PCI device ID xx:xx.x or xxxx:xx:xx.x from lspci   On computing node, add pci_passthrough_whitelist = {\"vendor_id\":\"xxxx\",\"product_id\":\"xxxx\"} to nova.conf (see nova-compute) On controller node, add pci_alias = {\"vendor_id\":\"xxxx\",\"product_id\":\"xxxx\", \"name\":\"GPU\"} to nova.conf (see nova-api) On controller node, enable PciPassthroughFilter in the scheduler (see nova-scheduler) Create new flavors with pci_passthrough:alias (or add key to existing flavor), e.g. openstack flavor set m1.large --property \"pci_passthrough:alias\"=\"GPU:2\"  GPU description in flavor metadata Users should be able to easily discover the flavors that provide GPUs (or accelerators in general). The following table describes the agreed metadata for EGI providers to add to those flavors:\n   Metadata Definition Comments     Accelerator:Type Type of accelerator (e.g. GPU) Possible values: GPU, MIC, FPGA, TPU, NPU   Accelerator:Number Number of accelerators available in the flavor (e.g. 1.0) Non integers allowed for the case of sharing GPU between VMs   Accelerator:Vendor Name of accelerator Vendor (e.g. NVIDIA)    Accelerator:Model Model of accelerator (e.g. Tesla V100) Need to make consensus and enforce. A100 is usually marketed without “Tesla” classname. Similarly, RTX A6000 usually marketed without “GeForce”. For clarity, full names should be used: “Tesla A100” and “GeForce RTX A6000”   Accelerator:Version Version of the accelerator Some cards have different versions, e.g. A100 PCIe and NVLink. Openstack does not allow empty value, so we should give 0 if no version is specified   Accelerator:Memory RAM in GBs of the accelerator    Accelerator:VirtualizationType Type of virtualisation used (e.g. PCI passthrough) Not relevant for accounting, but may be still useful in some cases    There are some extra fields that are defined in the GLUE2.1 schema but not so relevant for GPUs and therefore not considered at the moment. These are listed below for completeness:\n   Metadata Definition Comments     Accelerator:ComputeCapability Compute capabilities Defined by GLUE2.1, e.g. floating point type, NVLink, … may be used informally so far   Accelerator:ClockSpeed Clockspeed of accelerator Defined by GLUE2.1, not so relevant, as ClockSpeed no longer related to performance. May be reserved for other types of accelerators   Accelerator:Cores Number of cores of the accelerator Not so useful as there are several types of cores now (CUDA, tensor). May be reserved for other types of accelerators    Adding metadata to flavors has no effects on site operations. End users can see the metadata easily via openstack flavor list --long or openstack flavor show \u003cflavor id\u003e commands without any additional tools, e.g.:\n$ fedcloud openstack flavor show gpu1cpu2 --site IISAS-GPUCloud --vo eosc-synergy.eu -f json Site: IISAS-GPUCloud, VO: eosc-synergy.eu { \"OS-FLV-DISABLED:disabled\": false, \"OS-FLV-EXT-DATA:ephemeral\": 0, \"access_project_ids\": null, \"disk\": 40, \"id\": \"a8082202-f647-4d1f-9b97-4f5ddb38ae8e\", \"name\": \"gpu1cpu2\", \"os-flavor-access:is_public\": false, \"properties\": \"Accelerator:Version='0', Accelerator:Memory='5', Accelerator:Model='Tesla K20m', Accelerator:Number='1.0', Accelerator:Type='GPU', Accelerator:Vendor='NVIDIA', Accelerator:VirtualizationType='PCI passthrough', pci_passthrough:alias='GPU:1'\", \"ram\": 8192, \"rxtx_factor\": 1.0, \"swap\": \"\", \"vcpus\": 2 } ","categories":"","description":"Configuring GPU flavors\n","excerpt":"Configuring GPU flavors\n","ref":"/providers/cloud-compute/openstack/gpu/","tags":"","title":"GPU Flavors"},{"body":"Document control    Property Value     Title MAN05 top and site BDII High Availability   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement Deploying top or site BDII service in High Availability   Owner SDIS team    Top BDII and Site BDII with High Availability This document objective is to provide guidelines to improve the availability of the information system, addressing three main areas:\n Requirements to deploy a top or site BDII service High Availability from a client perspective Configuration of a High Availability top or site BDII service  Requirements to deploy a top or site BDII service Hardware  dual core CPU 10GB of hard disk space 2-3 GB RAM. If you decide to set BDII_RAM_DISK=yes in your YAIM configuration, it’s advisable to have 4GB of RAM.  Co-hosting  Due to the critical nature of the information system with respect to the operation of the grid, the top or site BDII should be installed as a stand-alone service to ensure that problems with other services do not affect the BDII. In no circumstances should the BDII be co-hosted with a service which has the potential to generate a high load.  Physical vs Virtual Machines  There is no clear vision on this topic. Some managers complain that there are performance issues related to deploying a top or site BDII service under a virtual machine. Others argue that such performance issues are related to the configuration of the service itself. The only agreed feature is that the management and disaster recovery of any service deployed under a virtual machine is more flexible and easier. This could be an important point to take into account considering the critical importance of the top or site BDII service.  Best practices from a client perspective for top BDII  In gLite 3.2 and EMI you can set up redundancy of top BDIIs for the clients (WNs and UIs) setting up a list of top BDII instances to support the automatic failover in the GFAL clients. If the first Top level BDII fails to be contacted, the second will be used in its place, and so on. This mechanism is implemented defining the BDII_LIST YAIM variable according to the following syntax:  BDII_LIST=topbdii.domain.one:2170[,topbdii.domain.two:2170[...]].  After running YAIM, the client enviroment should contain the following definition:  LCG_GFAL_INFOSYS=topbdii.domain.one:2170,topbdii.domain.two:2170   The data management tools (lcg_utils) contact the information system for every operation (lcg-cr, lcg-cp, …). So, if you have your client properly configured with redundancy for the information system, the lcg_utils tools will use that mechanism in a transparent way. Be aware that lcg-infosites doesn’t work with multiple BDIIs. Only gfal, lcg_utils, lcg-info and glite-sd-query.\n  Site administrators should configure their services with this failover mechanism where the first top BDII of the list should be the default top BDII provided by their NGI.\n  Best practices for a top or site BDII High Availability service  The best practice proposal to provide a high availability site or top BDII service is based on two mechanisms working as main building blocks:   DNS round robin load balancing A fault tolerance DNS Updater  We will provide a short introduction to some of these DNS mechanisms but for further information on specific implementations, please contact your DNS administrator.\nDNS round robin load balancing   Load balancing is a technique to distribute workload evenly across two or more resources. A load balancing method, which does not necessarily require a dedicated software or hardware node, is called round robin DNS.\n  We can assume that all transactions (queries to top or site BDII generate the same resource load. For an effective load balancing, all top or site BDII instances should have the same hardware configurations. In other case, a load balancing arbiter is needed.\n  Simple round robin DNS load balancing is easy to deploy. Assuming that there is a primary DNS server (dns.domain.tld) where the DNS load balancing will be implemented, one simply has to add multiple A records mapping the same hostname to multiple IP addresses under the core.top.domain DNS zone. It is equally applicable to site BDII.\n  # In dns.domain.tld: Add multiple A records mapping the same hostname to multiple IP addresses Zone core.domain.tld topbdii.core.domain.tld IN A x.x.x.x topbdii.core.domain.tld IN A y.y.y.y topbdii.core.domain.tld IN A z.z.z.z   The 3 records are always served as answer but the order of the records will rotate in each DNS query\n  This does NOT provide fault tolerance against problems in the top or site BDIIs themselves\n   if one top or site BDII fails its DNS “A” record will still be served one in each three DNS queries will provide the failed top or site BDII first answer  Fault tolerance DNS Updater  The DNS Updater is a mechanism (to be implemented by you) which tests the different top or site BDIIs and decides to remove or add DNS entries through DNS dynamic updates. The fault tolerance is implemented by dynamically nsupdate introduced in bind V8 offers the possibility of changing DNS records dynamically:   The nsupdate tool connects to a bind server on port 53 (TCP or UDP) and can update zone records Updates are authorized based on keys Updates can only be performed on the DNS primary server In the DNS bind implementation, the entire zone is rewritten by the DNS server upon “stop” to reflect the changes. Therefore, the zone should not be managed manually; and the changes are kept in a zone journal file until a “stop” happens.  Implementation  There are several alternatives to implement the DNS Updater:   NAGIOS based tests a demonized service scripts running as crons  What to test: BDII metrics  Status information about the BDII is available by querying the o=infosys root for the UpdateStats object. This entry contains a number of metrics relating to the latest update such as the time to update the database and the total number of entries. And example of such entry is shown below.  $ ldapsearch -x -h \u003cTopBDII/siteBDII\u003e -p 2170 -b \"o=infosys\" (...) dn: Hostname=localhost,o=infosys objectClass: UpdateStats Hostname: lxbra2510.cern.ch FailedDeletes: 0 ModifiedEntries: 4950 DeletedEntries: 1318 UpdateTime: 150 FailedAdds: 603 FailedModifies: 0 TotalEntries: 52702 QueryTime: 8 NewEntries: 603 DBUpdateTime: 11 ReadTime: 0 PluginsTime: 4 ProvidersTime: 113  More extensive information can be obtained (modifyTimestamp,createTimestamp) adding the +:  $ ldapsearch -x -h \u003cTopBDII/siteBDII\u003e -p 2170 -b \"o=infosys\" + (...) # localhost, infosys dn: Hostname=localhost,o=infosys structuralObjectClass: UpdateStats entryUUID: 09bf40e0-7b23-4992-af55-fd74f036a454 creatorsName: o=infosys createTimestamp: 20110612223435Z entryCSN: 20110615120723.216201Z#000000#000#000000 modifiersName: o=infosys modifyTimestamp: 20110615120723Z entryDN: Hostname=localhost,o=infosys subschemaSubentry: cn=Subschema hasSubordinates: FALSE  The following table shows the meaning of the most relevant metrics:     Metric Desciption     ModifiedEntries The number of objects to modify   DeletedEntries The number of objects to delete   UpdateTime To total update time in seconds   FailedAdds The number of add statements which failed   FailedModifies The number of modify statements which failed   TotalEntries The total number of entries in the database   QueryTime The time taken to query the database   NewEntries The number of new objects   DBUpdateTime The time taken to update the database in seconds   ReadTime The time taken to read the LDIF sources in seconds   PluginsTime The time taken to run the plugins in seconds   ProvidersTime The time taken to run the information providers in seconds      Previous BDII metrics can be checked to take a decision regarding the reliability and availability of a top or site BDII instance.\n  More information is available in gLite-BDII_top Monitoring.\n  DNS caching  DNS records obtained in queries are cached by the DNS servers (usually during 24 hours). Therefore to propagate DNS changes fast enough it is important to have very short TTL lifetimes. DNS has not been built to have very short TTL values and these may increase highly the number of queries and as result increase the load of the DNS server The TTL lifetime to be used will have to be tested. If the top BDII are only used by sites in the region and if queries are only from the DNS servers of these few sites then the number of queries may be low enough to allow for a very small TTL. This value should not be lower than 30s - 60s.  Example 1: The IGI Nagios based mechanism   In IGI, the DNS update of the number of instances participating in the DNS round robin mechanism depends on the results provided by a Nagios instance.\n  When Nagios needs to check the status of a service it will execute a plugin and pass information about what needs to be checked. The plugin verifies the operational state of the service and reports the results back to the Nagios daemon.\n  Nagios will process the results of the service check and take appropriate action as necessary (e.g. send notifications, run event handlers, etc).\n  Each instance is checked every 5 minutes. If a failure occurs, Nagios runs the event handler to restart the BDII service AND remove the instance from the DNS round robin set using dnsupdate:\n   an email is sent as notification; If 4 (out of 5) instances are failing, a SMS message is sent as notification;   If a failed instance appears to be restored, Nagios will re-add it to the DNS round robin mechanism.   This approach has some single points of failures:   The Nagios instance can fail The master DNS where the DNS entries are updated can fail  Example 2: The IBERGRID scripting based mechanism   In IBERGRID, an application (developed by LIP) verifies the health of each top BDII. The application can connect to the DNS servers and remove the “A” records of top BDIIs that become unavailable (non responsive to tests).\n  The monitoring application (nsupdater) is a simple program that performs tests, and based on their result acts upon DNS entries\n   Written in perl Can be run as daemon or at the command prompt The tests are programs that are forked Tests are added in a “module” fashion way Can be used to manage several DNS round robin scenarios Can manage multiple DNS servers   To remove the DNS single point of failure as in previous example, one could configure all DNS servers serving the core.ibergrid.eu domain as primary   Three primary servers would then exist for core.ibergrid.eu All three DNS servers could be dynamically updated independently The monitoring application should also have three instances, one running at each site The downside is that DNS information can become incoherent. It would be up to the monitoring application to manage the three DNS servers content and their coherence  ","categories":"","description":"Deploying the BDII service in High Availability","excerpt":"Deploying the BDII service in High Availability","ref":"/providers/operations-manuals/man05_top_and_site_bdii_high_availability/","tags":"","title":"MAN05 top and site BDII High Availability"},{"body":"What is it? EGI Service Monitoring keeps an eye on the performance of the EGI services to quickly detect and resolve issues.\nThe service monitors the infrastructure by collecting data generated by functional probes. The raw data is merged into statistics and available through the user interface in a user-friendly way. It provides automated reporting tools with minimal development or operational effort for setting up monitoring.\nNote Documentation for Service Monitoring is available on the ARGO site.  ","categories":"","description":"Monitor performance of EGI services\n","excerpt":"Monitor performance of EGI services\n","ref":"/internal/monitoring/","tags":"","title":"Service Monitoring"},{"body":"Information to provide when registering a new service (the fields marked with an asterisk are mandatory):\n Hosting Site * (select the appropriate RC under which you are registering the service) Service Type * (select the appropriate service type) Service URL (Alphanumeric and $-_.+!*'(),:) Hostname * (valid FQDN format) Host IP a.b.c.d Host IPv6 (0000:0000:0000:0000:0000:0000:0000:0000[/int]) (optional [/int] range) Host DN (/C=…/OU=…/…) Description * (Alphanumeric and basic punctuation) Host Operating System (Alphanumeric and basic punctuation) Host Architecture (Alphanumeric and basic punctuation) Is it a beta service (formerly PPS service)? Y/N Is this service in production? Y/N Is this service monitored? Y/N Contact email * (valid email format)  Scope Tags  ✓ Optional Tags (At least 1 optional tags must be selected): EGI Local FedCloud ✓ Reserved Tags Inheritable from Parent Site: none ✓ Reserved Tags Directly Assigned (WARNING - If deselected you will not be able to reselect the tag - it will be moved to the ‘Protected Reserved Tags’ list): none ✗ Protected Reserved Tags (Can only be assigned on request): alice atlas cms elixir lhcb tier1 tier2 wlcg  ","categories":"","description":"Information required for registering a service into the Configuration Database","excerpt":"Information required for registering a service into the Configuration …","ref":"/internal/configuration-database/service-registration-requirements/","tags":"","title":"Service registration requirements"},{"body":"OpenStack providers of the EGI Cloud Compute service offer native OpenStack features via native APIs integrated with EGI Check-in accounts.\nThe extensive OpenStack user documentation includes details on every OpenStack project, most providers offer access to:\n Keystone, for identity Nova, for VM management Glance, for VM image management Cinder, for block storage Swift, for object storage Neutron, for network management Horizon, as a web dashboard  Web-dashboard of the integrated providers can be accessed using your EGI Check-in credentials directly: select OpenID Connect or EGI Check-in in the Authenticate using drop-down menu of the login screen. You can explore Horizon dashboard documentation for more information on how to manage your resources from the browser. The rest of this guide will focus on CLI/API access.\nInstallation The OpenStack client is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure.\nLinux / Mac   Windows    Installation of the OpenStack client can be done using:\npip install openstackclient   As there are non-pure Python packages needed for installation, the Microsoft C++ Build Tools is a prerequisite, please make sure it’s installed with the following options selected:\n C++ CMake tools for Windows C++ ATL for latest v142 build tools (x86 \u0026 x64) Testing tools core features - Build Tools Windows 10 SDK (\u003clatest\u003e)  In case you prefer to use non-Microsoft alternatives for building non-pure packages, please see here.\nInstallation of the OpenStack client can be done using:\npip install openstackclient    Add IGTF CA to python's CA store:\ncat /etc/grid-security/certificates/*.pem \u003e\u003e $(python -m requests.certs) Authentication Check the documentation at the authentication and authorisation section on how to get the right credentials for accessing the providers.\nOpenStack token for other clients Most OpenStack clients allow authentication with tokens, so you can easily use them with EGI Cloud providers just doing a first step for obtaining the token. With the OpenStack client you can use the following command to set the OS_TOKEN variable with the needed token:\n$ OS_TOKEN=$(openstack --os-auth-type v3oidcaccesstoken \\  --os-protocol openid --os-identity-provider egi.eu \\  --os-auth-url \u003ckeystone url\u003e \\  --os-access-token \u003cyour access token\u003e \\  --os-project-id \u003cyour project id\u003e token issue -c id -f value) You can easily obtain an OpenStack token with the fedcloud client:\nfedcloud openstack --site \u003cNAME_OF_THE_SITE\u003e --vo \u003cNAME_OF_VO\u003e token issue Useful commands with OpenStack CLI Usage of the OpenStack client is described in detail here.\nPlease refer to the nova documentation for a complete guide on the VM management features of OpenStack. We list in the sections below some useful commands for the EGI Cloud.\nRegistering an existing ssh key It's possible to register an ssh key that can later be used as the default ssh key for the default user of the VM (via the --key-name argument to openstack server create:\nopenstack keypair create --public-key ~/.ssh/id_rsa.pub mykey Creating a VM openstack flavor list FLAVOR=\u003cFLAVOR_NAME\u003e openstack image list IMAGE_ID=\u003cIMAGE_ID\u003e openstack network list # Pick FedCloud network NETWORK_ID=\u003cNETOWRK_ID\u003e openstack security group list openstack server create --flavor $FLAVOR --image $IMAGE_ID \\  --nic net-id=$NETWORK_ID --security-group default \\  --key-name mykey oneprovider # Creating a floating IP openstack floating ip create \u003cNETOWRK_NAME\u003e # Assigning floating IP to server openstack server add floating ip \u003cSERVER_ID\u003e \u003cIP\u003e # Removing floating IP from server openstack server show \u003cSERVER_ID\u003e # Deleting server openstack server remove floating ip \u003cSERVER_ID\u003e \u003cIP\u003e openstack server delete \u003cSERVER_ID\u003e # Deleting floating IP openstack floating ip delete \u003cIP\u003e  OpenStack: launch an instance on the provider network OpenStack: Manging IP addresses  Using cloud-init openstack server create --flavor m3.medium \\  --image d0a89aa8-9644-408d-a023-4dcc1148ca01 \\  --user-data userdata.txt --key-name My_Key server01.example.com  OpenStack: providing user data (cloud-init) cloudinit documentation  Shell script data as user data #!/bin/sh adduser --disabled-password --gecos \"\" clouduser cloud-config data as user data #cloud-confighostname:mynodefqdn:mynode.example.commanage_etc_hosts:true Official cloud-config examples Cloud-init example  Creating a snapshot image from running VM You can create a new image from a snapshot of an existing VM that will allow you to easily recover a previous version of your VM or to use it as a template to clone a given VM.\nopenstack server image create \u003cyour VM\u003e --name \u003cname of the snapshot\u003e Once the snapshot is ready (openstack image show \u003cname of the snapshot\u003e will give your the details you can use it as any other image at the provider:\nopenstack server create --flavor \u003cflavor\u003e \\  --image \u003cname of the snapshot\u003e \\  \u003cname of the new VM\u003e You can override files in the snapshot if needed, e.g. changing the SSH keys:\nopenstack server create --flavor \u003cflavor\u003e \\  --image \u003cname of the snapshot\u003e \\  --file /home/ubuntu/.ssh/authorized_keys=my_new_keys \\  \u003cname of the new VM\u003e Terraform Terraform supports EGI Cloud OpenStack providers by using valid access tokens for Keystone. For using this, just configure your provider as usual in Terraform, but do not include user/password information:\n# Configure the OpenStack Provider provider \"openstack\" { project_id = \"\u003cyour project id\u003e\" auth_url = \"http://\u003cyour keystone url\u003e/v3\" }# Create a server resource \"openstack_compute_instance_v2\" \"test-server\" {# ... } when launching Terraform, set the OS_TOKEN environment variable to a valid token as shown in :ref:OpenStack token for other clients. You may also set the Keystone URL and project ID in the OS_AUTH_URL and OS_PROJECT_ID environment variables:\nprovider \"openstack\" { } data \"openstack_images_image_v2\" \"ubuntu16\" { most_recent = true properties { APPLIANCE_MPURI = \"https://appdb.egi.eu/store/vo/image/8df7ba00-8467-57aa-bf1e-05754a2a73bf:6428/\" } } data \"openstack_compute_flavor_v2\" \"small\" { vcpus = 1 ram = 2048 disk = 20 } resource \"openstack_compute_instance_v2\" \"vm\" { name = \"testvm\" image_id = \"${data.openstack_images_image_v2.ubuntu16.id}\" flavor_id = \"${data.openstack_compute_flavor_v2.small.id}\" security_groups = [\"default\"] } # this will export OS_AUTH_URL and OS_PROJECT_ID $ eval \"$(fedcloud site show-project-id --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e)\" # now get a valid token $ export OS_TOKEN=$(fedcloud openstack --site \u003cNAME_OF_SITE\u003e --vo \u003cNAME_OF_VO\u003e \\  token issue -c id -f value) $ terraform plan Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.openstack_compute_flavor_v2.small: Refreshing state... data.openstack_images_image_v2.ubuntu16: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: + openstack_compute_instance_v2.vm id: \u003ccomputed\u003e access_ip_v4: \u003ccomputed\u003e access_ip_v6: \u003ccomputed\u003e all_metadata.%: \u003ccomputed\u003e availability_zone: \u003ccomputed\u003e flavor_id: \"2\" flavor_name: \u003ccomputed\u003e force_delete: \"false\" image_id: \"ceb0434d-37af-4d1f-9efe-13f6f9937df2\" image_name: \u003ccomputed\u003e name: \"testvm\" network.#: \u003ccomputed\u003e power_state: \"active\" region: \u003ccomputed\u003e security_groups.#: \"1\" security_groups.3814588639: \"default\" stop_before_destroy: \"false\" Plan: 1 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. Note that as in the example above you can get images using information from AppDB if needed.\nlibcloud Apache libcloud supports OpenStack and EGI authentication mechanisms by setting the ex_force_auth_version to 3.x_oidc_access_token or 2.0_voms respectively. Check the libcloud docs on connecting to OpenStack for details. See below two code samples for using them\nOpenID Connect import requests from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver refresh_data = { 'client_id': '\u003cyour client_id\u003e', 'client_secret': '\u003cyour client_secret\u003e', 'grant_type': 'refresh_token', 'refresh_token': '\u003cyour refresh_token\u003e', 'scope': 'openid email profile', } r = requests.post(\"https://aai.egi.eu/oidc/token\", auth=(client_id, client_secret), data=refresh_data) access_token = r.json()['access_token'] OpenStack = get_driver(Provider.OPENSTACK) # first parameter is the identity provider: \"egi.eu\" # Second parameter is the access_token # The protocol 'openid' is specified in ex_tenant_name # and tenant/project cannot be selected :( driver = OpenStack('egi.eu', access_token, ex_tenant_name='openid', ex_force_auth_url='https://keystone_url:5000', ex_force_auth_version='3.x_oidc_access_token') VOMS from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver OpenStack = get_driver(Provider.OPENSTACK) # assume your proxy is available at /tmp/x509up_u1000 # you can obtain a proxy with the voms-proxy-init command # no need for username driver = OpenStack(None, '/tmp/x509up_u1000', ex_tenant_name='EGI_FCTF', ex_force_auth_url='https://sbgcloud.in2p3.fr:5000', ex_force_auth_version='2.0_voms') ","categories":"","description":"How to interact with the OpenStack providers APIs in the EGI Cloud\n","excerpt":"How to interact with the OpenStack providers APIs in the EGI Cloud\n","ref":"/users/cloud-compute/openstack/","tags":"","title":"Using OpenStack providers"},{"body":"In this page you can find a summary of the needed steps for supporting a new VO in your OpenStack infrastructure.\nLocal project creation The usual method of supporting a VO is by creating a local project for it. You should assign quotas to this project as agreed in the OLA defining the support for the given VO.\n  Create a group where users belongig to the VO will be mapped to: :\ngroup_id=$(openstack group create -f value -c id \u003cnew_group\u003e)   Add that group to the desired local project: :\n$ openstack role add member --group $group_id --project \u003cyour project\u003e   Keystone Mapping Expand your mapping.json with the VO membership to the created group (substitute group_id and entitlement as appropriate). The expected mappings for the VOs are listed in vo-mappings.yaml of fedcloud-catchall-operations repository:\n[ \u003cexisting mappings\u003e, { \"local\": [ { \"user\": { \"name\": \"{0}\" }, \"group\": { \"id\": \"\u003cgroup_id\u003e\" } } ], \"remote\": [ { \"type\": \"HTTP_OIDC_SUB\" }, { \"type\": \"HTTP_OIDC_ISS\", \"any_one_of\": [ \"https://aai.egi.eu/oidc/\" ] }, { \"type\": \"OIDC-eduperson_entitlement\", \"regex\": true, \"any_one_of\": [ \"^\u003centitlement\u003e$\" ] } ] } ] And update the mapping in your Keystone IdP:\n$ openstack mapping set --rules mapping.json egi-mapping Accounting Add the project supporting the VO to cASO:\n  In the projects field of /etc/caso/caso.conf :\nprojects = vo_project1, vo_project2, \u003cyour_new_vo_project\u003e   and as a new mapping in /etc/caso/voms.json :\n{ \"\u003cyour new vo\u003e\": { \"projects\": [\"\u003cyour new vo project\u003e\"] } }   Be sure to include the user running cASO as member of the project if it does not have admin privileges:\nopenstack role add member --user \u003cyour caso user\u003e --project \u003cyour new vo project\u003e Information system Add the mapping to your site configuration with a new Pull Request to the fedcloud-catchall-operations repository\n---vos:- name:\u003cvo name\u003eauth:project_id:\u003cyour new vo project\u003eVM Image Management cloudkeeper-core Add the new image list to the cloudkeeper configuration in /etc/cloudkeeper/cloudkeeper.yml (or /etc/cloudkeeper/image-lists.conf if using the appliance), new entry should look similar to:\nhttps://\u003cAPPDB_TOKEN\u003e:x-oauth-basic@vmcaster.appdb.egi.eu/store/vo/\u003cyour new vo\u003e/image.list\ncloudkeeper-os Add the user configured in cloudkeeper-os as member of the new project:\n$ openstack role add member \\  --user \u003cyour cloudkeeper-os user\u003e \\  --project \u003cyour new vo project\u003e Add the mapping of the project to the VO in /etc/cloudkeeper-os/mapping.json:\n{ \"\u003cyour new vo\u003e\": { \"tenant\": \"\u003cyour new vo project\u003e\" } } ","categories":"","description":"Summary of steps for configuring new VOs in OpenStack\n","excerpt":"Summary of steps for configuring new VOs in OpenStack\n","ref":"/providers/cloud-compute/openstack/vo_config/","tags":"","title":"VO Configuration guide"},{"body":"Overview The EGI DataHub allows to:\n Bring data close to the computing to exploit it efficiently. Publish a dataset and make it available to a specific community or worldwide across federated sites.  The main features offered by the DataHub are:\n Discovery of data via a central portal. Access to data conforming to required policies which may be:  unauthenticated open access; access after user registration or access restricted to members of a Virtual Organization (VO).\nThis access may be via a GUI (e.g. a webpage) or an API (e.g. programmatic access to the data)   Replication of data from data providers for resiliency and availability purposes. Replication may take place either on­demand or automatically. Replication will require the introduction of a file catalogue to enable tracking of logical and physical copies of data. Access to data from the AppDB to enable VOs to associate appropriate data with matching Virtual Appliances Authentication and Authorization Infrastructure (AAI) integration between the EGI DataHub and with other EGI components and with user communities existing infrastructure File catalog to track replication of data: logical and physical file  It is based on the OneData technology.\nMotivations  Putting up a (scalable) distributed data infrastructure needs specific expertise, resources and knowledge No easy way to discover and transfer data No easy way of making data (publicly) accessible without transferring it with a sharing service No easy way of combining multiple datasets from different data providers Users need to access data locally and from compute resources  Components and concepts  Space  a virtual volume where users will organize their data. A space is supported by one or multiple Oneproviders providing actual storage resources\n Onezone  a central component federating providers. It takes care of Authentication and Authorization and other management tasks (like space creation). EGI DataHub is a Onezone instance.\n EGI DataHub  the central Onedata Onezone instance of the EGI Federation. Single Sign On (SSO) with all the connected storage providers (Oneprovider) is guaranteed through EGI Check-in\n Oneprovider  a data management component deployed in the data centres, provisioning data and managing transfers. A Oneprovider is typically deployed at a site near the local storage resources, and can access local storage resources over multiple connectors (CEPH, POSIX,...). A default one is operated for EGI by CYFRONET.\n Oneclient  a client application providing access to the spaces through a FUSE mount point (local POSIX access). Spaces are accessible as if they were part of the local file system. Oneclient can be used from VM, containers, desktop,...\nWeb interfaces and APIs are also available\n  Highlighted features Using the EGI DataHub web interface it's possible to manage the space.\nUsing Oneclient it's possible to mount a space locally, and access it over a POSIX interface, using files as they were stored locally. The file's blocks are downloaded on demand.\nIn Onedata the file distribution is done on a block basis, blocks will be replicated on the fly, and it's possible to instrument the replication.\nThree different formats of metadata can be attached to files: basic (key/value), JSON and RDF. The metadata can be managed using the Web interface and the APIs. It's also possible to create indexes and query them.\nIt's possible to view the popularity of a file and manage smart caching.\n","categories":"","description":"Documentation related to [EGI DataHub Service](https://www.egi.eu/services/datahub/)","excerpt":"Documentation related to [EGI DataHub …","ref":"/users/datahub/","tags":"","title":"DataHub"},{"body":"Once the site services are registered in GOCDB (and flagged as \"monitored\") they will appear in the EGI service monitoring tools. EGI will check the status of the services (see Infrastructure Status for details). Check if your services are present in the EGI service monitoring tools and passing the tests; if you experience any issues (services not shown, services are not OK...) please contact back EGI Operations or your reference Resource Infrastructure.\nExtra checks for your installation:\n  Check in ARGO-Mon2 that your services are listed and are passing the tests. If all the tests are OK, your installation is already in good shape.\n  Check that you are publishing cloud information in your site BDII: :\nldapsearch -x -h \u003csite bdii host\u003e -p 2170 -b Glue2GroupID=cloud,Glue2DomainID=\u003cyour site name\u003e,o=glue   Check that all the images listed in the AppDB for the VOs you support (e.g. AppDB page for fedlcoud.egi.eu VO) are listed in your BDII. This sample query will return all the template IDs registered in your BDII: :\nldapsearch -x -h \u003csite bdii host\u003e -p 2170 -b Glue2GroupID=cloud,Glue2DomainID=\u003cyour site name\u003e,o=glue objectClass=GLUE2ApplicationEnvironment GLUE2ApplicationEnvironmentRepository   Try to start one of those images in your cloud. You can do it with [onetemplate instantiate]{.title-ref} or OCCI commands, the result should be the same.\n  Execute the site certification manual tests against your endpoints.\n  Check in the accounting portal that your site is listed and the values reported look consistent with the usage of your site.\n  ","categories":"","description":"Validate your installation\n","excerpt":"Validate your installation\n","ref":"/providers/cloud-compute/validation/","tags":"","title":"Installation Validation"},{"body":"Document control    Property Value     Title MAN06 Failover for MySQL grid-based services   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement Implementing failover of MySQL grid-based services.   Owner SDIS team    Introduction   Several critical grid services such as the Logical File Catalogue (LFC) or the VO Management Service (VOMS) server represent single points of failure in a grid infrastructure. When unavailable, a user can no longer access to the infrastructure since it is prevented from issuing new proxies, or is no longer able to access to the physical location of his data.\n  However, those services rely on MySQL backends which opens a window to replicate the relevant databases to different / backup services which could be used when the primary instances are unavailable. The MySQL DB replication process is one of the ways to get scalability and higher availability.\n  Architecture In this document we propose to follow a Master-Slave architecture for the MySQL replication, consisting in keeping DB copies of a main host (MASTER) in a secondary host (SLAVE). The slave host will only have read access to the Database entries.\nSecurity   For better availability, it is preferable to deploy the Master and the Slave services in different geographical locations, which normally means exposing the generated traffic to the internet. In that case, you will have to find a mechanism to encrypt the communication between the two hosts.\n  In this document, we propose to use Stunnel:\n   Stunnel is a free multi-platform computer program, used to provide universal TLS/SSL tunneling service. Stunnel can be used to provide secure encrypted connections for clients or servers that do not speak TLS or SSL natively. It runs on a variety of operating systems, including most Unix-like operating systems and Windows. Stunnel relies on a separate library such as OpenSSL or SSLeay to implement the underlying TLS or SSL protocol. Stunnel uses Public-key cryptography with X.509 digital certificates to secure the SSL connection. Clients can optionally be authenticated via a certificate too For more references, please check www.stunnel.org   There are other possibilities for encryption, like enabling SSL Support directly in MySQL, but these approach was not tested. Details can be obtained here.  MySQL replication Assumptions We are assuming that both grid services instances were previously installed and configured (manually or via YAIM) so that they support the same VOs.\nGeneric information    Description Value     MASTER hostname server1.domain.one   MASTER username root   MASTER mysql superuser root   MASTER mysql replication user db_rep   SLAVE hostname server2.domain.two   SLAVE username root   SLAVE mysql superuser root   DB to replicate DB_1 DB_2 …    Setup the MySQL MASTER for replication  Step 1: Install stunnel. It is available in the SL5 repositories.  $ yum install stunnel (...) $ date; rpm -qa | grep stunnel Wed Jun 15 16:00:23 WEST 2011 stunnel-4.15-2.el5.1  Step 2: Configure stunnel (via /etc/stunnel/stunnel.conf) to:   Accept incoming connections on port 3307, and allow to connect to port 3306 Use the server X509 certificates to encrypt data  $ cat /etc/stunnel/stunnel.conf # Authentication stuff verify = 2 CApath = /etc/grid-security/certificates cert = /etc/grid-security/hostcert.pem key = /etc/grid-security/hostkey.pem # Auth fails in chroot because CApath is not accessible #chroot = /var/run/stunnel #debug = 7 output = /var/log/stunnel.log pid = /var/run/stunnel/master.pid setuid = nobody setgid = nobody # Service-level configuration [mysql] accept = 3307 connect = 127.0.0.1:3306  Step 3: Start the stunnel service and add it to rc.local  $ mkdir /var/run/stunnel $ chown nobody:nobody /var/run/stunnel $ stunnel /etc/stunnel.conf $ echo 'stunnel /etc/stunnel.conf' \u003e\u003e /etc/rc.local  Step 4: Setup the firewall to allow connections from the SLAVE instance (server2.domain.two with IP XXX.XXX.XXX.XXX) on port 3307 TCP  # MySQL replication -A INPUT -s XXX.XXX.XXX.XXX -m state --state NEW -m tcp -p tcp \\  --dport 3307 -j ACCEPT  Step 5: Configure MySQL (via /etc/my.cnf) setting up the Master server-id (usually 1), and declaring the path for the MySQL binary log and the DBs to be replicated. Please make sure that the path declared under log-bin has the mysql:mysql ownerships, and that the binary log exists.  $ cat /etc/my.cnf (...) [mysqld] server-id=1 log-bin = /path_to_log_file/log_file.index binlog-do-db=DB_2 binlog-do-db=DB_1  Step 6: Restart MySQL  $ /etc/init.d/mysqld restart  Step 7: Add a specific user for the MySQL replication  mysql \u003e GRANT REPLICATION SLAVE ON *.* TO 'db_rep'@'127.0.0.1' \\  IDENTIFIED BY '16_char_password';  Step 8: Backup the databases using the following command. Please note tha the option --master-data=2 writes a comment on dump.sql that shows the log file and the ID to be used on the Slave setup. This option also locks all the tables while they are being copied, avoiding problems.  $ mysqldump -u root -p --default-character-set=latin1 \\  --master-data=2 --databases DB_1 DB_2 \u003e dump.sql Setup the MySQL SLAVE for replication Step 1: Configure stunnel (via /etc/stunnel/stunnel.conf) to:\n Accept incoming SSL connections on port 3307, and allow to connect to server1.domain.one on port 3307 Use the server X509 certificates to encrypt data  $ cat /etc/stunnel/stunnel.conf # Authentication stuff verify = 2 CApath = /etc/grid-security/certificates cert = /etc/grid-security/hostcert.pem key = /etc/grid-security/hostkey.pem # Auth fails in chroot because CApath is not accessible #chroot = /var/run/stunnel #debug = 7 output = /var/log/stunnel.log pid = /var/run/stunnel/master.pid setuid = nobody setgid = nobody # Use it for client mode client = yes # Service-level configuration [mysql] accept = 127.0.0.1:3307 connect = server1.domain.one:3307  Step 2: Start stunnel and add it to rc.local  $ mkdir /var/run/stunnel $ chown nobody:nobody /var/run/stunnel $ stunnel /etc/stunnel.conf $ echo 'stunnel /etc/stunnel.conf' \u003e\u003e /etc/rc.local  Step 3: Configure MySQL to include the SLAVE server-id (typically 2) and the replicated databases  $ cat /etc/my.cnf (...) [mysqld] server-id=2 replicate-do-db=DB_1 replicate-do-db=DB_2  Step 4: Restart MySQL and insert the dump.sql created on the Master  $ /etc/init.d/mysql restart $ mysql -u root -p \u003c dump.sql  Step 5: Start the Slave logging in the mysql of slave, and running the following queries, changing the values xxxxx and yyyyy by the values on top of dump.sql file:  mysql \u003e CHANGE MASTER TO MASTER_HOST='127.0.0.1', MASTER_PORT=3307, \\  MASTER_USER='db_rep', MASTER_PASSWORD='16_char_password', \\  MASTER_LOG_FILE='xxxxx', MASTER_LOG_POS=yyyyy; mysql \u003e SLAVE START;  Step 6: Your replication should be up and running. In case of troubles, check the Troubleshooting section.  Troubleshooting On the Slave  Check if you can connect to the Master on port 3307 from the Slave  #root@server2.domain.two]$ telnet server1.domain.one 3307  Check the /var/log/stunnel.log on the Slave to see if stunnel is working. For established connections, you should find a message like:  2011.05.30 11:57:45 LOG5[9000:1076156736]: mysql connected from XXX.XXX.XXX.XXX:PORTY 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=1, /DC=COUNTRY/DC=CA/CN=NAME 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=0, /DC=COUNTRY/DC=CA/O=INSTITUTE/CN=host/HOSTNAME  Check the MySQL process list. You should get an answer like the one bellow:  mysql\u003e SHOW PROCESSLIST; +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ | Id | User | Host | db | Command | Time | State | Info | +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+ | 1 | system user | | NULL | Connect | 1236539 | Waiting for master to send event | NULL | | 2 | system user | | NULL | Connect | 90804 | Slave has read all relay log; waiting for the slave I/O thread to update it | NULL | +----+-------------+------+------+---------+---------+-----------------------------------------------------------------------------+------+  Check the status of the slave  mysql\u003e SHOW SLAVE STATUS; +----------------------------------+-------------+------------------+---------------------+ | Slave_IO_State | Master_Port | Master_Log_File | Read_Master_Log_Pos | +----------------------------------+-------------+------------------+---------------------+ | Waiting for master to send event | 3307 | mysql-bin.000001 | 126167682 | +----------------------------------+-------------+------------------+---------------------+ On the Master  Check the /var/log/stunnel.log on the Slave to see if stunnel is working. For established connections, you should find a message like:  2011.05.30 11:57:45 LOG5[9000:1076156736]: mysql connected from XXX.XXX.XXX.XXX:PORTY 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=1, /DC=COUNTRY/DC=CA/CN=NAME 2011.05.30 11:57:45 LOG5[9000:1076156736]: VERIFY OK: depth=0, /DC=COUNTRY/DC=CA/O=INSTITUTE/CN=host/HOSTNAME  Check if you have established connections from the Slave on port 3307  $ netstat -tapn | grep 3307 tcp 0 0 0.0.0.0:3307 0.0.0.0:* LISTEN 9000/stunnel tcp 0 0 XXX.XXX.XXX.XXX:3307 YYY.YYY.YYY.YYY:34378 ESTABLISHED 9000/stunnel  Check the Master status on MySQL. You should get an answer like:  mysql\u003e SHOW MASTER STATUS; +------------------+-----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+-----------+--------------+------------------+ | mysql-bin.000001 | 126167682 | DB_1 | | +------------------+-----------+--------------+------------------+ 1 row in set (0.00 sec)  Check the MySQL process list. You should get an answer like the one bellow:  mysql\u003e SHOW PROCESSLIST; +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ | Id | User | Host | db | Command | Time | State | Info | +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ | 2778 | ibrepifca | localhost:42281 | NULL | Binlog Dump | 1400477 | Has sent all binlog to slave; waiting for binlog to be updated | NULL | +------+-----------+-----------------+------+-------------+---------+----------------------------------------------------------------+------+ The LFC case An working example Using the MySQL replication mechanism, you can setup a read-only LFC which can start to operate if the primary LFC is unavailable. The following workflow tries to demonstrate how the mechanism is working:\n Only the primary LFC is available in the information system  $ lcg-infosites --vo ict.vo.ibergrid.eu lfc lfc01.ncg.ingrid.pt  If LFC_HOST env variable is not defined, lcg_utils will query the information system to get the VO LFC  06/15/11-18:19:53 --\u003e unset LFC_HOST env variable 06/15/11-18:19:53 --\u003e copy data to SE and register it on the catalogue 06/15/11-18:19:53 --\u003e running command: lcg-cr -v --vo ict.vo.ibergrid.eu -d se01-tic.ciemat.es -l lfn:/grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt \u003cfile:/home/ingrid/csys/goncalo/scriptInputData.jdl\u003e Using grid catalog type: lfc Using grid catalog : lfc01.ncg.ingrid.pt Checksum type: None SE type: SRMv2 Destination SURL : srm://se01-tic.ciemat.es/dpm/ciemat.es/home/ict.vo.ibergrid.eu/generated/2011-06-15/filea0e24e4a-4caf-4d31-a871-da0138bca1b6 Source SRM Request Token: da0f6de5-3dad-4d14-bd17-a5ae601043ec Source URL: \u003cfile:/home/ingrid/csys/goncalo/scriptInputData.jdl\u003e File size: 565 VO name: ict.vo.ibergrid.eu Destination specified: se01-tic.ciemat.es Destination URL for copy: gsiftp://se01-tic.ciemat.es/se01-tic.ciemat.es:/storage10/ict.vo.ibergrid.eu/2011-06-15/filea0e24e4a-4caf-4d31-a871-da0138bca1b6.1488163.0 # streams: 1 565 bytes 1.19 KB/sec avg 1.19 KB/sec inst Transfer took 1020 ms Using LFN: lfn:/grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt Using GUID: guid:f602407d-bc36-4c0b-8346-219fb14f830b Registering LFN: /grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt (f602407d-bc36-4c0b-8346-219fb14f830b) Registering SURL: srm://se01-tic.ciemat.es/dpm/ciemat.es/home/ict.vo.ibergrid.eu/generated/2011-06-15/filea0e24e4a-4caf-4d31-a871-da0138bca1b6 (f602407d-bc36-4c0b-8346-219fb14f830b) guid:f602407d-bc36-4c0b-8346-219fb14f830b  We now explicitly switch to the backup LFC (not being published in the information system), and we do see that the file already appears there registered.  `06/15/11-18:20:01 --\u003e export LFC_HOST to backup LFC` `06/15/11-18:20:01 --\u003e running command: export LFC_HOST=ibergrid-lfc.ifca.es` `06/15/11-18:20:01 --\u003e list the backup LFC` `06/15/11-18:20:01 --\u003e running command: lfc-ls /grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt` `/grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt`  We can copy data around reading entries from the backup LFC  06/15/11-18:20:01 --\u003e Copy data using the backup LFC 06/15/11-18:20:01 --\u003e running command: lcg-cp -v --vo ict.vo.ibergrid.eu lfn:/grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt \u003cfile:/home/ingrid/csys/goncalo/scriptInputData.jdl2\u003e Using grid catalog type: LFC Using grid catalog : ibergrid-lfc.ifca.es VO name: ict.vo.ibergrid.eu Checksum type: None Trying SURL srm://se01-tic.ciemat.es/dpm/ciemat.es/home/ict.vo.ibergrid.eu/generated/2011-06-15/filea0e24e4a-4caf-4d31-a871-da0138bca1b6 ... Source SE type: SRMv2 Source SRM Request Token: 3e0be4f1-f9e9-40f4-8f9f-321a115840ba Source URL: /grid/ict.vo.ibergrid.eu/goncalo_borges/mytest.txt File size: 565 Source URL for copy: gsiftp://se01-tic.ciemat.es/se01-tic.ciemat.es:/storage10/ict.vo.ibergrid.eu/2011-06-15/filea0e24e4a-4caf-4d31-a871-da0138bca1b6.1488163.0 Destination URL: \u003cfile:/home/ingrid/csys/goncalo/scriptInputData.jdl2\u003e # streams: 1 0 bytes 0.00 KB/sec avg 0.00 KB/sec inst Transfer took 1030 ms  However, we can not register new files in the backup LFC  06/15/11-18:30:10 --\u003e copy data to SE and register it on the catalogue 06/15/11-18:30:10 --\u003e running command: lcg-cr -v --vo ict.vo.ibergrid.eu -d se01-tic.ciemat.es -l lfn:/grid/ict.vo.ibergrid.eu/goncalo_borges/mytest2.txt \u003cfile:/home/ingrid/csys/goncalo/scriptInputData.jdl\u003e Using grid catalog type: lfc Using grid catalog : ibergrid-lfc.ifca.es Checksum type: None SE type: SRMv2 Destination SURL : srm://se01-tic.ciemat.es/dpm/ciemat.es/home/ict.vo.ibergrid.eu/generated/2011-06-15/file8a30add9-b247-4037-9d42-981728ae3e76 Source SRM Request Token: 4029fcc2-281f-4e9a-a843-e8a3a67623ff Source URL: \u003cfile:/home/ingrid/csys/goncalo/scriptInputData.jdl\u003e File size: 565 VO name: ict.vo.ibergrid.eu Destination specified: se01-tic.ciemat.es Destination URL for copy: gsiftp://se01-tic.ciemat.es/se01-tic.ciemat.es:/storage17/ict.vo.ibergrid.eu/2011-06-15/file8a30add9-b247-4037-9d42-981728ae3e76.1488174.0 # streams: 1 565 bytes 1.28 KB/sec avg 1.28 KB/sec inst Transfer took 1020 ms Using LFN: lfn:/grid/ict.vo.ibergrid.eu/goncalo_borges/mytest2.txt Using GUID: guid:e1b977c7-290a-410f-ad31-be4feeb67b11 Registering LFN: /grid/ict.vo.ibergrid.eu/goncalo_borges/mytest2.txt (e1b977c7-290a-410f-ad31-be4feeb67b11) [LFC][lfc_creatg][] ibergrid-lfc.ifca.es: guid:e1b977c7-290a-410f-ad31-be4feeb67b11: Read-only file system srm://se01-tic.ciemat.es/dpm/ciemat.es/home/ict.vo.ibergrid.eu/generated/2011-06-15/file8a30add9-b247-4037-9d42-981728ae3e76: Registration failed, please register it by hand, when the problem will be solved guid:e1b977c7-290a-410f-ad31-be4feeb67b11 lcg_cr: Communication error on send Swapping between LFCs   Unfortunately the middleware does not offer an automatic way to swap to the backup LFC in case the primary fails. Therefore, it is up to the user (or to the application) to determine if a given LFC is available or not.\n  In the previous example, we have followed the approach where only the primary LFC is available through the information system. One can assume that, if the primary LFC is not available in the information system, it means that either it is down or unreachable. In that case, the user or application could switch to the backup LFC.\n  $ export LFC_HOST=lcg-infosites --vo ict.vo.ibergrid.eu lfc $ if [ \"X$LFC_HOST\" == \"X\" ]; then export LFC_HOST=\u003cbackup LFC FQDN\u003e; fi The VOMS case A working example   It is also possible to deploy a backup VOMS server with a MySQL replica of the main VOMS server. This will enable users to still start proxies even if the main VOMS server is down.\n  The VOMS Admin interface of the backup VOMS server should be switched off so that new users can only request registration via the main VOMS Admin Web interface.\n  User interfaces should be configured to use both VOMS servers\n  $ voms-proxy-init --voms ict.vo.ibergrid.eu Enter GRID pass phrase: Your identity: /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=Goncalo Borges Creating temporary proxy .............................................................. Done Contacting voms01.ncg.ingrid.pt:40008 [/C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=voms01.ncg.ingrid.pt] \"ict.vo.ibergrid.eu\" Done Creating proxy .......................................................................................................................................................... Done Your proxy is valid until Thu Jun 16 07:27:31 2011 $ voms-proxy-init --voms ict.vo.ibergrid.eu Enter GRID pass phrase: Your identity: /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=Goncalo Borges Creating temporary proxy ............................................................. Done Contacting ibergrid-voms.ifca.es:40008 [/DC=es/DC=irisgrid/O=ifca/CN=host/ibergrid-voms.ifca.es] \"ict.vo.ibergrid.eu\" Done Creating proxy ............................................................................ Done Your proxy is valid until Thu Jun 16 07:27:37 2011 ","categories":"","description":"Implementing failover of MySQL grid-based services.","excerpt":"Implementing failover of MySQL grid-based services.","ref":"/providers/operations-manuals/man06_failover_for_mysql_grid_based_services/","tags":"","title":"MAN06 Failover for MySQL grid-based services"},{"body":"Document control    Property Value     Title MAN07 VOMS Replication   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement How to implement a MySQL VOMS server replication   Owner SDIS team    Introduction In this manual we will show you how to implement a MySQL VOMS server replication: you need one master server, on which you can perform writing operations, and you can have from 1 to “n” replica servers that will work in read-only mode. In such a scenario you can do a whatever intervention on one of the servers without breaking the service, i.e. proxies creation and grid-mapfile downloads: just the users registration and the usual VOs management operations might be forbidden during an intervention on the master server (because it is the only server in writing mode).\nThis failover procedure is simply based on MySQL replication therefore every MySQL setting is referred to the current MySQL version (5.0.77 in this moment)\nSettings on the MASTER SERVER In order to allow the replica server to read the master database, you have to create an user with which the slave will connect to the master. Suppose the replica hostname is vomsrep.cnaf.infn.it, the user is bonjovi and the password is always. What you have to launch on the master server is:\n$ mysql -p -e \"grant super, reload, replication slave, replication client \\ on *.* to bonjovi@'vomsrep.cnaf.infn.it' identified by 'always'\" ; Then for each DB (VO) you want to replicate, you have to assign the right permissions, by launching:\nmysql -p -e \"grant select, lock tables on voms_myvo.* to \\ bonjovi@'vomsrep.cnaf.infn.it'\" Eventually you have to modify the file /etc/my.cnf by adding the following lines into the section [mysqld]:\nlog-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 It is important that on the master server it is set server-id=1: it is the identification number that distinguish a master from its several slaves (each slave will have a unique number starting from 2)\nFor example, the content of my.cnf file may appear like this:\n# less /etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock user=mysql # Default to using old password format for compatibility with mysql 3.x # clients (those using the mysqlclient10 compatibility package). old_passwords=1 max_connections = 800 log-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 # Disabling symbolic-links is recommended to prevent assorted security risks; # to do so, uncomment this line: # symbolic-links=0 [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid At this point, you have to restart MySQL, by launching:\n$ service mysqld restart In order to check that on the master side the mechanism is working, you can launch for example:\nmysql\u003e show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 | 24844 | | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) Eventually, through the web interface, in the ACL section of each VO you want to replicate, add an entry granting all the permissions to the slave server:\n select “a non VO member” from the menu fill in the replica server DN and a reference email address select “all” for the permissions and tick the “Propagate entry to children contexts” option  In this way, when the slave server copies the DB, it will have the proper permissions on acting on the DB. Moreover, in order to avoid the sending of notification to the email address you filled in before, connect to the MySQL database and do the following:\nmysql\u003e use voms_myvo; mysql\u003e update admins set email_address=NULL where \\  email_address=\"what you filled before\"; Settings on the SLAVE SERVER Install a VOMS server as usual, configuring the VOs you want to replicate: keep in mind that every modification done on the slave DB breaks the replica mechanism, so that on this server disable the users registration, by setting the yaim variable:\nVOMS_ADMIN_WEB_REGISTRATION_DISABLE=true Then ask the VO managers to not perform any action on the slave server web interface.\nThen launch the following scripts:\n first_replica.sh for the first database you want to replicate or in the case it is the only one next_replicas.sh for the next databases (one database for each launch)  For both the scripts, set the following variables:\n master_host, master_mysql_user, master_mysql_pwd that refers to the master server and to the user created on it mysql_username_admin and mysql_password_admin that refers to the slave  Example:\nvoms_database=\"\" # VOMS database (leave unset) master_host=\"voms.cnaf.infn.it\" # Master hostname master_mysql_user=\"bonjovi\" # Master MySQL admin user for replication master_mysql_pwd=\"always\" # Master MySQL admin pass for replication user master_log_file=\"\" # Master LOG file (leave unset) master_log_pos=\"\" # Master LOG file (leave unset) mysql_username_admin=\"root\" # Slave MySQL admin username mysql_password_admin=\"secret\" # Slave MySQL admin pass With the launch of first-replica.sh, the file /etc/my.cnf will be properly written; if you need to replicate further databases, modify /etc/my.cnf adding the following lines related to the db you are replicating (similar to the first db you’ve replicated):\nreplicate-do-db=\u003cmaster_vo_db_name\u003e replicate-ignore-table=\u003cmaster_vo_db_name\u003e.seqnumber replicate-ignore-table=\u003cmaster_vo_db_name\u003e.realtime replicate-ignore-table=\u003cmaster_vo_db_name\u003e.transactions replicate-ignore-table=\u003cslave_vo_db_name\u003e.seqnumber replicate-ignore-table=\u003cslave_vo_db_name\u003e.realtime replicate-ignore-table=\u003cslave_vo_db_name\u003e.transactions Having set the variables in the way shown above, for replicating the first database the scripts launch syntax is the following:\n$ ./first_replica.sh --master-db=voms_myvo --db=voms_myvo In your /etc/my.cnf file you will find lines like the following:\n# Connection with master server-id=2 master-host=voms.cnaf.infn.it master-user=bonjovi master-password=always # Replicas settings replicate-do-db=voms_myvo replicate-ignore-table=voms_myvo.seqnumber replicate-ignore-table=voms_myvo.realtime replicate-ignore-table=voms_myvo.transactions replicate-ignore-table=voms_myvo.seqnumber replicate-ignore-table=voms_myvo.realtime replicate-ignore-table=voms_myvo.transactions Now you may want to replicate a second database, let’s say voms_hervo: therefore in my.cnf file add the following lines:\nreplicate-do-db=voms_hervo replicate-ignore-table=voms_hervo.seqnumber replicate-ignore-table=voms_hervo.realtime replicate-ignore-table=voms_hervo.transactions replicate-ignore-table=voms_hervo.seqnumber replicate-ignore-table=voms_hervo.realtime replicate-ignore-table=voms_hervo.transactions Modify the script next_replicas.sh in according to the VO parameters and launch it:\n$ ./next_replicas.sh --master-db=voms_hervo --db=voms_hervo When you finished to replicate all the desired VOs, in order to make active the database modifications, restart voms and voms-admin:\n$ /etc/init.d/voms-admin stop $ /etc/init.d/voms stop $ /etc/init.d/voms start $ /etc/init.d/voms-admin start Keep in mind that every modification done on the slave DB breaks the replica mechanism, so that on this server disable the users registration, by setting the yaim variable:\nVOMS_ADMIN_WEB_REGISTRATION_DISABLE=true And ask the VO managers to not perform any action on the slave server web interface.\n","categories":"","description":"How to implement a MySQL VOMS server replication","excerpt":"How to implement a MySQL VOMS server replication","ref":"/providers/operations-manuals/man07_voms_replication/","tags":"","title":"MAN07 VOMS Replication"},{"body":"Overview This tutorial describes how to create your first Virtual Machine in the EGI Federation.\nStep 1: Signing up Create an EGI account with Check-In.\nStep 2: Enrolling to a Virtual Organisation Once your EGI account is ready you need to join a Virtual Organisation (VO). Here are the steps to join a VO. Explore the list of available VOs in the Operations Portal. We have a dedicated VO called vo.access.egi.eu for piloting purposes. If you are not sure about which VO to enrol to, please request access to the vo.access.egi.eu VO with your EGI account by visiting the enrollment URL.\nStep 3: Creating a VM Once your membership to a VO has been approved you are ready to create your first Virtual Machine. There are several ways to achieve this. The simplest way is to use a web dashboard like VMOps Dashboard or Infrastructure Manager. On the other hand, advanced users may prefer to use the command-line interface.\nTo know more about the Cloud Compute Service in EGI please visit its dedicated section.\nAsking for help If you find issues please do not hesitate to contact us.\n","categories":"","description":"Step by step guide to get your first Virtual Machine up and running\n","excerpt":"Step by step guide to get your first Virtual Machine up and running\n","ref":"/users/tutorials/create-your-first-virtual-machine/","tags":"","title":"Create your first Virtual Machine"},{"body":"Overview The EGI Data Transfer Service is based on the FTS3 service, developed at CERN. It allows you to move any type of data files asynchronously from one storage to another. The service includes dedicated interfaces to display statistics of on-going transfers and manage storage resources parameters.\nThe EGI Data Transfer is ideal to move large amounts of files or very large files as the service has mechanisms to verify checksums and ensure automatic retry in case of failures.\nEager to test this service? Have a look at our tutorial on how to transfer data in the grid.\nFeatures  Simplicity  Easy user interfaces for submitting transfers: CLI, Python Bindings, WebFTS and Web Monitoring.\n Reliability  Checksums and retries are provided per transfer\n Flexibility  Multi-protocol support (Webdav/https, GridFTP, xrootd, SRM, S3, GCloud).\n Intelligence  Parallel transfers optimization to get the most from network without burning the storages. Priorities/Activities support for transfers classification.\n  Components  FTS3 Server  The service is responsible of the asynchronous execution of the file transfer, checksumming and retries in case of errors\n FTS3 REST  The RESTFul server which is contacted by clients via REST APIs, CLI and Python bindings\n FTS3 Monitoring  A Web interface to monitor transfers activity and server parameters\n WebFTS  A web interface that provides a file transfer and management solution in order to allow users to invoke reliable, managed data transfers on distributed infrastructures\n  Service Instances EGI has signed OLAs with 2 Providers, CERN and STFC, in order to access their FTS3 Service instances.\nThe following endpoints are available:\nCERN  FTS REST FTS Mon WebFTS - N.B. Needs personal X.509 certificate installed in your Browser  STFC  FTS REST FTS Mon  N.B. if you access the endpoints via Browser the following CA certificates need to be installed:\n CERN CA certificates UK eScience CA certificates  ","categories":"","description":"Documentation related to [EGI Data Transfer](https://www.egi.eu/services/data-transfer/)","excerpt":"Documentation related to [EGI Data …","ref":"/users/data-transfer/","tags":"","title":"Data Transfer"},{"body":"Why joining the EGI Cloud?  To support international communities supported by EGI (e.g. these research communities and applications or these research infrastructures in EOSC-hub or these business pilots in the EOSC Digital Innovation Hub. To participate in e-Infrastructure projects (H2020, EOSC) as an EGI compliant IaaS cloud provider. To participate in resource allocation and in pay-for-use campaigns run by EGI. To align access policies and operational model of your cloud with international good practices. To adopt best practices of multi-cloud federation for the benefit of your local users.  Do I lose control on who can access my resources if I join federated cloud? No. EGI uses the concept of Virtual Organisation (VO) to group users. The resource provider has complete control on which VOs he wants to allow on its resources and which quotas or restrictions to assign to each VO. In the case of OpenStack, each VO is mapped to a regular OpenStack project that can be managed as any other and are isolated to other projects you may have configured in your deployment. Although not recommended, you can even restrict the automatic access of users within a VO and manually enable individual members.\nHow many components do I have to install? Depending on your cloud management framework and the kind of integration this will vary.\nIn general, the federation requires your cloud management framework to be configured to support Federated AAI with EGI Check-in. This may require changes in your current setup.\nOther components are designed to access your cloud management framework public APIs and do not require modification of your deployment. For OpenStack, these components can be run on a single VM that encapsulates them for convenience.\nWhich components of my cloud will interact with the federated cloud components? For OpenStack they are:\n Keystone Nova Glance Swift (optional)  Users will also interact with:\n Neutron Cinder  to perform their regular activities.\nHow will my daily operational activities change? For the most part daily operations will not change.\nA resource centre part of the EGI Federation, and supporting international communities, needs to provide support through the EGI channels. This means following up GGUS tickets. This includes requests from user communities and tickets triggered by failures detected by the monitoring infrastructure.\nA resource centre needs to maintain the services federated in EGI properly configured with the EGI AAI.\nThe resource centre will have to comply with the operational and security requirements. All the EGI policies aim at implementing service provisioning best practices and common requirements. EGI operations may conduct campaigns targeted to mitigate security vulnerabilities and to update unsupported operating system and software. These activities are part of the regular activities of a resource centre anyways (also for the non-federated ones). EGI and the Operations Centres coordinate these actions in order to have them implemented in a timely manner.\nIn summary, most of the site activities that are coordinated by EGI and the NGIs are already part of the work plan of a well-maintained resource centre, the additional task for a site manager is to acknowledge to EGI that the task has been performed.\n","categories":"","description":"Frequently Asked Questions\n","excerpt":"Frequently Asked Questions\n","ref":"/providers/cloud-compute/faq/","tags":"","title":"FAQ"},{"body":"GPU resources on EGI Cloud GPUs resources are available on selected providers of the EGI Cloud. These are available as specific flavours that when used to instantiate a Virtual Machine will make the hardware available to the user.\nThe table below summarises the available options:\n   Site VM configuration options Flavors Supported VOs with GPUs Access conditions     IISAS-FedCloud up to 2 NVIDIA Tesla K20m g1.c08r31-K20m, g1.c16r62-2xK20m acc-comp.egi.eu, eosc-synergy.eu, enmr.eu, training.egi.eu Sponsored access for limited testing, conditions to be negotiated for long-term usage   IFCA-LCG2 up to 2 NVIDIA T4, up to 2 NVIDIA V100   Pay-per-use   CESNET-MCC up to 2 NVIDIA T4 hpc.8core-64ram-nvidia-1080-glados, hpc.19core-176ram-nvidia-1080-glados, hpc.38core-372ram-nvidia-1080-glados, hpc.19core-176ram-nvidia-2080-glados, hpc.38core-372ram-nvidia-2080-glados vo.clarin.eu, biomed, eosc-synergy.eu, peachnote.com Sponsored, conditions to be negotiated    Access to GPU resources on EGI Cloud GPUs sites can be accessed in different ways: via site-specific dashboards and endpoints or via common federated-cloud services like the OpenStack Horizon dashboards, VMOps dashboard, or Infrastructure manager.\nIt is also possible to use the fedcloudclient for CLI access. Below is an example on how to use the fedcloud command to show the GPU properties of the available GPU flavors on all sites for the specific VO in the command:\nfedcloud openstack flavor list --long --site ALL_SITES --vo vo.access.egi.eu --json-output | \\  jq -r 'map(select(.\"Error code\" == 0)) | map(.Result = (.Result| map(select(.Properties.\"Accelerator:Type\" == \"GPU\")))) | map(select(.Result | length \u003e 0))' Site-specific dashboards and endpoints are described in the following table:\n   Site Openstack Horizon dashboard Keystone endpoint     IISAS-FedCloud https://cloud.ui.savba.sk https://cloud.ui.savba.sk:5000/v3/   IFCA-LCG2 https://portal.cloud.ifca.es https://api.cloud.ifca.es:5000/   CESNET-MCC https://dashboard.cloud.muni.cz https://identity.cloud.muni.cz/    A VM image with pre-installed NVIDIA driver and Docker is available at AppDB. Some VOs (acc-comp.egi.eu, eosc-synergy.eu) have the image included in the VO image list.\nFor a more detailed presentation on how to access GPUs in the EGI Federation please have a look at the EGI Webinar on 27th November 2020. There is also a video recording available on YouTube.\n","categories":"","description":"GPU resources on EGI Cloud\n","excerpt":"GPU resources on EGI Cloud\n","ref":"/users/cloud-compute/gpgpu/","tags":"","title":"GPUs"},{"body":"Document control    Property Value     Title Accounting data publishing   Policy Group Operations Management Board (OMB)   Document status Approved   Procedure Statement How to publish accounting information for different middleware   Owner SDIS team    Introduction In this manual we will show you how to publish accounting information from different middleware.\nGeneral information Publishing with the APEL Client/SSM To start sending accounting records:\n register each endpoint sending the accounting records to the central repository as ‘gLite-APEL’ endpoint in GOCDB with host DN information  either HTCondorCE or ARC-CE endpoints it is needed to authorise the endpoint to use the ARGO Message Service (AMS) Changes in GOCDB can take up to 4 hours to make it to AMS.   install the APEL client and APEL SSM on your publisher host and edit client.cfg and sender.cfg install the APEL parser relevant to your batch system on each CE and edit parser.cfg  documentation for APEL here  GitHub apel/apel GitHub apel/ssm     The old ActiveMQ network was dismissed: have a look at the new settings to properly publish the accounting records via AMS configure the client to publish data from the start of the current month. Run the parser(s) and client. If there are no errors messages and the client says it has sent messages then wait a day and you should see summaries here If it doesn’t open a GGUS ticket for the APEL Team.  Monitoring the accounting data publication In order to monitor the regular publication of accounting data, it is enough registering only one CE with the APEL service type.\nSee here the details about the related Nagios probes.\nPublishing summarized records or single ones Sites can send either but the preferred option is summaries. Larger sites are recommended to send summaries.\nThe frequency for sending aggregated/summary records to APEL database? We recommend sending data daily for all sites, whether sending summaries or individual records. I think this is in the interests of people who are using the portal so that what they see is accurate. The summaries are for a complete month or the current month so far.\nAuthorization and authentication Authorization and authentication is made by the host certificate (which is signed by a trusted CA from the ca_policy_core package). Certificate should be registered in GOCDB. The certificate must not have the X509 extension: Netscape Cert Type: SSL Server because the message brokers will reject it.\nARC ARC uses its own system to publish the accounting data through AMS, so please refer to the NorduGrid ARC 6 documentation:\n Information relevant only for 6.4 ARC releases and beyond: Accounting-NG The old ActiveMQ network was dismissed. ARC 6.12 introduces new settings for publishing the accounting records via AMS.  HTCondor-CE To collect and publish the accounting data you need to install the APEL software as explained in the general information section.\n","categories":"","description":"Publishing accounting information for different middleware.","excerpt":"Publishing accounting information for different middleware.","ref":"/providers/operations-manuals/man09_accounting_data_publishing/","tags":"","title":"MAN09 Accounting data publishing"},{"body":"Overview This tutorial describes the EGI Data Transfer using FTS transfers services and WebFTS.\nPrerequisites As first step please make sure that you have installed the FTS client as described in Data Transfer, and in particular Clients for the command-line FTS and to have your certificate installed in your browser to use WebFTS browser based client.\nTo access services and resources in the EGI Federated Cloud, you will need:\n An EGI Check-in account, you can sign up here Enrollment into a Virtual Organisation (VO) that has access to the services and resources you need  Using the FTS client Step 1 Configuration check To verify that everything is configured properly you can check with the following command pointing to the cerficates directly:\n$ fts-rest-whoami --key ~/.globus/userkey.pem --cert ~/.globus/usercert.pem \\  -s https://fts3-public.cern.ch:8446/ User DN: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu VO: AndreaCristoforiac@egi.eu@tcs.terena.org VO id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Delegation id: XXXXXXXXXXXXXXXX Base id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX In general the commands can be used by specifying the user public and private key like shown in the example or by creating a proxy certificate as described in the following section.\nProxy creation As you have seen in the previous section it is possible to use the FTS commands by specifying the location of the user public and private key. With the use of voms-proxy-init it is possible to create a proxy certificate for the user. With this you don’t need to specify the location of the public and private key for each FTS command. When running voms-proxy-init it’s possible to specify the location of the public and private key. If this are not included as options, the tool expect to find them in:\n ~/.globus/usercert.pem for the public key ~/.globus/userkey.pem for the private key with read access only for the owner  Following is an example of running this command with the public and private key already setup as described:\n$ voms-proxy-init Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu Creating proxy ........................................... Done Your proxy is valid until Wed Aug 25 04:18:14 2021 The output of the command shows, a proxy certificate valid for 12 hours has been generated This is the default behaviour and can be usually increased, for example to 48 hours, with the following option:\n$ voms-proxy-init -valid 48:00 Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu Creating proxy ................................... Done Your proxy is valid until Thu Aug 26 16:23:01 2021 To verify for how long the proxy is still valid you can use the following command: command:\n$ voms-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu/CN=1451339003 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu type : RFC compliant proxy strength : 1024 bits path : /tmp/x509up_u1000 timeleft : 19:59:57 When the timeleft reaches zero the same command will produce the following message:\n$ fts-rest-whoami -s https://fts3-public.cern.ch:8446/ Error: Proxy expired! The last option that you need to use is specify the VO that you want to use for the proxy being created. In the following example the dteam VO has been used:\n$ voms-proxy-init --voms dteam Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Andrea Cristofori ac@egi.eu Creating temporary proxy ................................................................... Done Contacting voms2.hellasgrid.gr:15004 [/C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr] \"dteam\" Done Creating proxy .............................................................................. Done Your proxy is valid until Wed Sep 8 04:37:07 2021 With a proxy now available for the user it is now possible to execute the FTS commands without specifying the public and private keys as it will be shown in the following sections.\nFind the storage In general, the source and destination storage for a specific project should be already known. However, to discover the available source or destination endpoints to be used for a transfer, you can use the VAPOR service.\nOnce the page is loaded on the left column it is possible to filter by VO or scroll the list and click the desired VO as show in the following picture:\nOnce selected, you can see all the resources associated with the specific VO. In particular in this case you are interested in the information on the status, capacity, type of storage, etc. Following is a screenshot of the visualisation of the list of storage available to dteam.\nStarting a transfer Once you have identified the source and destination storage needed for the transfer you can proceed with the transfer between the two endpoints. To do that you can use a command of this type, returning the job ID corresponding to the transfer that you started:\n$ fts-transfer-submit -s https://fts3-public.cern.ch:8446/ \\  --source https://dc2-grid-64.brunel.ac.uk/dpm/brunel.ac.uk/home/dteam/1M \\  --destination https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/dteam/1M -o cfc884f8-1181-11ec-b9c7-fa163e5dcbe0 To check the status of the transfer you can use the returned job ID and use the following command specifying the server controlling the transfer, the source and the transfer itself:\n$ fts-transfer-status -s https://fts3-public.cern.ch:8446/ cfc884f8-1181-11ec-b9c7-fa163e5dcbe0 FINISHED The last option -o specify that the file should be overwritten if present on the destination. If this option is not present and a file with the same name exists on the destination, the transfer itself will fail. If you use this option you should make sure that it is safe to do so.\nUsing the WebFTS Data Transfer interface Accessing the WebFTS interface The WebFTS is accessible at this CERN FTS URL. Similarly to what has been done from the command-line interface you need to provide our private key for delegation of the credential. To do that you use the following command:\n$ openssl pkcs12 -in yourCert.p12 -nocerts -nodes | openssl rsa Enter Import Password: writing RSA key (...) Which extract the private key in RSA format and you can paste it in the windows that opens:\nAnd select the desired VO. Once the delegation is set it’s possible to move to the following steps.\nWarning: please be careful and avoid sharing this information with any third party or saving this information in plain text. WebFTS uses the key to acquire a proxy certificate on your behalf as described previously and does not store it.\nSubmitting a transfer The tab Submit a transfer is divided in two parts in which is possible to add two endpoints that can be used both as source or destination. After adding the URL for the two endpoints, it is possible to browse and select the files and directories to be transferred. In the destination select the destination directory. In the following example the file 1MB has been selected and by simply clicking the arrow in the middle directing to the right you are instructing the system to copy the file from the storage and path on the left to the one on the right:\nSimilarly to what can be done with the command-line interface, there is the option to overwrite the destination if it already exists. To enable this option tick the Overwrite Files below the arrow for the transfer. On the top of the page is also shown a confirmation that the transfer has been submitted successfully. This same web page shows the status of the current transfers in the My jobs tab shown in the following screenshot:\nEach line on the list shows a different job. When clicked it will expand to show additional details. The reason for the failure of a job which can be seen by moving the mouse pointer over the File ID on the detailed view.\n","categories":"","description":"Use EGI Data transfer to handle data in grid storage\n","excerpt":"Use EGI Data transfer to handle data in grid storage\n","ref":"/users/tutorials/data-transfer-grid-storage/","tags":"","title":"Data transfer with grid storage"},{"body":"The Dynamic DNS service provides a unified, federation-wide Dynamic DNS support for VMs in EGI infrastructure. Users can register their chosen meaningful and memorable DNS hostnames in given domains (e.g. my-server.vo.fedcloud.eu) and assign to public IPs of their servers.\nBy using Dynamic DNS, users can host services in EGI Cloud with their meaningful service names, can freely move VMs from sites to sites without modifying server/client configurations (federated approach), can request valid server certificates in advance (critical for security) and many other advantages.\nDynamic DNS GUI portal The Dynamic DNS offers a web GUI portal where users can login using their Check-in credentials. For doing so, click on the Login link (top left) and then click on the egi button.\nOnce logged in, you will be presented with the following page:\nTo register a new DNS host name:\n  click on Overview and then on Add Host.\n  Type in the hostname you’d like to register and select the domain to use.\n  The portal will then show you a secret than can be used for updating the host ip whenever needed. Note it down so you can use it later.\n  From the VM you’d like to assign the name to, run a command like follows:\ncurl \"https://\u003chostname\u003e:\u003csecret\u003e@nsupdate.fedcloud.eu/nic/update\" where \u003chostname\u003e is the full hostname generated before, e.g. myserver.fedcloud-tf.fedcloud.eu and \u003csecret\u003e is the secret generated in the previous step. You can add that as a boot command in your cloud-init configuration:\n#cloud-configruncmd:- [curl, \"https://\u003chostname\u003e:\u003csecret\u003e@nsupdate.fedcloud.eu/nic/update\" ]  You can also manually edit your registered hostnames in the Overview menu by clicking on the hostname you’d like to manage\n  Note   Hostnames/IP addresses are not expired so no need to refresh IP addresses if no changes, it is enough to run once. You can add the following command curl https://HOSTNAME:SECRET@nsupdate.fedcloud.eu/nic/update to cloud-init as described above to assign hostname automatically at VM start\n  DNS server set Time-to-Live (max time for caching DNS records) to 1 min for dynamic DNS, but MS Windows seems to not respect that. You can clear DNS cache in Windows with ipconfig /flushdns command with Administrator account\n   API Dynamic DNS update server uses dydns2 protocol, compatible with commercial providers like dyn.com, and noip.com. The API is specified as follows:\nGET /nic/update?hostname=yourhostname\u0026myip=ipaddress Host: nsupdate.fedcloud.eu Authorization: Basic base64-encoded-auth-string User-Agent: where:\n base64-encoded-auth-string: base64 encoding of username:password username: your hostname password: your host secret hostname in the parameter string can be omitted or must be the same as username myip in the parameter string if omitted, the IP address of the client performing the GET request will be used  Security   For updating IP address, only hostname and its secret are needed. No user information is stored on VM in any form for updating IP.\n  NS-update server uses HTTPS protocol, hostname/secret are encrypted as data and not visible during transfer so it is secure to use the update URL\n  NS-update portal does not store host secret in recoverable form. If you forget the secret of your hostname, simply generate new one via “Show configuration” button in the host edit page. The old secret will be invalid.\n  ","categories":"","description":"Dynamic DNS for VMs in EGI Cloud\n","excerpt":"Dynamic DNS for VMs in EGI Cloud\n","ref":"/users/cloud-compute/dynamic-dns/","tags":"","title":"Dynamic DNS"},{"body":"What is it? High Throughput Compute (HTC) is a computing paradigm that focuses on the efficient execution of a large number of loosely-coupled tasks (e.g. data analysis jobs). HTC systems execute independent tasks that can be individually scheduled on many different computing resources, across multiple administrative boundaries. Users submit these tasks to the infrastructure as jobs. After a job have been scheduled and executed, the output can be collected from the service(s) that executed the job.\nTarget users The target customers for EGI High Throughput Compute are research communities that need to share, store, process, and produce large sets of data. Typically, their research collaborations involve organizations across Europe and the World.Some may already have local resources (e.g. universities, research institutions) that can only be accessed by local users in accordance to the respective organisation’s access policies.\nIn case of local compute resources researchers can request access to the local compute cluster from their IT department. However, when researchers join collaborations that need to share their research activities, data collections, and repositories, they need a homogenous and coordinated operation of the compute resources, which are not uniformly accessible. In addition, nowadays many research collaborations generate large amounts of data, and managing such data volumes is time consuming and error-prone.\nThe EGI High Throughput Compute service provides access to compute resources, and offers a set of high-level tools that allow managing large amounts of data in a collaborative way (e.g authorization and access control tools can be regulated by the research collaboration in a central manner, data can be uniformly distributed in the EGI Cloud, etc.).\nFeatures EGI High Throughput Compute provides easy, uniform access to shared computating and data services of EGI service providers. Most software deployed in the distributed resource centers is based on open standards and open source middleware services.\nThe main features of the EGI High Throughput Compute are:\n Access to high-quality computing resources Integrated monitoring and accounting tools provide information about the availability and resource consumption Workload and data management tools to manage all computational tasks Large amounts of processing capacity over long periods of time Faster results for your research Shared resources enable collaborative research  The EGI HTC infrastructure The EGI High Throughput Compute infrastructure is the federation of GRID resources provided by EGI providers. Its aim is to share in a secure way the distributed IT resources that are part of the EGI Cloud. It comprises of:\n Compute Resources – execution environment for computing tasks, organized into clusters distributed across multiple resource centers in Europe and the World. Data Infrastructure – storage servers from different resource centers where users can store their data/files in a distributed manner. Federated Operations – global operational tasks (e.g., AAI, accounting, helpdesk) needed to federate the heterogeneous resources of resource centers and their operational activities. User Support – EGI provides the central user support and coordinates support activities of EGI providers, who offer user support for the services/resources they contribute to the EGI ecosystem.  Architecture and service components The key components of the EGI High Throughput Compute architecture are:\n Data Transfer service (FTS) Online Storage services Computing Elements (CEs) are compute resources made available through GRID interfaces. The most common implementations of CEs in the EGI infrastructure are HTCondor-CE and ARC-CE.  Access model Access to HTC resources in the EGI infrastructure is based on X.509 certificates and Virtual Organisations (VOs).\nVOs are fully managed by research communities, allowing communitites to manage their users and grant access to their services and resources. This means communities can either own their resources and use EGI services to share (federate) them, or can use the resources available in the EGI infrastructure for their scientific needs.\nBefore users can access EGI HTC services, they have to:\n Obtain an X.509 certficate. The certificates are issued by Certification Authorities (CAs) part of the European Policy Management Authority for Grid Authentication (EUGridPMA), which is also part of the International Global Trust Federation (IGTF). Enroll into a VO before they can use the services, as users are not individually granted access to resources. Add the certificate to their internet browser of choice, or import it into the appropriate certificate store of their local machine (on Windows). Proceed to the Workload Manager to submit HTC jobs or retrieve job results, login using EGI Check-in when prompted.  ","categories":"","description":"EGI High Throughput Compute service\n","excerpt":"EGI High Throughput Compute service\n","ref":"/users/high-throughput-compute/","tags":"","title":"High Throughput Compute"},{"body":"EC3 (Elastic Cloud Compute Cluster) is a tool to create elastic virtual clusters on Infrastructure as a Service (IaaS) providers, either public (such as Amazon Web Services, Google Cloud or Microsoft Azure) or on-premises (such as OpenNebula and OpenStack). It supports the provisioning of clusters running TORQUE, SLURM, HTCondor, Mesos, Nomad, Kubernetes and others that will be automatically resized to fit the load (e.g. number of jobs at the batch system).\nThe following section of the documentation will guide you on how to:\n Deploy simple EC3 elastic cluster on top of the IaaS providers of the EGI Cloud, either using the web interface or the command-line interface, Run pre-configured scientific applications in the EC3 elastic cluster.  EC3 was presented in one of the EGI Webinars. Please see more details on the Indico page and the video recording on YouTube.\n","categories":"","description":"Using the Elastic Cloud Compute Cluster on EGI Cloud\n","excerpt":"Using the Elastic Cloud Compute Cluster on EGI Cloud\n","ref":"/users/cloud-compute/ec3/","tags":"","title":"EC3"},{"body":"The more you go in data analysis, the more you understand that the most suitable tool for coding and visualizing is not a pure code, or SQL IDE, or even simplified data manipulation diagrams (aka workflows or jobs). From some point you realize that you need a mix of these all – that’s what “notebook” platforms are, with Jupyter being the most popular notebook software out there.\nNotebooks is an 'as a Service' environment based on the Jupyter technology, offering a browser-based, scalable tool for interactive data analysis. The Notebooks environment provides users with notebooks where they can combine text, mathematics, computations and rich media output. EGI Notebooks is a multi-user service and can scale to multiple servers based on the EGI Cloud service.\nEGI Notebooks Unique Features EGI Notebooks provides the well-known Jupyter interface for notebooks with the following added features:\n Integration with EGI Check-in for authentication, login with any EduGAIN or social accounts (e.g. Google, Facebook) Persistent storage associated with each user, available in the notebooks environment. Customisable with new notebook environments, expose any existing notebook to your users. Runs on EGI e-Infrastructure so can easily use EGI compute and storage from your notebooks.  Service Modes We offer different service modes depending on your needs:\n  Individual users can use the centrally operated service from EGI. Users can login, write and play and re-play notebooks by:\n creating an EGI account enrolling to the vo.notebooks.egi.eu VO accessing https://notebooks.egi.eu/  This instance has limits on the amount of resources available for each user (2 CPU core, 4 GB RAM and 20 GB of storage). It will also kill inactive sessions after 1 hour.\n  User communities can have their customised EGI Notebooks service instance. EGI offers consultancy, support, and can operate the setup as well. A community specific setup allows the community to use the community's own Virtual Organisation (i.e. federated compute and storage sites) for Jupyter, add custom libraries into Jupyter (e.g. discipline-specific analysis libraries) or have fine grained control on who can access the instance (based on the information available to the EGI Check-in AAI service).\n  ","categories":"","description":"Documentation related to [EGI Notebooks](https://www.egi.eu/services/notebooks/)","excerpt":"Documentation related to [EGI …","ref":"/users/notebooks/","tags":"","title":"Notebooks"},{"body":"Overview Online Storage includes a wide category of services that allow users to store, share and access data using the EGI infrastructure. Different categories of storage are available, depending on how data is stored, the technology used to access and consume data, and the foreseen usage.\nThree major service offerings are available:\n Block Storage is block level storage that can be attached to virtual machines (VMs) as volumes, a simple solution for durable data that does not need to be shared beside a single VM. Grid Storage is serving file access and storage for EGI High Throughput Compute (HTC) scenarios. Object Storage is persistent storage for cases when data needs to be exposed within portals or shared between different steps of processing workflows.  Comparison of storage types The differences between Block, Grid, and Object Storage are summarized below:\n   Type Sharing Accounting Usage     Block Only from within VMs, only at the same site the VM is located For the entire block POSIX access, use as local disk   Grid From any device connected to the internet For the data stored Grid protocols and HTTP/WebDAV   Object From any device connected to the internet For the data stored HTTP requests to REST API    The following guides offer a more detailed description of each storage service.\n","categories":"","description":"[EGI Online Storage services](https://www.egi.eu/services/online-storage/)\n","excerpt":"[EGI Online Storage …","ref":"/users/online-storage/","tags":"","title":"Online Storage"},{"body":" Note Additional Operations Manuals and How-tos are available on the EGI Wiki.  Manuals The EGI Operations Manuals are approved technical documents that provide prescriptive guidelines on how to complete a given task. These documents are periodically reviewed, and need to be followed by all partners (as opposed to a best practice documents that provide optional guidelines).\n MAN01 How to publish Site Information MAN02 Service intervention management MAN04 Tool intervention management MAN05 Top and site BDII High Availability MAN06 failover for MySQL grid-based service MAN07 VOMS replication MAN09 Accounting data publishing  How-Tos Miscellaneous collection of short How-Tos which are relevant to various other documentation and procedures.\n HOWTO01 Site Certification Required Information: Information required in the GOCDB when registering a site/resource centre HOWTO02 Site Certification Required Documentation HOWTO03 Site Certification GIIS Check HOWTO04 Site Certification Manual tests  ","categories":"","description":"Operations manuals for Service Providers","excerpt":"Operations manuals for Service Providers","ref":"/providers/operations-manuals/","tags":"","title":"Operations manuals and How-Tos"},{"body":"The training infrastructure is a resource pool within the EGI Federated Cloud infrascture providing IaaS as well as access services (login, application catalogue and application management portal) for face-to-face events, online training courses or self-paced learning modules.\nThe training infrastructure is integrated with Check-in allowing trainers to generate short-lived user accounts for training participants. Such accounts can identify students individually, and for a limited lifetime - typically few hours or days, depending on the length of the training event - allow them to interact with the services.\nThe infrastructure currently includes enough capacity to scale up to class-room size audiences, approximately up to 100 participants.\nAn introductory slideset and poster of the infrastructure from EGI Community Forum 2015, Bari. (Outdated in some parts).\nUsage models The training infrastructure is suitable for two types of courses:\n Cloud computing courses: Such courses teach students about IaaS clouds and on how Virtual Appliances, Virtual Machines, block storage and other types of ‘low level’ resources are managed. For such courses, the trainer does not need to deploy applications or online services in advance of the course. The applications/services will be deployed by the students themselves as training exercises. Such courses typically target developers or other rather technical members of scientific communities or projects. Scientific courses: Such courses teach scientists or developers about a specific software suite relevant for their work. For example a specific gene sequence analysis application, an earthquake visualisation tool, or a data processing pipeline. In this operational mode, the trainer deploys the domain specific application/tool on the training infrastructure before the training and the students interact directly with those applications/tools without even knowing where those are deployed and running. Depending on how computationally or data intensive the exercises are, multiple students may share a single software deployment instance, or each student can have their own. The configuration can be controlled by the trainer when the setup is deployed.  In both cases the deployment of applications/tools/services can happen in the form of ‘Virtual Appliances’ (VAs), and block storage - the latter basically behaving like a virtual USB drive that can be attached/detached to VMs to provide data and storage space for applications.\nThe AppDB has a growing catalogue of Virtual Appliances that includes both basic applications (e.g. latest version of clean Linux distribution) and more specialised applications (e.g. Jupyter Notebook). The list of VAs available on the training infrastructure is configurable and listed in the training.egi.eu VO entry of AppDB.\nThe VMOps can be used as web interface for both trainers and students to deploy and manage VMs.\nAvailable resources The available resources are offered by a set of providers included in the training.egi.eu VO Operation Level Agreement (OLA). Check the document for the exact amount of resources and conditions of access for each provider.\nJoin the training infrastructure! Do you want to join as a resource provider? Please email at support _at_ egi.eu.  The list of providers and VAs is also discoverable in the training.egi.eu VO entry of AppDB. The VO is also described at the EGI Operations Portal training.egi.eu VO id card.\nBooking the infrastructure The infrastructure currently includes enough capacity to scale up to class-room size audiences, approximately up to 100 participants.\nDo you want to book the infrastructure for a course? Please send a request through our site.\n","categories":"","description":"The training infrastructure on EGI Cloud\n","excerpt":"The training infrastructure on EGI Cloud\n","ref":"/users/training/infrastructure/","tags":"","title":"Training Infrastructure"},{"body":"Authentication Some EGI services authentication is based on X.509 certificates. The certificates are issued by Certification Authorities (CAs) part of the EUGridPMA federation which is also part of IGTF (International Global Trust Federation).\nThe role of a Certification Authoritie (CA) is to guarantee that users are who they claim to be and are entitled to own their certificate. It is up to the users to discover which CA they should contact. In general, CAs are organised geographically and by research institutes. Each CA has its own procedure to release certificates.\nEGI sites, endpoints and tools accept certificates part of the EUGridPMA distribution. If your community VO is enabled on that site, your certificate will be accepted by that site since all certificates are recognized at site level.\nUsually, a certificate can be installed by command-line tools, but they can also be stored in the web browser to access EGI web tools and services.\nGet a Certificate The list of EGI recognised CAs provides a clickable map to find your nearby CA. Several of these offer the option to get an ‘eScience Personal’ certificate online from the Terena Certificate Service CA. Check the countries where this is available.\nIf eScience Personal certificate is not available in your country, then request a certificate from a regular IGTF CA. The request is normally generated using either a web-based interface or console commands. Details of which type of request a particular CA accepts can be found on each CA’s site.\nFor a web-based certificate request, a form must usually be filled in with information such as the name of the user, home institute, etc. After submission, a pair of private and public keys are generated, together with a request for the certificate containing the public key and the user data. The request is then sent to the CA, while the private key stays in the browser, hence the same browser must be used to retrieve the certificate once it is issued.\nUsers must usually install the CA root certificate in their browser first. This is because the CA has to sign the user certificate using its private key, and the user’s browser must be able to validate the signature.\nFor some CAs, the certificate requests are generated using a command line interface. The details of the exact command and the requirements of each CA will vary and can be found on the CA’s site.\nOnce received the request, the CA will have to confirm your authenticity through your certificate. This usually involves a physical meeting or a phone call with a Registration Authority (RA). A RA is delegated by the CA to verify the legitimacy of a request, and approve it if it is valid. The RA is usually someone at your home institute, and will generally need some kind of ID to prove your identity.\nInstall a Certificate After approval, the certificate is generated and delivered to you. This can be done via e-mail, or by giving instructions to you to download it from a web page.\nBrowser installation Install the certificate in your browser. If you don’t know how to upload your certificate in your browser have a look at the examples.\nHost installation To use EGI services with your certificate, you must first save your certificate to disk.\nThe received certificate will usually be in one of two formats:\n Privacy Enhanced Mail Security Certificate (PEM) with extension .pem or Personal Information Exchange File (PKCS12) with extensions .p12 or .pfx.  The latter is the most common for certificates exported from a browser (e.g. Internet Explorer, Mozilla and Firefox), but the PEM format is currently needed on EGI user interface. The certificates can be converted from one format to the other using the openssl command.\nIf the certificate is in PKCS12 format, then it can be converted to PEM using pkcs12:\n  First you will need to create the private key, use -nocerts. Open your terminal, enter the following command:\nopenssl pkcs12 -nocerts -in my_cert.p12 -out userkey.pem where:\n   Filename Description     my_cert.p12 is the input PKCS12 format file;   userkey.pem is the output private key file;   usercert.pem is the output PEM certificate file.    When prompted to “Enter Import Password”, simply press enter since no password should have been given when exporting from keychain. When prompted to “Enter PEM pass phrase”, enter the pass phrase of your choice, e.g. 1234.\n  Now you can create the certificate, use -clcerts, (use -nokeys hereu will not output private key), and the command is:\nopenssl pkcs12 -clcerts -nokeys -in my_cert.p12 -out usercert.pem When prompted to “Enter Import Password”, simply press enter since no password should have been given when exporting from keychain.\nFor further information on the options of the pkcs12 command, consult man pkcs12\n  It is strongly recommended that the names of all these files are kept as shown. Once in PEM format, the two files, userkey.pem and usercert.pem, should be copied to a User Interface (UI). For example, the ‘standard’ location for Mac would be .globus directory in your $HOME. I.e. $HOME/.globus/\nRenewing the Certificate CAs issue certificates with a limited duration (usually one year); this implies the need to renew them periodically. The renewal procedure usually requires that the certificate holder sends a request for renewal signed with the old certificate and/or that the request is confirmed by a phone call; the details depend on the policy of the CA. The certificate usually needs to be renewed before the old certificate expires; CAs may send an email to remind users that renewal is necessary, but users should try to be aware of the renewal date, and take appropriate action if they are away for extended periods of time.\nTaking Care of Private Keys A private key is the essence of your identity. Anyone who steals it can impersonate the owner and if it is lost, it is no longer possible to do anything. Certificates are issued personally to individuals, and must never be shared with other users. To user EGI services, users must agree to an Acceptable Use Policy, which among other things requires them to keep their private key secure.\nOn a UNIX UI, the certificate and private key are stored in two files. Typically they are in a directory called $HOME/.globus and are named usercert.pem and userkey.pem, and it is strongly recommended that they are not changed. The certificate is public and world-readable, but the key must only be readable by the owner. The key should be stored on a disk local to the user’s UI rather than, for example, an NFS-mounted disk. If a certificate has been exported from a browser, a PKCS12-format file (.p12 or .pfx), which contains the private key, will have been locally stored and this file must be either encrypted, hidden or have its access rights restricted to only the owner.\nIf a private key is stored under the Andrew File System (AFS), access is controlled by the AFS Access Control Lists (ACL) rather than the normal file permissions, so users must ensure that the key is not in a publicly-readable area.\nWeb browsers also store private keys internally, and these also need to be protected. The details vary depending on the browser, but password protection should be used if available; this may not be the default (it is not with Internet Explorer). The most secure mode is one in which every use of the private key needs the password to be entered, but this can cause problems as some sites ask for the certificate many times. Reaching a compromise between security and convenience is vital here, so that neither come too short.\nIt is important not to lose the private key, as this implies loss of all access to the services, and registration will have to be started again from scratch. Having several securely protected copies in different places is strongly advised, so the certificate can be used from a web browser and several UI machines.\nA private key stored on a UI must be encrypted, meaning that a passphrase must be typed whenever it is used. A key must never be stored without a passphrase. The passphrase should follow similar rules to any computer password. Users should be aware of the usual risks, like people watching them type or transmitting the passphrase over an insecure link.\nAuthorisation The sites authorise the access to their resources to a VO according to their own access policies, resource location, how many resources is the VO allowed to use. There are finer authorization policies, including groups, roles, in this way, the users can be structured in a VO. So, it is not a 0/1 authorization policy.\nThe community has full control of the access to the VO according to community authorization policies. The VO membership, groups and roles are managed by VO managers (Privileged VO members) independently by using the Virtual Organization Membership Service (VOMS).\nVOMS The Virtual Organization Membership Service (VOMS) is an attribute authority which serves as central repository for VO user authorization information, providing support for sorting users into group hierarchies, keeping track ofu their roles and other attributes in order to issue trusted attribute certificates and SAML assertions used in the Grid environment for authorization purposes. VOMS is composed of two main components:\n the VOMS core service, which issues attribute certificates to authenticated clients the VOMS Admin service, which is used by VO manager to administer VOs and manage user membership details.  How does it work? Usually, users submit tasks/jobs to the infrastructure that are attached with their own credential, and the credential is attached with a proxy certificate that is a short-term credential signed with the user certificate and is extended with the VO attributes. In general speaking, a user credential is just an ID, and a proxy contains the VO details, so a resource site by receiving the proxy can recognize that the user is part of such a VO with such a role from such a group. A user can be part of multiple VO, thus can generate multiple proxies.\nRegister to a VO Visit Operation Portal to search for existing VOs\n  If there are any community VOs matching your requirements (with Registry System is VOMS), then click Action-\u003e Details to look at the VO information. In the VO ID Card page, click the link for Enrollment URL, it will take you to the VO VOMS page. You should have already discussed with the EGI support team, they would help you to contact the VO managers and get approval for your access.   If there are no relevant VOs, you can send a request to register a new VO. (Note, for EGI services, you should request for VOMS configuration, once VO is configured, you will be notified about your VO VOMS link). More information can be found at Guideline for VO registration. Again, this is usually guided by the EGI support team. You should already have a meeting with them to discuss your requirements. They will help you to get resources from EGI providers, and sign SLA with you.\n  Request your VO membership at VO VOMS page. You will have to enter required information and then wait for approval.\n  Creating a proxy VOMS configuration Every VO needs two different pieces of information:\n the vomses configuration files, where the details of the VO are stored (e.g. name, server, ports). These are stored by default at /etc/vomses and are normally named following this convention: \u003cvo name\u003e.\u003cserver name\u003e (e.g. for fedcloud.egi.eu VO, you would have fedcloud.egi.eu.voms1.grid.cesnet.cz and fedcloud.egi.eu.voms2.grid.cesnet.cz. the .lsc files that describe the trust chain of the VOMS server. These are stored at /etc/grid-security/vomsdir/\u003cvo name\u003e and there should be one file for each of the VOMS server of the VO.  You can check specific configuration for your VO at the Operations portal. Normally each VOMS server has a Configuration Info link where the exact information to include in the vomses and .lsc files is shown.\nProxy creation Once you have the VO information configured (vomses and .lsc) and your certificate available in your $HOME/.globus directory you can create a VOMS proxy to be used with clients with:\nvoms-proxy-init --voms \u003cname of the vo\u003e --rfc See for example, using fedcloud.egi.eu VO:\nvoms-proxy-init --voms fedcloud.egi.eu --rfc Enter GRID pass phrase: Your identity: /DC=org/DC=terena/DC=tcs/C=NL/O=EGI/OU=UCST/CN=Enol Fernandez Creating temporary proxy ......................................................... Done Contacting voms1.grid.cesnet.cz:15002 [/DC=cz/DC=cesnet-ca/O=CESNET/CN=voms1.grid.cesnet.cz] \"fedcloud.egi.eu\" Done Creating proxy ................................................................... Done Your proxy is valid until Mon Feb 4 23:37:21 2019 ","categories":"","description":"X.509 / VOMS based authentication and authorisation\n","excerpt":"X.509 / VOMS based authentication and authorisation\n","ref":"/users/check-in/vos/voms/","tags":"","title":"VOMS"},{"body":"Documents required for Resource Centre registration and certification.\nThe Resource Centre administrators should read and understand the first five documents. It is a further requirement that the OLA between the NGI and RC, and the Service Operations Policy are accepted by the Site Operations Manager. Further, documents referred within the Service Operations Policy should also be understood.\nDocumentation  PROC09 Resource Centre Registration and Certification: Procedure document describing the steps required to register new Resource Centres and certify new and suspended ones. Service Operations Policy: This security policy presents the conditions that apply to anyone running a Service on the Infrastructure, or to anyone providing a Service that is part of the Infrastructure. Resource Centre Operational Level Agreement (OLA) PROC11 Resource Centre Decommissioning Procedure: Procedure document describing the steps required to decommission a Resource Centre. PROC12 Production Service Decommissioning Procedure: Procedure document describing the steps required to decommission a production service. Helpdesk and FAQs: A collection of documentation relevant to the EGI Helpdesk (GGUS). The User Guide and FAQ for direct site notification are probably the most helpful.  Tools  Configuration Database (GOCDB) Operations Portal ARGO Monitoring: RCs status, A/R reports: The ARGO Monitoring service periodically checks the functionality of the production services. EGI Operations Architecture: Document describing the roles and responsibilities of the various layers in EGI and also the architectural nomenclature.  ","categories":"","description":"Documents required for Resource Center registration and certification.","excerpt":"Documents required for Resource Center registration and certification.","ref":"/providers/operations-manuals/howto02_site_certification_required_documentation/","tags":"","title":"HOWTO02 Site Certification Required Documentation"},{"body":"What is the EGI Workload Manager? The EGI Workload Manager is a service provided to the EGI community to efficiently manage and distribute computing workloads on the EGI infrastructure. The service, based on the DIRAC technology, is configured to support a number of HTC and cloud resource pools from the EGI Federation. This pool of computing resources can be easily extended and customized to support the needs of new scientific communities. In the LHCb experiment the service has proven production scalability up to peaks of more than 100,000 concurrently running jobs. WeNMR, the structural biology community, uses the service for a number of community services. The community reported an improvement of jobs submission in the infrastructure from previous 70% to 99% with the EGI Workload Manager service. The delivery of the service is coordinated by the EGI Foundation and IN2P3 provides the resources and operates the service.\nMain Features The EGI Workload Manager:\n  Maximizes usage efficiency by choosing appropriately computing and storage resources on real-time.\n  Provides a large-scale distributed environment to manage and handle data storage, data movement, accessing and processing.\n  Handles job submission and workload distribution in a transparent way.\n  Improves the general job throughput compared with native management of EGI Grid or Cloud computing resources.\n  Offers pilot-based task scheduling method, that submits pilot jobs to resources to check the execution environment before to start the user’s jobs. From a technical standpoint, the user’s job description is delivered to the pilot, which prepares its execution environment and executes the user application. The pilot-based scheduling feature solves many problems of using heterogeneity and unstable distributed computing resources.\n  Includes easy extensions to customize the environment checks to address the needs of a particular community. Users can choose appropriately computing and storage resources maximising their usage efficiency for particular user requirements.\n  Handles different storage supporting both cloud and grid capacity.\n  Provides a user-friendly interface that allows users to choose among different DIRAC services.\n  Target User Groups The service suits for the established Virtual Organization communities, long tail of users, SMEs and Industry\n EGI and EGI Federation participants Research communities  Architecture The EGI Workload Manager service is a cluster of DIRAC services running on EGI resources (HTC, Cloud, HPC) supporting multi-VO. All the DIRAC services are at or above TRL8. The main service components include:\n  Workload Management System (WMS) architecture is composed of multiple loosely coupled components working together in a collaborative manner with the help of a common Configuration Services ensuring reliable service discovery functionality. The modular architecture allows to easily incorporate new types of computing resources as well as new task scheduling algorithms in response to evolving user requirements. DIRAC services can run on multiple geographically distributed servers which increases the overall reliability and excellent scalability properties.\n  REST server providing language neutral interface to DIRAC service.\n  Web portal provides simple and intuitive access to most of the DIRAC functionalities including management of computing tasks and distributed data. It also has a modular architecture designed specifically to allow easy extension for the needs of particular applications.\n  The DIRAC Web portal\nHow to access the EGI Workload Manager service There are several options to access the service:\n Members of a scientific community whose resources pool is already configured in the EGI Workload Manager instance can use the EGI Workload Manager web portal to access the service, or use DIRAC Client. Individual researchers who want to do some number crunching for a limited period of time, with a reasonable (not too high) number of CPUs \u003e can use the catch-all VO resource pool (vo.access.egi.eu). Submit a request through the EGI Marketplace selecting:\nCompute \u003e Workload Manager from the top menu. Representatives of a community who want to try DIRAC and EGI \u003e Same as #2. Representative of a community who wants to request DIRAC for the community’s own resource pool \u003e Submit a request through the EGI Marketplace selecting\nCompute \u003e Workload Manager from the top menu.  Getting Started Submit a service order via the Marketplace User can request access to the service submitting a service-order to use the EGI HTC service directly from the EOSC or the EGI Marketplace:\n EOSC Marketplace: Workload Manager EGI Marketplace: Workload Manager  Service orders are usually handled within 5 working days by the EGI User Support Team on shift.\nBefore starting Apply for your user credentials DIRAC uses X.509 certificates to identify and authenticate users. These certificates are delivered to each individual by trusted certification authorities. If you have a personal certificate issued by a EUGridPMA-certified authority you can use it for this tutorial. Otherwise refer to the information available in this section, to obtain a certificate. Your certificate may take a few days to be delivered, so please ask for your certificate well in advance and in any case, before the tutorial starts.\nInstall your credentials Your personal certificate is usually delivered to you via a site and is automatically loaded in your browser. You need to export it from the browser and put it in the appropriate format for DIRAC to use. This is a one-time operation. Please follow the instructions in detailed in VOMS documentation page to export and in install your certificate.\nSend your certificate’s subject to the DIRAC team In order to configure the DIRAC server so that you gets registered as a user, the team needs to know your certificate’s subject.\nPlease use the command below on any Unix machine and send its output to\ndirac-support \u003cAT\u003e mailman.egi.eu\nopenssl x509 -in $HOME/.globus/usercert.pem -subject -noout The EGI Workload Manager Web Portal To access the EGI Workload Manager open a web browser to: https://dirac.egi.eu/DIRAC/\nThe EGI Workload Manager service Web portal\n  If you are a new user, you can see the welcome page where you can find links to user documentations.\n  VO options: you can switch to different VOs that you have membership.\n  Log In options: the service supports both X.509, Certificate and Check-in log-in.\n  View options: allow to choose either desktop or tabs layout.\n  Menu: a list of tools that enable the selected VO.\n  Upload Proxy Before submitting your job, you need to upload your Proxy. Login to the portal. Go to:\nMenu \u003e Tools \u003e Proxy Upload, enter your certificates .p12 file and the passphrase, click Upload.\nThe wizard to upload the .p12 proxy certificate\nJob Submission Go to:\nMenu \u003e Tools \u003e Job Launchpad. First check the Proxy Status, click it until it shows Valid in green color.\nIn the Job Launchpad, you can select your jobs from the list; add parameters, indicating the output Sandbox location.\nNow, select Helloworld from the job list, and click Submit, you just launch your very first job to the EGI HTC cluster.\nSubmit a job with the Job Launchpad\nMonitor Job status Go to:\nMenu \u003e Applications \u003e Job Monitor.\nThe left panel gives all kinds of search options for your jobs. Set your search criteria, and click Submit, the jobs will list on the right panel.\nTry the various options to view different information about the jobs.\nMonitor the job execution with the Job Monitor panel\nGet Results from Sandbox Once the job has been successfully processed, the Status of the job will change to green. Right click the job, select:\nSandbox \u003e Get Output file(s), you can get the result file(s).\nFull User Guide for DIRAC Web Portal For further instructions, please refer to DIRAC Web Portal Guide\nThe DIRAC client tool The easiest way to install the client is via Docker Container. If you have a Docker client installed in your machine, install the DIRAC CLI as follows:\ndocker run -it -v $HOME:$HOME -e HOME=$HOME diracgrid/client:egi Once the client software is installed, it should be configured in order to access the EGI Workload Manager service:\nsource /opt/dirac/bashrc To proceed further a temporary proxy of the user certificate should be created. This is necessary to get information from the central Configuration Service:\n$ dirac-proxy-init -x Generating proxy... Enter Certificate password: ... Now the client can be configured to work in conjunction with the EGI Workload Manager service:\n$ dirac-configure defaults-egi.cfg Executing: /home/jdoe/DIRAC/DIRAC/Core/scripts/dirac-configure.py defaults-egi.cfg Checking DIRAC installation at \"/home/jdoe/DIRAC\" Created vomsdir file /home/jdoe/DIRAC/etc/grid-security/vomsdir/vo.formation.idgrilles.fr/cclcgvomsli01.in2p3.fr.lsc [..] Created vomsdir file /home/jdoe/DIRAC/etc/grid-security/vomsdir/fedcloud.egi.eu/voms2.grid.cesnet.cz.lsc Created vomses file `/home/jdoe/DIRAC/etc/grid-security/vomses/fedcloud.egi.eu` Generate the proxy containing the credentials of your VO. Specify the VO in the --group option:\nIn this example, we are going to use the resources allocated for the WeNMR project.\n$ dirac-proxy-init --debug --group wenmr_user -U --rfc $ dirac-proxy-init --debug --group wenmr_user -U --rfc Generating proxy... Enter Certificate password: Contacting CS... Checking DN /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu Username is jdoe Creating proxy for jdoe@wenmr_user (/DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu) Requested adding a VOMS extension but no VOMS attribute defined for group wenmr_user Uploading proxy for wenmr_user... Uploading wenmr_user proxy to ProxyManager... Loading user proxy Uploading proxy on-the-fly Cert file /home/jdoe/.globus/usercert.pem Key file /home/jdoe/.globus/userkey.pem Loading cert and key User credentials loaded Uploading... Proxy uploaded Proxy generated: subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu/CN=0123456789 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu timeleft : 23:59:58 DIRAC group : wenmr_user rfc : True path : /tmp/x509up_u0 username : jdoe properties : LimitedDelegation, GenericPilot, Pilot, NormalUser Proxies uploaded: DN | Group | Until (GMT) /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | access.egi.eu_user | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | fedcloud_user | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | access.egi.eu_admin | 2021/09/14 23:54 /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu | wenmr_user | 2021/09/14 23:54 As a result of this command a user proxy with the same validity period of the certificate is uploaded to the DIRAC ProxyManager service.\nFor checking the details of you proxy, run the following command:\n$ dirac-proxy-info subject : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu/CN=0123456789 issuer : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu identity : /DC=org/DC=terena/DC=tcs/C=NL/O=Stichting EGI/CN=Jane Doe jdoe@egi.eu timeleft : 23:59:26 DIRAC group : wenmr_user rfc : True path : /tmp/x509up_u0 username : jdoe properties : LimitedDelegation, GenericPilot, Pilot, NormalUser Managing simple jobs    DIRAC commands Note     dirac-wms-job-status To check the status of a job   dirac-wms-job-delete To delete a job   dirac-wms-job-logging-info To retrieve history of transitions for a DIRAC job   dirac-wms-job-get-output To retrieve the job output   dirac-wms-job-submit To submit a job    DIRAC commands\nHave a look at the official command reference documentation for the complete list of the Workload Management commands.\nIn general, you can submit jobs, check their status, and retrieve the output. For example:\nCreate a simple JDL file (test.jdl) to submit the job:\n[ JobName = \"Simple_Job\"; Executable = \"/bin/ls\"; Arguments = \"-ltr\"; StdOutput = \"StdOut\"; StdError = \"StdErr\"; OutputSandbox = {\"StdOut\",\"StdErr\"}; ] Submit the job:\ndirac-wms-job-submit test.jdl JobID = 53755998 Check the job status:\n$ dirac-wms-job-status 53755998 JobID=23844073 Status=Waiting; MinorStatus=Pilot Agent Submission; Site=ANY; $ dirac-wms-job-status 53755998 JobID=53755998 Status=Done; MinorStatus=Execution Completed; Site=EGI.NIKHEF.nl; Site=EGI.HG-08-Okeanos.gr; Retrieve the outputs of the job (when the status is Done):\n$ dirac-wms-job-get-output --Dir joboutput/ 53755998 Job output sandbox retrieved in joboutput/53755998/ Jobs with Input Sandbox and Output Sandbox In most cases the job input data or executable files are available locally and should be transferred to the grid to run the job. In this case the InputSandbox attribute can be used to move the files together with the job.\nCreate the InputAndOuputSandbox.jdl\nJobName = \"InputAndOuputSandbox\"; Executable = \"testJob.sh\"; StdOutput = \"StdOut\"; StdError = \"StdErr\"; InputSandbox = {\"testJob.sh\"}; OutputSandbox = {\"StdOut\",\"StdErr\"}; Create a simple shell script (testJob.sh)\n#!/bin/bash /bin/hostname /bin/date /bin/ls -la After creation of JDL file the next step is to submit the job, using the command:\ndirac-wms-job-submit InputAndOuputSandbox.jdl JobID = XXXXXXXX List of supported VOs  acc-comp.egi.eu, beapps, compchem, eiscat.se, eli-laser.eu, eli-beams.eu, eng.vo.ibergrid.eu, enmr.eu, fedcloud.egi.eu, hungrid, km3net.org, lofar, opencoast.eosc-hub.eu, see, training.egi.eu, virgo, vlemed, vo.formation.idgrilles.fr, vo.plgrid.pl, vo.access.egi.eu, auger, biomed, bitp, eng_cloud and breakseq_cloud  More details  JDL language and simple jobs submission: JDLs and Job Management Basic Submitting Parametric and MPI jobs, using DIRAC API: Advanced Job Management Past tutorials  Technical Support   DIRAC User Guide: https://dirac.readthedocs.io/en/latest/UserGuide/\n  For technical issues and bug reports, please submit a ticket in GGUS, in Assign to support unit, indicate:\nEGI Services and Service Components \u003e Workload Manager (DIRAC).\n  ","categories":"","description":"The [EGI Workload Manager](https://www.egi.eu/services/workload-manager/)\n","excerpt":"The [EGI Workload …","ref":"/users/workload-manager/","tags":"","title":"Workload Manager"},{"body":"Basics How can I get access to the cloud compute service? There is a VO available for 6 months piloting activities that any researcher in Europe can join. Just place an order into the EGI Marketplace.\nHow can I get an OAuth2.0 token? Authentication via CLI or API requires a valid Check-in token. The FedCloud Check-in client allows you to get one as needed. Check the Authentication and Authorisation guide for more information.\nIs OCCI still supported? OCCI is now deprecated as API for the EGI Cloud providers using OpenStack. Some providers still support OCCI (a list of active endpoints can be queried at GOCDB) but it should note be used for any new developments.\nMigration from rOCCI CLI to OpenStack CLI is quite straightforward, we summarize the main commands in rOCCI and OpenStack equivalent in the table below:\n   Action rOCCI OpenStack     List images occi -a list -r os_tpl openstack image list   Describe images occi -a describe -r \u003cimage_id\u003e openstack image show \u003cimage_id\u003e   List flavors occi -a list -r resorce_tpl openstack flavor list   Describe flavors occi -a describe -r \u003ctemplate_id\u003e openstack flavor show \u003cimage_id\u003e   Create VM occi -a create -r compute -t occi.core.title=\"MyFirstVM\" -M \u003cflavor id\u003e -M \u003cimage id\u003e -T user_data=\"file://\u003cfile\u003e\" openstack server create --flavor \u003cflavor\u003e --image \u003cimage\u003e --user-data \u003cfile\u003e MyFirstVM   Describe VM occi -a describe -r \u003cvm id\u003e openstack server show \u003cvm id\u003e   Delete VM occi -a delete -r \u003cvm id\u003e openstack server delete \u003cvm id\u003e   Create volume occi -a create -r storage -t occi.storage.size='num(\u003csite in GB\u003e)' -t occi.core.title=\u003cstorage_resource_name\u003e openstack volume create --size \u003csize in GB\u003e \u003cstorage resource name\u003e   List volume occi -a list -r storage openstack volume list   Attach volume occi -a link -r \u003cvm_id\u003e -j \u003cstorage_resource_id\u003e openstack server add volume \u003cvm id\u003e \u003cvolume id\u003e   Dettach volume occi -a unlink -r \u003cstorage_link_id\u003e openstack server remove volume \u003cvm id\u003e \u003cvolume id\u003e   Delete volume occi -a delete -r \u003cvolume id\u003e openstack volume delete \u003cvolume id\u003e   Attach public IP occi -a link -r \u003cvm id\u003e --link /network/public openstack server add floating ip \u003cvm id\u003e \u003cip\u003e    If you still rely on OCCI for your access, please contact us at support _at_ egi.eu for support on the migration.\nDiscovery How can I get the list of the EGI Cloud providers? The list of certified providers is available in GOCDB. The following command with the fedcloud client can help you to get that list:\n$ fedcloud site list 100IT BIFI CESGA CESNET-MCC CETA-GRID CLOUDIFIN CYFRONET-CLOUD DESY-HH GSI-LCG2 IFCA-LCG2 IISAS-FedCloud IISAS-GPUCloud IN2P3-IRES INFN-CATANIA-STACK INFN-CLOUD-BARI INFN-PADOVA-STACK Kharkov-KIPT-LCG2 NCG-INGRID-PT SCAI TR-FC1-ULAKBIM UA-BITP UNIV-LILLE fedcloud.srce.hr The providers also generate dynamic information about their characteristics via the Argo Messaging System which is easily browsable from AppDB.\nHow can I choose which site to use? Sites offer their resources to users through Virtual Organisations (VO). First, you need to join a Virtual Organisation that matches your research interests, see authorisation section on how VOs work. AppDB shows the supported VOs and for each VO you can browse the resource providers that support it.\nHow can I get information about the available VM images? The Application Database contains information about the VM images available in the EGI Cloud. Within the AppDB Cloud Marketplace, you can look for a VM and get all the information about which VO the VM is associated, the sites where the VM is available and the endpoints and identifiers to use it in practice.\nManaging VMs The disk on my VM is full, how can I get more space? There are several ways to increase the disk space available at the VM. The fastest and easiest one is to use block storage, creating a new storage disk device and attaching it to the VM. Check the storage guide for more information.\nHow can I keep my data after the VM is stopped? After a VM has been stopped and unless backed up in a block storage volume, all data in the VM is destroyed and cannot be recovered. To ensure your data will be available after the VM is deleted, you need to use some form of persistent storage.\nHow can I assign a public IP to my VM? Some providers do not automatically assign a public IP address to a VM during the creation phase. In this case, you can attach a public IP by first allocating a new public IP and then assigning it to the VM.\nHow can I assign a DNS name to my VM? If you need a domain name for your VMs, we offer a Dynamic DNS service that allows any EGI user to create names for VMs under the fedcloud.eu domain.\nJust go to EGI Cloud nsupdate and login with your Check-in account. Once in, you can click on \"Add host\" to register a new hostname in an available domain.\nWhat is contextualisation? Contextualisation is the process of installing, configuring and preparing software upon boot time on a predefined virtual machine image. This way, the predefined images can be stored as generic and small as possible, since customisations will take place on boot time.\nContextualisation is particularly useful for:\n Configuration not known until instantiation (e.g. data location). Private Information (e.g. host certs) Software that changes frequently or under development.  Contextualisation requires passing some data to the VMs on instantiation (the context) and handling that context in the VM.\nHow can I inject my public SSH key into the machine? The best way to login into the virtual server is to use SSH keys. If you don't have one, you need to generate it with the ssh-keygen command:\nssh-keygen -f fedcloud This will generate two files:\n fedcloud, the private key. This file should never be shared fedcloud.pub, the public key. That will be sent to your VM.  To inject the public SSH key into the VM you can use the key-name option when creating the VM in OpenStack. Check keypair management option in OpenStack documentation. This key will be available for the default configured user of the VM (e.g. ubuntu for Ubuntu, centos for CentOS).\nYou can also create users with keys with a contextualisation file:\n#cloud-configusers:- name:cloudadmsudo:ALL=(ALL) NOPASSWD:ALLlock-passwd:truessh-import-id:cloudadmssh-authorized-keys:- \u003cpaste here the contents of your SSH key pub file\u003e Warning YAML format requires that the spaces at the beginning of each line is respected in order to be correctly parsed by cloud-init.  How can I use a contextualisation file? If you have a contextualisation file, you can use it with the --user-data option to server create in OpenStack.\nopenstack server create --flavor \u003cyour-flavor\u003e --image \u003cyour image\u003e \\  --user-data \u003cyour contextualisation file\u003e \\  \u003cserver name\u003e  Note We recommend using cloud-init for contextualisation. EGI images in AppDB do support cloud-init. Check the documentation for more information.  How can I pass secrets to my VMs? EGI Cloud endpoints use HTTPS so information passed to contextualise the VMs can be assumed to be safe and only readable within your VM. However, take into account that anyone with access to the VM may be able to access also the contextualisation information.\nWarning Take into account that anyone with access to the VM may be able to access also the contextualisation information, so ensure that no sensitive data like clear text passwords is used during contextualisation.  How can I use ansible? Ansible relies on ssh for accessing the servers it will configure. VMs at EGI Cloud can be also accessed via ssh, just make sure you inject the correct public keys in the VMs to be able to access.\nIf you don't have public IPs for all the VMs to be managed, you can also use one as a gateway as described in the Ansible FAQ.\nHow can I release resources without destroying my data? Whenever you delete a VM, the ephemeral disks associated with it will be also deleted. If you don’t plan to use your VM for some time, there are several ways to release resources consumed by the VM (e.g. CPU, RAM) and recover the the data or boot your VM in a previous state when you need it back. We list below the main strategies you can use:\n  Use a volume to store the data to be kept: Check the Storage section of the documentation to learn how to use volumes. If you start your VM from a volume, the VM can be destroyed and recreated easily. OpenStack documentation cover how to start a VM from a volume with CLI or using the Horizon dashboard\n  Suspend or shelve instance: Suspending a VM will pause a VM, releasing CPU and memory, and allowing to resume later in time at the exact same state. Shelving shuts down the VM, thus RAM contents will be lost but disk will be kept. This releases more resources from the provider while still allows to easily boot the VM back without losing disk contents.\n  Create snapshot of instance: a snapshot will create a new VM image at your provider that can be used to boot a new instance of the VM with the same disk content. You can use this technique for creating a base template image that can be later re-used to start similar VMs easily.\n  ","categories":"","description":"Frequenlty Asked Questions\n","excerpt":"Frequenlty Asked Questions\n","ref":"/users/cloud-compute/faq/","tags":"","title":"FAQ"},{"body":"Be sure that Resource Centre GIIS URL is contained in the BDII you use for certification.\nCheck the consistency of the published information These are the main branches of the LDAP tree:\n GlueSiteUniqueID GlueSubClusterUniqueID GlueCEUniqueID GlueCESEBind GlueSEUniqueID GlueServiceUniqueID  It is recommended to use the Apache Studio LDAP browser, although in this page ldapsearch queries are shown.\nContact information Under the branch GlueSiteUniqueID check the values of the following fields:\n GlueSiteName GlueSiteUserSupportContact GlueSiteSysAdminContact GlueSiteSecurityContact GlueSiteOtherInfo  Example:\n$ ldapsearch -x -LLL -H ldap://sibilla.cnaf.infn.it:2170 \\  -b mds-vo-name=INFN-CNAF,o=grid 'objectClass=GlueSite' \\  GlueSiteName GlueSiteUserSupportContact GlueSiteSysAdminContact \\  GlueSiteSecurityContact GlueSiteOtherInfo dn: GlueSiteUniqueID=INFN-CNAF,Mds-Vo-name=INFN-CNAF,o=grid GlueSiteSecurityContact: mailto:grid-sec@cnaf.infn.it GlueSiteSysAdminContact: mailto:grid-operations@lists.cnaf.infn.it GlueSiteName: INFN-CNAF GlueSiteOtherInfo: CONFIG=yaim GlueSiteOtherInfo: EGEE_SERVICE=prod GlueSiteOtherInfo: EGI_NGI=NGI_IT GlueSiteOtherInfo: GRID=WLCG GlueSiteOtherInfo: GRID=EGI GlueSiteOtherInfo: WLCG_TIER=3 GlueSiteUserSupportContact: mailto:grid-operations@lists.cnaf.infn.it Information related to software environment Under the branch GlueSubClusterUniqueID check the values of the following fields:\n Check that the GlueHostApplicationSoftwareRunTimeEnvironment contains a list of software tags supported by the site.  The list can include VO-specific software tags. In order to ensure backwards compatibility it should include the entry ‘LCG-2’, the current middleware version and the list of previous middleware tags (i.e. LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 LCG-2_3_0 LCG-2_3_1 LCG-2_4_0 LCG-2_5_0 LCG-2_6_0 LCG-2_7_0 GLITE-3_0_0 GLITE-3_1_0 GLITE-3_2_0 R-GMA). If the site supports MPI, check MAN03   GlueHostProcessorOtherDescription (see FAQ HEP SPEC06) GlueHostOperatingSystemName, GlueHostOperatingSystemVersion and GlueHostOperatingSystemRelease (see HOWTO05)  Example:\n$ ldapsearch -x -LLL -H `\u003cldap://virgo-ce.roma1.infn.it:2170\u003e` \\  -b mds-vo-name=resource,o=grid 'objectClass=GlueSubCluster' \\  GlueHostProcessorOtherDescription dn: GlueSubClusterUniqueID=virgo-ce.roma1.infn.it,GlueClusterUniqueID=virgo-ce.roma1.infn.it,Mds-Vo-name=resource,o=grid GlueHostProcessorOtherDescription: Cores=4, Benchmark=7.83-HEP-SPEC06 Information about the batch system Under the branch GlueCEUniqueID check the values of the following fields:\n GlueCEInfoTotalCPUs: Check that the value is higher than 0. GlueCEStateWaitingJobs: If there is a “44444”, the information providers are not working properly. GlueCEInfoLRMSType: any supported batch system (sge, pbs, lsf…) GlueCEStateStatus: Production, Draining, Queuing or Closed are accepted values. GlueCEAccessControlBaseRule: VOs enabled on the queue GlueCECapability  Example:\n$ ldapsearch -x -LLL -H ldap://virgo-ce.roma1.infn.it:2170 \\  -b mds-vo-name=INFN-ROMA1-VIRGO,o=grid 'objectClass=GlueCE' \\  GlueCEInfoTotalCPUs GlueCEInfoJobManager GlueCEImplementationName dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-theophys,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-cert,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-virgoglong,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-argo,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 dn: GlueCEUniqueID=virgo-ce.roma1.infn.it:2119/jobmanager-lcgpbs-virgogshort,Mds-Vo-name=INFN-ROMA1-VIRGO,o=grid GlueCEImplementationName: LCG-CE GlueCEInfoJobManager: lcgpbs GlueCEInfoTotalCPUs: 8 $ ldapsearch -x -LLL -H ldap://cmsrm-bdii.roma1.infn.it:2170 \\  -b mds-vo-name=INFN-ROMA1-CMS,o=grid 'objectclass=GlueCE' GlueCECapability dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsgcert,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsglong,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce01.roma1.infn.it:2119/jobmanager-lcglsf-cmsgshort,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsglong,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsgcert,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 dn: GlueCEUniqueID=cmsrm-ce02.roma1.infn.it:2119/jobmanager-lcglsf-cmsgshort,Mds-Vo-name=INFN-ROMA1-CMS,o=grid GlueCECapability: CPUScalingReferenceSI00=1515 GlueCECapability: Share=cms:100 Information on Computing Element about Storage Elements For each SE, on the CEs the following values must be present:\n GueCESEBindSEUniqueID.  GlueCESEBindCEAccesspoint and GlueCESEBindMountInfo.    Example:\n$ ldapsearch -x -LLL -H ldap://cremino.cnaf.infn.it:2170 \\  -b mds-vo-name=resource,o=grid 'objectClass=GlueCESEBind' \\  GlueCESEBindSEUniqueID GlueCESEBindCEUniqueID GlueCESEBindMountInfo dn: GlueCESEBindSEUniqueID=sunstorm.cnaf.infn.it,GlueCESEBindGroupCEUniqueID=cremino.cnaf.infn.it:8443/cream-pbs-cert,Mds-Vo-name=resource,o=grid GlueCESEBindSEUniqueID: sunstorm.cnaf.infn.it GlueCESEBindMountInfo: n.a GlueCESEBindCEUniqueID: cremino.cnaf.infn.it:8443/cream-pbs-cert dn: GlueCESEBindSEUniqueID=sunstorm.cnaf.infn.it,GlueCESEBindGroupCEUniqueID=cremino.cnaf.infn.it:8443/cream-pbs-prod,Mds-Vo-name=resource,o=grid GlueCESEBindSEUniqueID: sunstorm.cnaf.infn.it GlueCESEBindMountInfo: n.a GlueCESEBindCEUniqueID: cremino.cnaf.infn.it:8443/cream-pbs-prod Information on Storage Elements Under the branch GlueSEUniqueID check the values of the following fields:\n GlueSALocalID: VO information GlueSEAccessProtocolLocalID : rfio, srm_v2, gsiftp, gsidcap GlueSEImplementationName (deprecated) GlueSEArchitecture GlueSAStateUsedSpace GlueSAStateAvailableSpace GlueSACapability  Example:\n$ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\  -b mds-vo-name=resource,o=grid 'objectclass=GlueSE' dn: GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSEImplementationVersion: 1.7.4 GlueSETotalOnlineSize: 8795 GlueSEStatus: Production objectClass: GlueTop objectClass: GlueSE objectClass: GlueKey objectClass: GlueSchemaVersion GlueSETotalNearlineSize: 0 GlueSEArchitecture: multidisk GlueSESizeTotal: 8795 GlueSESizeFree: 5458 GlueSEName: INFN-PAVIA DPM server GlueSchemaVersionMinor: 3 GlueSEUsedNearlineSize: 0 GlueForeignKey: GlueSiteUniqueID=INFN-PAVIA GlueSEUsedOnlineSize: 3336 GlueSchemaVersionMajor: 1 GlueSEImplementationName: DPM GlueSEUniqueID: grid-se.pv.infn.it $ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\  -b mds-vo-name=resource,o=grid 'objectclass=GlueSA' \\  GlueSAAccessControlBaseRule GlueSACapability dn: GlueSALocalID=storage:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSAAccessControlBaseRule: VO:atlas GlueSAAccessControlBaseRule: VO:dteam GlueSAAccessControlBaseRule: VO:infngrid GlueSAAccessControlBaseRule: VO:ops GlueSACapability: InstalledOnlineCapacity=8258 GlueSACapability: InstalledNearlineCapacity=0 dn: GlueSALocalID=ATLASHOTDISK:SR:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSAAccessControlBaseRule: VOMS:/atlas/Role=production GlueSACapability: InstalledOnlineCapacity=536 GlueSACapability: InstalledNearlineCapacity=0 $ ldapsearch -x -LLL -H ldap://grid-se.pv.infn.it:2170 \\  -b mds-vo-name=resource,o=grid '(\u0026(objectclass=GlueSA)(GlueSALocalID=storage:replica:online))' \\  GlueSAReservedNearlineSize GlueSAFreeNearlineSize \\  GlueSATotalNearlineSize GlueSAUsedNearlineSize GlueSACapability \\  GlueSATotalOnlineSize GlueSAFreeOnlineSize \\  GlueSAReservedOnlineSize GlueSAStateAvailableSpace \\  GlueSAUsedOnlineSize GlueSAStateUsedSpace dn: GlueSALocalID=storage:replica:online,GlueSEUniqueID=grid-se.pv.infn.it,Mds-Vo-name=resource,o=grid GlueSATotalNearlineSize: 0 GlueSAFreeOnlineSize: 4921 GlueSAUsedNearlineSize: 0 GlueSAFreeNearlineSize: 0 GlueSAReservedNearlineSize: 0 GlueSAStateAvailableSpace: 4921376492 GlueSAReservedOnlineSize: 0 GlueSAUsedOnlineSize: 3336 GlueSAStateUsedSpace: 3336991982 GlueSATotalOnlineSize: 8258 GlueSACapability: InstalledOnlineCapacity=8258 GlueSACapability: InstalledNearlineCapacity=0 Information about other services There is a branch GlueServiceUniqueID for each service published by the site (WMS, LFC, DPM, GRIDICE, LB, MYPROXY, BDII, etc): what discriminates the services are the values of GlueServiceType, example:\n lcg-file-catalog org.glite.wms.WMProxy org.glite.lb.Server srm_v1, SRM  Example:\n$ ldapsearch -x -LLL -H ldap://sibilla.cnaf.infn.it:2170 \\  -b mds-vo-name=INFN-CNAF,o=grid 'objectClass=GlueService' \\  GlueServiceType GlueServiceEndpoint GlueServiceName dn: GlueServiceUniqueID=lfcserver.cnaf.infn.it,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: lfcserver.cnaf.infn.it GlueServiceName: INFN-CNAF-lfc GlueServiceType: lcg-file-catalog dn: GlueServiceUniqueID=local-lfcserver.cnaf.infn.it,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: lfcserver.cnaf.infn.it GlueServiceName: INFN-CNAF-lfc GlueServiceType: lcg-local-file-catalog dn: GlueServiceUniqueID=\u003chttp://lfcserver.cnaf.infn.it:8085/,Mds-Vo-name=INFN-CNAF,o=grid\u003e GlueServiceEndpoint: \u003chttp://lfcserver.cnaf.infn.it:8085/\u003e GlueServiceName: INFN-CNAF-lfc-dli GlueServiceType: data-location-interface dn: GlueServiceUniqueID=myproxy.cnaf.infn.it_MyProxy_4027652676,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: myproxy://myproxy.cnaf.infn.it:7512/ GlueServiceName: INFN-CNAF-MyProxy GlueServiceType: MyProxy dn: GlueServiceUniqueID=sibilla.cnaf.infn.it_bdii_site_3877936872,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003cldap://sibilla.cnaf.infn.it:2170/mds-vo-name=INFN-CNAF,o=grid\u003e GlueServiceName: INFN-CNAF-bdii_site GlueServiceType: bdii_site dn: GlueServiceUniqueID=local-http://lfcserver.cnaf.infn.it:8085/,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttp://lfcserver.cnaf.infn.it:8085/\u003e GlueServiceName: INFN-CNAF-lfc-dli GlueServiceType: local-data-location-interface dn: GlueServiceUniqueID=mon-it.cnaf.infn.it_Regional-NAGIOS_2937827985,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://mon-it.cnaf.infn.it:443/nagios\u003e GlueServiceName: INFN-CNAF-Regional-NAGIOS GlueServiceType: Regional-NAGIOS dn: GlueServiceUniqueID=httpg://sunstorm.cnaf.infn.it:8444/srm/managerv2,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: httpg://sunstorm.cnaf.infn.it:8444/srm/managerv2 GlueServiceName: INFN-CNAF-SRM GlueServiceType: SRM dn: GlueServiceUniqueID=albalonga.cnaf.infn.it_org.glite.lb.server_889826742,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://albalonga.cnaf.infn.it:9003/\u003e GlueServiceName: INFN-CNAF-server GlueServiceType: org.glite.lb.server dn: GlueServiceUniqueID=gridit-ce-001.cnaf.infn.it_org.edg.gatekeeper_715226072,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: gram://gridit-ce-001.cnaf.infn.it:2119/ GlueServiceName: INFN-CNAF-gatekeeper GlueServiceType: org.edg.gatekeeper dn: GlueServiceUniqueID=egee-wms-01.cnaf.infn.it_org.glite.wms.WMProxy_2200630265,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://egee-wms-01.cnaf.infn.it:7443/glite_wms_wmproxy_server\u003e GlueServiceName: INFN-CNAF-WMProxy GlueServiceType: org.glite.wms.WMProxy dn: GlueServiceUniqueID=cremino.cnaf.infn.it_org.glite.ce.CREAM_860197007,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://cremino.cnaf.infn.it:8443/ce-cream/services\u003e GlueServiceName: INFN-CNAF-CREAM GlueServiceType: org.glite.ce.CREAM dn: GlueServiceUniqueID=cremino.cnaf.infn.it_org.glite.ce.Monitor_2670664997,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003chttps://cremino.cnaf.infn.it:8443/ce-monitor/services/CEMonitor\u003e GlueServiceName: INFN-CNAF-Monitor GlueServiceType: org.glite.ce.Monitor dn: GlueServiceUniqueID=top-bdii01.cnaf.infn.it_bdii_top_1813027130,Mds-Vo-name=INFN-CNAF,o=grid GlueServiceEndpoint: \u003cldap://egee-bdii.cnaf.infn.it:2170/mds-vo-name=local,o=grid\u003e GlueServiceName: INFN-CNAF-bdii_top GlueServiceType: bdii_top [...] See the Site Certification Manual tests HOWTO04\nSee to Resource Centre registration and certification procedure PROC09.\n","categories":"","description":"How to test a Resource Centre during certification","excerpt":"How to test a Resource Centre during certification","ref":"/providers/operations-manuals/howto03_site_certificatoin_giis_check/","tags":"","title":"HOWTO03 Site Certification GIIS Check"},{"body":"Objectives ","categories":"","description":"Training on using EGI resources and services\n","excerpt":"Training on using EGI resources and services\n","ref":"/users/training/","tags":"","title":"Training"},{"body":"This page provides instructions on how to test manually the functionality of the grid and cloud services offered by a site. These checks are executed by the EGI Operations Team for sites that want to be formally included into EGI Production infrastructure. The site must successfully pass either the grid or the cloud certification tests to become part of the EGI Production infrastructure.\nCheck the functionality of the grid elements Be sure that the site’s GIIS URL is contained in the Top level BDII/Information System your NGI will use for your certification.\nNote that the examples here use the Italian NGI and sites. Please substitute with YOUR OWN NGI and site credentials when running the test.\nCream-CE checks Open in your browser the following URL: https://\u003chostname-of-cream-ce\u003e:8443/ce-cream/services\nA page with link to the CREAM WSDL should be shown.\nTry a gsiftp (e.g. using globus-url-copy or uberftp) towards that CREAM CE. E.g.:\n$ globus-url-copy \\  gsiftp://\u003chostname-of-cream-ce\u003e/opt/glite/yaim/etc/versions/glite-yaim-core \\  file:/tmp/yaim-version-test Try the following command:\n$ glite-ce-allowed-submission \u003chostname-of-cream-ce\u003e:8443 It should report: Job Submission to this CREAM CE is enabled.\nTry a submission to Cream-CE using the glite-ce-job-submit command, e.g.:\n$ cat sleep.jdl [ executable=\"/bin/sleep\"; arguments=\"1\"; ] $ glite-ce-job-submit -a -r \u003chostname-of-cream-ce\u003e:8443/\u003cqueue\u003e sleep.jdl $ glite-ce-job-submit -a -r ce-cr-02.ts.infn.it:8443/cream-lsf-cert sleep.jdl https://ce-cr-02.ts.infn.it:8443/CREAM127814374 Check the status of that job, which eventually should be DONE-OK:\n$ glite-ce-job-status https://ce-cr-02.ts.infn.it:8443/CREAM127814374 2010-07-27 11:55:37,986 WARN - No configuration file suitable for loading. Using built-in configuration ****** JobID=[https://ce-cr-02.ts.infn.it:8443/CREAM127814374] Status = [DONE-OK] ExitCode = [0] Try a submission to that CE using the glite-ce-job-submit command, and then tries to cancel it (using the glite-ce-job-cancel command):\n$ cat sleep2.jdl [ executable=\"/bin/sleep\"; arguments=\"1000\"; ] $ glite-ce-job-submit -a -r cecream-cyb.ca.infn.it:8443/cream-lsf-poncert sleep2.jdl https://cecream-cyb.ca.infn.it:8443/CREAM126335182 $ glite-ce-job-cancel https://cecream-cyb.ca.infn.it:8443/CREAM126335182 $ glite-ce-job-status https://cecream-cyb.ca.infn.it:8443/CREAM126335182 2010-07-27 12:18:26,973 WARN - No configuration file suitable for loading. Using built-in configuration ****** JobID=[https://cecream-cyb.ca.infn.it:8443/CREAM126335182] Status = [CANCELLED] ExitCode = [] Description = [Cancelled by user] ARC CE checks A first test can be done using ARC’s ngstat command:\n$ /usr/bin/ngstat -q -l -c \u003cCE hostname\u003e -t 20 ... ... plenty of output ... If a monitoring host of your NGI is available, then the probes can easily be executed from there:\nCheck the status of the CE with:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-status \\  -H \u003cCE hostname\u003e -x /etc/nagios/globus/userproxy.pem-ops Status is active Test gsiftp:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-auth -H \u003cCE hostname\u003e \\  -x /etc/nagios/globus/userproxy.pem-ops gsiftp OK Test the versions of the CA’s:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-caver -H \u003cCE hostname\u003e \\  -x /etc/nagios/globus/userproxy.pem-ops version = 1.38 - All CAs present Check the versions of ARC and Globus:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-softver \\  -H \u003cCE hostname\u003e \\  -x /etc/nagios/globus/userproxy.pem-ops nordugrid-arc-0.8.3.1, globus-5.0.3 Copy a file:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-gridftp -H \u003cCE hostname\u003e \\  -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Submit a test job:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-jobsubmit \\  -H \u003cCE hostname\u003e \\  --vo ops -x /etc/nagios/globus/userproxy.pem-ops Job submission successful Check the LFC:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-lfc -H \u003cCE hostname\u003e \\  -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Check the SRM:\n$ /usr/libexec/grid-monitoring/probes/org.ndgf/ARCCE-srm -H \u003cCE hostname\u003e \\  -x /etc/nagios/globus/userproxy.pem-ops Job finished successfully Before continuing, you may want to make sure that the probes for all services which the CE intends to offer, do actually succeed.\nStorage Element (SE) checks Check if gridftp server on SE works:\n$ uberftp inaf-se-01.ct.pi2s2.it For STORM SE: check if SRM client works (on the published information you can find the right port to use)\n$ /opt/storm/srm-clients/bin/clientSRM \\  ping -e httpg://sunstorm.cnaf.infn.it:8444 ============================================================ Sending Ping request to: httpg://sunstorm.cnaf.infn.it:8444 ============================================================ Request status: statusCode=\"SRM_SUCCESS\"(0) explanation=\"SRM server successfully contacted\" ============================================================ SRM Response: versionInfo=\"v2.2\" otherInfo (size=2) [0] key=\"backend_type\" [0] value=\"StoRM\" [1] key=\"backend_version\" [1] value=\"\u003cFE:1.5.0-1.sl4\u003e\u003cBE:1.5.3-4.sl4\u003e\" ============================================================ Try to write on SE. Be sure your UI is pointing to an IS the SE is contained in (you may use your certification BDII)\n1) Setting a top-bdii that is publishing the SE you have to test\n$ export LCG_GFAL_INFOSYS=\u003cTopBDII hostname\u003e:2170 2) Copy a file from the local filesystem to the SE, registering it in the LFC. This command output will return a SURL that you can use latter for other tests.\nA SURL is a path of the type: srm://srm01.ncg.ingrid.pt/ibergrid/iber/generated/2011-02-01/file4034a935-8d7a-48f4-914f-16f2634d4802\n$ lcg-cr -v --vo \u003cVO\u003e-d\u003cYour SE\u003e \\  -l lfn:/grid/\u003cVO\u003e/test.txt file:\u003c/path/to/your/local/file\u003e 3) Create a new replica in other SE (to check the third-party transfer between 2 SEs)\n$ lcg-rep -v --vo \u003cVO\u003e-d\u003cOther SE\u003e \u003cSURL\u003e 4) List Replicas\n$ lcg-lr -v --vo \u003cVO\u003e lfn:/grid/\u003cVO\u003e/test.txt 5) Delete all replicas\n$ lcg-del -v --vo \u003cVO\u003e-a\u003cguid\u003e Job submission Submit a test job to Cream-CE through the WMS, i.e. using the glite-wms-job-submit command. In case, submit a mpi test job. The NGI_IT certification WMS is gridit-cert-wms.cnaf.infn.it.\nRegistration into 1st level HLR NOTE: this step is needed if your infrastructure uses DGAS as accounting system\nAfter the site entered in production, it needs to register the site resources in the HLR. Ask the site admins to open a ticket towards the HLR administrators, passing them the following information:\n grid queues names, in the form:  gridit-ce-001.cnaf.infn.it:2119/jobmanager-lcgpbs-cert   not-grid queues names, in the form:  hostname:queue   Name, surname ad certificate subject of each site admin Certificate subject of Computing Element  Eventually, the site admins have to open a ticket to DGAS support unit asking to enable the forwarding of accounting data from the 2° level HLR to APEL.\nCertification Job The test job checks several things, like the environment on WN and installed RPMs. Moreover it performs some replica management tests. With a grep TEST you may get a summary of the results: in case of errors, you have to see in detail what is gone wrong!\nAs already said, if the site supports any flavour of MPI, launch a MPI test job, like this.\nDon’t forget to set a reasonable value in CPUNumber: most important is that your job starts running quickly.\nIf you want less stuff in the .out and .err files, in the file mpi-start-wrapper.sh comment the line\n$ export I2G_MPI_START_DEBUG=1 A successful output will look like the following one (extract)\n[...] mpi-start [DEBUG ]: using user supplied startup : '/opt/mpich-1.2.7p1/bin/mpirun ' mpi-start [DEBUG ]: =\u003e MPI_SPECIFIC_PARAMS= mpi-start [DEBUG ]: =\u003e I2G_MPI_PRECOMMAND= mpi-start [DEBUG ]: =\u003e MPIEXEC=/opt/mpich-1.2.7p1/bin/mpirun mpi-start [DEBUG ]: =\u003e I2G_MACHINEFILE_AND_NP=-machinefile /tmp/tmp.iBypc12521 -np 6 mpi-start [DEBUG ]: =\u003e I2G_MPI_APPLICATION=/home/dteam022/globus-tmp.t3-wn-13.11955.0/https_3a_2f_2falbalonga.cnaf.infn.it_3a9000_2fI06uWaKi1evxL3tTF-DTOg/hello mpi-start [DEBUG ]: =\u003e I2G_MPI_APPLICATION_ARGS= mpi-start [DEBUG ]: /opt/mpich-1.2.7p1/bin/mpirun -machinefile /tmp/tmp.iBypc12521 -np 6 /home/dteam022/globus-tmp.t3-wn-13.11955.0/https_3a_2f_2falbalonga.cnaf.infn.it_3a9000_2fI06uWaKi1evxL3tTF-DTOg/hello Process 4 on t3-wn-37.pn.pd.infn.it out of 6 Process 3 on t3-wn-34.pn.pd.infn.it out of 6 Process 1 on t3-wn-13.pn.pd.infn.it out of 6 Process 2 on t3-wn-34.pn.pd.infn.it out of 6 Process 5 on t3-wn-37.pn.pd.infn.it out of 6 Process 0 on t3-wn-13.pn.pd.infn.it out of 6 [...] Globus checks  These checks should be executed depending on the services registered in GOCDB under a Resource Centre. Not all services are compulsory for a RC, but upon registration of new ones, the corresponding tests should be executed.\n GSISSH Initialize grid proxy and check if GSISSH works:\n$ grid-proxy-init $ gsissh USER@HOST -p 2222 /bin/date (Debug with: USER@HOST -vvv -p 2222 /bin/date) GridFTP Check if upload works:\n$ globus-url-copy file:/tmp/test.txt gsiftp://HOST:2811/tmp/test.txt (Debug with: globus-url-copy -dbg -v -vb file:/tmp/test.txt gsiftp://HOST:2811/tmp/test.txt) Check if download works:\n$ globus-url-copy gsiftp://HOST:2811/tmp/test.txt file:/tmp/test.txt (Debug with: globus-url-copy -dbg -v -vb gsiftp://HOST:2811/tmp/test.txt file:/tmp/test.txt) Delete the remote file:\n$ uberftp HOST 'rm /tmp/test.txt' (Debug with: uberftp HOST 'rm /tmp/test.txt' -debug 3) GRAM Check authentication:\n$ globusrun -a -r HOST:2119 Check job submission:\n$ globusrun -s -r HOST:2119 \"\u0026(executable=\"/bin/date\")\" Unicore checks This testing manual assumes that the test instance has not been added to the “Global” registry. “Global” registry does not have to be global (for the whole infrastructure) - is a register used by a group of site which work together. For example each Resource Infrastructure Provider can have own “Global” registry.\nIt is suggested to add the instance to the “Global” registry only if it was tested and works properly. For this reason this instruction refers to the local registry.\nPreliminary testing After installation and configuration, start all the services and see if functioning properly. To avoid errors/warnings in the logs first start the TSI and the Gateway and then the Unicore/X (requires two other servers to operate). The first step of verification is to verify proper configuration of log files for all services whether they running. Logs for Unicore/X and Gateway are in standard locations /var/log/unicore/unicorex/unicorex.log and /var/log/unicore/gateway/gateway.log. In the case where there is no log file, check the file /var/log/unicore/unicorex/unicorex-startup.log or /var/log/unicore/gateway/gateway-startup.log - those file contain the servers' standard output output, and can be useful in case of generic, system-wide issues as missing Java virtual machine. Log files should be checked carefully for warnings and errors. They should show only the information about the start of the service, without any warnings (the WARN label) or errors (the ERROR label). In case of problems, you should proceed according to the information found in the log files. If they are unclear you should increase logging detail (for Unicore/X and Gateway). This is set in the file /etc/unicore/gateway/logging.properties and /etc/unicore/unicorex/logging.properties. UNICORE uses log4j logging subsystem. When you change the login parameters is not required to restart the component. After the successful initialization of all services you can begin to test them in practice. Please connect to the site via any UNICORE client (URC or UCC). Since the registration of newly created VSite was initially turned off in the global registry, you should use the local registry. The local registry address is: https://GATEWAY-ADDESS/VSITE_NAME/services/Registry?res=default_registry. Is recommended for test script execution, which displays the user. This should be a user associated with the certificate.\nTesting using the URC  Testing should start from setting up the user’s credentials, A local registry should be added in URC Grid Browser view.. The registry contents should be listed, by double clicking on its node. It is worth to enable the display of all sites by clicking on the Grid Browser the “Show” button and selecting from the list “All services”. If you see a red cross on the service, click on it and see the details of the error message in the URC and the error on the server-side. If all services are available, you can send the job. At the same time it is recommended to monitor the logs Unicore/X and TSI for errors.  Testing using the UCC  Configure UCC credentials Configure the registry in UCC preferences file (the registry property). Invoke:  /ucc shell ucc\u003e connect You can access 1 target system(s). ucc\u003e list-sites VSITE_NAME https://GATEWAY_ADDRESS/VSITE_NAME/services/TargetSystemService?res=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX ucc\u003e list-storages SHARE https://GATEWAY_ADDRESS/VSITE_NAME/services/services/StorageManagement?res=default_storage ucc\u003e ls u6://SHARE /a5063ea0-ecbe-4097-9abc-f55ec9437376 /3f501d37-5851-4c9e-a1da-5ad7b9f16633 /3bf169c2-2149-4564-b827-0b6560a3dd35 ... ucc\u003e list-applications Applications on target system \u003cVSITE_NAME\u003e R 2.10.0 BLAST 2.2.22 POVRay 3.6.1 ... We should get a message similar to the above. Then test the file transfer:\nucc\u003e put-file -s LOCAL_FILE_PATH \\  -t https://GATEWAY_ADDRESS/VSITE_NAME/services/StorageManagement?res=default_storage#TARGET_FILE_NAME And job submission:\nucc\u003e run -s VSITE_NAME JOB_FILE_PATH.u SUCCESSFUL exit code: 0 If an error occurs, you can on each of these commands add the “-v” flag, what increases UCC verbosity. As in the URC case it is advised to simultaneously monitor Unicore / Xa and TSI log files.\nAfter testing If testing was successful, you can unlock the registration system in the global registry.\nQCG checks QCG Computhng checks The presented tests of QCG-Computing service use the qcg-comp, the client program for QCG-Computing, that may be installed from provided RPMS. In order to connect to QCG-Computing the grid proxy must be created.\nGenerate user’s proxy:\n$ grid-proxy-init Your identity: /C=PL/O=GRID/O=PSNC/CN=Mariusz Mamonski Enter GRID pass phrase for this identity: Creating proxy ............................ Done Your proxy is valid until: Fri Jun 10 06:23:32 2011 Query the QCG-Computing service:\n# the xmllint is used only to present the result in more pleasant way $ qcg-comp -G | xmllint --format - \u003cbes-factory:FactoryResourceAttributesDocument xmlns:bes-factory=\"http://schemas.ggf.org/bes/2006/08/bes-factory\"\u003e … a lot of information … \u003c/bes-factory:FactoryResourceAttributesDocument\u003e Submit a sample job:\n$ qcg-comp -c -J /opt/plgrid/qcg/share/qcg-comp/doc/examples/date.xml Activity Id: ccb6b04a-887b-4027-633f-412375559d73 Query its status:\n$ qcg-comp -s -a ccb6b04a-887b-4027-633f-412375559d73 status = Executing $ qcg-comp -s -a ccb6b04a-887b-4027-633f-412375559d73 status = Finished exit status = 0 QCG Notification checks The tests of QCG-Notification require qcg-ntf-client program to be installed in a system. The program is provided in RPM package.\nCreate a sample subscription:\n$ qcg-ntf-client -d \\  -S \"cons=http://127.0.0.1:2212 top=http://schemas.qoscosgrid.org/comp/2011/04/notification/topic;//*;Full\" ... INF May 17 14:15:51 1128 0xa0262720 [qcg-client-gsoa] Subscribed, subRef: '810917963' ... Remove the created subscription:\n$ qcg-ntf-client -d -U \"id=810917963\" ... INF May 17 14:41:48 3318 0xa0262720 [qcg-client-gsoa] Unsubscribed: '810917963' … Checking the connection with QCG-Computing: In one shell run ‘tail -f’ on the QCG-Computing log file and in the other try to submit a sample job using the qcg-comp program (as described above). Check the tail output if there are no error messages on sending notifications. E.g. the following lines means that the connection problems occurred:\n$ tail -f /opt/qcg/var/log/qcg-comp/qcg-compd.log INF Oct 04 10:55:33 18929 0x2adadc2abe30 [notification_ws] Sending notify: 320f014c-3181-4daf-bbd9-1824b7d8216a -\u003e Queued NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultCode: 'SOAP-ENV:Client' NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultString: 'smcm:ActivityState' NOT Oct 04 10:55:33 18929 0x2adadc2abe30 [.....ntf_client] FaultDetail: '\u003cSOAP-ENV:Detail xmlns:SOAP-ENV=\"http://schemas.xmlsoap.org/soap/envelope/\"\u003econnect failed in tcp_connect()\u003c/SOAP-ENV:Detail\u003e' ERR Oct 04 10:55:33 18929 0x2adadc2abe30 [notification_ws] Failed to send notification to http://grass1.man.poznan.pl:19011/ QCG Broker checks The basic tests of QCG-Broker service may be proceeded with help of qcg-simple-client, the software that provides a set of commands for interaction with QCG-Broker. qcg-simple-client may be installed from RPMs.\nCreate a sample job description:\n$ cat \u003e sleep.qcg \u003c\u003c EOF #!/bin/bash #QCG queue=plgrid #QCG host=nova.wcss.wroc.pl #QCG persistent sleep 30 EOF Submit a job:\n$ qcg-sub sleep.qcg https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl Your identity: C=PL,O=GRID,O=PSNC,CN=Bartosz Bosak Enter GRID pass phrase for this identity: Creating proxy, please wait... Proxy verify OK Your proxy is valid until Tue Mar 12 14:50:27 CET 2013 UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 59 Minutes 58 Seconds jobId = J1360936230540__0152 Check the job statuses:\n$ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 59 Minutes 49 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: ProxyLifetime: PT0S Status: PREPROCESSING StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: PREPROCESSING StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: LocalFinishTime: $ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 59 Minutes 23 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: ProxyLifetime: PT0S Status: RUNNING StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: RUNNING StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: Fri Feb 15 14:50:47 CET 2013 LocalFinishTime: $ qcg-info https://qcg-broker.man.poznan.pl:8443/qcg/services/ /C=PL/O=GRID/O=PSNC/CN=qcg-broker/qcg-broker.man.poznan.pl UserDN = /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak ProxyLifetime = 24 Days 23 Hours 56 Minutes 10 Seconds Command translated to: \"task_info\" \"J1360936230540__0152\" \"task\" Note: UserDN: /C=PL/O=GRID/O=PSNC/CN=Bartosz Bosak TaskType: SINGLE SubmissionTime: Fri Feb 15 14:50:31 CET 2013 FinishTime: Fri Feb 15 14:52:17 CET 2013 ProxyLifetime: PT0S Status: FINISHED StatusDesc: StartTime: Fri Feb 15 14:50:33 CET 2013 Allocation: HostName: nova.wcss.wroc.pl ProcessesCount: 1 ProcessesGroupId: Status: FINISHED StatusDescription: SubmissionTime: Fri Feb 15 14:50:32 CET 2013 FinishTime: Fri Feb 15 14:52:12 CET 2013 LocalSubmissionTime: Fri Feb 15 14:50:37 CET 2013 LocalStartTime: Fri Feb 15 14:50:47 CET 2013 LocalFinishTime: Fri Feb 15 14:52:09 CET 2013 Check the functionality of the cloud elements Sites can provide any (not necessarily all) of the interfaces listed below:\n OCCI for VM Management OpenStack Compute for VM Management CDMI for Object Storage  Cloud Compute (OCCI/OpenStack) checks prerequisites AppDB integration NOTE: Prerequisite: run the following commands is the installation of the EGI rOCCI CLI environment, according to this guide.\nGo to AppDB and look for a OS image member of the fedcloud.egi.eu VO (all sites should support), e.g. EGI CentOS 7 image\nCheck that the site is visible in to the AppDB “Availability and Usage” panel for the image. If not, probably the site has not registered the FedCloud VO into their middleware (vmcatcher) or it did not properly configured the BDII provider script.\nFrom that “Availability and Usage” panel, click on the Site name, then on the latest VM Image version, select a resource template (preferably with the smallest quantity of resources (RAM \u0026 CPU)) and click on the “get IDs” button on the right of the resource template. You will get the “Site Endpoint”, “Template ID” and “OCCI ID”. Save these values since they will be needed in the next steps.\nCredentials Generate a set of keys for your user (it is not required to set a phassphrase for the keys, since these are just temporary keys for the test), make sure to set key permissions to 400:\n$ ssh-keygen -t rsa -b 2048 -f tempkey Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in tempkey. Your public key has been saved in tempkey.pub. The key fingerprint is: (...) $ chmod 400 tempkey Create a simple contextualization script, to setup access keys on the machine and test contextualization\n$ cat \u003c\u003c EOF \u003e ctx.txt Content-Type: multipart/mixed; boundary=\"===============4393449873403893838==\" MIME-Version: 1.0 --===============4393449873403893838== Content-Type: text/x-shellscript; charset=\"us-ascii\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\"deploy.sh\" #!/bin/bash echo \"OK\" \u003e /tmp/deployment.log --===============4393449873403893838== Content-Type: text/cloud-config; charset=\"us-ascii\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\"userdata.txt\" #cloud-config users: - name: testadm sudo: ALL=(ALL) NOPASSWD:ALL lock-passwd: true ssh-import-id: testadm ssh-authorized-keys: - `cat tempkey.pub` --===============4393449873403893838==-- EOF Create a proxy in RFC format for your tests:\n$ voms-proxy-init -rfc -voms fedcloud.egi.eu Your identity: /DC=es/DC=irisgrid/O=ifca/CN=Enol-Fernandez-delCastillo Creating temporary proxy .......................................................................... Done Contacting voms2.grid.cesnet.cz:15002 [/DC=org/DC=terena/DC=tcs/OU=Domain Control Validated/CN=voms2.grid.cesnet.cz] \"fedcloud.egi.eu\" Done Creating proxy .............................................................................. Done Your proxy is valid until Fri Nov 14 04:59:26 2014 OCCI checks (eu.egi.cloud.vm-management.occi service type) NOTE: Prerequisite: run the following commands is the installation of the EGI EGI CLI environment, according to this guide.\nDescribe OS templates (occi_endpoint is the one retrieved from AppDB). Check also that the OCCI ID provided by in AppDB is listed:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X -a describe -r os_tpl [...] [[/_http://occi.carach5.ics.muni.cz/occi/infrastructure/os_tpl#uuid_ubuntu_server_14_04_lts_fedcloud_dukan_84_| http://occi.carach5.ics.muni.cz/occi/infrastructure/os_tpl#uuid_ubuntu_server_14_04_lts_fedcloud_dukan_84 ]] title: Ubuntu-Server-14.04-LTS@fedcloud-dukan term: uuid_ubuntu_server_14_04_lts_fedcloud_dukan_84 location: /mixin/os_tpl/uuid_ubuntu_server_14_04_lts_fedcloud_dukan_84/ [...] Describe the resource templates, and check that the resource ID provided but AppDB is returned:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X -a describe \\  -r resource_tpl [...] [[/_http://schema.fedcloud.egi.eu/occi/infrastructure/resource_tpl#small_| http://schema.fedcloud.egi.eu/occi/infrastructure/resource_tpl#small ]] title: Small Instance - 1 core and 2 GB RAM term: small location: /mixin/resource_tpl/small/ Start a VM (using the OCCI ID and Resource ID as provided by AppDB for the mixins, and for context the file created above). The returned ID will be used in the commands below:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a create -r compute -M \u003cresource_id\u003e -M \u003cocci_id\u003e \\  -t occi.core.title=\"My OCCI VM\" \\  -T user_data= \u003cfile://$PWD/ctx.txt\u003e https://carach5.ics.muni.cz:11443/compute/46934 Check that the VM is active by describing it (it may take a few minutes):\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X -a describe -r \u003cvm_id\u003e [...] occi.compute.state = active [...] occi.networkinterface.address = 147.251.3.62 [...] If the VM does not have a public IP, you will need to get an IP for it\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X -a link \\  -r \u003cvm_id\u003e -j /network/public ssh to the machine to the provided IP (the options avoid problems when different VMs have the same IP, don’t use them in production) and check that contextualization script was executed:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\  testadm@ \u003cip_addr\u003e \"sudo cat /tmp/deployment.log\" OK Create a block storage entity of 1 GB and save the ID returned:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a create -r storage -t occi.storage.size=num(1) \\  -t occi.core.title=storage_test https://carach5.ics.muni.cz:11443/storage/296 Describe the newly created storage ID:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a describe -r \u003cstorage_id\u003e [...] occi.storage.size = 1.0 occi.storage.state = online [...] Link that storage to the running VM:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a link -r \u003cvm_id\u003e -j \u003cstorage_id\u003e https://carach5.ics.muni.cz:11443/link/storagelink/compute_46936_disk_2 Check the device where the storage is attached:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a describe -r \u003cstorage_link_id\u003e [...] occi.storagelink.deviceid = /dev/xvdb [...] And login into the machine to create a filesystem, mount it, create a file, and umount:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\  testadm@ \u003cip_addr\u003e testadm $ sudo mke2fs \u003cdevice_id\u003e mke2fs 1.42.9 (4-Feb-2014) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 65536 inodes, 262144 blocks 13107 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=268435456 8 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Writing superblocks and filesystem accounting information: done testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ touch /mnt/test testadm $ ls -l /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Unlink the storage from that VM and start a new one with that storage attached:\n$ occi -a unlink -r \u003cstorage_link_id\u003e $ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a create -r compute -M \u003cresource_id\u003e -M \u003cocci_id\u003e \\  -t occi.core.title=\"My OCCI VM\" \\  -T user_data= \u003cfile://$PWD/ctx.txt\u003e \\  -j \u003cstorage_id\u003e** https://carach5.ics.muni.cz:11443/compute/46936 Check the IP of the new VM and login:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a describe -r \u003cnew_vm_id\u003e** [...] occi.compute.state = active [...] \u003e\u003e location: /link/storagelink/compute_46936_disk_2 [...] occi.storagelink.deviceid = /dev/xvdb [...] occi.networkinterface.address = 147.251.3.63 [...] $ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\  testadm@ \u003cnew_vm_ip_addr\u003e testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ ls -ltr /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Clean up by unlinking the storage, and removing VMs and storage:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a unlink -r \u003cnew_storage_link_id\u003e $ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a delete -r \u003cvm_id\u003e $ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a delete -r \u003cnew_vm_id\u003e $ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X \\  -a delete -r \u003cstorage_id\u003e Finally check that the VMs and storage are not listed by occi:\n$ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X -a list -r compute $ occi -e \u003cocci_endpoint\u003e -n x509 -x $X509_USER_PROXY -X -a list -r storage OpenStack Compute checks (org.openstack.nova service type) NOTE: Prerequisite: run the following commands is the installation of the OpenStack CLI with VOMS extensions, as shown in on the API and SDK guide.\nExport the following variables on your shell (keystone URL can be obtained from GOCDB URL of the endpoint)\n$ export OS_AUTH_URL= \u003ckeystone URL\u003e $ export OS_AUTH_TYPE=v2voms $ export OS_X509_USER_PROXY=$X509_USER_PROXY Get the list of tenants supporting your proxy\n$ keystone_tenants Tenant id: 999f045cb1ff4684a15ebb338af69460 Tenant name: VO:fedcloud.egi.eu Enabled: True Description: VO fedcloud.egi.eu Export the tenant name as shown from the command in the following variable:\n$ export OS_PROJECT_NAME=\"VO:fedcloud.egi.eu\" Describe the available flavors, check also that the template ID provided by AppDB is listed:\n$ openstack flavor list +----+-----------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +----+-----------+-------+------+-----------+-------+-----------+ | 1 | m1.tiny | 512 | 0 | 0 | 1 | True | | 2 | m1.small | 2000 | 10 | 20 | 1 | True | | 3 | m1.medium | 4000 | 10 | 40 | 2 | True | | 4 | m1.large | 7000 | 20 | 80 | 4 | True | | 5 | m1.xlarge | 14000 | 30 | 160 | 8 | True | +----+-----------+-------+------+-----------+-------+-----------+ Describe available images, again check that the ID provided by AppDb is listed\n$ openstack image list +--------------------------------------+----------------------------------------------+ | ID | Name | +--------------------------------------+----------------------------------------------+ | 1414f242-d6d1-4a8c-8b26-8c0ada32f343 | IFCA Fedora Cloud 23 | | 8a700834-04b4-4e91-a3d5-9246ef95167e | LW Jupyter-R Ubuntu 14.04 | | 7f361fba-21d6-40ca-892d-17aa60b63a66 | IFCA CentOS 7 | | f3544cc8-421f-4d93-ac35-eba7fdc75329 | IFCA CentOS 6 | ... +--------------------------------------+----------------------------------------------+ Start a VM (using the flavor and images checked above and the context file created previously). The returned ID will be used in the following commands:\n$ openstack server create \\  --flavor \u003cflavor\u003e --image \u003cimage id\u003e \\  --user-data ctx.txt test +--------------------------------------+------------------------------------------------------+ | Field | Value | +--------------------------------------+------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | config_drive | | | created | 2016-03-01T13:07:10Z | | flavor | m1.tiny (1) | | hostId | | | id | 5d3ed7d6-d5ac-4f09-a353-0c2bd0fbd0ea | | image | IFCA CentOS 7 (7f361fba-21d6-40ca-892d-17aa60b63a66) | | key_name | None | | name | test | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | project_id | 999f045cb1ff4684a15ebb338af69460 | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2016-03-01T13:07:11Z | | user_id | a31b8c452b594369a49a8329103e241a | +--------------------------------------+------------------------------------------------------+ Check that the VM is active by describing it (it may take a few minutes):\n$ openstack server show \u003cvm id\u003e +---------------------+--------+ | Field | Value | +---------------------+--------+ (...) | OS-EXT-STS:vm_state | active | (...) +---------------------+--------+ If the VM does not have a public IP, you will need to get an IP for it\n$ openstack ip floating pool list +-------------+ | Name | +-------------+ | nova | +-------------+ $ openstack ip floating pool create \u003cpool name\u003e +-------------+----------------+ | Field | Value | +-------------+----------------+ | fixed_ip | None | | id | 1265 | | instance_id | None | | ip | 193.146.75.245 | | pool | nova | +-------------+----------------+ $ openstack ip floating add \u003cip\u003e \u003cvm id\u003e The VM should have now a public IP available when shown:\n$ openstack server show \u003cvm id\u003e +-----------+-------------------------------------+ | Field | Value | +-----------+-------------------------------------+ (...) | addresses | private=172.16.8.14, 193.146.75.245 | (...) +-----------+-------------------------------------+ ssh to the machine to the provided IP (the options avoid problems when different VMs have the same IP, don’t use them in production) and check that contextualization script was executed:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\  testadm@ \u003cip\u003e \"cat /tmp/deployment.log\" Warning: Permanently added '193.146.75.245' (ECDSA) to the list of known hosts. OK Create a storage volume:\n$ openstack volume create --size 1 test +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | created_at | 2016-03-01T13:29:44.392423 | | display_description | None | | display_name | test | | id | 8f46046d-cd9b-4219-9ce7-f0abe30ad992 | | properties | | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | None | +---------------------+--------------------------------------+ And show its status:\n$ openstack volume show \u003cvol id\u003e Attach it to the running VM:\n$ openstack server add volume \u003cvm id\u003e \u003cvol id\u003e And check it’s attached:\n$ openstack volume show 8f46046d-cd9b-4219-9ce7-f0abe30ad992 +---------------------+---------------------------------------------------------+ | Field | Value | +---------------------+---------------------------------------------------------+ | attachments | [{u'device': u'/dev/vdb', | | | u'server_id': u'21f6123f-926c-4816-b4fb-53df91907a63', | | | u'id': u'8f46046d-cd9b-4219-9ce7-f0abe30ad992', | | | u'volume_id': u'8f46046d-cd9b-4219-9ce7-f0abe30ad992'}] | | availability_zone | nova | | bootable | false | | created_at | 2016-03-01T13:29:44.000000 | | display_description | None | | display_name | test | | id | 8f46046d-cd9b-4219-9ce7-f0abe30ad992 | | properties | | | size | 1 | | snapshot_id | None | | source_volid | None | | status | in-use | | type | None | +---------------------+---------------------------------------------------------+ And login into the machine to create a filesystem, mount it, create a file, and umount:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\  testadm@ \u003cip_addr\u003e testadm $ sudo mke2fs \u003cdevice_id\u003e (...) testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ touch /mnt/test testadm $ ls -l /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Delete the VM:\n$ openstack server delete \u003cvm id\u003e And create a new one with the volume attached directly (substitute vdb for the same device name shown above):\n$ openstack server create \\  --flavor \u003cflavor\u003e --image \u003cimage id\u003e \\  --block-device-mapping vdb= \u003cvolume id\u003e \\  --user-data ctx.txt test +-----------------------------+---------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | 9imSRC9EZhhX | | config_drive | | | created | 2016-03-01T13:45:21Z | | flavor | m1.tiny (1) | | hostId | | | id | 10000d50-239b-4e86-bbd8-224143d6d346 | | image | Image for EGI Centos 6 [CentOS/6/KVM]_egi | | key_name | None | | name | test | | progress | 0 | | project_id | fffd98393bae4bf0acf66237c8f292ad | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2016-03-01T13:45:23Z | | user_id | 6c254b295af64644904a813db0d3d88a | +-----------------------------+---------------------------------------------+ Assign the public IP (if it does not have one already):\n$ openstack ip floating add \u003cip\u003e \u003cvm id\u003e And check that the volume is attached and usable:\n$ ssh -i tempkey -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\  testadm@ \u003cnew_vm_ip_addr\u003e testadm $ sudo mount \u003cdevice_id\u003e /mnt testadm $ ls -ltr /mnt/ total 16 drwx------ 2 root root 16384 Nov 13 17:43 lost+found -rw-r--r-- 1 root root 0 Nov 13 17:45 test testadm $ sudo umount /mnt testadm $ exit Finally delete VM, volume and IP address (NOTE: use the id of the IP as returned by openstack ip floating list\\!): $ openstack server delete \u003cvm id\u003e $ openstack volume delete \u003cvolume id\u003e $ openstack ip floating delete \u003cip id\u003e Cloud Storage (CDMI) checks (eu.egi.cloud.storage-management.cdmi service type) NOTE: Prerequisite: run the following commands is the installation of the EGI CLI environment, according to this guide.\nList the content of the repository:\n$ bcdmi -e \u003ccdmi_endpoint\u003e list / Create a test directory:\n$ bcdmi -e \u003ccdmi_endpoint\u003e mkdir test { \"completionStatus\": \"Complete\", \"objectName\": \"test/\", \"capabilitiesURI\": \"/cdmi/AUTH_df37f5b1ebc94604964c2854b9c0551f/cdmi_capabilities/container/\", \"parentURI\": \"/cdmi/AUTH_df37f5b1ebc94604964c2854b9c0551f/\", \"objectType\": \"application/cdmi-container\", \"metadata\": {} } Create a test file and upload it to the created directory:\n$ echo \"TEST OK\" \u003e testfile $ bcdmi -e \u003ccdmi_endpoint\u003e put -T testfile test/test.txt** Try to download back the file and compare to previous:\n$ bcdmi -e \u003ccdmi_endpoint\u003e get test/test.txt -o testfile.downloaded $ diff testfile testfile.downloaded \u0026\u0026 echo \"Files are equal\" \\  || echo \"Files differ\" Files are equal Delete the file:\n$ bcdmi -e \u003ccdmi_endpoint\u003e delete test/test.txt Check that the file is not present anymore\n$ bcdmi -e \u003ccdmi_endpoint\u003e list test/ Upload the file again and recursively delete the directory:\n$ bcdmi -e \u003ccdmi_endpoint\u003e put -T testfile test/test.txt $ bcdmi -e \u003ccdmi_endpoint\u003e delete -r test/ Check that the folder does not exist anymore\n$ bcdmi -e \u003ccdmi_endpoint\u003e list / See Site Certification GIIS Check HOWTO03.\nSee to Resource Centre registration and certification procedure PROC09.\n","categories":"","description":"Manual testing of services offered by a site","excerpt":"Manual testing of services offered by a site","ref":"/providers/operations-manuals/howto04_site_certification_manual_tests/","tags":"","title":"HOWTO04 Site Certification Manual tests"},{"body":"The following tutorials show you how to perform common tasks in the EGI infrastructure. These describe how to set up the EGI services needed for each use case, and how to connect or operate them together.\n","categories":"","description":"Tutorials for common uses cases in the EGI infrastructure\n","excerpt":"Tutorials for common uses cases in the EGI infrastructure\n","ref":"/users/tutorials/","tags":"","title":"Tutorials"},{"body":"This section contains guidelines for software development to be considered when developing a product for the EGI Federation.\nNote Those guidelines are providing a set of aspects to consider together with some reference documentation, and are not meant to be exhaustive, further research is welcome.  Licensing  Adopt an OSI-approved license; we recommend a business compatible license such as MIT or Apache 2.0 The license should provide unlimited access rights to the EGI Community  Source code access  Maintain the source code in a publicly-accessible software repository like GitHub You can request to use the EGI Federation GitHub organisation  If using another repository, a copy can be kept synchronized under the EGI Federation GitHub organisation   All releases must be tagged appropriately  Code style  Style guidelines must be defined and documented. A general style guide may be made available by EGI as a default. If you are extended an existing software component, then you must use the code style defined by the related product team If you are developing a new component, then you must use the code style practices for the programming language of your choice Code style compliance should be checked by automated means for every change  Best practices  The industry best practices should be adopted As far as possible adopt 12 factor application pattern Configuration Management modules to deploy and configure the products should be provided and distributed through the corresponding distribution channels  Ansible is recommended, roles can be hosted under the EGI Federation organization in Ansible Galaxy    Security best practices  Security best practices must be taken into account Security-related aspects must be considered from the beginning Security issues must be addressed in priority and following the EGI SVG recommendations and must take into account the points mentioned in the SVG Secure Coding and Software Security Checklist The Open Web Application Security Project (OWASP) provides extensive documentation, standards (such as ASVS) and tools to ensure that your software has capabilities to defend against common attacks.  Suggested references  Secure Software Development Framework | CSRC NIST: Cybersecurity Cybersecurity \u0026 Infrastructure Security Agency Microsoft Security Development Lifecycle (SDL)  Tooling and telemetry  If the project is an application or an infrastructure component, it should follow as close as possible the monitoring guidelines set by the Site Reliability Engineer book  Testing  Unit tests should be provided Unit testing should be automated Code coverage should be computed as part of the continuous integration When possible functional and integration tests should be automated If it’s not possible for some components the product team should provide report about those tests for the new releases  Code Review  A team of code reviewers shall be specified for each project Changes must be reviewed by the code review team prior to be merged using a Pull Request-like workflow  Documentation  Documentation must be treated like code  Written in a plain text-based format Management in a repository and versioning Markdown and reStructuredText formats are recommended   The documentation for an EGI Service should be submitted for publishing on the EGI Documentation site using the related GitHub repository A “Community First” approach should be followed  Contributing, onboarding and community guidelines should be available from the start of the project   Documentation should be available for  Developers Administrators (Deployment and administration) End users    Artefacts Release and delivery  Artefacts should be tagged according to Semantic Versioning or to Calendar Versioning that can be more appropriate for OS images Artefacts should have a DOI and associated short writeup (see Documentation above) Artefacts should be published in publicly available repositories  EGI Application Database can be used for this UMD can be used to distribute middleware components   It should be possible to automatically build production-grade distribution artefacts from the repository using provided build scripts and files Where appropriate, native packages for EGI Federation-supported Operating System should be provided:  CentOS 7 (rpm) Ubuntu 20.04 LTS (deb)   Containers are accepted  Containers must be compatible with EGI Cloud services    Request for information You can ask for more information about them by contacting us via our site.\n","categories":"","description":"Guidelines for software development","excerpt":"Guidelines for software development","ref":"/internal/guidelines-software-development/","tags":"","title":"Guidelines for software development"},{"body":"","categories":"","description":"","excerpt":"","ref":"/_footer/","tags":"","title":""},{"body":"The EGI documentation is written in Markdown, uses the Docsy theme, and is built using Hugo.\nNote The documentation covering the EGI Services is maintained by the EGI Community and coordinated by the EGI Foundation. Everyone is invited to participate by following the Contributing Guide.  ","categories":"","description":"About EGI Documentation","excerpt":"About EGI Documentation","ref":"/about/","tags":"","title":"About"},{"body":"Welcome to the EGI Documentation!  EGI is an international e-Infrastructure providing advanced computing and data analytics services for research and innovation. The EGI Federation offers a wide range of services for compute, storage, data and support. This is the start page for the user and provider documentation of the EGI Services.           For Users   How to access and use the EGI services to benefit from advanced computing.    For Providers   How to join the EGI Federation and operate services for advanced computing.          Learn about EGI   The EGI Federation is supporting many different user communities and open science projects.    Contribute!   We use a GitHub contributions workflow, new authors are always welcome!    Follow on Twitter   Find out about new EGI features and see how our users are using EGI services.     ","categories":"","description":"Documentation related to EGI activities","excerpt":"Documentation related to EGI activities","ref":"/","tags":"","title":"EGI documentation"},{"body":"Introduction The Operations Start Guide will help you start with EGI Operations duties. It covers the responsibilities of the various parties involved in the running of the EGI infrastructure and guide how to join operations. As a newcomer, you need to understand the structure of the infrastructure and roles of operators at different levels. Reading the whole document will give you a complete overall picture of daily operations within EGI.\nResource Centres and Resource Infrastructures Resources are geographically distributed and are contributed by Resource Centres. A Resource Centre is the smallest resource administration domain within EGI. It can be either localized or geographically distributed. A Resource Centre is also known as a site. It provides a minimum set of local or remote IT Services, such as High Throughput Compute (HTC), Cloud Compute, Storage and Data Transfer, compliant to well-defined IT Capabilities necessary to make resources accessible to authorised users. Users can access the services using common interfaces.\nThe EGI Federation is a Resource Infrastructure federating Resource Centres to constitute a homogeneous operational domain, and the Resource Infrastructure Provider is the legal organisation that is responsible of establishing, managing and of operating directly or indirectly the operational services to an agreed level of quality needed by the Resource Centres and the user community. It holds the responsibility of integrating them in EGI to enable uniform resource access and sharing for the benefit of their consuming end users. Examples of a Resource infrastructure Provider are the European Intergovernmental Research Organisations (EIRO) and the NGIs.\nIn Europe, Resource Centres are required to be affiliated to the respective NGIs, which (a) have a mandate to represent their national users community in all matters falling within the scope of the EGI Infrastructure, and (b) are the only organization having the mandate described in (a) for its country and thus provide a single contact point at the national level.\nOperations Centres A Resource Infrastructure Provider is responsible for delivering, to different groups of customers, a number of services, either technical or humans : services supporting the activities of the given Resource Infrastructure Provider and facilitating and making secure the access to the provided resources and services.\nIn order to contribute resources to EGI, a Resource Infrastructure Provider must be associated to an Operations Centre which will deliver the services on behalf of them.\nA single Operations Centre can be federated and operate services for multiple Resource Infrastructure Providers. Federation of operations can be a cost-effective approach for Resource Infrastructure Providers wishing to share their effort and services, or in order to operate the infrastructures in an early stage of maturity.\nThe Operations Centre provides services in collaboration with the respective Resource Centres via the Resource Centre Operations Manager, and globally with EGI Foundation and other Operations Centres.\n Locally, Operations Centres are responsible for supporting the Resource Centres, for monitoring their Quality of Service (QoS), for collecting requirements and representing them in the EGI operations boards. Globally they are in charge of contributing to the evolution of the EGI Infrastructure.  Roles The following sections covers the roles that are commonly involved in the operations of the EGI Infrastructure. The correspondent roles defined in Configuration Database (GOCDB) give specific rights in the Configuration Database itself and in other EGI services. There are roles whose scope is limited to the operation of a Resource Centre. A number of other roles act on a higher level, involving the operations activities related either to the NGI (Regional level) or to the EGI infrastructure as a whole (global level). Other terms and definitions can be found in the EGI Glossary.\nResource Centre level Resource Centre Administrator An individual responsible for installing, operating, maintaining and supporting one or more Resources or IT Services in a Resource Centre. In the scope of Operations, Resource Centres (RC) administrators primarily receive and react on incidents at their Resource Centre and on service requests notified through tickets created on the EGI Helpdesk service. They should respond to the tickets in a suitable time frame as defined in the Resource Centre Operational Level Agreement (OLA) and be aware of the alarms at their site, eg. through the Operations Dashboard. Sites MUST only operate supported middleware versions. This implies upgrading it regularly. Emergency releases are treated in a special way. Resource Centre Administrators MUST react to security issues that are at a global level, but affect their site. See SEC03 EGI-CSIRT Critical Vulnerability Handling.\nResource Centre Operations Manager An individual who leads the Resource Centre operations, who is the official technical contact person in the connected organisation, and who is locally supported by a team of Resource Centre Administrators. The Resource Centre Operations Manager is responsible for the site at the political and legal levels and for signing the Operational Level Agreement) between the Resource Centre and its hosting NGI. The Resource Centre Operations Manager is also responsible for enforcing the EGI policies and procedures by the Resource Centre and for assigning and approving the other site roles in the Configuration Database. Further, they should ensure that administrators are subscribed to the relevant mailing lists.\nResource Centre Security Officer The person responsible for keeping the site compliant with the EGI security policies. They are the primary contact for the NGI Security officer and EGI Computer Security Incident Response Team (CSIRT). The Site Security Officer deals with security incidents and shall respond to enquiries in a timely fashion as defined in the collection of security procedures and policies.\nRegional level Regional Operator on Duty (ROD) A team responsible for solving problems and incidents in the infrastructure according to agreed procedures. ROD teams monitor the Resource Centres in their region, react to incidents identified by the monitoring tools, and oversee incidents and related problems through to their resolution. They ensure that incidents are properly recorded and that the solutions progress according to specified time lines. They also provide support to Resource Centres and Virtual Organisations (VOs) and provide support to oversight bodies in cases of unresponsive Resource Centres. They ensure that all the necessary information is available to all parties. The team is provided by each NGI and requires procedural knowledge on the process (rather than technical skills) for their work. New ROD team members are required to read the ROD Welcome page and be familiar with ROD wiki page.\nNGI Security officer The NGI Security Officer is member of the EGI-CSIRT.\nThe NGI Security Officer coordinates the security activities within its NGI and serves as the primary contact for all security related requests, in particular from EGI-CSIRT’s IRTF concerning issues with the sites within the NGI.\nFurther, the NGI Security Officer is responsible for overseeing the security related aspects of the operations of the NGI and coordinates the security activities within its NGI.\nNGIs and Sites MUST respond in a timely manner to the security requests and alerts coming from the EGI-CSIRT’s IRTF.\nThe NGI Security Officer name and contact address needs to be registered in the Configuration Database (GOCDB) and the information maintained by the NGI.\nNGI Operations manager The NGI Operations manager is the contact point for all operational matters and represents the NGI within the Operations Management Board (OMB).\nThey are mainly responsible for:\n keeping up to date the NGI entry in the Configuration Database and managing the status of all sites under their NGI, and ensuring their information is also kept current addressing problems with Site availability or reliability. The reports are issued on a monthly basis and the NGI operations managers have 10 days to respond to identified problems attending regular Operations Management Board meetings  All NGI operations management responsibilities are listed in the Resource Infrastructure Provider OLA document.\nGlobal level Chief Operations Officer The Chief Operations Officer leads EGI Operations, and is responsible for coordinating the operations of the infrastructure across the project.\nEGI-CSIRT EGI-CSIRT is the official security coordination team and contact point at project level.\n EGI-CSIRT site EGI-CSIRT profile according to RFC-2350 EGI-CSIRT Terms of References  EGI-CSIRT’s IRTF The Incident Response Team for the Federation (IRTF) is a subgroup of EGI-CSIRT which acts as the primary contact for all security related requests concerning the Federation and the projects the EGI Foundation is involved in.\nEGI-CSIRTs IRTF provides the Security Officer on Duty role on a weekly rota basis. Details are in the EGI-CSIRT Terms of References.\nEGI Security Officer The EGI Security Officer, having the EGI CSIRT Officer role in the Configuration Database, is leading and coordinating EGI-CSIRT.\nThe role of the EGI Security Officer is provided by a member of EGI-CSIRT’s IRTF on a weekly rota basis.\nEGI Foundation SDIS team The EGI Foundation Service Delivery and Information Security (SDIS) team, formerly known as the EGI Operations team, is responsible of coordinating and supporting the operational activities of all the EGI Infrastructure.\nVO A Virtual Organisation (VO) is a group of users and, optionally, resources, often not bound to a single institution or national borders, who, by reason of their common membership and in sharing a common goal, are given authority to use a set of resources. Each VO member signs the VO Acceptable Usage Policy (AUP) (during registration) which is the policy describing the goals of the VO thereby defining the expected and acceptable use of the resources by the users of the VO. User documentation can be found in the users section and in the EGI Wiki.\nVO manager An individual responsible for the management of the membership registry of the VO ensuring its accuracy and integrity.\nJoining operations In order to join any of the organisational groups in your NGI, you will need to go through the following steps in order:\nAuthentication In general the authentication in EGI infrastructure works either with X509 personal certificates or through federated identities using Check-in. For some services (HTC and Storage) the access is granted only by using a X509 personal certificate due to legacy reasons: the process of moving the authentication and authorisation mechanism to federated identities has started but it will takes time before having everything compliant with federated identities.\nObtaining a X509 personal certificate If you do not already have a X509 personal certificate the EUGridPMA worlmap provides a map of all certification authorities according to the country (or NGI). Select your country on the map to find out who is your local CA. Follow the procedure of your local CA to request a certificate. When you have received your certificate, install it into your web browser.\nIf case of setting up a new Resource Centre please request Host certificates.\nEUGridPMA provides a web page allowing to test your certificate. Please use this resource and contact your CA if your certificate does not work.\nCreate a federated identity: registration in EGI Check-in As soon as you try to access an EGI service with your federated identity, you will be requested to register an account in EGI Check-in if not existing yet. The Check-in sign up guide explains how to sign up for an EGI account. If you already own an account, you will be simply asked to login through EGI Check-in.\nJoining dteam VO It is recommended to join the dteam VO at the dteam Registration page. You should request group membership for /dteam and /dteam/YOUR_NGI. The dteam group manager will then be notified and review your application. The membership to dteam VO is possible only by using a X09 personal certificate and it is useful to test the RCs and to debug related issues.\nRequesting GOCDB access  Read Input System User Documentation first. Go to the Configuration Database and follow the instruction  All members should notify their NGI operations manager about their role requests, to be sure they are considered on time.\nRegistering into GGUS To register into GGUS please follow the Central GGUS registration link. GGUS can be accessed either with your X509 personal certificate or with your federated identity. Do not forget to apply for the support role as well. (The GGUS support staff will approve you quickly as they get the notification automatically). To get the supporter role with your federated identity, please enroll to the GGUS Supporters group in Check-in.\nIf your NGI also have a local helpdesk interfaced with GGUS, please ensure that you are properly registered also there: your NGI managers will take care of that.\nSubscribing to mailing lists NGIs and Sites have local mailing lists for ROD team members and Site Administrators respectively. Please ensure that you subscribe to them. Depending on your role ask your NGI operations manager or Site operations manager to have you included on the necessary mailing lists if there is no automatic subscription process.\nNGI operations manager should contact operations@egi.eu and state that wish to be subscribed to noc-managers mailing list noc-managers@mailman.egi.eu.\nDocumentation Procedures and policies are accessible on the EGI Policies and Procedures space. Additional documentation relevant to EGI operations is available at EGI Documentation wiki page.\nTools A list of services relevant to EGI operations can be found at the section about internal services.\n","categories":"","description":"","excerpt":"Introduction The Operations Start Guide will help you start with EGI …","ref":"/providers/getting-started/operations_start_guide/","tags":"","title":"Operations Start Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"Support for EGI services is available through the EGI Helpdesk, an internal EGI service.\nThe EGI Helpdesk is a distributed tool with central coordination, which provides the information and support needed to troubleshoot product and service problems. Users can report incidents, bugs or request changes.\nThe support activities are grouped into first and second level support.\nNote Support is also available by contacting us at support \u003cat\u003e egi.eu.  ","categories":"","description":"Support for EGI services","excerpt":"Support for EGI services","ref":"/support/","tags":"","title":"Support"}]